{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 1. Предобработка текстовых данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data/imgs/photo-1472737817652-4120ab61af6c.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Компьютеры не понимают человеческие языки, поэтому текстовые данные нужно каким-то образом преобразовывать. Это часто не простой и не тривиальный процесс. На этом занятии мы разберем основные способы очистки данных, нормализации и векторизации. Также мы посмотрим на готовые интрументы, предназначенные для работы с русским языком. В конце занятия мы попробуем порешать задачу автоматического определения тональности твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Очистка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто в данных, с которыми нам нужно работать помимо текста присутствует ещё какая-то лишняя информация: тэги, ссылки, код, разметка. От всего этого нужно избавляться."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сразу импортируем все нужные библиотеки\n",
    "# подробнее о каждой из них я расскажу по ходу\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize, wordpunct_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import re\n",
    "mystem = Mystem()\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>Ошибки:</font>\n",
    "Если возникает ошибка **ImportError**, значит у вас не установлен какой-то пакет.\n",
    "выполните в консоли *\"pip install package_name\"* (или pip3, если у вас две разные версии питона).\n",
    "Также это можно сделать и внутри ноутбука, просто поставьте **!** перед командой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install nltk pymystem3 pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ругается **nltk**, нужно запустить *nltk.download()* и скачать нужные модели или данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем в качестве примера несколько статей с Хабрахабра. Они были скачаны автоматически и в них остались некоторые тэги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим статьи в список\n",
    "habr_texts = [open('data/habr/habr_{}.txt'.format(i)).read() for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на статью\n",
    "print(habr_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В html все тэги заключаются в угловые скобки. Мы можем использовать это, чтобы легко избавиться от всех тэгов сразу. Напишем простую регулярку, которая будет удалять всё, что попадает между символами **<** и **>** и не является **>**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re - модуль регулярных выражений в питоне\n",
    "# функция sub заменяет все, что подходит под шаблон, на указанный текст\n",
    "def remove_tags_1(text):\n",
    "    return re.sub(r'<[^>]+>', '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим как работает наша функция."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_tags_1(habr_texts[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно заметить, что в некоторых местах удаление тэгов приводит к тому, что между точкой и началом следующего \n",
    "предложения нет пробела, а это может помешать правильно токенизировать текст,\n",
    "поэтому сделаем так, чтобы тэг заменялся пробелом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_2(text):\n",
    "    return re.sub(r'<[^>]+>', ' ', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_tags_2(habr_texts[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь куски текста не слипаются, но появились последовательности из нескольких пробелов, чтобы убрать их, добавим ещё одно регулярное выражение и применим его к тексту, из которого уже удалили тэги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags_3(text):\n",
    "    no_tags_text = re.sub(r'<[^>]+>', ' ', text)\n",
    "    no_space_sequences_text = re.sub('  +', ' ', no_tags_text)\n",
    "    return no_space_sequences_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(remove_tags_3(habr_texts[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь текст более менее чистый."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из текста теперь нужно выделить интересующие нас объекты. Чаще всего нужно просто слова."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой способ поделить текст на слова — стандартный питоновский ***str.split*** метод.\n",
    "    По умолчанию он разбивает текст по последовательностям пробелов\n",
    " (т.е. даже со второй версией remove_tags всё бы хорошо разделилось)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем на какой-нибудь очищенной статье."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_habr_texts = [remove_tags_2(text) for text in habr_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_habr_texts[1].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Большая часть слов отделяется, но знаки препинания лепятся к словам.\n",
    "Можно пройтись по всем словам и убрать из них пунктцацию с методом str.translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#основные знаки преминания хранятся в питоноском модуле string.punctuation\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем словарь, который будет всем знакам препинания сопостовлять None\n",
    "# и преобразуем текст\n",
    "table = str.maketrans({ch: None for ch in string.punctuation})\n",
    "[word.translate(table) for word in clean_habr_texts[3].split()][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все разобралось очень хорошо, остались слова в кавычках-елочках и лапках, длинное тире, но наверное это и к лучшему.\n",
    "Не к лучшему то, что многоточие не удаляется.\n",
    "Ещё можно отметить то, что такой способ будет, например, удалять апострофы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"father's\".translate(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исправить и то и другое можно добавив или удалив какие-то знаки пунктуации к/из ***string.punctuation***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_extended = string.punctuation + '«»—…“”'\n",
    "table = str.maketrans({ch: None for ch in punct_extended})\n",
    "[word.translate(table) for word in clean_habr_texts[3].split()][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_no_apostrophie = re.sub(\"'\", '', string.punctuation)\n",
    "table = str.maketrans({ch: None for ch in punct_no_apostrophie})\n",
    "print(\"father's\".translate(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть готовые токенизаторы из nltk. Они не удаляют пунктуацию, а выделяют её отдельным токеном\n",
    "\n",
    "Например ***wordpunct_tokenizer*** разбирает по регулярке - '**\\w+|[^\\w\\s]+**'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordpunct_tokenize(clean_habr_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё есть ***word_tokenize***. Он также построен на регулярках, но они там более сложные (учитывается последовательность некоторых \n",
    "символов, символы начала, конца слова и т.д). \n",
    "\n",
    "Специально подобранного под русский язык токенизатора там нет, \n",
    "но и с английским всё работает достаточно хорошо:\n",
    "сокращения типа **т.к** собираются в один токен, дефисные слова тоже не разделяются. Многоточия тут тоже не отделяются, но это можно поправить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(clean_habr_texts[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нормализация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто после выделения слов (или других вещей) нужно привести разные формы одного слова к одной стандартной или получить подробную информацию об отдельном слове."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эта задача уже в разы сложнее предыдущих. Простой функцией тут не отделаешься. К счастью, есть готовые инструменты.\n",
    "\n",
    "Для русского основых варианта два: Mystem и Pymorphy.\n",
    "\n",
    "Майстем работает немного лучше и сам токенизирует,\n",
    "поэтому можно в него засовывать сырой текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystem.lemmatize функция лемматизации в майстеме\n",
    "# сам объект mystem нужно заранее инициализировать\n",
    "# мы сделали это в начале тетрадки строчкой \"mystem = Mystem()\"\n",
    "mystem.lemmatize(clean_habr_texts[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужна грамматическая информация или надо сохранить ненормализованный текст,\n",
    "# есть функция mystem.analyze\n",
    "words_analized = mystem.analyze(clean_habr_texts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# возвращает она список словарей\n",
    "# каждый словарь имеет либо одно поле 'text' (когда попался пробел) или text и analysis\n",
    "# в analysis снова список словарей с вариантами разбора (первый самый вероятный)\n",
    "# поля в analysis - 'gr' - грамматическая информация, 'lex' - лемма\n",
    "# analysis - может быть пустым списком\n",
    "words_analized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Слово - ', words_analized[0]['text'])\n",
    "print('Разбор слова - ', words_analized[0]['analysis'][0])\n",
    "print('Лемма слова - ', words_analized[0]['analysis'][0]['lex'])\n",
    "print('Грамматическая информация слова - ', words_analized[0]['analysis'][0]['gr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#леммы можно достать в одну строчку\n",
    "[parse['analysis'][0]['lex'] for parse in words_analized if parse.get('analysis')][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Недостатки Mystem: это продукт Яндекса с некоторыми ограничениями на использование, больше он не развивается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pymorphy - открытый и развивается (можно поучаствовать на гитхабе)\n",
    "\n",
    "Ссылка на репозиторий: https://github.com/kmike/pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У него нет втстроенной токенизации и он расценивает всё как слово (это одновременно плюс и минус)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# основная функция - pymorphy.parse\n",
    "words_analized = [morph.parse(token) for token in word_tokenize(clean_habr_texts[2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Она похожа на analyze в майстеме только возрващает список объектов Parse\n",
    "# Первый в списке - самый вероятный разбор (у каждого есть score)\n",
    "# Информация достается через атрибут (Parse.word - например)\n",
    "# Грамматическая информация хранится в объекте OpencorporaTag и из него удобно доставать\n",
    "# части речи или другие категории\n",
    "print('Cлово - ', words_analized[0][0].word)\n",
    "print('Разбор слова - ', words_analized[0][0])\n",
    "print('Лемма слова - ', words_analized[0][0].normal_form)\n",
    "print('Грамматическая информация слова - ', words_analized[0][0].tag)\n",
    "print('Часть речи слова - ', words_analized[0][0].tag.POS)\n",
    "print('Род слова - ', words_analized[0][0].tag.gender)\n",
    "print('Число  слова - ', words_analized[0][0].tag.number)\n",
    "print('Падеж слова - ', words_analized[0][0].tag.case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дополнительная очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пунктуация часто совсем не нужна и поэтому можно выбросить её заранее. Если нужно обрабатывать много текста, это может немного ускорить процесс.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Оставим только буквено-численные токены\n",
    "# Это не самый лучший вариант, так как удалятся сокращения с точкой, слова через дефис\n",
    "text = 'В этом случае слова вроде т.к. и по-другому не пройдут фильтр и будут удалены.'\n",
    "good_tokens = [word for word in word_tokenize(text) if word.isalnum()]\n",
    "[morph.parse(token)[0].normal_form for token in good_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#можно сделать фильтр по длине, или оставить всё, что не целиком состоит из знаков препинания\n",
    "good_tokens = [word for word in word_tokenize(text) if len(word) > 1 \n",
    "                                                    and not all([ch in punct_extended for ch in word])]\n",
    "[morph.parse(token)[0].normal_form for token in good_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если не хочется делать нормализацию, то можно хотя бы просто привести все слова к одному регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('СпАсИбО'.lower())\n",
    "print('СпАсИбО'.upper())\n",
    "print('СпАсИбО'.capitalize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После нормализации можно убрать стоп-слова (предлоги, союзы, местоимения, частотные слова)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# стоп-слова есть в nltk\n",
    "stops = stopwords.words('russian')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список не идеальный и его можно расширять под свои задачи (в примере можно бы ещё удалить слово \"это\" и \"вроде\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_normalized = [morph.parse(token)[0].normal_form for token in good_tokens]\n",
    "words_normalized_no_stops = [word for word in words_normalized if word not in stops]\n",
    "print(words_normalized_no_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем теперь применить эти знания на настоящей задаче. Возьмем данные Dialog Evaluation по анализу тональности твитов пользователей в адрес телекоммуникационных компаний. В изначальной формулировке нужно определять тональность относительно упомянутой компании, но мы упростим её до простого определения тональности твита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом нам нужно по тексту твита приписать ему класс - негативный, нейтральный или положительный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# я заранее подготовил для вас данные\n",
    "train_data = pd.read_csv('data/sentiment_twitter/train_sentiment_ttk.tsv', sep='\\t')\n",
    "test_data = pd.read_csv('data/sentiment_twitter/test_sentiment_ttk.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные, чтобы убедиться, что всё нормально"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим распределение классов (-1 - негативный отзыв, 1 - положительный, 0 - отрицательный)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.label.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что положительный заметно меньше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на какой-нибудь негативный твит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'][10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И на положительный"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бейзлайн"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В начале стоит попробовать самый простой возможный вариант. \n",
    "Count_vectorizer - без какой-либо нормализации подойдет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Почти у всего в sklearn индентичный интерфейс. \n",
    "1. Нужно инициализировать нужный объект\n",
    "\n",
    "    ***`vectorizer = CountVectorizer()`***\n",
    "\n",
    "2. \"Обучить\" модель на наших данных.\n",
    "    \n",
    "    ***`vectorizer.fit(texts)`***\n",
    "    \n",
    "3. Преобразовать с помощью обученной модели наши данные в вектора.\n",
    "    \n",
    "    ***`X = vectorizer.transform(texts)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим какие опции есть у count_vectorizer\n",
    "?CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у count_vectorizer есть встроенный токенизатор, поэтому можно подавать текст напрямую\n",
    "# обучим векторайзер на обучающей выборке и преобразуем тексты в векторы\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit(train_data.text.values) \n",
    "\n",
    "X_train = count_vectorizer.transform(train_data.text.values)\n",
    "X_test = count_vectorizer.transform(test_data.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим на размеры матрицы\n",
    "# первое число - количество твитов\n",
    "# второе - размер каждого вектора (равен размеру словаря)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нужно чтобы второе число совпадало в обучающей и тестовой выборке\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# выделим классы в отдельные переменные\n",
    "y_train = train_data.label.values\n",
    "y_test = test_data.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качесте классификатора будем использовать Логистическую регрессию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерфейс почти в точности такой же. \n",
    "1. Нужно инициализировать нужный объект\n",
    "\n",
    "    ***`clf = LogisticRegression()`***\n",
    "\n",
    "2. Обучить модель на наших заранее преобразованных данных.\n",
    "    \n",
    "    ***`clf.fit(X)`***\n",
    "    \n",
    "3. Предсказать классы на новых данных.\n",
    "    \n",
    "    ***`preds = clf.predict(texts)`***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#также можно для начала посмотреть параметры\n",
    "#основной параметр это С, коэффициент регуляризации.\n",
    "#Регуляризация нужна для того, чтобы ограничить значения выучиваемых параметров, чтобы избежать переобучения\n",
    "# l2 стоит по умолчанию и обычно лучше работает, но\n",
    "# l1 зануляет ненужные признаки и увеличивает значения важных\n",
    "# поэтому её можно использовать для отбора признаков\n",
    "#попробуйте перебрать значения (0.01, 0.1, 1(по умолчанию), 10, 100)\n",
    "?LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty=\"l1\", C=0.1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# предскажем значения тестовых твитов\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# замерим качество классификации\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Макросредняя F1 мера - ',f1_score(y_test, y_pred, average='macro'))\n",
    "print('Микросредняя F1 мера - ',f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У некоторых классификаторов можно посмотреть значимость признаков. У логрега они хранится в ***clf.coef_*** . Это массив размером (количество классов, количество признаков). \n",
    "\n",
    "Признаки можно достать из векторайзера с помощью метода ***get_feature_names***.\n",
    "\n",
    "Вместе их можно соотнести встроеной функцией *zip*. При использовании **L1** регуляризации значений признака можно интерпретировать как важность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_important(vectorizer, clf, topn=10):\n",
    "    features = vectorizer.get_feature_names()\n",
    "    classes = clf.classes_\n",
    "    importances = clf.coef_\n",
    "    for i, cls in enumerate(classes):\n",
    "        print('Значимые слова для класса - ', cls)\n",
    "        important_words = sorted(list(zip(features, importances[i])), key=lambda x: abs(x[1]), reverse=True)[:topn]\n",
    "        print([word for word,_ in important_words])\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_important(count_vectorizer, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для визуализации важных слов можно ещё использовать библиотеку **wordcloud**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 150\n",
    "features = count_vectorizer.get_feature_names()\n",
    "importances = clf.coef_\n",
    "classes = clf.classes_\n",
    "words_with_weights = sorted(list(zip(features, importances[0])),key=lambda x: abs(x[1]), reverse=True)\n",
    "only_words = [word for word,_ in words_with_weights][:top]\n",
    "\n",
    "cloud = WordCloud(width=1000, height=500).generate(' '.join(only_words))\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Попробуем теперь TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# интерфейс точно такой же, но немного отличаются параметры\n",
    "# токенизация по умолчанию также включена, поэтому подаём текст как есть\n",
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(train_data.text.values)\n",
    "X_train = tfidf.transform(train_data.text.values)\n",
    "X_test = tfidf.transform(test_data.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l1')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# замерим качество классификации\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('Макросредняя F1 мера - ',f1_score(y_test, y_pred, average='macro'))\n",
    "print('Микросредняя F1 мера - ',f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также бывает полезно посмотреть на confision matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для красоты визуализируем с помощью библиотеки seaborn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = clf.classes_\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "sns.heatmap(data=confusion_matrix(y_test, y_pred), annot=True, \n",
    "            fmt=\"d\", xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# если не устанавливается, посмотрите просто так\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_important(tfidf, clf, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение качества классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### По самым характерным словам видно, что в корпусе есть мусор, стоп слова, разные формы одного слова. Чтобы улучшить результат попробуйте почистить данные и добавить нормализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    \"\"\"\n",
    "    функция нормализации\n",
    "    \n",
    "    ::парметры::\n",
    "    @text - ненормализованный текст (string)\n",
    "    \n",
    "    ::returns::\n",
    "    нормализованный текст (string)\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = # ваш код здесь\n",
    "    lemmas = # ваш код здесь\n",
    "    \n",
    "    return ' '.join(lemmas)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы применить нормализацию ко всему корпусу, воспользуйтесь функцией apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['normalized'] = train_data['text'].apply(normalize)\n",
    "test_data['normalized'] = test_data['text'].apply(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите новую модель на нормализованных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "tfidf.fit(train_data['normalized'].values)\n",
    "\n",
    "X_train = tfidf.transform(train_data['normalized'].values)\n",
    "X_test = tfidf.transform(test_data['normalized'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "print('Макросредняя F1 мера - ',f1_score(y_test, y_pred, average='macro'))\n",
    "print('Микросредняя F1 мера - ',f1_score(y_test, y_pred, average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
