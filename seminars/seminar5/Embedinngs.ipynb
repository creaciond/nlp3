{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Векторные представления слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные из тех же файлов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = open('data/train_pos.out').read().split('\\n\\n') + \\\n",
    "        open('data/test_pos.out').read().split('\\n\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для word2vec'а лучше использовать лемматизированные слова, а для fasttext'а подойдут и сырые слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраним целые предложения на будущее\n",
    "sents_ = []\n",
    "\n",
    "# не будем учитывать слова короче 4 (так заодно выкинтся все знаки препинания)\n",
    "lemmas = []\n",
    "for sent in sents:\n",
    "    sent_lemmas = []\n",
    "    for line in sent.split('\\n'):\n",
    "        lemma = line.split('\\t')[1]\n",
    "        if len(lemma) > 3:\n",
    "            sent_lemmas.append(lemma.lower())\n",
    "    lemmas.append(sent_lemmas)\n",
    "\n",
    "raw = []\n",
    "for sent in sents:\n",
    "    sent_lemmas = []\n",
    "    s = []\n",
    "    for line in sent.split('\\n'):\n",
    "        lemma = line.split('\\t')[0]\n",
    "        s.append(lemma)\n",
    "        if len(lemma) > 3:\n",
    "            sent_lemmas.append(lemma.lower())\n",
    "    raw.append(sent_lemmas)\n",
    "    sents_.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим параметры\n",
    "?gensim.models.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.Word2Vec(lemmas, size=200, max_vocab_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('женщина', 0.9966326951980591),\n",
       " ('представлять', 0.9890692234039307),\n",
       " ('казаться', 0.9889426827430725),\n",
       " ('позволить', 0.9874495267868042),\n",
       " ('вообразить', 0.9870392084121704),\n",
       " ('комфортно', 0.9853262901306152),\n",
       " ('ребёнок', 0.9849750995635986),\n",
       " ('душа', 0.984269380569458),\n",
       " ('обманывать', 0.9840550422668457),\n",
       " ('красивый', 0.983880877494812)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим что за модель получилась, посмотрев на близкие слова\n",
    "w2v.wv.most_similar('мужчина')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим параметры\n",
    "?gensim.models.FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft = gensim.models.FastText(raw, min_n=3, max_n=6, sg=1, size=100, max_vocab_size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('кухня', 0.9960958361625671),\n",
       " ('вдаль', 0.9941439628601074),\n",
       " ('чуть_не', 0.993897557258606),\n",
       " ('папу', 0.9938607215881348),\n",
       " ('кухню', 0.9933991432189941),\n",
       " ('спирт', 0.9931646585464478),\n",
       " ('звонко', 0.9929766654968262),\n",
       " ('капусту', 0.9929111003875732),\n",
       " ('неподалеку', 0.9927726984024048),\n",
       " ('папа', 0.9926959276199341)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# проверим что за модель получилась, посмотрев на близкие слова\n",
    "ft.wv.most_similar('варенье')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Как ни странно, но из одельных векторов слов можно сделать хорошие представления целых предложений.\n",
    "\n",
    "Для этого просто усредним векторы отдельных векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(lemmas), 200))\n",
    "\n",
    "for i, sent in enumerate(lemmas):\n",
    "    try:\n",
    "        vectors = w2v[sent]\n",
    "    except (KeyError, ValueError):\n",
    "        continue\n",
    "    \n",
    "    average_vector = np.mean(vectors, axis=0)\n",
    "    X[i] = average_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно также искать похожие предложения с помощью косинусной близости (или каких-то других метрик)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Со', 'времени', 'ее', 'отъезда', 'прошло', 'три', 'года', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   26, 67394, 35566, ..., 60574, 32887, 41573]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_distances(X[26], X).argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['С', 'тех', 'пор', 'прошло', 'немало', 'времени', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_[54970]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для наглядности кластеризуем предложения и посмотрим какие получаются кластеры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init 1/3 with method: k-means++\n",
      "Inertia for init 1/3: 1.092780\n",
      "Init 2/3 with method: k-means++\n",
      "Inertia for init 2/3: 0.992682\n",
      "Init 3/3 with method: k-means++\n",
      "Inertia for init 3/3: 0.648972\n",
      "Minibatch iteration 1/83200: mean batch inertia: 0.035449, ewa inertia: 0.035449 \n",
      "Minibatch iteration 2/83200: mean batch inertia: 0.043664, ewa inertia: 0.035469 \n",
      "Minibatch iteration 3/83200: mean batch inertia: 0.042421, ewa inertia: 0.035486 \n",
      "Minibatch iteration 4/83200: mean batch inertia: 0.036229, ewa inertia: 0.035488 \n",
      "Minibatch iteration 5/83200: mean batch inertia: 0.032004, ewa inertia: 0.035479 \n",
      "Minibatch iteration 6/83200: mean batch inertia: 0.036316, ewa inertia: 0.035481 \n",
      "Minibatch iteration 7/83200: mean batch inertia: 0.034248, ewa inertia: 0.035478 \n",
      "Minibatch iteration 8/83200: mean batch inertia: 0.030571, ewa inertia: 0.035467 \n",
      "Minibatch iteration 9/83200: mean batch inertia: 0.038757, ewa inertia: 0.035474 \n",
      "[MiniBatchKMeans] Reassigning 50 cluster centers.\n",
      "Minibatch iteration 10/83200: mean batch inertia: 0.039470, ewa inertia: 0.035484 \n",
      "Minibatch iteration 11/83200: mean batch inertia: 0.042732, ewa inertia: 0.035502 \n",
      "Converged (lack of improvement in inertia) at iteration 11/83200\n",
      "Computing label assignment and total inertia\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MiniBatchKMeans(batch_size=100, compute_labels=True, init='k-means++',\n",
       "        init_size=None, max_iter=100, max_no_improvement=10,\n",
       "        n_clusters=1000, n_init=3, random_state=None,\n",
       "        reassignment_ratio=0.01, tol=0.0, verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = MiniBatchKMeans(1000, verbose=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clustes_.txt', 'w')\n",
    "\n",
    "labels = km.labels_ # в km.labels_ тут хранятся кластеры для каждого примера\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "for i in range(len(sents_)):\n",
    "    clusters[labels[i]].append(sents_[i])\n",
    "\n",
    "for cluster in clusters:\n",
    "    f.write('\\n\\n\\nCLUSTER - {}\\n'.format(cluster))\n",
    "    for sent in clusters[cluster]:\n",
    "        f.write(' '.join(sent) + '\\n')\n",
    "f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простое усреднение работает очень хорошо. Если найти корпус побольше и подобрать параметры, векторы предложения будут очень хорошие и подойдут для классификации.\n",
    "\n",
    "Есть несколько способоб сделать векторы ещё лучше.\n",
    "\n",
    "\n",
    "Про них можно почитать вот тут:\n",
    "\n",
    "https://github.com/nlptown/sentence-similarity/blob/master/Simple%20Sentence%20Similarity.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
