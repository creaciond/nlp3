1.5. MODELS 15
.
.
Constants: K, number of mixture components; n, number of examples
Parameters:  2 RK
, i  0,
PK
iD1 i D 1; i 2 Rd
, ‚Ä†i 2 Rdd
, i 2 f1; : : : ; Kg
Latent variables: Zj for j 2 f1; : : : ; ng
Observed variables: Xj for j 2 f1; : : : ; ng
.
‚Ä¢ For j 2 f1; : : : ; ng
1:
 Generate zj 2 f1; : : : ; Kg from Multinomial./
2:
 Generate xj from Normal.zj
; ‚Ä†zj
/
3:
Generative Story 1.1: ÓÄÄe generative story for a Gaussian mixture model.
Generative models are often contrasted with discriminative models, in which the underlying
statistical model does not deÔ¨Åne a probability distribution over the input space. Instead, the model
is conditional, and we model the probability of a predicted structure given an element in the input
space.‚Å∂
ÓÄÄe joint distributions that generative models describe often entail independence assump-
tions between the various random variables in the distribution. ÓÄÄis means that the chain rule
can be applied to the joint distribution, with a certain ordering over the random variables, so that
the joint distribution is written in a more compact way as a product of factors‚Äîeach describing a
conditional distribution of a random variable or a set of random variables given a small, local set
of random variables.
It is often the case that describing a joint distribution based only on the form of its fac-
tors can be confusing, or not suÔ¨Éciently detailed. Representations such as graphical models (see
Section 1.5.4) can be even more restrictive and less revealing, since they mostly describe the in-
dependence assumptions that exist in the model.
In these cases, this book uses a verbal generative story description, as a set of bullet points
that procedurally describe how each of the variables is generated in the model. For example,
generative story 1.1 describes a mixture of Gaussian models.
Behind lines 1 and 3 lurk simple statistical models with well-known distributions. ÓÄÄe
Ô¨Årst line assumes a probability distribution p.Zj j/. ÓÄÄe second assumes a Gaussian probability
distribution p.Xj jzj
; ‚Ä†zj
/. Combined together, and taking into account the loop in lines 1‚Äì3,
they yield a joint probability distribution:
‚Å∂Any generative model can be transformed into a discriminative model by conditioning on a speciÔ¨Åc input instance. ÓÄÄe reverse
transformation from a discriminative model to a generative model is not possible without introducing an underspeciÔ¨Åed factor,
a probability distribution over the input space.
16 1. PRELIMINARIES
p.X1; : : : ; Xn; Z1; : : : ; Znj; 1; : : : ; K; ‚Ä†1; : : : ; ‚Ä†K/ D
n
Y
j D1
p.Xj ; Zj j; 1; : : : ; K; ‚Ä†1; : : : ; ‚Ä†K/:
In this book, generative story boxes are included with some information about the variables,
constants and parameters that exist in the model. ÓÄÄis can be thought of as the signature of the
generative story. ÓÄÄis signature often also tells, as in the above case, which variables are assumed
to be observed in the data, and which are assumed to be latent. Clearly, this is not a property of
the joint distribution itself, but rather depends on the context and the use of this joint distribution
as a statistical model.
As mentioned above, generative stories identify a joint distribution over the parameters
in the model, where this joint distribution is a product of several factors. ÓÄÄis is related to the
chain rule (Section 1.3). ÓÄÄe generative story picks an ordering for the random variables, and
the chain rule is applied using that order to yield the joint distribution. Each factor can theoreti-
cally condition on all possible random variables that were generated before, but the independence
assumptions in the model make some of these variables unnecessary to condition on.
Latent Variables in Generative Models
Latent variables are random variables in the model which are assumed to be unobserved when
doing inference from data (see also Section 1.5.3). As such, latent variables can generally refer to
three distinct types of random variables:
‚Ä¢ Random variables that augment, reÔ¨Åne or link (as a cause) between observed random vari-
ables and the target predictions (this is especially true when the target predictions are ob-
served during learning, in the supervised setting. See Section 1.6).
‚Ä¢ Random variables that represent the target predictions, because these are not observed dur-
ing learning (in the unsupervised setting, for example).
‚Ä¢ In the Bayesian setting, random variables that represent the parameters of the generative
model.
1.5.4 INDEPENDENCE ASSUMPTIONS IN MODELS
How do we actually construct a generative model given a phenomeon we want to model? We Ô¨Årst
have to decide which random variables compose the model. Clearly, the observed data need to
be associated with a random variable, and so do the predicted values. We can add latent variables
as we wish, if we believe there are hidden factors that link between the observed and predicted
values.
1.5. MODELS 17
Often, the next step is deciding exactly how these random variables relate to each other with
respect to their conditional independence. At this point, we are not yet assigning distributions to the
various random variables, but just hypothesizing about the information Ô¨Çow between them. ÓÄÄese
independence assumptions need to balance various trade-oÔ¨Äs. On one hand, the weaker they are
(i.e., the more dependence there is), the more expressive the model family is‚Äîin other words, the
model family includes more distributions. On the other hand, if they are too expressive, we might
run into problems such as overÔ¨Åtting with small amounts of data, or technical problems such as
computationally expensive inference.
Looking at various models in NLP, we see that the independence assumptions we make
are rather strong‚Äîit is usually the case that a given random variable depends on a small number
of other random variables. In that sense, the model has ‚Äúlocal factors‚Äù in its joint distribution.
For example, context-free grammars (see Chapter 8) make the independence assumption that
any future rewrites of a partial derivation depend only on the current foot nonterminal of that
partial derivation. Similarly, hidden Markov models (HMMs) make the assumption that ‚Äúfuture
observations‚Äù and ‚Äúpast observations‚Äù are conditionally independent given the identity of the state
that links them together. ÓÄÄis strong assumption is often mitigated by using a higher order HMM
(with the transition to the next state depending on the last two, or more, previous states, and not
just the last one).
Once the independence assumptions in the model are determined (either by writing the
joint distribution with its factors, or perhaps through a graphical representation with a Bayesian
network), we can proceed to describe an exact distribution for each factor, i.e., a conditional dis-
tribution in the joint distribution. ÓÄÄis is where well-known distributions are typically used, such
as Gaussians, multinomials or featurized log-linear models. ÓÄÄe parameters of these conditional
distributions depend on all of the random variables that are being conditioned on. For example,
in generative story 1.1, the distribution function for xj depends on the values previously drawn
for zj .
1.5.5 DIRECTED GRAPHICAL MODELS
As mentioned above, detailing the full generative story or joint distribution is necessary to fully
comprehend the inner workings of a given statistical model. However, in cases where we are
just interested in describing the independence assumptions that exist in the model, graphical
representations can help to elucidate these assumptions.
Given that Bayesian models are often generative, the most important type of graphical
representation for them is ‚Äúdirected graphical models‚Äù (or ‚ÄúBayesian networks‚Äù). See Murphy
(2012) for an introduction to other types of graphical models (GMs), such as undirected graphical
models. In a Bayesian network, each random variable in the joint distribution is represented as a
vertex in a graph. ÓÄÄere are incoming edges to each vertex X from all random variables that X
18 1. PRELIMINARIES
.
.
. Z
. W
.
Àõ
.
Àá
.
M
.
N
.
K
Figure 1.1: Graphical model for the latent Dirichlet allocation model. Number of topics denoted by
K, number of documents denoted by M and number of words per document denoted by N.‚Å∑
conditions on, when writing down the joint distribution and inspecting the factor that describes
the distribution over X.
ÓÄÄe basic type of independence assumption a Bayesian network describes is the following.
A random variable X is conditionally independent of all its ancestors when conditioned on its
immediate parents. ÓÄÄis type of property leads to an extensive calculus and a set of graph-theoretic
decision rules that can assist in determining whether one set of random variables in the model is
independent of another set when conditioned on a third set (i.e., the random variables in the third
set are assumed to be observed for that conditional independence test). ÓÄÄe calculus includes a few
logical relations, including symmetry: if X and Y are conditionally independent given Z, then Y
and X are conditionally independent given Z; decomposition: if X and Y [ W are conditionally
independent given Z, then X and Y are conditionally independent given Z; contraction: if X
and Y are conditionally independent given Z, and X and Y are conditionally independent given
Y [ Z, then X is conditionally independent of W [ Y given Z; weak union: if X and Y [ Z are
conditionally independent given W , then X and Y are conditionally independent given Z [ W .
Here, X, Y , Z and W are subsets of random variables in a probability distribution. See Pearl
(1988) for more information.
Bayesian networks also include a graphical mechanism to describe a variable or unÔ¨Åxed
number of random variables, using so-called ‚Äúplate notation.‚Äù With plate notation, a set of random
variables is placed inside plates. A plate represents a set of random variables with some count. For
example, a plate could be used to describe a set of words in a document. Figure 1.1 provides an
example of a use of the graphical plate language. Random variables, denoted by circles, are the
basic building blocks in this graphical language; the plates are composed of such random variables
(or other plates) and denote a ‚Äúlarger object.‚Äù For example, the random variable W stands for a
word in a document, and the random variable Z is a topic variable associated with that word. As
such, the plate as a whole denotes a document, which is indeed a larger object composed from N
random variables from the type of Z and W .
‚Å∑Adapted from a drawing by Luis Pedro.
1.6. LEARNING FROM DATA SCENARIOS 19
In this notation, edges determine the conditional independence assumptions in the model.
ÓÄÄe joint distribution over all random variables can be read by topologically sorting the elements
in the graphical representation, and then multiplying factors, starting at the root, such that each
random variable (or a group of random variables in a plate) conditions on its parents, determined
by the edges in the graph. For example, the graphical model in Figure 1.1 describes the following
joint distribution:
M
Y
j D1
p.W .j/
1 ; : : : ; W .j/
n ; Z.j/
1 ; : : : ; Z.j/
N ; .j/
jÀá1; : : : ; ÀáK; Àõ/
D
M
Y
j D1
p..j/
jÀõ/
n
Y
iD1
p.Z.j/
i j/p.W .j/
i jZ.j/
i ; .j/
; Àá1; : : : ; ÀáK/
!
: (1.5)
ÓÄÄis joint distribution describes the latent Dirichlet allocation model (a model for generat-
ing M documents, with each document having N words), described more in detail in Chapter 2.
Plates can also be nested (with several indices indexing random variables) or intersected.
Perhaps with somewhat of an abuse of notation, graphical models often also include ver-
tices for the parameters of the model, even when they are not random variables (in a frequentist
setting) or hyperparameters (in the Bayesian setting; see Chapter 3). Such nodes in the graphical
model will never have incoming edges, only outgoing edges. ÓÄÄe plate for the vector Àá1; : : : ; ÀáK
in Figure 1.1 is an example of this. ÓÄÄe same goes for the Àõ node in the same Ô¨Ågure. (A similar
abuse of notation is often done when writing down distributions. For example, we would write
p.jÀõ/, conditioning on Àõ, even though Àõ is not a random variable, but a Ô¨Åxed hyperparameter
value.)
ÓÄÄis book makes little use of graphical models in describing statistical models, and the
reader is instead referred to a machine learning book (e.g., Koller and Friedman, 2009), for more
information about graphical models. Graphical modeling is a rich area of research, but goes be-
yond the scope of this book. Given the mechanism we have introduced to describe generative
stories in the previous section, there is also little use of it in the material covered in this book.
1.6 LEARNING FROM DATA SCENARIOS
We are now equipped with the basic concepts from probability theory and from statistics so that
we can learn from data. We can now create a statistical model, where the data we have is mapped
to ‚Äúobserved‚Äù random variables. ÓÄÄe question remains: how is our data represented? In NLP,
researchers usually rely on Ô¨Åxed datasets from annotated or unannotated corpora. Each item in a
Ô¨Åxed corpus is assumed to be drawn from some distribution. ÓÄÄe data can be annotated (labeled)
or unannotated (unlabeled). Learning can be supervised, semi-supervised or unsupervised. ÓÄÄe
various scenarios and prediction goals for each are described in Table 1.1.
20 1. PRELIMINARIES
Table 1.1: Common procedures for learning from data. Observed data comes from the X random
variable distribution, target predictions from the Z random variable distribution (indexed as appro-
priately for diÔ¨Äerent instances).
Learning Setting Learning Input Data Learning Output
Supervised (inductive) (x(1) , z(1)), ... , (x(n) , z(n))
mechanism to predict z
values on arbitrary input
instances
Supervised (transductive)
(x(1) , z(1)), ... , (x(n) , zn)
and x(1), ... , x(m)
predictions of z values on
x(i )
, i œµ {1, ... ,m}
Semi-supervised
(x(1) , z1), ... , (x(n) , zn)
and x(1), ... , x(m)
mechanism to predict z
values on arbitrary input
instances
Unsupervised (instance-general) x(1), ... , x(n)
mechanism to predict z
values on arbitrary input
instances
Unsupervised (instance-specific) x(1) , ... , x(n)
predictions of z on
x(1) , ... , x(n)
1
1
1
1
0 0
0 0
1
An important concept common to all of these learning settings is that of marginal like-
lihood. ÓÄÄe marginal likelihood is a quantity that denotes the likelihood of the observed data
according to the model. In the Bayesian setting, marginalization is done over the parameters
(taking into account the prior) and the latent variables.
Here are some learning cases along with their marginal likelihood.
‚Ä¢ We are interested in doing unsupervised part-of-speech tagging. ÓÄÄerefore, the observed
data points are x.1/
; : : : ; x.n/
, which are sentences. In addition, there is a distribu-
tion over POS sequences. Each x.i/
is associated with a random variable Z.i/
that de-
notes this POS sequence. ÓÄÄe likelihood of a pair .X; Z/ is determined by p.X; Zj/ D
p.Zj/p.XjZ; /‚Äîi.e., in the generative story we Ô¨Årst generate the POS tag sequence,
and then the sentence. ÓÄÄere is a prior over . ÓÄÄerefore, the Ô¨Ånal likelihood is:
L.x.1/
; : : : ; x.n/
/ D
Z

0
@
n
Y
iD1
X
z.i/
p.z.i/
j/p.x.i/
jz.i/
; /
1
A p./d:
1.6. LEARNING FROM DATA SCENARIOS 21
‚Ä¢ We are interested in doing transductive‚Å∏ supervised part-of-speech tagging. ÓÄÄerefore, the
observed data points are .x.1/
; z.1/
/; : : : ; .x.n/
; z.n/
/ (labeled data) and x0.1/
; : : : ; x0.m/
(sen-
tences for which predictions are needed). ÓÄÄe distributions over the X and Z variables are
the same as in the previous case. Same, as well, for the prior. ÓÄÄe marginal likelihood is
(where Z0.i/
are the predicted sequences for X0.i/
):
L.x.1/
; : : : ; x.n/
; z.1/
; : : : ; z.n/
; x0.1/
; : : : ; x0.m/
/
D
Z

n
Y
iD1
p.z.i/
j/p.x.i/
jz.i/
; /p./
!

0
@
m
Y
iD1
X
z0.i/
p.z0.i/
j/p.x0.i/
jz0.i/
; /
1
A p./d:
‚Ä¢ We are interested in doing inductive supervised part-of-speech tagging. ÓÄÄerefore, the ob-
served data points are .x.1/
; z.1/
/; : : : ; .x.n/
; z.n/
/. ÓÄÄe distributions over the X and Z vari-
ables is the same as in the previous case. Same, again, for the prior. ÓÄÄe marginal likelihood
is:
L.x.1/
; : : : ; x.n/
; z.1/
; : : : ; z.n/
/ D
Z

n
Y
iD1
p.z.i/
j/p.x.i/
jz.i/
; /
!
p./d:
Note that here, the marginal likelihood is not used to predict any value directly. But if the
prior, for example, is parametrized as p.jÀõ/, this marginal likelihood could be maximized
with respect to Àõ. See Chapter 4.
‚Ä¢ We are interested in doing inductive supervised part-of-speech tagging, where the statisti-
cal model for the POS sequences assumes an additional latent variable for each sequence.
ÓÄÄerefore, the likelihood is deÔ¨Åned over X, Z and H, the latent variable. For example, H
could be a reÔ¨Ånement of the POS labels‚Äîadding an additional latent category that describes
types of coarse POS tags such as nouns, verbs and prepositions. ÓÄÄe observed data points
are .x.1/
; z.1/
/; : : : ; .x.n/
; z.n/
/. ÓÄÄe marginal likelihood is:
L.x.1/
; : : : ; x.n/
; z.1/
; : : : ; z.n/
/
D
Z

0
@
n
Y
iD1
X
h.i/
p.h.i/
; z.i/
j/p.x.i/
jh.i/
; z.i/
; /
1
A p./d:
‚Å∏ÓÄÄe scenario in transductive learning is such that training data in the form of inputs and outputs are available. In addition,
the inputs for which we are interested in making predictions are also available during training.
22 1. PRELIMINARIES
ÓÄÄe concept of likelihood is further explored in this book in various contexts, most promi-
nently in Chapters 4 and 6.
Log-likelihood and marginal log-likelihood can also be used as an ‚Äúintrinsic‚Äù measure of
evaluation for a given model. ÓÄÄis evaluation is done by holding out part of the observed data,
and then evaluating the marginal log-likelihood on this held-out dataset. ÓÄÄe higher this log-
likelihood is for a given model, the better ‚ÄúÔ¨Åt‚Äù it has to the data.
ÓÄÄe reason for doing this kind of evaluation on a held-out dataset is to ensure that the gen-
eralization power of the model is the parameter being tested. Otherwise, if we were to evaluate the
log-likelihood on the observed data during learning and initial inference, we could always create
a dummy, not-so-useful model that would give higher log-likelihood than any other model. ÓÄÄis
dummy model would be created by deÔ¨Åning a probability distribution (unconstrained to a speciÔ¨Åc
model family) which assigns probability 1=n to each instance in the observed data. Such distribu-
tion, however, would not generalize well for complex sample spaces. Still, evaluation of marginal
log-likelihood on the training data could be used for other purposes, such as hyperparameter
tuning.
1.7 BAYESIAN AND FREQUENTIST PHILOSOPHY (TIP OF
THE ICEBERG)
ÓÄÄe main diÔ¨Äerence between the Bayesian approach and the frequentist approach is the interpre-
tation of the concept of ‚Äúprobability.‚Äù ÓÄÄe frequentist view, as its name implies, views probability
(of an event) as a number that denotes its ‚Äúrelative frequency‚Äù in a large number of repeated
identical experiments. ÓÄÄe Bayesian view, on the other hand, views probability as a number that
denotes our state of knowledge about the event. Within the supporters of Bayesian statistics,
there are two camps. ÓÄÄose who view Bayesian probability as an objective, rational measure of the
state of knowledge (objectivists), and those who view it as as indication of personal belief (sub-
jectivists). ÓÄÄis means that objectivists advocate a rather uniform inference from data between
modelers who share the same knowledge about the problem, while subjectivists advocate that this
inference may diÔ¨Äer and highly depend on personal beliefs.
Both subjectivists and objectivists perform inference by applying Bayes‚Äô rule, and inverting
the relationship between the data and the hypotheses or the model. Objectivists, however, want
to minimize the inÔ¨Çuence that a person has on the inference process, so that the Ô¨Ånal conclusions
are determined, as much as possible, based on the data. ÓÄÄe attempt to minimize this inÔ¨Çuence
is sometimes done by introducing ‚Äúreference‚Äù or non-informative priors, such as JeÔ¨Äreys prior
(Section 3.3.2).
In science, the frequentist approach is associated with the hypothetico-deductive approach.
ÓÄÄis means that a hypothesis is formed, tested and Ô¨Ånally accepted or rejected. ÓÄÄe frequentist
approach is based on the idea of falsiÔ¨Åcation of a theory, and is conducted through statistical
tests, hypothesis testing and other methods which support this idea. ÓÄÄe Bayesian approach, on
1.8. SUMMARY 23
the other hand, is more typically associated with inductive inference. Data is collected, and then
we make an update to our beliefs about the theory at hand. Some might argue that the Bayesian
approach can also be thought of as hypothetico-deductive (see for example Gelman and Shalizi
(2013)). For a discussion of the Bayesian and frequentist philosophy, see Jaynes (2003).
1.8 SUMMARY
In this chapter we described the fundamental concepts for Probability and Statistics required for
understanding this book. Some of the concepts we described include:
‚Ä¢ Probability measures, probability distributions and sample spaces.
‚Ä¢ Random variables (which are functions over the sample space).
‚Ä¢ Joint probability distributions (which deÔ¨Åne a probability distribution over several random
variables) and conditional probability distributions (which deÔ¨Åne a probability distribution
over a set of random variables, while permitting clamping for the values of others).
‚Ä¢ Independence between random variables (which implies that two sets of random variables
are not informative about one another) and conditional independence between random vari-
ables (which implies that two sets of random variables are not informative about one another
assuming we know the value of a third set of random variables).
‚Ä¢ Bayes‚Äô rule and the chain rule.
‚Ä¢ Expectations (which compute the mean of a function, weighted against a probability dis-
tribution).
‚Ä¢ Models (which are sets of probability distributions).
‚Ä¢ Estimation (identifying a speciÔ¨Åc member of a model family based on data; most commonly
used in the frequentist setting).
‚Ä¢ Bayesian statistical inference (which requires the use of Bayes‚Äô rule to compute a probability
distribution, the posterior, over the parameter space, given observed data).
24 1. PRELIMINARIES
1.9 EXERCISES
1.1. Let p be a probability measure over  and A, B and C three events (subsets of ). Using
the axioms of probability measures, prove that
p.A [ B [ C/
D p.A/ C p.B/ C p.C/ p.A \ B/ p.A \ C/ p.B \ C/ C p.A \ B \ C/:
(ÓÄÄis formula can be generalized to an arbitrary number of events, based on the inclusion-
exclusion principle.)
1.2. Let  be the set of natural numbers. Can you deÔ¨Åne a probability measure p over  such
that p.x/ D p.y/ for any x; y 2 ? If not, show that such measure does not exist.
1.3. Describe the distribution of two random variables X and Y , such that X and Y are
uncorrelated but not independent.
1.4. For a given n, describe n random variables such that any subset of them is independent,
but all n random variables together are not.
1.5. Consider the graphical model in Figure 1.1 and its joint distribution in Equation 1.5.
Fix j 2 f1; : : : ; N g. Using the deÔ¨Ånition of conditional independence, show that W .j/
i is
conditionally independent of W .j/
k
for k ¬§ i given Z.j/
i . In addition, show that Z.j/
i is
conditionally independent of Z.k/
`
for k ¬§ j and any `, given .j /
. Is Z.j/
i conditionally
independent of Z.j/
`
given .j/
for ` ¬§ i?
C H A P T E R 2
Introduction
Broadly interpreted, Natural Language Processing (NLP) refers to the area in Computer Sci-
ence that develops tools for processing human languages using a computer. As such, it borrows
ideas from ArtiÔ¨Åcial Intelligence, Linguistics, Machine Learning, Formal Language ÓÄÄeory and
Statistics. In NLP, natural language is usually represented as written text (as opposed to speech
signals, which are more common in the area of Speech Processing).
ÓÄÄere is another side to the exploration of natural language using computational means‚Äî
through the Ô¨Åeld of Computational Linguistics. ÓÄÄe goal of exploring language from this angle
is slightly diÔ¨Äerent than that of NLP, as the goal is to use computational means to scientiÔ¨Å-
cally understand language, its evolution, acquisition process, history and inÔ¨Çuence on society. A
computational linguist will sometimes Ô¨Ånd herself trying to answer questions that linguists are
attempting to answer, only using automated methods and computational modeling. To a large ex-
tent, the study of language can be treated from a computational perspective, since it involves the
manipulation of symbols, such as words or characters, similarly to the ways other computational
processes work.
Computational Linguistics and NLP overlap to a large extent, especially in regards to the
techniques they use to learn and perform inference with data. ÓÄÄis is also true with Bayesian
methods in these areas. Consequently, we will refer mostly to NLP in this book, though most
of the technical descriptions in this book are also relevant to topics explored in Computational
Linguistics.
Many of the eÔ¨Äorts in modern Natural Language Processing address written text at the
sentential level. Machine translation, syntactic parsing (the process of associating a natural lan-
guage sentence with a grammatical structure), morphological analysis (the process of analyzing
the structure of a word and often decompose it into its basic units such as morphemes) and seman-
tic parsing (the process of associating a natural language sentence with a meaning-representation
structure) all analyze sentences to return a linguistic structure. Such predicted structures can be
used further in a larger natural language application. ÓÄÄis book has a similar focus. Most of the
Bayesian statistical models and applications we discuss are developed at the sentential level. Still,
we will keep the statistical models used more abstract, and will not necessarily commit to deÔ¨Åning
distributions over sentences or even natural language elements.
Indeed, to give a more diverse view for the reader, this introductory chapter actually dis-
cusses a simple model over whole documents, called the ‚Äúlatent Dirichlet allocation‚Äù (LDA)
model. Not only is this model not deÔ¨Åned at the sentential level, it originally was not framed
26 2. INTRODUCTION
in a Bayesian context. Yet there is a strong motivation behind choosing the LDA model in this
introductory chapter. From a technical point of view, the LDA model is simple, but demonstrates
most of the fundamental points that repeatedly appear in Bayesian statistical modeling in NLP.
We also discuss the version of LDA used now, which is Bayesian.
Before we begin our journey, it is a good idea to include some historical context about the
development of Bayesian statistics in NLP and introduce some motivation behind its use. ÓÄÄis is
the topic of the next section, followed by a section about the LDA model.
2.1 OVERVIEW: WHERE BAYESIAN STATISTICS AND NLP
MEET
Bayesian statistics had been an active area of research in statistics for some time before it was
introduced to NLP. It has a rich and colorful history that dates back to the 1700s, with seminal
ideas introduced by luminaries such as ÓÄÄomas Bayes and Pierre-Simon Laplace.
NLP, on the other hand, has a shorter history. It dates back perhaps as early as the 1950s, but
statistical techniques for NLP in its current form were introduced much later, in the mid 1980s.¬π
Early work on statistical analysis of language enabled rich Bayesian NLP literature to emerge a
few decades later. In the early days of statistical NLP, most of the data-driven techniques used the
frequentist approach, and were based on parameter estimation driven by objective functions such
as the log-likelihood, or otherwise some information-theoretic criteria. ÓÄÄis seemed to have laid
the groundwork for the subtle introduction of Bayesian statistics in the form of Bayesian point
estimation (Chapter 4).
Indeed, the earliest use of the Bayesian approach in NLP did not exploit the Bayesian
approach to its fullest extent, but rather did so only superÔ¨Åcially. ÓÄÄis approach was mostly based
on maximum a posteriori estimation (see Section 4.2.1), in which a Bayesian prior was used as a
penalty term added to the log-likelihood in order to bias a point estimate so that it had better
predictive power on unseen data. In these cases, the Bayesian approach was more of an additional
interpretation to an existing frequentist method. Bayesian techniques in NLP were also often
referred to at that point, or were implicitly considered to be, smoothing techniques. For example,
additive smoothing was heavily used in these early days (see Section 4.2.1).
ÓÄÄe more intensive use of Bayesian techniques and Bayesian machinery in NLP started
later, around the mid 2000s, in tandem with the increasing popularity of Bayesian methods in
the machine learning community. At Ô¨Årst, the literature focused on describing how to approach
well-known problems in NLP, such as part-of-speech tagging and context-free parsing using the
‚Äúfully Bayesian approach.‚Äù At that point, generic models such as hidden Markov models (HMMs)
and probabilistic context-free grammars (PCFGs) were mostly used (Goldwater and GriÔ¨Éths,
2007, Johnson et al., 2007a). Later on, Bayesian models emerged for very speciÔ¨Åc NLP prob-
lems, including Bayesian models for segmentation problems at various levels of text (Eisenstein
¬πÓÄÄe idea of statistical analysis of language had been explored before that by people such as Warren Weaver, Claude Shannon,
Victor Yngve, and others, but not in the way and level it has been done in modern NLP.
2.1. OVERVIEW: WHERE BAYESIAN STATISTICS AND NLP MEET 27
and Barzilay, 2008); morphological analysis (Dreyer and Eisner, 2011, Johnson et al., 2007b);
multilingual learning (Snyder and Barzilay, 2008, Snyder et al., 2008); machine translation (Blun-
som and Cohn, 2010a); syntactic parsing, both supervised (Shindo et al., 2012) and unsupervised
(Blunsom and Cohn, 2010b); discourse problems such as entity or event coreference resolution
(Bejan et al., 2009, Haghighi and Klein, 2007) and document-level discourse (Chen et al., 2009);
and even linguistic discovery (Daume III, 2009, Daume III and Campbell, 2007, Lin et al., 2009).
As mentioned earlier, the focus of this book is models that are developed at the sentential
level; these models are the principal part of what we refer to as ‚ÄúBayesian NLP‚Äù in this book. ÓÄÄe
predictions are usually made with respect to some linguistic structure. However, there has been a
signiÔ¨Åcant amount of work done on other kinds of text processing that has developed Bayesian
models and machinery. Most notably, the topic modeling community has been making heavy use
of Bayesian statistics since the introduction of the latent Dirichlet allocation model (Blei et al.,
2003).
Most of the recent eÔ¨Äorts to use Bayesian statistics with NLP have focused on the unsu-
pervised learning setting, in which only example inputs are available to the learner, without any
examples for the predicted structures. More generally, most models have been developed for use
with latent variables. (Unsupervised learning is a speciÔ¨Åc case of latent variable learning, in which
the whole predicted structure is missing during learning or inference.) In the partially supervised
setting these latent variables are just used as auxiliary variables to improve model expressivity.
‚ÄúPartially supervised‚Äù refers to the case in which annotated data of the same type one wants to
predict are available during inference or training, but the model includes additional random vari-
ables which are unobserved. Machine translation can be framed in such a setting, with the target
and source language sentences available during learning, but not alignment or other structures
that link them. A milder version of such latent-variable learning is that of learning PCFGs with
latent heads‚Äîa full parse tree is given during training, but the states that reÔ¨Åne the syntactic
categories in the tree are not given (see Chapter 8).
From a technical point of view, there is no reason to abstain from using Bayesian statistics
in the supervised learning setting. In fact, most introductory text about Bayesian statistics usually
assume complete data are available. Still, in NLP, the use of Bayesian statistics organically devel-
oped for scenarios with missing data and latent variables (such as unsupervised learning), likely
for the following reasons.
‚Ä¢ Discriminative models generally do better than generative models in the supervised set-
ting. In the supervised case with complete data available, discriminative models‚Äîin which
the input to the prediction algorithm is not statistically modeled‚Äîusually lead to better
performance for a variety of natural language problems. Most frequently, the discriminative
setting is used with conditional MaxEnt models (Berger et al., 1996) that have a log-linear
form‚Äîi.e., the distributions for these models are normalized (to form a probability), expo-
nentiated linear functions of various features extracted from the objects the distributions are
deÔ¨Åned over. ÓÄÄe Bayesian approach in NLP, on the other hand, is inherently generative.
28 2. INTRODUCTION
ÓÄÄe joint distribution is deÔ¨Åned over the parameters, over the predicted structure and over
the inputs as well. However, this comes with a modeling cost. Generative models usually
require stating more explicitly what are the independence assumptions that are being made
in the model. All interactions between the various components in the model are speciÔ¨Åed
as a graphical model or as a generative story (Section 1.5.3). In the discriminative setting,
arbitrary overlapping features are often used.¬≤
‚Ä¢ Priorsplayamuchmoreimportantroleintheunsupervisedsettingthaninthesupervised
setting. In the supervised setting, even when the generative case is considered, the log-
likeliood function gives a strong signal for the identiÔ¨Åcation of the underlying parameters
of the model. In the unsupervised setting, on the other hand, priors (especially those that are
structured and incorporate prior knowledge about the domain of the problem) have larger
eÔ¨Äects on the estimation problem.
‚Ä¢ ÓÄÄe parameters of the model in the Bayesian setting already introduce latent variables.
Since the parameters of the model in Bayesian NLP are a random variable that is never
observed, it comes more naturally to add other latent variables to the model and perform
inference over them as well. ÓÄÄis is especially true because of the advanced inference tech-
niques that have been developed in the Bayesian setting with latent variables.
Much of the technical Bayesian machinery developed for NLP relies strongly on recent
developments in Statistics and Machine Learning. For example, variational Bayesian inference
became popular in the NLP community after it had already been discovered and used in the
Statistics and Machine Learning community for some time. ÓÄÄe machinery developed in NLP
is more speciÔ¨Åc to scenarios that arise with language, but still includes general machinery for
important structured models that can be used outside of NLP.
Bayesian statistics in general has several advantages over the more classic frequentist ap-
proach to statistics. First and foremost, the theory behind it is simple. Bayesian statistics provides
a principle to combine data with existing information (or prior beliefs) all through a simple ap-
plication of Bayes‚Äô rule. ÓÄÄe uncertainty about the choice of model or parameters is all handled
through distributions, more speciÔ¨Åcally, the posterior distribution. Mathematically speaking, the
theory is therefore very elegant. Unlike the case with frequentist statistics, Bayesian statistics has
a uniÔ¨Åed approach to handling data both when large and small amounts of data are available.
Computationally, we may treat each case diÔ¨Äerently (because of limited computational power),
but the basic principle remains the same.
¬≤Here, ‚Äúoverlapping features‚Äù refers to features that are not easy to describe in a clean generative story‚Äîbecause they contain
overlapping information, and therefore together ‚Äúgenerate‚Äù various parts of the structure or the data multiple times. In princi-
ple, there is no issue with specifying generative models with overlapping features or models with complex interaction such as
log-linear models that deÔ¨Åne distribution both over the input and output spaces. However, such generative models are often
intractable to work with, because of the normalization constant which requires summing an exponential function both over
the input space and the output space. ÓÄÄerefore, general log-linear models are left to the discriminative setting, in which the
normalization constant is only required to be computed by summing over the output space for a given point in the input space.
2.2. FIRST EXAMPLE: THE LATENT DIRICHLET ALLOCATION MODEL 29
One of the greatest advantages of using Bayesian statistics in NLP is the ability to introduce
a prior distribution that can bias inference to better solutions. For example, it has been shown in
various circumstances that diÔ¨Äerent prior distributions can model various properties of natural
language, such as the sparsity of word occurrence (i.e., most words in a dictionary occur very few
times or not at all in a given corpus), the correlation between reÔ¨Åned syntactic categories and the
exponential decay of sentence length frequency. However, as is described below and in Chapter 3,
the current state of the art in Bayesian NLP does not exploit this degree of freedom to its fullest
extent. In addition, nonparametric methods in Bayesian statistics give a principled way to identify
appropriate model complexity supported by the data available.
Bayesian statistics also serves as a basis for modeling cognition, and there is some overlap
between research in Cognitive Science and NLP, most notably through the proxy of language ac-
quisition research (Doyle and Levy, 2013, Elsner et al., 2013, Frank et al., 2014, 2013, Fullwood
and O‚ÄôDonnell, 2013, Pajak et al., 2013). For example, Bayesian nonparametric models for word
segmentation were introduced to the NLP community (B√∂rschinger and Johnson, 2014, John-
son, 2008, Johnson et al., 2014, 2010, Synnaeve et al., 2014) and were also used as models for
exploring language acquisition in infants (Goldwater et al., 2009, 2006). For reviews of the use
of the Bayesian framework in Cognitive Science see, for example, GriÔ¨Éths et al. (2010, 2008),
Perfors et al. (2011), Tenenbaum et al. (2011).
Bayesian NLP has been Ô¨Çourishing, and its future continues to hold promise. ÓÄÄe rich
domain of natural language oÔ¨Äers a great opportunity to exploit the basic Bayesian principle of
encoding into the model prior beliefs about the domain and parameters. Much knowledge about
language has been gathered in the linguistics and NLP communities, and using it in a Bayesian
context could potentially improve our understanding of natural language and its processing using
computers. Still, the exploitation of this knowledge through Bayesian principles in the natural
language community has been somewhat limited, and presents a great opportunity to develop
this area in that direction. In addition, many problems in machine learning are now approached
with a Bayesian perspective in mind; some of this knowledge is being transferred to NLP, with
statistical models and inference algorithms being tailored and adapted to its speciÔ¨Åc problems.
ÓÄÄere are various promising directions for Bayesian NLP such as understanding better the
nature of prior linguistic knowledge about language that we have, and incorporating it into prior
distributions in Bayesian models; using advanced Bayesian inference techniques to scale the use
of Bayesian inference in NLP and make it more eÔ¨Écient; and expanding the use of advanced
Bayesian nonparametric models (see Section 7.5) to NLP.
2.2 FIRST EXAMPLE: THE LATENT DIRICHLET
ALLOCATION MODEL
We begin the technical discussion with an example model for topic modeling: the latent Dirichlet
allocation (LDA) model. It demonstrates several technical points that are useful to know when
approaching a problem in NLP with Bayesian analysis in mind. ÓÄÄe original LDA paper by Blei
30 2. INTRODUCTION
et al. (2003) also greatly popularized variational inference techniques in Machine Learning and
Bayesian NLP, to which Chapter 6 is devoted.
LDA elegantly extends the simplest computational representation for documents‚Äîthe
bag-of-words representation. With the bag-of-words representation, we treat a document as a
multiset of words (or potentially as a set as well). ÓÄÄis means that we dispose of the order of the
words in the document and focus on just their isolated appearance in the text. ÓÄÄe words are as-
sumed to originate in a Ô¨Åxed vocabulary that includes all words in all documents (see Zhai and
Boyd-graber (2013) on how to avoid this assumption).
ÓÄÄe bag-of-words representation is related to the ‚Äúunigram language model,‚Äù which also
models sentences by ignoring the order of the words in these sentences.
As mentioned above, with the bag-of-words model, documents can be mathematically rep-
resented as multisets. For example, assume there is a set V of words (the vocabulary) with a special
symbol Àò, and a text such as:¬≥
Goldman Sachs said ÓÄÄursday it has adopted all 39 initiatives it proposed to
strengthen its business practices in the wake of the 2008 Ô¨Ånancial crisis, a step de-
signed to help both employees and clients move past one of most challenging chapters
in the company‚Äôs history. Àò
ÓÄÄe symbol Àò terminates the text. All other words in the document must be members of
V n fÀòg. ÓÄÄe mathematical object that describes this document, d, is the multiset fw W cg, where
the notation w W c is used to denote that the word w appears in the document c times. For example,
for the above document, business W 1 belongs to its corresponding multiset, and so does both W 1. A
bag of words can have even more extreme representation, in which counts are ignored, and c D 1
is used for all words. From a practical perspective, the documents are often preprocessed, so that,
for example, function words or extremely common words are removed.
To deÔ¨Åne a probabilistic model over these multisets, we Ô¨Årst assume a probability distribu-
tion over V , p.W jÀá/. ÓÄÄis means that Àá is a set of parameters for a multinomial distribution such
that p.wjÀá/ D Àáw. ÓÄÄis vocabulary distribution induces a distribution over documents, denoted
by the random variable D (a random multiset), as follows:
p.D D djÀá/ D
Y
.wWc/2d
p.wjÀá/c
D
Y
.wWc/2d
.Àáw/c
: (2.1)
ÓÄÄis bag-of-words model appears in many applications in NLP, but is usually too weak to
be used on its own for the purpose of modeling language or documents. ÓÄÄe model makes an
extreme independence assumption‚Äîthe occurrence of all words in the document are independent
of each other. Clearly this assumption is not satisÔ¨Åed by text because there are words that tend to
co-occur. A document about soccer will tend to use the word ‚Äúgoal‚Äù in tandem with ‚Äúplayer‚Äù and
¬≥ÓÄÄe text is taken from ÓÅêe Wall Street Journal Risk and Compliance Journal and was written by Justin Baer (May 23, 2013).
2.2. FIRST EXAMPLE: THE LATENT DIRICHLET ALLOCATION MODEL 31
‚Äùball,‚Äù while a document about U.S. politics will tend to use words such as ‚Äúpresidential,‚Äù ‚Äúsenator‚Äù
and ‚Äúbill.‚Äù ÓÄÄis means that the appearance of the word ‚Äúpresidential‚Äù in a document gives us a lot of
information about what other words may appear in the document, and therefore the independence
assumption that bag-of-words models make fails. In fact, this extreme independence assumption
does not even capture the most intuitive notion of word repetition in a document‚Äîactual content
words‚Äîespecially words denoting entities, which are more likely to appear later in a document if
they already appeared earlier in the document.
ÓÄÄere is no single remedy to this strict independence assumption that the bag-of-words
model makes. ÓÄÄe rich literature of document modeling is not the focus of this book, but many of
the current models for document modeling are subject to the following principle, devised in the
late 1990s. A set of ‚Äútopics‚Äù is deÔ¨Åned. Each word is associated with a topic, and this association
can be made probabilistic or crisp. It is also not mutually exclusive‚Äîwords can belong to various
topics with diÔ¨Äerent degrees of association. In an inference step, given a set of documents, each
document is associated with a set of topics, which are being learned from the data. Given that
the topics are learned automatically, they are not labeled by the model (though they could be in a
post-processing step after inference with the topic model), but the hope is to discover topics such
as ‚Äúsoccer‚Äù (to which the word ‚Äúgoal‚Äù would have strong association, for example) or ‚Äúpolitics.‚Äù
Topics are discovered by assembling a collection of words for each topic, with a corresponding
likelihood for being associated with that topic.
ÓÄÄis area of topic modeling has Ô¨Çourished in the recent decade with the introduction of
the latent Dirichlet allocation model (Blei et al., 2003). ÓÄÄe idea behind the latent Dirichlet
allocation model is extremely intuitive and appealing, and it builds on previous work for modeling
documents, such as the work by Hofmann (1999b). ÓÄÄere are K topics in the model. Each topic
z 2 f1; : : : ; Kg is associated with a conditional probability distribution over V , p.w j z/ D Àáz;w.
(Àáz is the multinomial distribution over V for topic z). LDA then draws a document d in three
phases (conditioned on a Ô¨Åxed number of words to be sampled for the document, denoted by N)
using generative story 2.1.‚Å¥
LDA weakens the independence assumption made by the bag-of-words model in Equa-
tion 2.1: the words are not completely independent of each other, but are independent of each
other given their topic. First, a distribution over topics (for the entire document) is generated.
Second, a list of topics for each word in the document is generated. ÓÄÄird, each word is generated
according to a multinomial distribution associated with the topic of that word index.
Consider line 1 in the description of the LDA generative model (generative story 2.1).
ÓÄÄe distribution from which  is drawn is not speciÔ¨Åed. In order to complete the description of
the LDA, we need to choose a ‚Äúdistribution over multinomial distributions‚Äù such that we can
‚Å¥To make the LDA model description complete, there is a need to draw the number of words N in the document, so the LDA
model can generate documents of variable length. In Blei‚Äôs et al. paper, the number of words in the document is drawn from
a Poission distribution with some rate . It is common to omit this in LDA‚Äôs description, since the document is assumed
to be observed, and as a result, the number of words is known during inference. ÓÄÄerefore, it is not necessary to model N
probabilistically.
32 2. INTRODUCTION
.
.
Constants: K, N integers
Parameters: Àá
Latent variables: , zi for i 2 f1; : : : ; N g
Observed variables: w1; : : : ; wN (d)
.
‚Ä¢ Draw a multinomial distribution  over f1; : : : ; Kg. ( 2 RK
, i  0 for i 2
f1; : : : ; Kg,
PK
iD1 i D 1).
1:
2:
‚Ä¢ Draw a topic zi  Multinomial./ for i 2 f1; : : : ; N g (zi 2 f1; : : : ; Kg).
3:
‚Ä¢ Draw a word wi  Multinomial.Àázi
/ for i 2 f1; : : : ; N g (wi 2 V ).
4:
‚Ä¢ ÓÄÄe document d consists of the multiset generated from w1; : : : ; wN :
5:
d D fw W cjw 2 V; c D
N
X
jD1
I.wj D w/g;
where I./ is 1 if proposition  is true and 0 otherwise.
GenerativeStory2.1: ÓÄÄe generative story for the latent Dirichlet allocation model. Figure 1.1 gives a
graphical model description for the LDA. ÓÄÄe generative model here assumes M D 1 when compared
to the graphical model in Figure 1.1 (i.e., the model here generates a single document). An outer loop
is required to generate multiple documents. ÓÄÄe distribution over  is not speciÔ¨Åed in the above, but
with LDA it is drawn from the Dirichlet distribution.
draw from it a topic distribution. Each instance  is a multinomial distribution with z  0 and
PK
zD1 z D 1. ÓÄÄerefore, we need to Ô¨Ånd a distribution over the set
f j 8z z  0 ;
K
X
zD1
z D 1g:
LDA uses the Dirichlet distribution for deÔ¨Åning a distribution over this probability sim-
plex.‚Åµ ÓÄÄis means that the distribution over  is deÔ¨Åned as follows:
‚ÅµA K-dimensional simplex is a K-dimensional polytope, i.e., the convex hull of K C 1 vertices. ÓÄÄe vertices that deÔ¨Åne a
probability simplex are the basis vectors ei for i 2 f1; : : : ; Kg where ei 2 Rk is a vector such that it is 0 everywhere. but is 1
in the ith coordinate.
2.2. FIRST EXAMPLE: THE LATENT DIRICHLET ALLOCATION MODEL 33
p.1; : : : ; KjÀõ1; : : : ; ÀõK/ D C.Àõ/
K
Y
kD1

Àõk 1
k
; (2.2)
where the function C.Àõ/ is deÔ¨Åned in Section 2.2.1 (Equation 2.3, see also Appendix B), and
serves as the normalization constant of the Dirichlet distribution.
ÓÄÄe Dirichlet distribution depends on K hyperparameters, Àõ1; : : : ; ÀõK, which can be de-
noted using a vector Àõ 2 RK
. It is notationally convenient to denote the Dirichlet distribution
then by p.1; : : : ; K j Àõ/ to make this dependence explicit. ÓÄÄis does not imply that Àõ itself is a
random variable or event that we condition on, but instead is a set of parameters that determines
the behavior of the speciÔ¨Åc instance of the Dirichlet distribution (see note about this notation in
Section 1.5.5).
ÓÄÄe reason for preferring the Dirichlet distribution is detailed in Chapter 3, when the
notion of conjugacy is introduced. For now, it is suÔ¨Écient to say that the choice of the Dirichlet
distribution is natural because it makes inference with LDA much easier‚Äîdrawing a multinomial
from a Dirichlet distribution and then subsequently drawing a topic from this multinomial is
mathematically and computationally convenient. ÓÄÄe Dirichlet distribution has other desirable
properties which are a good Ô¨Åt for modeling language, such as encouraging sparse multinomial
distributions with a speciÔ¨Åc choice of hyperparameters (see Section 3.2.1).
Natural language processing models are often constructed using multinomial distributions
as the basic building blocks for the generated structure. ÓÄÄis includes parse trees, alignments, de-
pendency trees and others. ÓÄÄese multinomial distributions compose the parameters of the model.
For example, the parameters of a probabilistic context-free grammar (see Chapter 8) are a set of
multinomial distributions for generating the right-hand side of a rule conditioned on the left-
hand side.
One of the core technical ideas in the Bayesian approach is that the parameters are consid-
ered to be random variables as well, and therefore the generative process draws values for these
model parameters. It is not surprising, therefore, that the combination of convenience in inference
with Dirichlet-multinomial models and the prevalence of multinomial distributions in generative
NLP models yields a common and focused use of the Dirichlet distribution in Bayesian NLP.
ÓÄÄere is one subtle dissimilarity between the way the Dirichlet distribution is used in
Bayesian NLP and the way it was originally deÔ¨Åned in the LDA. ÓÄÄe topic distribution  in
LDA does not represent the parameters of the LDA model. ÓÄÄe only parameters in LDA are the
topic multinomials Àák for k 2 f1; : : : ; Kg. ÓÄÄe topic distribution  is an integral part of the model,
and is drawn separately for each document. To turn LDA into a Bayesian model, one should draw
Àá from a Dirichlet distribution (or another distribution). ÓÄÄis is indeed now a common practice
with LDA (Steyvers and GriÔ¨Éths, 2007). A graphical model for this fully Bayesian LDA model
is given in Figure 2.1.
34 2. INTRODUCTION
.
.
. z
. w
.
Àõ
.
Àá
.
.
M
.
N
.
K
Figure 2.1: ÓÄÄe fully Bayesian version of the latent Dirichlet allocation model. A prior over Àá is added
(and Àá now are random variables). Most commonly, this prior is a (symmetric) Dirichlet distribution
with hyperparameter .
Since the Dirichlet distribution is so central to Bayesian NLP, the next section is dedi-
cated to an exploration of its basic properties. We will also re-visit the Dirichlet distribution in
Chapter 3.
2.2.1 THE DIRICHLET DISTRIBUTION
ÓÄÄe Dirichlet distribution is a multivariate distribution over the probability simplex of a Ô¨Åxed
dimension. ÓÄÄis means it deÔ¨Ånes a distribution over K continuous random variables, 0  k  1
for k 2 f1; : : : ; Kg such that:
K
X
kD1
k D 1:
Its probability density depends on K positive real values, Àõ1; : : : ; ÀõK. ÓÄÄe PDF appears in
Equation 2.2 where C.Àõ/ is a normalization constant deÔ¨Åned as:
C.Àõ/ D
¬Ä.
PK
kD1 Àõk/
¬Ä.Àõ1/ : : : ¬Ä.ÀõK/
; (2.3)
with ¬Ä.x/ being the Gamma function for x  0 (see also Appendix B)‚Äîa generalization of the
factorial function such that whenever x is natural number it holds that ¬Ä.x/ D .x 1/≈†.
Vectors in the ‚Äúprobability simplex,‚Äù as the name implies, can be treated as probability
distributions over a Ô¨Ånite set of size K. ÓÄÄis happens above, with LDA, where  is treated as a
probability distribution over the K topics (each topic is associated with one of the K dimensions
of the probability simplex), and used to draw topics for each word in the document.
Naturally, the Ô¨Årst and second moments of the Dirichlet distribution depend on Àõ. When
denoting Àõ
D
PK
kD1 Àõk, it holds that:
2.2. FIRST EXAMPLE: THE LATENT DIRICHLET ALLOCATION MODEL 35
E≈ík¬ç D
Àõk
Àõ
;
var.k/ D
Àõk.Àõ
Àõk/
.Àõ/2.Àõ C 1/
;
Cov.j ; k/ D
Àõj Àõk
.Àõ/2.Àõ C 1/
:
ÓÄÄe mode‚Å∂ of the Dirichlet distribution (when Àõk > 1 for all k) is
mode.k/ D
Àõk 1
Àõ K
:
ÓÄÄe mode is not deÔ¨Åned if any Àõk < 1, since in that case, the density of the Dirichlet
distribution is potentially unbounded.
ÓÄÄe Beta Distribution In the special case where K D 2, the Dirichlet distribution is also called
the Beta distribution, and has the following density:
p.1; 2 j Àõ1; Àõ2/ D
¬Ä.Àõ1 C Àõ2/
¬Ä.Àõ1/¬Ä.Àõ2/
Àõ1 1
1 Àõ2 1
2 :
Since 1 C 2 D 1, the Beta distribution can be described as a univariate distribution over
0
2 ≈í0; 1¬ç:
p.0
j Àõ1; Àõ2/ D
¬Ä.Àõ1 C Àõ2/
¬Ä.Àõ1/¬Ä.Àõ2/
.0
/Àõ1 1
.1 0
/Àõ2 1
: (2.4)
Symmetric Dirichlet It is often the case that instead of using K diÔ¨Äerent parameters, the
Dirichlet distribution is used with Àõ such that Àõ1 D Àõ2 D : : : D ÀõK D Àõ0
2 RC
. In this case, the
Dirichlet distribution is also called a symmetric Dirichlet distribution. ÓÄÄe Àõ0
hyperparameter is
called the concentration hyperparameter.
ÓÄÄe reason for collapsing Àõ1; : : : ; ÀõK into a single parameter is two-fold: (i) this consider-
ably simpliÔ¨Åes the distribution, and makes it easier to tackle learning, and (ii) a priori, if latent
variables exist in the model, and they are drawn from a multinomial (which is drawn from a
Dirichlet distribution), it is often the case that the role of various events in the multinomial are
interchangeable. ÓÄÄis is the case, for example, with the LDA model. Since only the text is ob-
served in the data (without either the topic distribution or the topics associated with each word),
the role of the K topics can be permuted. If Àõ1; : : : ; ÀõK are not estimated by the learning algo-
‚Å∂ÓÄÄe mode of a probability distribution is the most likely value according to that distribution. It is the value(s) at which the
PMF or PDF obtain their maximal value.
36 2. INTRODUCTION
.
.
Parameters: 
Latent variables: Z (and also )
Observed variables: X
.
‚Ä¢ Draw a set of parameters  from p./.
1:
‚Ä¢ Draw a latent structure z from p.zj/.
2:
‚Ä¢ Draw the observed data x from p.xjz; /.
3:
Generative Story 2.2: ÓÄÄe generative story for a Bayesian model.
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0
Density
Œ∏
0.1
0.5
1.0
2.0
3.0
Figure 2.2: ÓÄÄe Beta distribution density function when Àõ1 D Àõ2 D Àõ0
for Àõ0
2 f0:1; 0:5; 1; 2; 3g.
rithm, but instead clamped at a certain value, it makes sense to keep the role of Àõk symmetric by
using a symmetric Dirichlet.
Figure 2.2 plots the density in Equation 2.4 when Àõ1 D Àõ2 D Àõ0
2 R for several values of
Àõ0
, and demonstrates the choice of the name ‚Äúconcentration parameter‚Äù for Àõ0
. ÓÄÄe closer Àõ0
is to 0,
2.2. FIRST EXAMPLE: THE LATENT DIRICHLET ALLOCATION MODEL 37
Figure 2.3: A plot of sampled data from the symmetric Dirichlet distribution with K D 3, with
various Àõ. Top-left: Àõ D 10, top-right: Àõ D 1, bottom-left: Àõ D 0:1, bottom-right: Àõ D 0:01. ÓÄÄe plot
demonstrates that for Àõ < 1, the Dirichlet distribution is centered on points in the probability simplex
which are sparse. For the value Àõ D 1, the probability distribution should be uniform.
the more sparse the distribution is, with most of the mass concentrated on near-zero probability.
ÓÄÄe larger Àõ0
is, the more concentrated the distribution around its mean value. Since the Ô¨Ågure
describes a symmetric Beta distribution, the mean value is 0.5 for all values of Àõ0
. When Àõ0
D 1,
the distribution is uniform.
ÓÄÄe fact that small values for Àõ0
make the Dirichlet distribution sparse is frequently ex-
ploited in the NLP Bayesian literature. ÓÄÄis point is discussed at greater length in Section 3.2.1.
It is also demonstrated in Figure 2.3.
38 2. INTRODUCTION
2.2.2 INFERENCE
As was brieÔ¨Çy mentioned earlier, in topic modeling the topics are considered to be latent. While
datasets exist in which documents are associated with various human-annotated topics, the vast
majority of document collections do not have such an annotation‚Äîcertainly not in the style of
the LDA model, where each word has some degree of association with each topic. In fact, asking
an annotator to annotate topics the way they are deÔ¨Åned in an LDA-style topic model is probably
an ill-deÔ¨Åned task because these topics are often not crisp or fully interpretable in their word
association (see Chang et al. (2009) for a study on human interpretation of topic models; also see
Mimno et al. (2011) and Newman et al. (2010) for automatic topic coherence evaluation). For
LDA, this means that the distribution over topics, , and the topic identity for each word are
latent variables‚Äîthey are never observed in the data, which are just pure text.
ÓÄÄis is typical in Bayesian NLP as well. Usually, there is a random variable X (a document
or a sentence, for example) which is associated with a predicted structure, denoted by a random
variable Z. ÓÄÄe generation of X and Z is governed by some distribution parametrized by . ÓÄÄe
parameters  themselves are a random variable that is governed, for example, by the Dirichlet
distribution, or more generally by a distribution p./. ÓÄÄis distribution is also called the prior
distribution. Generative story 2.2 describes this process.
ÓÄÄere is a striking similarity to the LDA generative process, where the topic distribution
plays the role of the set of parameters, the topic assignments play the role of the latent structure,
and the words in the document play the role of the observed data.
ÓÄÄe generative process above dictates the following joint probability distribution
p.X; Z; /:
p.x; z; / D p./p.z j /p.x j ; z/:
ÓÄÄe goal of Bayesian inference is naturally to either infer the latent structure z (or a distri-
bution over it), or infer the parameters  (or a distribution over them). More generally, the goal of
Bayesian inference is to obtain the posterior distribution over the non-observed random variables
in the model, given the observed data x. For the general Bayesian model above, the posterior is
p.Z; jx/.
Note that the predictions are managed through distributions (such as the posterior) and
not through Ô¨Åxed values. Bayesian inference, at its basic level, does not commit to a single z or
. (However, it is often the case that we are interested in a point estimate for the parameters, see
Chapter 4, and even more often‚Äîa Ô¨Åxed value for the predicted structure.)
To identify this posterior, Bayesian statistics exploits the treatment of  as a random variable
through a basic application of Bayes‚Äô rule. More speciÔ¨Åcally, the posterior is identiÔ¨Åed as:
p.z;  j x/ D
p./p.z j /p.x j z; /
p.x/
: (2.5)
2.3. SECOND EXAMPLE: BAYESIAN TEXT REGRESSION 39
ÓÄÄe quantity p.x/ acts as a marginalization constant that ensures that the probability in
Equation 2.5 integrates (and sums) to one. ÓÄÄerefore, assuming  is continuous, as is usually the
case, the following holds:
p.x/ D
Z

p./
X
z
p.z j /p.x j z; /
!
d: (2.6)
Mathematically, Bayesian inference is easy and elegant. It requires inverting the conditional
distributions of p.x; z; / using Bayes‚Äô rule so that the posterior is computed. All of the quantities
in Equation 2.5 are theoretically known. ÓÄÄere is only the need to rely on the simplest, most basic
results from probability theory.
Still, Bayesian inference is not always trivial to implement or to computationally execute.
ÓÄÄe main challenge is computing the marginalization constant (Equation 2.6), which is required
to make the predictions. ÓÄÄe marginalization constant requires summing over a discrete (possibly
inÔ¨Ånite) set and integrating over a continuous set. ÓÄÄis is often intractable, but prior conjugacy
can alleviate this problem (Chapter 3). Even when only one of these marginalizations is required
(such as the case with variational inference, see Chapter 6), inference can still be complex.
ÓÄÄis intractability is often overcome by using approximate inference methods, such as
Markov chain Monte Carlo methods or variational inference. ÓÄÄese are discussed in Chapters 5
and 6, respectively.
2.2.3 SUMMARY
ÓÄÄe core ideas in LDA modeling and inference have striking similarities to some of the principles
used in Bayesian NLP. Most notably, the use of the Dirichlet distribution to deÔ¨Åne multinomial
distributions is common in both.
LDA requires inferring a posterior over topic assignments for each word in each document
and the distribution over topics for each document. ÓÄÄese are the two latent variables in the LDA
model. Analogously, in Bayesian NLP, we often require inferring a latent structure (such as a parse
tree or a sequence) and the parameters of the model.
Inference of the kind described in this chapter, with LDA, with Bayesian models and more
generally, with generative models, can be thought of as the reverse-engineering of a generative
device, that is the underlying model. ÓÄÄe model is a device that continuously generates samples of
data, of which we see only a subset of the Ô¨Ånal output that is generated by this device. In the case
of LDA, the device generates raw text as output. Inference works backward, trying to identify the
missing values (topic distributions and the topic themselves) that were used to generate this text.
2.3 SECOND EXAMPLE: BAYESIAN TEXT REGRESSION
Even though Bayesian NLP has focused mostly on unsupervised learning, Bayesian inference
in general is not limited to learning from incomplete data. It is also often used for prediction
40 2. INTRODUCTION
problems such as classiÔ¨Åcation and regression where the training examples include both the inputs
and the outputs of the model.
In this section, we demonstrate Bayesian learning in the case of text regression, predicting
a continuous value based on a body of text. We will continue to use the notation from Section 2.2
and denote a document by d, as a set of words and word count pairs. In addition, we will assume
some continuous value that needs to be predicted, denoted by the random variable Y . To ground
the example, D can be a movie review, and Y can be a predicted average number of stars the
movie received by critics or its revenue (Joshi et al., 2010). ÓÄÄe prediction problem is therefore to
predict the number of stars a movie receives from the movie review text.
One possible way to frame this prediction problem is as a Bayesian linear regression prob-
lem. ÓÄÄis means we assume that we receive as input for the inference algorithm a set of examples
.d.i/
; y.i/
/ for i 2 f1; : : : ; ng. We assume a function f .d/ that maps a document to a vector in
RK
. ÓÄÄis is the feature function that summarizes the information in the document as a vector,
and on which the Ô¨Ånal predictions are based. For example, K could be the size of the vocabulary
that the documents span, and ≈íf .d/¬çj could be the count of the j th word in the vocabulary in
document d.
A linear regression model typically assumes that there is a stochastic relationship between
Y and d:
Y D   f .d/ C ;
where  2 RK
is a set of parameters for the linear regression model and  is a noise term (with
zero mean), most often framed as a Gaussian variable with variance 2
. For the sake of simplicity,
we assume for now that 2
is known, and we need not make any inference about it. As a result,
the learning problem becomes an inference problem about .
As mentioned above,  is assumed to be a Gaussian variable under the model, and as such
Y itself is a Gaussian with mean value   f .d/ for any Ô¨Åxed  and document d. ÓÄÄe variance of
Y is 2
.
In Bayesian linear regression, we assume a prior on , a distribution p.jÀõ/. Consequently,
the joint distribution over  and Y .i/
is:
p.; Y .1/
D y.1/
; : : : ; Y .n/
D y.n/
j d.1/
; : : : ; d.n/
; Àõ/ D p. j Àõ/
n
Y
iD1
p.Y .i/
D y.i/
j ; d.i/
/:
In this case, Bayesian inference will use Bayes‚Äô rule to Ô¨Ånd a probability distribution over
 conditioned on the data, y.i/
and d.i/
for i 2 f1; : : : ; ng. It can be shown that if we choose a
conjugate prior to the likelihood over Y .i/
, which in this case is also a normal distribution, then
the distribution p. j y.1/
; : : : ; y.n/
; d.1/
; : : : ; d.n/
/ is also a normal distribution.
ÓÄÄis conjugacy between the normal distribution and itself is demonstrated in more detail
in Chapter 3. Also see exercise 6 in this chapter. ÓÄÄere are two natural extensions to the scenario
2.4. CONCLUSION AND SUMMARY 41
described above. One in which Y .i/
are multivariate, i.e., y.i/
2 RM
. In that case,  is a matrix in
RMK
. ÓÄÄe other extension is one in which the variance 2
(or the covariance matrix controlling
the likelihood and the prior, in the multivariate case) is unknown. Full derivations of Bayesian
linear regression for these two cases are given by Minka (2000).
2.4 CONCLUSION AND SUMMARY
Earlier use of Bayesian statistics in NLP mostly relied on Bayesian point estimation. See Chapter 4
for a survey of techniques to obtain a point estimate in the Bayesian setting. Modern use of
Bayesian statistics in NLP makes use of both the theoretical and technical machinery that is
available and has been recently developed in the statistics and machine learning communities.
Bayesian machinery speciÔ¨Åcally tailored for NLP models, such as PCFGs or HMMs, is also
sometimes described. See Chapter 8.
Beyond NLP, from a practical point of view, both the classic frequentist methods and the
Bayesian methods have advantages and disadvantages. Bayesian inference gives a natural and prin-
cipled way of combining prior beliefs with data, through Bayes‚Äô rule. Inference means inferring
a posterior distribution, which in turn can be used as a prior when new data is available; it also
provides interpretable results (for example, with Bayesian conÔ¨Ådence intervals). Conceptually,
the Bayesian approach also provides inference that depends on the data and is exact. However,
sometimes it is computationally intractable to perform this kind of inference, and approximation
methods must be used. ÓÄÄe Bayesian approach also relies on a selection of a prior distribution, but
does not instruct on exactly how it should be chosen. For a thorough discussion of the advantages
and disadvantages in Bayesian analysis, see, for example Berger (1985).
42 2. INTRODUCTION
2.5 EXERCISES
2.1. Consider the generative story below.
Can the posterior p.Z.1/
; : : : ; Z.n/
; jx.1/
; : : : ; x.n 1/
/ be analytically identiÔ¨Åed? If so,
write down its expression.
.
.
Constants: n integer
Hyperparameters: Àõ > 0
Latent variables: Z.1/
; : : : ; Z.n/
Observed variables: X.1/
; : : : ; X.n/
.
‚Ä¢ Draw a multinomial  of size two from a symmetric Beta distribution with hyperpa-
rameter Àõ > 0.
1:
2:
‚Ä¢ Draw z.1/
; : : : ; z.n/
from the multinomial  where z.i/
2 f0; 1g.
3:
‚Ä¢ Set n 1 binary random variables x.1/
; : : : ; x.n 1/
such that x.i/
D z.i/
z.iC1/
.
4:
2.2 Consider the graphical model for Bayesian LDA (Figure 2.1). Write down an expression
for the joint distribution over the observed variables (the words in the document). (Use the
Dirichlet distribution for the topic distributions.)
2.3 Alice has a biased coin that lands more often on ‚Äútails‚Äù than ‚Äùheads.‚Äù She is interested in
placing a symmetric Beta prior over this coin with hyperparameter Àõ. A draw  from this
Beta distribution will denote the probability of tails. (1  is the probability of heads.)
What range of values for Àõ complies with Alice‚Äôs knowledge of the coin‚Äôs unfairness?
2.4 As mentioned in this chapter, choosing a hyperparameter Àõ < 1 with the symmetric Dirich-
let encourages sparse draws from the Dirichlet. What are some properties of natural lan-
guage that you believe are useful to mathematically model using such sparse prior distribu-
tion?
2.5 ÓÄÄe LDA assumes independence between the topics being drawn for each word in the
document. Can you describe the generative story and the joint probability distribution for
a model that assumes a bigram-like distribution of topics? ÓÄÄis means that each topic Z.i/
depends on Z.i 1/
. In what scenarios is this model more sensible for document modeling?
Why?
2.6 Complete the details of the example in Section 2.3. More speciÔ¨Åcally, Ô¨Ånd the posterior over
 given y.i/
and d.i/
for i 2 f1; : : : ; ng, assuming the prior over p. j ; 2
/ is a normal
distribution with mean  and variance 2
.
C H A P T E R 3
Priors
Priors are a basic component in Bayesian modeling. ÓÄÄe concept of priors and some of their me-
chanics must be introduced quite early in order to introduce the machinery used in Bayesian NLP.
At their core, priors are distributions over a set of hypotheses, or when dealing with parametric
model families, over a set of parameters. In essence, the prior distribution represents the prior
beliefs that the modeler has about the identity of the parameters from which data is generated,
before observing any data.
One of the criticisms of Bayesian statistics is that it lacks objectivity in the sense that diÔ¨Äer-
ent prior families will lead to diÔ¨Äerent inference based on the available data, especially when only
small amounts of data are available. ÓÄÄere are circumstances in which such a view is justiÔ¨Åed (for
example, the Food and Drug Administration held this criticism against the Bayesian approach
for some time (Feinberg, 2011); the FDA has since made use of Bayesian statistics in certain cir-
cumstances), but this lack of objectivity is less of a problem in solving engineering problems than
in natural language processing. In NLP, the Ô¨Ånal ‚Äútest‚Äù for the quality of a model (or decoder,
to be more precise) that predicts a linguistic structure given some input such as a sentence, often
uses an evaluation metric that is not directly encoded in the statistical model. ÓÄÄe existence of a
precise evaluation metric, coupled with the use of unseen data (in the supervised case; in the un-
supervised case, either the unseen data or the data on which inference is performed can be used)
to calculate this evaluation metric eliminates the concern of subjectivity.
In fact, the additional degree of freedom in Bayesian modeling, the prior distribution, can
be a great advantage in NLP. ÓÄÄe modeler can choose a prior that biases the inference and learning
in such a way that the evaluation metric is maximized. ÓÄÄis is not necessarily done directly as a
mathematical optimization problem, but through experimentation.
ÓÄÄis point has been exploited consistently in the Bayesian NLP literature, where priors are
chosen because they exhibit certain useful properties that are found in natural language. In the
right setting, the Dirichlet distribution is often shown to lead to sparse solutions (see Chapter 2).
ÓÄÄe logistic normal distribution, on the other hand, can capture relationships between the var-
ious parameters of a multinomial distribution. Other hand-crafted priors have also been used,
mirroring a speciÔ¨Åc property of language.
ÓÄÄis chapter covers the main types of priors that are used in Bayesian NLP. As such, it
discusses conjugate priors at length (Section 3.1) and speciÔ¨Åcally focuses on the Dirichlet distri-
bution (Section 3.2.1). ÓÄÄe discussion of the Dirichlet distribution is done in the context of priors
over multinomials (Section 3.2), as the multinomial distribution is the main modeling workhorse
in Bayesian NLP.
44 3. PRIORS
3.1 CONJUGATE PRIORS
Basic inference in the Bayesian setting requires computation of the posterior distribution (Chap-
ter 2)‚Äîthe distribution over the model parameters which is obtained by integrating the infor-
mation from the prior distribution together with the observed data. Without exercising caution,
and putting restrictions on the prior distribution or the likelihood function, this inference can
be intractable. When performing inference with incomplete data (with latent variables), this is-
sue becomes even more severe. In this case, the posterior distribution is deÔ¨Åned over both of the
parameters and the latent variables.
Conjugate priors eliminate this potential intractability when no latent variables exist, and
also help to a large extent when latent variables do exist in the model. A prior family is conjugate
to a likelihood if the posterior, obtained as a calculation of
posterior D
prior  likelihood
evidence
;
is also a member of the prior family.
We now describe this idea in more detail. We begin by describing the use of conjugate pri-
ors in the case of having empirical observations for all random variables in the model (i.e., without
having any latent variables). Let p. j Àõ/ be some prior with hyperparameters Àõ. ÓÄÄe hyperparam-
eters by themselves are parameters‚Äîonly instead of parametrizing the likelihood function, they
parametrize the prior. ÓÄÄey can be Ô¨Åxed and known, or be inferred. We assume the hyperparame-
ters are taken from a set of hyperparameters A. In addition, let p.X j / be a distribution function
for the likelihood of the observed data. We observe an instance of the random variable X D x.
Posterior inference here means we need to identify the distribution p.jx/. We say that the prior
family p.jÀõ/ is a a conjugate prior with respect to the likelihood p.X j / if the following holds
for the posterior:
p. j x; Àõ/ D p. j Àõ0
/;
for some Àõ0
D Àõ0
.x; Àõ/ 2 A. Note that Àõ0
is a function of the observation x and Àõ, the hyperpa-
rameter with which we begin the inference. (ÓÄÄis means that in order to compute the posterior,
we need to be able to compute the function Àõ0
.x; Àõ/.)
ÓÄÄe mathematical deÔ¨Ånition of conjugate priors does not immediately shed light on why
they make Bayesian inference more tractable. In fact, according to the deinÔ¨Åtion above, the use
of a conjugate prior does not guarantee computational tractability. Conjugate priors are useful
when the function Àõ0
.x; Àõ/ can be eÔ¨Éciently computed, and indeed this is often the case when
conjugate priors are used in practice.
When Àõ0
.x; Àõ/ can be eÔ¨Éciently computed, inference with the Bayesian approach is con-
siderably simpliÔ¨Åed. As mentioned above, to compute the posterior over the parameters, all we
need to do is compute Àõ0
.x; Àõ/, and this introduces a new set of hyperparameters that deÔ¨Åne the
posterior.
3.1. CONJUGATE PRIORS 45
ÓÄÄe following example demonstrates this idea about conjugate priors for normal variables.
In this example, contrary to our persistent treatment of the variable X as a discrete variable up to
this point, X is set to be a continuous variable. ÓÄÄis is done to demonstrate the idea of conjugacy
with a relatively simple, well-known example‚Äîthe conjugacy of the normal distribution to itself
(with respect to the mean value parameters).
Example 3.1
Let X be drawn from a normal distribution with expected value  and Ô¨Åxed known variance
2
(it is neither a parameter nor hyperparameter), i.e., the density of X for a point x is:
p.x j / D
1

p
2
exp
1
2

x 

2
!
:
In addition,  is drawn from a prior family that is also Gaussian, controlled by the hyperpa-
rameter set A D R  RC
where every Àõ 2 A is a pair .; 2
/. ÓÄÄe value of  denotes the expected
value of the prior and the value of 2
denotes the variance. Assume we begin inference with a
prior such that  2 R and 2
D 2
‚Äîi.e., we assume the variance of the prior is identical to the
variance in the likelihood. (ÓÄÄis is assumed for the simplicity of the posterior derivation, but it
is not necessary to follow this assumption to get a similar derivation when the variances are not
identical.) ÓÄÄe prior, therefore, is:
p. j Àõ/ D
1

p
2
exp
1
2

 

2
!
:
Assume a single observation x is observed, based on which the likelihood function is
formed. ÓÄÄe quantity of interest is the posterior distribution p.jx; Àõ/. Bayesian inference dictates
that it has the following form:
p.jx; Àõ/ D
p. j Àõ/p.xj/
R
 p. j Àõ/p.xj/d
: (3.1)
ÓÄÄe numerator equals:
p. j Àõ/p.xj/ D
1

p
2
exp
1
2

x 

2
!!

1

p
2
exp
1
2

 

2
!!
D
1
22
exp

1
2

.x /2
C . /2
2

: (3.2)
Algebraic manipulation shows that:
46 3. PRIORS
.x /2
C . /2
D


x C 
2
2
C 1
2
.x /2
1=2
:
ÓÄÄe term 1
2
.x /2
does not depend on , and therefore it will cancel from both the
numerator and the denominator. It can be taken out of the integral in the denominator. (We
therefore do not include it in the equation below.) ÓÄÄen, Equation 3.1 can be rewritten as:
p.jx; Àõ/ D
p. j Àõ/p.xj/
R
 p. j Àõ/p.xj/d
D
exp
0
B
B
B
@


x C 
2
2
2=2
1
C
C
C
A
C.x; Àõ/
: (3.3)
ÓÄÄe term in the denominator,
C.x; Àõ/ D
Z

exp
0
B
B
B
@


x C 
2
2
2=2
1
C
C
C
A
d;
is a normalization constant that ensures that p.jx; Àõ/ integrates to 1 over . Since the numerator
of Equation 3.3 has the form of a normal distribution with mean value
x C 
2
and variance
2
=2, this means that Equation 3.3 is actually the density of a normal distribution such that
Àõ0
.x; Àõ/ D

x C 
2
;

p
2

, where  and 2
are deÔ¨Åned by Àõ. See Appendix A for further detail.
ÓÄÄe normalization constant can easily be derived from the density of the normal distribution with
these speciÔ¨Åc hyperparameters.
ÓÄÄe conclusion from this example is that the family of prior distributions fp.jÀõ/jÀõ D
.; 2
/ 2 R  .0; 1/g is conjugate to the normally distributed likelihood with a Ô¨Åxed variance
(i.e., the likelihood is parametrized only by the mean value of the normal distribution).
In a general case, with Àõ D .; 0/ (i.e.,   Normal.; 2
0 / and n observations being iden-
tically distributed (and independent given ) with X.i/
 Normal.; 2
/), it holds that:
Àõ0
.x.1/
; : : : ; x.n/
; Àõ/ D
 C
Pn
iD1 x.i/
n C 1
;
q
.1=2
0 C n=2/ 1
!
; (3.4)
3.1. CONJUGATE PRIORS 47
i.e., the posterior distribution is a normal distribution with a mean and a variance as speciÔ¨Åed in
Equation 3.4. Note that full knowledge of the likelihood variance is assumed both in the example
and in the extension above. ÓÄÄis means that the prior is deÔ¨Åned only over  and not . When
the variances are not known (or more generally, when the covariance matrices of a multivariate
normal variable are not known) and are actually drawn from a prior as well, more care is required
in deÔ¨Åning a conjugate prior for the variance (more speciÔ¨Åcally, the Inverse-Wishart distribution
would be the conjugate prior in this case deÔ¨Åned over the covariance matrix space).
Example 3.1 and Equation 3.4 demonstrate a recurring point with conjugate priors and
their corresponding posteriors. In many cases, the hyperparameters Àõ take the role of ‚Äúpseudo-
observations‚Äù in the function Àõ0
.x; Àõ/. As described in Equation 3.4,  is added to the sum of
the rest of the observations and then averaged together with them. Hence, this hyperparameter
functions as an additional observation with value  which is taken into account in the posterior.
To avoid confusion, it is worth noting that both the prior and the likelihood were normal
in this example of conjugacy, but usually conjugate prior and likelihood are not members of the
same family of distributions. With Gaussian likelihood, the conjugate prior is also Gaussian (with
respect to the mean parameter) because of the speciÔ¨Åc algebraic properties of the normal density
function.
3.1.1 CONJUGATE PRIORS AND NORMALIZATION CONSTANTS
Consider posterior inference in Equation 3.1. ÓÄÄe key required calculation was computing the
normalization constant
R
 p.jÀõ/p.xj/d in order to fully identify the posterior distribution.¬π
ÓÄÄis normalization constant is also equal to p.xjÀõ/, since it is just a marginalization of  from
the joint distribution p.; xjÀõ/.
ÓÄÄerefore, the key step required in computing the posterior is calculating p.xjÀõ/, also called
‚Äúthe evidence.‚Äù ÓÄÄe posterior can be then readily evaluated at each point  by dividing the product
of the prior and the likelihood by the evidence.
ÓÄÄe use of conjugate prior in Example 3.1 eliminated the need to explicitly calculate this
normalization constant, but instead we were able to calculate it more indirectly. Identifying that
Equation 3.2 has the algebraic form (up to a constant) of the normal distribution immediately
dictates that the posterior is a normal variable with the appropriate Àõ0
.x; Àõ/. ÓÄÄerefore, explicitly
computing the evidence p.xjÀõ/ is unnecessary, because the posterior was identiÔ¨Åed as a (normal)
distribution, for which its density is fully known in an analytic form.
If we are interested in computing p.xjÀõ/, we can base our calculation on the well-known
density of the normal distribution. Equation 3.1 implies that for any choice of , it holds that
p.xjÀõ/ D
Z

p.jÀõ/p.xj/d D
p.jÀõ/p.xj/
p.jx; Àõ/
: (3.5)
¬πAs we see in later Chapters 5 and 6, we can avoid calculating this marginalization constant by using approximate inference
such as MCMC sampling and variational inference.
48 3. PRIORS
ÓÄÄis is a direct result of applying the chain rule in both directions:
p.x; jÀõ/ D p.xjÀõ/p.jÀõ; x/ D p.jÀõ/p.xj; Àõ/:
Note that even though the right-hand side of Equation 3.5 seems to be dependent on ,
if we were to algebraically manipulate the right-hand side, all terms that include  will cancel,
because the left-hand side does not introduce any dependence on . ÓÄÄe right-hand side, as
mentioned, is true for any choice of . In the case of the normal distribution from Example 3.1, all
three distributions in Equation 3.5 are normal distributions, for which we can calculate the density
using its formula. (It is important to note that the calculation of the normalization constant of
the normal distribution, either in univariate or multivariate form, is also known and can be easily
computed.)
ÓÄÄis kind of algebraic relationship exists for most conjugate priors with well-known den-
sities. ÓÄÄe algebraic convenience that often occurs with conjugate priors can lead to somewhat
inaccurate deÔ¨Ånitions of conjugate priors, the most common one being that ‚Äúconjugate priors are
priors which lead to a closed-form solution for the posterior.‚Äù Even though this is true in many
cases, the strict deÔ¨Ånition of conjugate priors is that the posterior that is obtained from a given
prior in the prior family and a given likelihood function belongs to the same prior family. As such,
conjugacy is always determined in the context of a likelihood function and a family of priors.
3.1.2 THE USE OF CONJUGATE PRIORS WITH LATENT VARIABLE
MODELS
Earlier in this section, it was demonstrated that conjugate priors make Bayesian inference tractable
when complete data is available. Example 3.1 demonstrated this by showing how the posterior
distribution can easily be identiÔ¨Åed when assuming a conjugate prior. Explicit computation of the
evidence normalization constant with conjugate priors is often unnecessary, because the product
of the likelihood together with the prior lead to an algebraic form of a well-known distribution.
As mentioned earlier, the calculation of the posterior normalization constant is the main
obstacle in performing posterior inference. If this is the case, we can ask: do conjugate priors help
in the case of latent variables being present in the model? With latent variables, the normalization
constant is more complex, because it involves the marginalization of both the parameters and the
latent variables. Assume a full distribution over the parameters , latent variables z and observed
variables x (both being discrete), which factorize as follows:
p.; z; x j Àõ/ D p. j Àõ/p.z j /p.x j z; /:
ÓÄÄe posterior over the latent variables and parameters has the form (see Section 2.2.2 for a
more detailed example of such posterior):
3.1. CONJUGATE PRIORS 49
p.; z j x; Àõ/ D
p. j Àõ/p.z j /p.x j z; /
p.x j Àõ/
;
and therefore, the normalization constant p.x j Àõ/ equals:
p.x j Àõ/ D
X
z
Z

p./p.z j /p.x j z; /d

D
X
z
D.z/; (3.6)
where D.z/ is deÔ¨Åned to be the term inside the sum above. Equation 3.6 demonstrates that
conjugate priors are useful even when the normalization constant requires summing over latent
variables. If the prior family is conjugate to the distribution p.X; Z j /, then the function D.z/
will be mathematically easy to compute for any z. However, it is not true that
P
z D.z/ is always
tractable, since the form of D.z/ can be quite complex.
On a related note, if the order of summation and integration is switched, then it holds that:
p.x j Àõ/ D
Z

p./
X
z
p.z j /p.x j z; /
!
d D
Z

p./D0
./d./;
where D0
./ is deÔ¨Åned as the term that sums over z. ÓÄÄen, it is often the case that for every ,
D0
./ can be computed using dynamic programming algorithms or other algorithms that sum
over a discrete space (for example, if the latent variable space includes parse trees with an un-
derlying PCFG grammar, then D0
./ can be computed using a variant of the CKY algorithm,
the inside algorithm. See Chapter 8 for further discussion.) Switching integration and summa-
tion does not make the problem of computing the marginalization constant p.xjÀõ/ tractable. ÓÄÄe
outside integral over the function D0
./ is often still infeasible.
Still, the fact that a tractable solution for the inner term results from switching the inte-
gration and the sum is very useful for approximate inference, especially for variational inference.
ÓÄÄis is exactly where the conjugacy of the prior comes in handy even with an intractable posterior
with latent variables. ÓÄÄis is discussed further in Chapter 6.
3.1.3 MIXTURE OF CONJUGATE PRIORS
Mixture models are a simple way to extend a family of distributions into a more expressive family.
If we have a set of distributions p1.X/; : : : ; pM .X/, then a mixture model over this set of distribu-
tions is parametrized by an M dimensional probability vector .1; : : : ; M / (i  0,
P
i i D 1)
and deÔ¨Ånes distributions over X such that:
p.Xj/ D
M
X
iD1
i pi .X/:
50 3. PRIORS
Section 1.5.3 gives an example of a mixture-of-Gaussians model. ÓÄÄe idea of mixture mod-
els can also be used for prior families. Let p. j Àõ/ be a prior from a prior family with Àõ 2 A.
ÓÄÄen, it is possible to deÔ¨Åne a prior of the form:
p. j Àõ1
; : : : ; ÀõM
; 1; : : : ; M / D
M
X
iD1
i p. j Àõi
/;
where i  0 and
PM
iD1 i D 1 (i.e.,  is a point in the M 1 dimensional probability simplex).
ÓÄÄis new prior family, which is hyperparametrized by Àõi
2 A and i for i 2 f1; : : : Mg will ac-
tually be conjugate to a likelihood p.x j / if the original prior family p. j Àõ/ for Àõ 2 A is also
conjugate to this likelihood.
To see this, consider that when using a mixture prior, the posterior has the form:
p. j x; Àõ1
; : : : ; ÀõM
; / D
p.x j /p. j Àõ1
; : : : ; ÀõM
; /
R
 p.x j /p. j Àõ1; : : : ; ÀõM ; /d
D
PM
iD1 i p.x j /p. j Àõi
/
PM
iD1 i Zi
;
where
Zi D
Z

p.x j /p. j Àõi
/d:
ÓÄÄerefore, it holds that:
p. j x; Àõ1
; : : : ; ÀõM
; / D
PM
iD1.i Zi /p. j x; Àõi
/
PM
iD1 i Zi
;
because p.xj/p.jÀõi
/ D Zi p.jx; Àõi
/. Because of conjugacy, each p.jx; Àõi
/ is equal to
p.jÀái
/ for some Àái
2 A (i 2 f1; : : : ; Mg). ÓÄÄe hyperparameters Àái
are the updated hyperpa-
rameters following posterior inference. ÓÄÄerefore, it holds:
p. j x; Àõ1
; : : : ; ÀõM
; / D
M
X
iD1
0
i p. j Àái
/;
for 0
i D i Zi =
PM
iD1 i Zi

.
When the prior family parametrized by Àõ is K-dimensional Dirichlet (see Section 2.2.1
and Equation 2.2), then:
3.1. CONJUGATE PRIORS 51
Zi D
QK
j D1 ¬Ä.Àõi
j C xj /
¬Ä.
PK
j D1 Àõi
j C xj /
:
We conclude this section about mixtures of conjugate priors with an example of using such
a mixture prior for text analysis. Yamamoto and Sadamitsu (2005) deÔ¨Åne a topic model, where
a mixture of Dirichlet distributions is deÔ¨Åned as the prior distribution over the vocabulary. Each
draw from this mixture provides a multinomial distribution over the vocabulary. Following that
draw, the words in the document are drawn independently in the generative process.
Yamamoto and Sadamitsu describe the mixture components of their Dirichlet mixture dis-
tribution as corresponding to topics. In this sense, there is a large distinction between their model
and LDA, which samples a topic distribution for each document separately. When measuring per-
formance on a held-out dataset using perplexity (see Appendix A and Section 1.6), their model
consistently scored better than LDA on a set of 100,000 newspaper articles in Japanese. ÓÄÄeir
model performance was also saturated for 20 topics, while the perplexity of the LDA continued
to decrease for a much larger number of topics. ÓÄÄis perhaps points to a better Ô¨Åt‚Äîtheir model
uses fewer topics (and therefore is simpler), but still has lower perplexity than LDA.
3.1.4 RENORMALIZED CONJUGATE DISTRIBUTIONS
In the previous section, we saw that one could derive a more expressive prior family by using a
basic prior distribution in a mixture model. Renormalizing a conjugate prior is another way to
change the properties of a prior family while still retaining conjugacy.
Let us assume that a prior p.jÀõ/ is deÔ¨Åned over some parameter space ‚Äö. It is sometimes
the case that we want to further constrain ‚Äö into a smaller subspace, and deÔ¨Åne p.jÀõ/ such that
its support is some ‚Äö0  ‚Äö. One way to do so would be to deÔ¨Åne the following distribution p0
over ‚Äö0:
p0
.jÀõ/ D
p.jÀõ/
R
02‚Äö0
p.0jÀõ/d0
: (3.7)
ÓÄÄis new distribution retains the same ratio between probabilities of elements in ‚Äö0 as p,
but essentially allocates probability 0 to any element in ‚Äö n ‚Äö0.
It can be shown that if p is a conjugate family to some likelihood, then p0
is conjugate to
the same likelihood as well. ÓÄÄis example actually demonstrates that conjugacy, in its pure form
does not necessitate tractability by using the conjugate prior together with the corresponding
likelihood. More speciÔ¨Åcally, the integral over ‚Äö0 in the denominator of Equation 3.7 can often
be diÔ¨Écult to compute, and approximate inference is required.
ÓÄÄe renormalization of conjugate distributions arises when considering probabilistic
context-free grammars with Dirichlet priors on the parameters. In this case, in order for the
52 3. PRIORS
prior to allocate zero probability to parameters that deÔ¨Åne non-tight PCFGs, certain multinomial
distributions need to be removed from the prior. Here, tightness refers to a desirable property of
a PCFG so that the total measure of all Ô¨Ånite parse trees generated by the underlying context-free
grammar is 1. For a thorough discussion of this issue, see Cohen and Johnson (2013).
3.1.5 DISCUSSION: TO BE OR NOT TO BE CONJUGATE?
It is worth noting that the development of conjugate priors came with the intention of deÔ¨Åning
priors that are not just analytically tractable (i.e., the posterior belongs to the prior family), but are
also (i) rich enough to express the modeler‚Äôs prior information and beliefs, and (ii) interpretable,
so that the modeler can understand what prior information is being injected into the posterior
when choosing a certain member of the prior family (RaiÔ¨Äa and Schlaifer, 1961). However, the
requirement for analytic tractability conÔ¨Ånes the set of possible priors for most likelihood func-
tions. Once the requirement of analytical tractability is met, it is diÔ¨Écult to verify that richness
and interpretability are met as well. In fact, one of the criticisms of Bayesian statistics is that in
many cases Bayesians rely on computationally convenient priors, and are therefore able to tackle
only the most simple examples with respect to richness and interpretability (Carlin and Louis,
2000).
In NLP, the need for computationally convenient priors is especially important. NLP
mainly predicts combinatorial structures such as trees and sequences, for which inference is com-
putationally expensive, and it is prohibitive to use priors that do not oÔ¨Äer computational tractabil-
ity. For this reason, for example, the Dirichlet distribution is often used in Bayesian NLP; the
Dirichlet distribution is conjugate to the multinomial likelihood and the multinomial family and,
in turn, serves as the most fundamental building block of generative models over sequences, trees,
and other structures that arise in NLP. Section 3.2 discusses this at length.
Still, one can argue that with the advent of new hardware and state-of-the-art approxi-
mation algorithms, it is possible to tackle problems in NLP with priors that are not necessarily
computationally convenient. MCMC sampling (Chapter 5) and variational inference (Chapter 6)
can be used in these cases.
It is also important to note that conjugacy of the prior to the likelihood does not guarantee
tractability of inference with the model. ÓÄÄe function Àõ0
.x; Àõ/ needs to be eÔ¨Éciently computable
in order for conjugacy to be useful. To demonstrate this point, consider a parameter space ‚Äö. ÓÄÄe
set of all distributions deÔ¨Åned over ‚Äö, denoted by P, is actually a conjugate prior family to any
distribution of the form p.X j / simply because it holds trivially that p. j X/ 2 P. Clearly, the
conjugate prior family P is intractable because Àõ0
.x; Àõ/ cannot be computed eÔ¨Éciently.
At the other extreme, a prior family that includes a single prior distribution that places all
of the probability mass on a single point in the parameter space would also be a conjugate prior
(to any model using this parameter space). ÓÄÄe posterior can be trivially computed in this case
(it is the distribution that is the single member in the prior family, because the support of the
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 53
posterior over the parameters is always subsumed by the support of the prior), but conjugacy in
this case is not useful, simply because the prior family is not suÔ¨Éciently rich.
3.1.6 SUMMARY
Conjugate priors are deÔ¨Åned in the context of a prior family and a distribution for the variables
in the model over the observations and latent variables. In many cases, conjugate priors ensure
the tractability of the computation of the normalization constant of the posterior. For example,
conjugate priors often lead to closed-form analytic solutions for the posterior given a set of ob-
servations (and values for the latent variables, if they exist in the model).
Conjugate priors are often argued to be too simplistic, but they are very helpful in NLP
because of the computational complexity of NLP models. Alternatives to conjugate priors are
often less eÔ¨Écient, but with the advent of new hardware and approximation algorithms, these
alternatives have become more viable.
3.2 PRIORS OVER MULTINOMIAL AND CATEGORICAL
DISTRIBUTIONS
ÓÄÄe nature of structures that are predicted in natural language processing is an excellent Ô¨Åt for
modeling using the categorical distribution. ÓÄÄe categorical distribution is a generalization of the
Bernoulli distribution, which speciÔ¨Åes how K outcomes (such as topics in a document or on the
right-hand sides of context-free rules headed by a non-terminal) are distributed. ÓÄÄe categori-
cal distribution is speciÔ¨Åed by a parameter vector  2 RK
, where  satisÔ¨Åes the following two
properties:
8k 2 f1; : : : ; Kg k  0; (3.8)
K
X
kD1
k D 1: (3.9)
ÓÄÄe space of allowed parameters for the categorical distribution over K outcomes,
‚Äö D f 2 RK
j satisÔ¨Åes Equations 3.8‚Äì3.9g;
is also called ‚Äúthe probability simplex of dimension K 1‚Äù‚Äîthere is one fewer degree of free-
dom because of the requirement for all probabilities to sum to 1. ÓÄÄe set ‚Äö deÔ¨Ånes a simplex, i.e.,
geometrically it is a K 1 dimensional polytope, which is the convex hull of K vertices; the ver-
tices are the points such that all probability mass is placed on a single event. All other probability
distributions can be viewed as a combination of these vertices. Each point in this simplex deÔ¨Ånes
a categorical distribution.
54 3. PRIORS
If X  Categorical./, then the probability distribution over X is deÔ¨Åned as:¬≤
p.X D ij/ D i ;
where i 2 f1; : : : ; Kg. While the categorical distribution generalizes the Bernoulli distribution,
the multinomial distribution is actually a generalization of the binomial distribution and describes
a distribution for a random variable X 2 NK
such that
PK
iD1 Xi D n for some Ô¨Åxed n, a natural
number, which is a parameter of the multinomial distribution. ÓÄÄe parameter n plays a similar
role to the ‚Äúexperiments count‚Äù parameter in the binomial distribution. ÓÄÄen, given  2 ‚Äö and n
as described above, the multinomial distribution is deÔ¨Åned as:
p.X1 D i1; : : : ; XK D iK/ D
n≈†
QK
j D1 ij ≈†
K
Y
j D1

ij
j ;
where
PK
j D1 ij D n.
Even though the categorical distribution and the multinomial distribution diÔ¨Äer, there is
a strong relationship between them. More speciÔ¨Åcally, if X distributes according to a categorical
distribution with parameters , then the random variable Y 2 f0; 1gK
deÔ¨Åned as:
Yi D I.X D i/; (3.10)
is distributed according to the multinomial distribution with parameters  and n D 1. It is often
the case that it is mathematically convenient to represent the categorical distribution as a multino-
mial distribution of the above form, using binary indicators. In this case, the probability function
p.Y j/ can be written as
QK
iD1 
yi
i .
ÓÄÄere have been various generalizations and extensions to the Dirichlet distribution. One
such example is the generalized Dirichlet distribution, which provides a richer covariance struc-
ture compared to the Dirichlet distribution (see also Section 3.2.2 about the covariance structure
of the Dirichlet distribution). Another example is the Dirichlet-tree distribution (Minka, 1999)
which gives a prior over distributions that generate leaf nodes in a tree-like stochastic process.
In the rest of this section, the categorical distribution will be referred to as a multinomial
distribution to be consistent with the NLP literature. ÓÄÄis is not a major issue, since most of the
discussion points in this section are valid for both distributions.
3.2.1 THE DIRICHLET DISTRIBUTION RE-VISITED
ÓÄÄe Dirichlet distribution is ubiquitous in Bayesian NLP, because it is the simplest conjugate
distribution to the categorical (and multinomial) distribution. Its deÔ¨Ånition and an example of
¬≤ÓÄÄe Bayesian NLP literature often refers to categorical distributions as ‚Äúmultinomial distributions,‚Äù but this is actually a
misnomer (a misnomer that is nonetheless used in this book, in order to be consistent with the literature).
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 55
its use with the latent Dirichlet allocation model for topic modeling are given in Section 2.2.
For completeness, we repeat Equation 2.2 below, and reiterate that the Dirichlet distribution is
parametrized by a vector Àõ 2 RK
such that:
p.1; : : : ; KjÀõ1; : : : ; ÀõK/ D C.Àõ/
K
Y
kD1

Àõk 1
k
;
where .1; : : : ; K/ is a vector such that i  0 and
PK
iD1 i D 1.
Here, we continue to provide a more complete description of the Dirichlet distribution and
its properties.
Conjugacy of the Dirichlet Distribution
ÓÄÄe conjugacy of the Dirichlet distribution to the categorical distribution is a direct result of ‚Äúalge-
braic similarity‚Äù between the density of the Dirichlet distribution and the multinomial probability
distribution.
Example 3.2 Let   Dirichlet.Àõ/ with Àõ D .Àõ1; : : : ; ÀõK/ 2 RK
. Let X be a binary random
vector of length K that deÔ¨Ånes a multinomial distribution as described in Equation 3.10. ÓÄÄe
parameter vector of this categorical distribution is . Assume that x, a sample of X, is observed.
ÓÄÄen:
p. j x; Àõ/ / p.jÀõ/p.x j / /
K
Y
iD1

Àõi 1
i
!

K
Y
iD1

xi
i
!
D
K
Y
iD1

Àõi Cxi 1
i : (3.11)
Note the use of / (i.e., ‚Äúproportional to‚Äù) instead of ‚Äúequal to.‚Äù Two normalization constants
in Equation 3.11 were omitted to simplify the identiÔ¨Åcation of the posterior. ÓÄÄe Ô¨Årst constant is
p.x j Àõ/, the marginalization constant. ÓÄÄe second constant is the normalization constant of the
Dirichlet distribution (Equation 2.3). ÓÄÄe reason we can omit them is that these constants do not
change with , and the distribution we are interested in is deÔ¨Åned over .
Equation 3.11 has the algebraic form (without the normalization constant) of a Dirich-
let distribution with Àõ0
.x; Àõ/ D Àõ C x. ÓÄÄis means that the posterior distributes according to a
Dirichlet distribution with hyperparameters Àõ C x.
Example 3.2 demonstrates again the principle of hyperparameters acting as pseudo-
observations. ÓÄÄe initial hyperparameters Àõ are added to the observations to derive the posterior,
as if outcome i was observed Àõi times (though, note that Àõi does not need to be integral).
In the general case of observing x.1/
; : : : ; x.n/
from a categorical distribution (where each
sample is independent of the other given ), the resulting posterior has hyperparameters Àõ C
P
iD1 x.i/
, where each x.i/
is a binary vector of length K.
56 3. PRIORS
ÓÄÄe Dirichlet Distribution and Sparsity
A symmetric Dirichlet distribution (Section 2.2.1) is hyperparametrized by Àõ > 0. It is a speciÔ¨Åc
case of the Dirichlet distribution in which the hyperparameter vector of the general Dirichlet dis-
tribution contains only identical values to Àõ. When the hyperparameter of a symmetric Dirich-
let distribution Àõ 2 R is chosen such that Àõ < 1, any point x 2 RK
drawn from the respective
Dirichlet will have most of its coordinates close to 0, and only a few will have a value signiÔ¨Åcantly
larger than zero.
ÓÄÄe intuition behind this property of the symmetric Dirichlet distribution can be under-
stood when inspecting the main term in the density of the Dirichlet distribution:
QK
iD1 Àõ 1
i .
When Àõ < 1, this product becomes
1
QK
iD1 Àá
i
for 0 < Àá D Àõ 1. Clearly, this product becomes
very large if one of the i is close to 0. If many of the i are close to 0, this eÔ¨Äect is multiplied,
which makes the product even larger. It is therefore true that most of the density for the sym-
metric Dirichlet with Àõ < 1 is concentrated around points in the probability simplex where the
majority of the i are close to 0.
ÓÄÄis property of the symmetric Dirichlet has been exploited consistently in the Bayesian
NLP literature. For example, Goldwater and GriÔ¨Éths (2007) deÔ¨Åned a Bayesian part-of-speech
tagging with hidden Markov models (Chapter 8), in which they used a Dirichlet prior as a prior
over the set of multinomials for the transition probabilities and emission probabilities in the tri-
gram hidden Markov model.
For the Ô¨Årst set of experiments, Goldwater and GriÔ¨Éths used a Ô¨Åxed sparse hyperparameter
for all transition probabilities and a Ô¨Åxed, diÔ¨Äerent hyperparameter for all emission probabilities.
ÓÄÄeir Ô¨Åndings show that choosing a small value for the transition hyperparameter (0:03) together
with a choice of hyperparameter 1 for the emission probabilities achieves the best prediction accu-
racy of the part-of-speech tags. ÓÄÄis means that the optimal transition multinomials are similarly
likely to be very sparse. ÓÄÄis is not surprising, since only a small number of part-of-speech tags
can appear in a certain context. However, the emission hyperparameter 1 means that the Dirichlet
distribution is simply a uniform distribution. ÓÄÄe authors argued that the reason a sparse prior
was not very useful for the emission probabilities is that all emission probabilities shared the same
hyperparameter.
Indeed, when they improved their model, and inferred a diÔ¨Äerent hyperparameter for each
emission distribution (hyperparameter inference is discussed in Chapters 5 and 6), the results
improved. In addition, the inferred hyperparameters for the emission distributions were quite
close to 0, implying a very sparse distribution for the set of emitted words for a given tag, as one
would intuit.
Toutanova and Johnson (2008) also described a model for a Bayesian semi-supervised¬≥
part-of-speech tagging that uses the Dirichlet distribution to encourage sparsity in the parameter
¬≥Toutanova and Johnson‚Äôs study is considered here to be in the semi-supervised realm because they used a part-of-speech
tagging dictionary for some of the words, i.e., they had a speciÔ¨Åcation of the parts of speech that these words can be associated
with. ÓÄÄis approach was also taken in some of the experiments that Goldwater and GriÔ¨Éths performed.
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 57
space. Unlike the model of Goldwater and GriÔ¨Éths, their model is not based on hidden Markov
models, but instead adapts the LDA model for the purpose of POS tagging. ÓÄÄe model includes
a multinomial component that generates tags conditioned on their words (as opposed to words
conditioned on their tags, as in HMMs). ÓÄÄis component was associated with a sparse Dirichlet
prior, to capture the notion that most words are associated with very few tags, mainly a single
part-of-speech tag. ÓÄÄe hyperparameter Àõ they used ranged between 0:2 to 0:5, depending on the
size of the dictionary used.
ÓÄÄey contrasted their model with an almost identical model that does not include a Bayesian
prior on the components of p.T j W / (where T denotes a random variable of tag sequences, and
W denotes a random variable for word sequences, i.e., sentences), to determine whether the sparse
Dirichlet prior helps to obtain better tagging accuracy. ÓÄÄe non-Bayesian model is similar to the
probabilistic latent semantic analysis model, PLSA (Hofmann, 1999a). ÓÄÄey reported that this
was indeed the case‚Äîthe LDA-like model achieved an error reduction of up to 36% compared
to PLSA.
In general, sparsity of a prior distribution can eÔ¨Äectively support models for natural lan-
guage. Most often, this property is a useful property for modeling language at the lexical level:
whenever associating a word with a set of clusters (such as syntactic categories), the word is as-
sociated with a relatively small number of clusters. ÓÄÄis property is a great Ô¨Åt for modeling with
a sparse Dirichlet distribution, and has been exploited in the Bayesian NLP literature multiple
times.
Gamma Representation of the Dirichlet
ÓÄÄe Dirichlet distribution has a reductive representation to the Gamma distribution. ÓÄÄis repre-
sentation does not contribute directly to better modeling, but helps to demonstrate the limitations
of the Dirichlet distribution, and suggest alternatives to it (such as the one described in the next
section).
Let i  ¬Ä.Àõi ; 1/ be K i.i.d. random variables distributed according to the Gamma dis-
tribution with shape Àõi > 0 and scale 1 (see also Appendix B). ÓÄÄen, the deÔ¨Ånition of
i D
i
PK
iD1 i
; (3.12)
for i 2 f1; : : : ; Kg yields a random vector  from the probability simplex of dimension K 1, such
that  distributes according to the Dirichlet distribution with hyperparameters Àõ D .Àõ1; : : : ; ÀõK/.
ÓÄÄe representation of the Dirichlet as independent, normalized, Gamma variables explains
a limitation inherent to the Dirichlet distribution. ÓÄÄere is no explicit parametrization of the rich
structure of relationships between the coordinates of . For example, given i ¬§ j , the ratio i =j ,
when treated as a random variable, is independent of any other ratio k=` calculated from two
other coordinates, k ¬§ `. (ÓÄÄis is evident from Equation 3.12: the ratio i D  is i D j , where
58 3. PRIORS
all i for i 2 f1; : : : ; Kg are independent.) ÓÄÄerefore, the Dirichlet distribution is not a good
modeling choice when the  parameters are better modeled even with a weak degree of dependence.
Natural language elements, however, have a large dependence between one another. For
example, consider a Bayesian unigram language model (i.e., a language model that treats a sen-
tence as a bag of words), where the model is parametrized through , which is a distribution over
K words in a vocabulary. When estimated from data, these parameters exhibit great dependence,
depending on the domain of the data. It is highly likely that an increase or decrease in the fre-
quency of words that are semantically related to each other (where these changes are compared
to a language model learned from another unrelated text) is simultaneous, comparing one data
domain to another. A text about veterinary science will have a simultaneous increase in the proba-
bility of words such as ‚Äúdog,‚Äù ‚Äúcat‚Äù and ‚Äúfur‚Äù compared to a text about religion‚Äîthough each word
separately can have a unique probability, high or low. A prior distribution on this unigram model
encapsulates our prior beliefs, varying across text domains, for example, about the parameters.
Using the Dirichlet distribution is unsatisfactory, because it is unable to capture a dependency
structure between the words in the vocabulary.
ÓÄÄe next section explains how this independence property of the Dirichlet distribution can
be partially remedied, through the use of a diÔ¨Äerent distribution as a prior over the multinomial
distribution.
Summary
ÓÄÄe Dirichlet distribution is often used as a conjugate prior to the categorical and multinomial
distributions. ÓÄÄis conjugacy makes it extremely useful in Bayesian NLP, because the categorical
distribution is ubiquitous in NLP modeling. ÓÄÄe Dirichlet distribution has the advantage of po-
tentially encouraging sparse solutions, when its hyperparameters are set properly. ÓÄÄis property
has been repeatedly exploited in the NLP literature, because distributions over linguistic elements
such as part-of-speech tags or words typically tend to be sparse. ÓÄÄe Dirichlet distribution also
comes with limitations. For example, this distribution assumes a nearly independent structure
between the coordinates of points in the probability simplex that are drawn from it.
3.2.2 THE LOGISTIC NORMAL DISTRIBUTION
ÓÄÄe logistic normal distribution was suggested by Aitchison (1986) in order to overcome the
limitations of the Dirichlet distribution with compositional data in the probability simplex. A
random vector  2 RK
is distributed according to the (additive) logistic normal distribution with
parameters Àõ D .; ‚Ä†/ where  D .1; : : : ; K 1/ 2 RK 1
and ‚Ä† 2 R.K 1/.K 1/
is a covariance
matrix (i.e., it is positive-deÔ¨Ånite and symmetric) if:
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 59
i D
exp.i /
1 C
PK 1
j D1 exp.j /
8i 2 f1; : : : ; K 1g; (3.13)
K D
1
1 C
PK 1
j D1 exp.j /
; (3.14)
for some  2 RK 1
random vector that distributes according to the multivariate normal distri-
bution with mean value  and covariance matrix ‚Ä†.
ÓÄÄerefore, the logistic normal distribution, as its name implies, is a multivariate normal
variable that has been transformed using the logistic transformation. ÓÄÄe reason that this multi-
variate normal variable needs to be K 1 dimensional instead of K dimensional is to eliminate a
redundant degree of freedom: had the logistic normal distribution used a K dimensional multi-
variate normal variable, one of them could have been canceled by choosing one of the coordinates
and subtracting them from all the others (the resulting subtracted vector would still be a multi-
variate normal variable).
An additional dependence structure that does not exist in the Dirichlet distribution ap-
pears in the logistic normal distribution because of the explicit dependence structure represented
through the covariance matrix ‚Ä†. ÓÄÄerefore, in light of the discussion in Section 3.2.1, the logistic
normal distribution is an alternative to the Dirichlet distribution. Unlike the Dirichlet distribu-
tion, the logistic normal distribution is not conjugate to the multinomial distribution.
Figure 3.1 provides a plot of the logistic normal distribution with various hyperparame-
ters for K D 3 and n D 5,000 (i.e., the number of samples drawn is 5,000). With independence
between the dimensions, and variance 1 for each dimension, the distribution is spread over the
whole probability simplex. When the correlation is negative and close to 1, we see a more nar-
row spread. With large variance (and independence between the coordinates), the logistic normal
behaves almost like a sparse distribution.
Properties of the Additive Logistic Normal Distribution
Using the Jacobian transformation method (see Appendix A), Aitchison (1986) shows that the
density of the additive logistic normal distribution is:
p. j ; ‚Ä†/
D
1
p
.2/Kdet.‚Ä†/
 .
K
Y
iD1
i / 1
exp

1
2
.log. K=K/ />
‚Ä† 1
log. K=K/ /

;
where  K D .1; : : : ; K 1/ and log. K=K/ 2 RK 1
with:
≈ílog. K=K/¬çi D log.i =K/ 8i 2 f1; : : : ; K 1g:
60 3. PRIORS
Figure 3.1: A plot of sampled data from the logistic normal distribution with K D 3, with various
‚Ä†. ÓÄÄe hyperparameter  is always .0; 0/. Top-left: ‚Ä† D
1 0
0 1
!
, top-right: ‚Ä† D
1 0:7
0:7 1
!
,
bottom-left: ‚Ä† D
1 0:7
0:7 1
!
, bottom-right: ‚Ä† D
5 0
0 5
!
.
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 61
ÓÄÄis density is only deÔ¨Åned on the probability simplex. Both the moments and the loga-
rithmic moments of the logistic normal distribution are well-deÔ¨Åned for all positive orders. ÓÄÄese
moments are E≈í
QK
iD1 
ai
i ¬ç and E≈í
QK
iD1.log i /ai ¬ç for ai > 0. Unfortunately, even though these
moments exist, there are no closed-form expressions for them.
Since the log-ratio between i and j is distributed according to the normal distribution,
the following holds (Aitchison, 1986):
E≈ílog.i =j /¬ç D i j ;
Cov.log.i =j /; log.k=`// D `k C ji i` kj:
In addition,
E≈íi =j ¬ç D exp

i j C
1
2
ii 2ij C jj


:
Uses of the Logistic Normal Distribution
ÓÄÄe use of the logistic normal (additive and multiplicative) distribution is not as common as the
use of the Dirichlet distribution (in Bayesian NLP). ÓÄÄe main reason for this is that inference with
the logistic normal is cumbersome, even with approximate inference such as the MCMC method
or variational inference (which are discussed in Chapters 6 and 5). More than being cumbersome,
this type of inference is also computationally intensive. Still, this does not preclude the use of the
logistic normal distribution in both the MCMC setting (Mimno et al., 2008) or the variational
setting (Blei and LaÔ¨Äerty, 2006).
A recent example of a use of the (additive) logistic normal distribution for text analysis is
the correlated topic model (CTM) of Blei and LaÔ¨Äerty (2006). Blei and LaÔ¨Äerty present a model
that is identical to the LDA model (see Section 2.2), only a logistic normal distribution is used to
draw topic distribution for each topic, instead of the Dirichlet distribution, as in the LDA model.
ÓÄÄe authors‚Äô main motivation was to model correlation between topics. ÓÄÄey assumed that
given a large corpus of documents, the topics in the corpus are related to each other. For example,
topics such as genetics and computational biology are both under the umbrella of biology, and
when a document is about one, it is more likely to be about the other. On the other hand, astron-
omy is usually weakly related, if at all, to biology, and therefore we expect a correlation close to
0, or even negative, between topics under the umbrella of astronomy and biology.
ÓÄÄe authors compared the LDA and CTM on articles from the journal Science. ÓÄÄeir
Ô¨Åndings were that the CTM achieved a better Ô¨Åt (when measured using average held-out log-
likelihood; see Section 1.6 for more detail) than LDA. In addition, CTM‚Äôs probability peaked
with K D 90 (i.e., 90 topics), while LDA‚Äôs probability peaked with 30 topics. ÓÄÄis implies that
the CTM was able to make better use of available topics for the dataset used.
62 3. PRIORS
ÓÄÄe additive logistic normal distribution was also used as a prior for structured problems,
such as dependency grammar induction. Cohen et al. (2009) explore the use of the logistic normal
distribution as a prior on the dependency model with valence (Klein and Manning, 2004), and
demonstrate signiÔ¨Åcant improvements in model estimation with this prior. Cohen et al. com-
pared it with the use of the Dirichlet distribution and to not using a prior at all. ÓÄÄe Dirichlet
distribution behaved quite similarly to the case of not having a prior at all. ÓÄÄe problem they
modeled, as mentioned above, is that of dependency grammar induction, in which the prediction
target is dependency trees (see Chapter 8) predicted from a sequence of part-of-speech tags.
ÓÄÄis study was further developed by Cohen and Smith (2010b), with suggestions for ex-
tensions of the logistic normal distribution that go beyond the locality of a single multinomial.
See the next section for further discussion.
ÓÄÄe Partitioned Logistic Normal Distribution
ÓÄÄe logistic normal distribution is a distribution over the probability simplex, which corresponds
to a single multinomial. In NLP, the generative models used often consist of a family of multino-
mial distributions. In this case, the parameters  consist of K subvectors, 1
; : : : ; K
, where each
k
is in an Nk-dimensional vector in the probability simplex. See Chapter 8 for a more detailed
explanation.
In these cases, the natural choice of a prior over the whole set of multinomials would be:
p./ D
K
Y
kD1
p.k
/;
where each distribution p.k
/ can be, for example, a Dirichlet or a logistic normal. However,
this decomposition does not introduce a covariance structure between events in diÔ¨Äerent multi-
nomials; there is a clear independence assumption in the prior between multinomials of diÔ¨Äerent
index k.
One way to overcome this issue is by using the partitioned logistic normal distribution
(Aitchison, 1986). ÓÄÄe partitioned logistic normal distribution is similar to the logistic normal
distribution, only it is deÔ¨Åned as a prior on a whole collection of multinomial distributions, such
as 1
; : : : ; K
. To generate such a collection of multinomials, the generative process is given in
generative story 3.1.
ÓÄÄe covariance matrix ‚Ä† now permits correlations between all components of the vector .
ÓÄÄe partitioned logistic normal is related to the shared logistic normal distribution, intro-
duced by Cohen and Smith (2009). Both incorporate a covariance structure that exists outside
of multinomial boundaries, as deÔ¨Åned by the natural factorization of a set of parameters into
multinomials. ÓÄÄe shared logistic normal encodes such covariance more implicitly, by averaging
several Gaussian variables (‚Äúnormal experts‚Äù) that are then exponentiated and normalized. See
also Cohen and Smith (2010b) for discussion.
3.2. PRIORS OVER MULTINOMIAL AND CATEGORICAL DISTRIBUTIONS 63
.
.
Constants: K, Nk for k 2 f1; : : : ; Kg integers
Hyperprameters:  2 R.
PK
kD1 Nk/ K
mean vector, ‚Ä† the corresponding covariance matrix
Target random variables: k
vectors in the probability simplex of dimension Nk 1 for
k 2 f1; : : : ; Kg
Auxiliary random variables: k
2 RNk 1
for k 2 f1; : : : ; Kg
.
‚Ä¢ Generate a multivariate normal variable  2 R
PK
iD1 Nk K
. ÓÄÄe multivariate normal
variable has mean  and covariance matrix ‚Ä† of size .
PK
iD1 Nk K/  .
PK
iD1 Nk
K/.
1:
2:
3:
‚Ä¢ Break  into K subvectors, each of length Nk 1.
4:
‚Ä¢ ÓÄÄe random vector  is set to:
5:
k
i D
exp.k
i /
QNk 1
j D1

1 C exp.k
j /
 8i 2 f1; : : : ; Nk 1g;
k
Nk
D
1
QNk 1
j D1

1 C exp.k
j /
:
Generative Story 3.1: ÓÄÄe generative story for the partitioned logistic normal distribution.
ÓÄÄe Multiplicative Logistic Normal Distribution
Another type of a logistic normal distribution is that of the multiplicative logistic normal. ÓÄÄe
deÔ¨Ånition of the multiplicative logistic normal distribution resembles that of the additive one.
ÓÄÄe diÔ¨Äerence is in Equations 3.13‚Äì3.14, which are substituted with:
i D
exp.i /
QK 1
j D1 1 C exp.j /
 8i 2 f1; : : : ; K 1g;
K D
1
QK 1
j D1 1 C exp.j /
:
ÓÄÄe multiplicative logistic normal distribution is not often used in Bayesian NLP models,
and is given here mostly for completeness.
64 3. PRIORS
ÓÄÄe Logistic Normal Distribution vs. the Dirichlet Distribution
According to Aitchison (1986), the family of logistic normal distributions and the family of
Dirichlet distributions are quite disparate, and it is hard to Ô¨Ånd a distribution in one family that
approximates another distribution in the other family in a useful manner.
Aitchison points out that the value of the minimal KL divergence (Kullback-Leibler
divergence, see Appendix A) between a Dirichlet distribution with hyperparameters Àõ D
.Àõ1; : : : ; ÀõK/ and the family of logistic normal distributions can be approximated as follows.
Whenever Àõi are relatively large values, then the minimal KL divergence is approximately
.1=12/
PK
iD1 Àõ 1
i C 2=.
PK
iD1 Àõi /

.
Dirichlet distributions tend to logistic normality as Àõi goes to inÔ¨Ånity. More speciÔ¨Å-
cally, when  is distributed from a Dirichlet distribution with hyperparameters Àõ, as Àõ ! 1,
it holds that p.jÀõ/ behaves very much like a logistic normal distribution with hyperparameters
 2 RK 1
and ‚Ä† 2 RK 1
 RK 1
such that:
i D .Àõi / .ÀõK/ i 2 f1; : : : ; K 1g
‚Ä†ii D 0
.Àõi / C .ÀõK/ i 2 f1; : : : ; K 1g
‚Ä†ij D 0
.ÀõK/ i ¬§ j; i; j 2 f1; : : : ; K 1g;
where is the digamma function and 0
is its derivative (see Appendix B).
Summary
Despite its lack of conjugacy to the categorical distribution, the logistic normal distribution is a
useful prior for the categorical distribution. While the Dirichlet exhibits an independence struc-
ture in the probability simplex, the logistic normal distribution exploits an explicit dependence
structure originating in a multivariate normal distribution.
ÓÄÄere are two variants of the logistic normal distribution: the additive and the multiplica-
tive. Usually, if a distribution is referred to as being a logistic normal, but no additional reference
is made to the type of logistic normal used, the additive is the one being referred to.
3.2.3 DISCUSSION
As Aitchison (1986) points out, several studies have been conducted in an attempt to generalize
the Dirichlet distribution to a family that has more dependence structure in it (see Section 3.2.1),
and that subsumes the family of Dirichlet distributions. Two such attempts are the scaled Dirich-
let distribution and the Connor-Mosimann distribution.
ÓÄÄe scaled Dirichlet distribution is parametrized by two vectors Àõ and Àá, all positive, of the
same length. Its density is then the following:
3.3. NON-INFORMATIVE PRIORS 65
p.jÀõ; Àá/ D
¬Ä.
Pd
iD1 Àõi /
Qd
iD1 ¬Ä.Àõi /

Qd
iD1 Àá
Àõi
i 
Àõi 1
i
Pd
iD1 Àái i
Pd
iD1 Àõi
;
where ¬Ä.x/ is the Gamma function. ÓÄÄe Connor-Mosimann distribution, on the other hand, has
the following density (it is also parametrized by Àõ; Àá 2 Rd
with all positive values):
p.jÀõ; Àá/ D
d
Y
iD1

Àõi 1
i
B.Àõi ; Àái /
;
where B.Àõi ; Àái /, is the Beta function, deÔ¨Åned as:
B.Àõi ; Àái / D
¬Ä.Àõi /¬Ä.Àái /
¬Ä.Àõi C Àái /
:
Both of these attempts only slightly improve the dependence structure of the Dirichlet
distribution, and as Aitchison points out, the problem of Ô¨Ånding a class of distributions that
generalizes the Dirichlet distribution and enriches it with more dependence structure is still open.
3.2.4 SUMMARY
Multinomial distributions are an important building block in NLP models, especially when con-
sidering generative modeling techniques. Most linguistic structures in NLP can be described in
terms of parts that originate in multinomial distributions. For example, each rule in a phrase-
structure tree, with a probabilistic context-free grammar model, originates in a multinomial dis-
tribution that generates the right-hand sides of rules from a multinomial associated with the
left-hand side nonterminal.
As such, there has been an extensive use of priors over multinomials in Bayesian NLP, most
notably, with the Dirichlet distribution. ÓÄÄe choice of the Dirichlet originates in its conjugacy to
the multinomial distribution, which leads to tractability, but it can also be used to encourage the
model to manifest properties such as sparsity.
ÓÄÄe second multinomial prior family discussed in this section is the family of logistic nor-
mal distributions. Unlike the Dirichlet, the logistic normal family of distributions incorporates
explicit covariance structure between the various parameters of the multinomial distribution. It
has additive and multiplicative versions.
3.3 NON-INFORMATIVE PRIORS
ÓÄÄe priors discussed up to this point are mostly informative. With informative priors, there is an
attempt on the part of the modeler to capture a certain belief about the parameters in the prior,
66 3. PRIORS
and incorporate it into the analysis. For example, with the Dirichlet prior, this belief can be about
the sparsity of the parameter space. With the logistic normal distribution, on the other hand, this
belief can be related to some parameter dependence structure in the parameter space.
It is sometimes the case that there is a preference for a non-informative prior‚Äîi.e., a prior
that does not bias the analysis in any way. ÓÄÄe need for such a type of prior emerges when there
are no deÔ¨Ånite prior beliefs about the parameters.
Using non-informative priors may seem counter-intuitive: in NLP, one of the reasons for
using the Bayesian approach is precisely to bias the predictions toward more plausible structures
through the use of a well-informed prior. If there are no prior beliefs, why should one use the
Bayesian approach to begin with?
ÓÄÄe answer to that question is not straightforward. ÓÄÄis question is the root of the debate
between ‚Äúsubjectivists‚Äù and ‚Äúobjectivists‚Äù‚Äîthose who view Bayesian probability as a degree of
personal belief, and those who believe it should denote an objective rational measure of knowledge,
and as such, should not be inÔ¨Çuenced by subjective prior choice. Non-informative priors Ô¨Åt better
the view of objectivists. Some of the supporters of this view derive Bayesian statistics through a
set of axioms (Cox, 1946, Jaynes, 2003), and believe that this view is better for scientiÔ¨Åc reasoning
than the subjective view. See also Section 1.7.
ÓÄÄe use of non-informative priors often leads to congruence with frequentist methodology,
as discussed in Chapter 4. But still, using a non-informative prior permits inference that explores
the space of parameters more eÔ¨Äectively if the posterior over the parameters is integrated when
making predictions about the structure.
ÓÄÄis point is demonstrated by Goldwater and GriÔ¨Éths (2007). When the authors compared
their HMM models for part-of-speech tagging (one model is a fully Bayesian HMM model
with a uniform prior over the parameters of the transition and the emission matrices, and the
other is a vanilla HMM model that is estimated using maximum likelihood), they discovered
that averaging their predictions uniformly over the space of parameters improved the prediction
of part-of-speech tags.
Still, non-informative priors in Bayesian NLP are most often used as hyperpriors‚Äîthat is,
as hierarchical priors on top of the hyperparameters, and not the parameters of the model. ÓÄÄis
is discussed in Section 3.5.
3.3.1 UNIFORM AND IMPROPER PRIORS
One approach to choosing a non-informative prior p./ over the parameter space ‚Äö follows
intuition: choose a prior that assigns the same density to all  2 ‚Äö.
For example, whenever the set ‚Äö is a subset of Rd
and has a Ô¨Ånite volume v.‚Äö/ (according
to Lebesgue measure), then the uniform prior over ‚Äö is simply the constant distribution p./ D
1=v.‚Äö/. However, forcing a uniform distribution over a set ‚Äö sometimes leads to improper priors.
An improper prior p./ is a ‚Äúprior‚Äù that violates the integrate-to-one requirement from p./:
3.3. NON-INFORMATIVE PRIORS 67
Z

p./d D 1:
For example, there is no uniform distribution on the real line R (or more generally, any
unbounded set in Rd
for d 2 N), simply because for any c > 0, the integral
R 1
1 cd diverges to
inÔ¨Ånity. As a consequence, any attempt to deÔ¨Åne a uniform prior on the real line will lead to an
improper prior.
Even when the prior is improper, it is still technically (or algebraically) possible to use Bayes‚Äô
rule to calculate the posterior,
p.jx/ D
p./p.x j /
R
 p./p.x j /d
;
and get a proper posterior distribution‚Äîif the integral
R
 p./p.x j /d converges. For this
reason, improper priors are sometimes used by Bayesians, as long as the posterior is well-deÔ¨Åned.
When using p./ D c for c > 0, i.e., a uniform (possibly improper) prior, there is an over-
lap between Bayesian statistics and maximum likelihood estimation, which is a purely frequentist
method. ÓÄÄis is discussed in Section 4.2.1.
When a Ô¨Çat uniform prior becomes an improper prior, it is possible to instead use a vague
prior. Such a prior is not improper, but it is also not uniform. Instead, it has a large spread, such
that its tail goes to 0 to avoid divergence of the prior when integrating over the parameter space.
ÓÄÄe tail is usually ‚Äúheavy‚Äù in order to retain a distribution that is as close as possible to uniform.
3.3.2 JEFFREYS PRIOR
One criticism of non-informative priors, as described above, is that they are not invariant to re-
parameterization. ÓÄÄis means that if  is transformed into a new representation for the parameters,
using a one-to-one (perhaps even smooth) mapping, the resulting non-informative prior will have
diÔ¨Äerent properties, and will not remain a uniform prior, for example.
Intuitively, if a prior is not informative about the parameters, the prior should stay consistent
under re-parametrization. ÓÄÄis means that the probability mass assigned to a set of parameters
should remain the same as the probability mass assigned to the set of these parameters after being
re-parametrized. For this reason, statisticians have sought out sets of priors that could still be
considered to be non-informative, but remain invariant to transformations of the parameters.
One example of such a prior is JeÔ¨Äreys prior (JeÔ¨Äreys, 1961).
JeÔ¨Äreys priors are deÔ¨Åned based on the Fisher information in the parameters. In the case of
multivariate parameter vector , coupled with a likelihood function p.xj/, the Fisher informa-
tion i./ is a function of the parameter values, which returns a matrix:
68 3. PRIORS
.i.//ij D E

@2
@i j
log p.xj/
 Àá
Àá
Àá
Àá 

:
When  is univariate, then the Fisher information reduces to be the variance of the score
function, which is the derivative of the log-likelihood function. JeÔ¨Äreys (1961) proposed to deÔ¨Åne
the following prior:
p./ /
p
det.i.//:
Eisenstein et al. (2011) used a JeÔ¨Äreys prior in their Sparse Additive Generative (SAGE)
model on a parameter that serves as a variance value for a draw from the Gaussian distribution.
Several draws from the Gaussian distribution, each with their own variance, are combined with
a ‚Äúbackground distribution‚Äù to produce a distribution over a set of words, representing a topic.
Eisenstein et al. claim that the use of the normal-JeÔ¨Äreys combination (compared to a normal-
exponential combination they also tried) encourages sparsity, and also alleviates the need to choose
a hyperparameter for the prior (the JeÔ¨Äreys prior they use is not parametrized).
Still, the use of JeÔ¨Äreys priors in current Bayesian NLP work is uncommon. It is more
common to use uniform non-informative priors or vague priors using the Gamma distribution.
However, the strong relationship between the Dirichlet and multinomial distributions appears
again in the context of JeÔ¨Äreys priors. ÓÄÄe symmetric Dirichlet with hyperparameter 1=2 (see
Section 3.2.1) is the JeÔ¨Äreys prior for the multinomial distribution.
In cases where a hierarchical prior is of interest (see Section 3.5), and the structure of the
model is Dirichlet-multinomial, one could choose to use a JeÔ¨Äreys prior for the Dirichlet distri-
bution hyperparameters (Yang and Berger, 1998). If the Dirichlet distribution is parametrized by
Àõ1; : : : ; ÀõK > 0, then JeÔ¨Äreys prior for over Àõ is p.Àõ/ /
p
det.i.Àõ1; : : : ; ÀõK// where:
≈íi.Àõ1; : : : ; ÀõK/¬çii D 0
.Àõi / 0
.
K
X
iD1
Àõi / i 2 f1; : : : ; Kg
≈íi.Àõ1; : : : ; ÀõK/¬çij D 0
.
K
X
iD1
Àõi / i ¬§ j I i; j 2 f1; : : : ; Kg:
3.3.3 DISCUSSION
ÓÄÄere is no clear agreement in the statistics community on what it means for a prior to be non-
informative. Some argue that uniform priors are non-informative because they assign an equal
probability to all parameters in the parameter space. However, when they exist, uniform priors
are not invariant to re-parametrization and therefore are not considered to be non-informative
by many statisticians. JeÔ¨Äreys priors, on the other hand, are invariant to re-parametrization, but
they can have a preference bias for certain parts of the parameter space.
3.4. CONJUGACY AND EXPONENTIAL MODELS 69
3.4 CONJUGACY AND EXPONENTIAL MODELS
ÓÄÄe exponential family is an important family of models that is useful in statistics and is highly
common in NLP as well. To begin, denote the parameter space of an exponential model by ‚Äö,
and the sample space by . For deÔ¨Åning a speciÔ¨Åc exponential model, we are required to deÔ¨Åne
the following functions:
‚Ä¢ W ‚Äö ! Rd
for some d
‚Ä¢ tW  ! Rd
(t is also called ‚Äúthe suÔ¨Écient statistics‚Äù)
‚Ä¢ hW  ! .RC
[ f0g/ (also called ‚Äúthe base measure‚Äù)
ÓÄÄese functions deÔ¨Åne the following model:
p.xj/ D h.x/ exp../  t.x/ A.//;
where A./ is used as a normalization constant (and is also called ‚Äúthe log-partition function‚Äù),
deÔ¨Åned as:
A./ D log
X
x
h.x/ exp../  t.x//
!
:
Many well-known distributions fall into this category of the exponential family. For ex-
ample, the categorical distribution for a space  with d events,  D f1; : : : ; dg, and parameters
1; : : : ; d , can be represented as an exponential model with:
i ./ D log.i /; (3.15)
ti .x/ D I.x D i/; (3.16)
h.x/ D 1: (3.17)
Many other distributions fall into that category, such as the Gaussian distribution, the
Dirichlet distribution, the Gamma distribution and others.
An exponential model can be reparametrized, where the new set of parameters is ./,
and  is replaced with the identity function. In this case, we say that the exponential model is in
natural form (with ‚Äúnatural parameters‚Äù). ÓÄÄe rest of the discussion focuses on exponential models
in natural form, such that:
p.xj/ D h.x/ exp.  t.x/ A.//: (3.18)
70 3. PRIORS
ÓÄÄere is a strong relationship between the log-partition function and the mean of the suf-
Ô¨Åcient statistics. More speciÔ¨Åcally, it can be shown that:
@A./
@i
D E≈íti .X/¬ç:
ÓÄÄis fact is used in NLP quite often when we are required to compute the gradient of a log-
linear model to optimize its parameters. In the case of  being a combinatorial discrete space,
such as a set of parse trees or labeled sequences, dynamic programming algorithms can be used
to compute these expectations. See Chapter 8 for more detail.
Since we discuss the Bayesian setting, it is natural to inquire about what a conjugate prior is
for an exponential model. A conjugate prior for the model in Equation 3.18 is also an exponential
model of the following general form:
p.j0; / D f .0; / exp.>
 0A.//;
where  2 Rd
, 0 2 R and f W Rd
! .RC
[ f0g/. ÓÄÄis general result can be used to prove many
of the conjugacy relationships between pairs of well-known distributions. (See the exercises at the
end of this chapter.) ÓÄÄere is also a strong relationship between exponential models in their natural
form and log-linear models. More information about log-linear models is found in Section 4.2.1.
3.5 MULTIPLE PARAMETER DRAWS IN MODELS
Consider the basic Bayesian model, in which parameters are drawn from a prior p./ and then the
data is drawn from a distribution p.Xj/. Here, the data is abstractly represented using a random
variable X, but it is often the case that the observed data is composed of multiple observations
x.1/
; : : : ; x.n/
, all drawn from the distribution p.Xj/.
In this case, one way to write the joint distribution over the data and parameters is:
p.; x.1/
; : : : ; x.n/
/ D p./
n
Y
iD1
p.x.i/
j/:
A more careful look into this equation reveals that there is an additional degree of freedom
in the placement of the prior. Instead of drawing the parameters once for all data points, one
could draw a set of parameters for each data point. In such a case, the joint distribution is:
p..1/
; : : : ; .n/
; x.1/
; : : : ; x.n/
/ D
n
Y
iD1
p..i/
/p.x.i/
j.i/
/:
ÓÄÄis type of model is also called a compound sampling model. With this approach to prior
modeling, the distribution
3.5. MULTIPLE PARAMETER DRAWS IN MODELS 71
(a) (b)
.
.
. .
.
Àõ .
. .
.
 .
.
.
X.1/ .
.
X.2/ .
.
X.3/ .
.
. .
.
Àõ .
.
.
.1/ .
.
.2/ .
.
.3/
.
.
X.1/ .
.
X.2/ .
.
X.3/
Figure3.2: A graphical depiction of the two levels at which a prior can be placed for a model with three
observations. (a) Parameters for all of the observations are being drawn once; (b) multiple parameters
are re-drawn for each observation, in an empirical Bayes style. Shaded nodes are observed.
p..1/
; : : : ; .n/
/ D
n
Y
iD1
p..i/
/ (3.19)
can be considered to be a single prior over the joint set of parameters ..1/
; : : : ; .n/
/. ÓÄÄese two
approaches are graphically depicted in Figure 3.2.
Conceptually, both approaches to prior placement have advantages and disadvantages when
modeling natural language. Drawing the parameters for each observation permits more Ô¨Çexibility
across the observations (or the predicted structures, in the case of latent variable models), allowing
the model to capture variation across the corpus, that arises, for example, because of diÔ¨Äerence in
authors or genres. Generating the parameter at the top level (only once) suggests that inference
needs to be done in a smaller space: there is a need to Ô¨Ånd the posterior over a single set of
parameters. ÓÄÄis reduces the complexity of the model.
When parameters are drawn separately for each datum (i.e., we use the prior in Equa-
tion 3.19), it is often useful to assume some kind of dependencies between these parameters. We
are also interested in inferring the hyperparameters, in order to infer dependencies between the
parameters. ÓÄÄere are two dominating approaches to conducting this type of inference. Both ap-
proaches assume that the prior p./ from which .i/
are drawn is controlled by hyperparameters,
so that we have p.jÀõ/ for some Àõ.
ÓÄÄe Ô¨Årst approach is called empirical Bayes (Berger, 1985). Empirical Bayes here means
that the hyperparameters are estimated as well, usually by using a maximum likelihood criterion.
For more information about emprical Bayes, see Section 4.3.
72 3. PRIORS
ÓÄÄe second approach is hierarchical Bayesian modeling. Hierarchical Bayesian models are
models in which the hyperparameters (which parametrize the prior) themselves are associated
with a (hyper)prior as well. A hierarchical Bayesian model would add an additional level of priors,
usually parametric (parametrized by  2 ∆í), p.Àõ j / such that the joint distribution is:
p.Àõ; .1/
; : : : ; .n/
; x.1/
; : : : ; x.n/
j/ D p.Àõ j /
n
Y
iD1
p..i/
jÀõ/p.x.i/
j/:
ÓÄÄe choice of a second stage prior (or hyperprior) has a less noticeable eÔ¨Äect than the choice
of a Ô¨Årst stage prior on the predictions the model makes. ÓÄÄerefore, vague priors and priors that
are mathematically convenient are more common as priors over hyperparameters, even though
they might not be the best Ô¨Åt for our beliefs about the model.
Unfortunately, many Bayesian NLP papers do not make it explicit in their model descrip-
tion whether parameters are drawn for each example or whether there is a single set of parameters
for all observations and latent structures. ÓÄÄis ambiguity mostly arises because typically, the gen-
erative process of a Bayesian NLP model is described for a single observation (or latent structure).
ÓÄÄe ‚Äúloop‚Äù over all observations is not made explicit in the description of the model.
With hidden Markov models, for example (Chapter 8), with a single draw of the parameters
for the HMM, there is an implicit treatment in many papers of the issue with multiple sequences
as datapoints. One can assume that all sequences in the data are concatenated together into a
single sequence, with a separator symbol between them. ÓÄÄen, we can proceed with inference for
this single sequence. ÓÄÄis leads to an equivalent scenario as having multiple sequences, where the
probabilities for transitioning from the separator symbol to any other symbol are treated as initial
probabilities.
As a general rule, readers of Bayesian NLP papers should usually assume that there is a
single set of parameters drawn for all observations unless the paper is set in the empirical Bayesian
setting or in a hierarchical Bayesian setting. ÓÄÄis general rule is overridden by other clues about
the placement of the prior, such as the derivation of the inference algorithm or other Bayesian
baselines that the method in the paper is compared against.
ÓÄÄis book tries to be as explicit as possible about the placement of the prior. ÓÄÄe prior is
abstractly deÔ¨Åned at the top level, but the reader should consider that in many cases,  actually
represents multiple draws of parameters, with the prior deÔ¨Åned in Equation 3.19.
Both hierarchical Bayesian modeling and empirical Bayes can be used in the case of a single
parameter draw for all parameters. However, they are most often used in NLP in the context of
multiple parameter draws, especially in the empirical Bayes setting.
3.6 STRUCTURAL PRIORS
A large body of work in Bayesian NLP focuses on cases in which the model structure is Ô¨Åxed.
Model structure here is not rigorously deÔ¨Åned, but it refers to the representation that makes the
3.7. CONCLUSION AND SUMMARY 73
core underlying independence assumptions in the model. It can be, for example, a context-free
grammar or a directed graphical model. For this structure to be functional in the context of data,
it is associated with parameters for its various components. PCFGs, for example, associated each
rule with a rule probability.
Since we usually assume the structure is Ô¨Åxed, the priors in Bayesian NLP are deÔ¨Åned
over the parameters of this structure. Setting a prior over the model structure is more complex.
One implicit way to do it is to have the structure make very few independence assumptions, and
incorporate many components that are potentially not used for a given Ô¨Åxed set of parameters
(such as all possible right-hand sides for a context-free grammar). ÓÄÄen, choosing a sparse prior
for this model (for example, a symmetric Dirichlet with a small concentration hyperparameter),
will essentially lead to selecting a subset of these components from the model.
ÓÄÄere are several more explicit examples of the use of structural priors in NLP. Eisner
(2002), for example, deÔ¨Ånes a structural ‚Äútransformational prior‚Äù over the right-hand sides of
context-free rules. Eisner‚Äôs goal was to introduce rules into the grammar that were never seen in
the training data. ÓÄÄe introduction of rules is done by introducing local edit operations to existing
rules. ÓÄÄe full set of rules is represented as a graph, where the nodes correspond to the rules, and
edges correspond to possible transitions from one rule to another using a local edit operation
(removing a nonterminal in the right-hand side, adding one or replacing it). ÓÄÄe edges in this
graph are weighted, and determine the probability of the rule in the Ô¨Ånal grammar.
Other cases for setting structural priors in a Bayesian setting include that of Stolcke
and Omohundro (1994). Stolcke was interested in learning the structure of a grammar using
a ‚ÄúBayesian merging model.‚Äù See Section 8.9 for more information.
3.7 CONCLUSION AND SUMMARY
ÓÄÄe prior distribution is the basic mechanism in Bayesian statistics used to help manage uncer-
tainty in the parameters. ÓÄÄe prior distribution has to be chosen with two trade-oÔ¨Äs in mind: ÓÄÄe
prior‚Äôs expressivity and ability to capture properties in the parameter space, and its tractability for
posterior inference.
Conjugate priors are one type of prior that focuses more on the second aspect of this trade-
oÔ¨Ä. Since Bayesian NLP models are usually based on categorical distributions, the use of the
Dirichlet distribution, which is a conjugate prior to the categorical distribution, is prevalent in
Bayesian NLP.
ÓÄÄe Dirichlet distribution has some limitations, and other priors over multinomials, such
as the logistic normal distribution, can be used instead of the Dirichlet. Such priors are harder to
tackle computationally, but this challenge can be overcome using approximate inference methods.
It is often the case in Bayesian NLP that hierarchical priors are used, and uncertainty about
the hyperparameters is actually managed using another prior. ÓÄÄis type of prior (also called hy-
perprior) is less susceptible to performance degradation as a result of an unsuitable choice of prior
74 3. PRIORS
family. ÓÄÄis is due to hyperprior‚Äôs location in the hierarchy of the model, which is farther from
the parameters than the direct prior over the parameters.
ÓÄÄere are two approaches in Bayesian NLP for placing the prior. ÓÄÄe Ô¨Årst option is to
put the prior at the top level, with parameters being drawn once for all observations. A second
option is for parameters to be drawn separately for each of the observations (or latent structures).
ÓÄÄis approach is often associated with hierarchical priors and empirical Bayes estimation (see
Section 4.3).
Priors are an important part of the mechanics behind Bayesian inference. More abstractly,
they should be viewed as a way to model the prior beliefs of the modeler about the various hy-
potheses (or parameters, for parametric models).
An unexplored territory that could be of special interest to Bayesian NLP is that of prior elic-
itation. With prior elicitation, priors are constructed based on expert knowledge (in the language
domain, such expert could be a linguist). In some areas of research, the expert would simply have
to pick a hyperparameter setting for the prior family. In NLP, such prior elicitation can perhaps
be used to incorporate linguistic information using a clear set of principles.
3.8. EXERCISES 75
3.8 EXERCISES
3.1. Following Section 3.1.4, show that if a distribution is conjugate to another, the relation-
ship is maintained when the Ô¨Årst distribution is renormalized to a subset of the parameter
space.
3.2. Verify that the multinomial or categorical distribution can be represented as an exponen-
tial model (i.e., show that Equations 3.15‚Äì3.17 are correct).
3.3. Show that the Dirichlet distribution can be represented as an exponential model, and use
Section 3.4 to show the Dirichlet-multinomial conjugacy.
3.4. Alice designed a model that is parametrized by a single integer parameter, p.Xj/ where
 2 N. She is now interested in deÔ¨Åning a prior p./. Alice does not have much in-
formation about the parameter or prior knowledge about the problem, so she wants to
deÔ¨Åne a uniform prior over N for . Show that any such prior is going to be improper.
3.5. Alice, from the previous question, is not happy with her prior being improper. She learned
more about the problem she is modeling, and now believes that  should be centered
around 0, and drop exponentially as jj increases. Can you suggest a proper prior for ?
(It should be hyperparametrized by a single parameter Àõ that decides on the rate of the
exponential decay.)
C H A P T E R 4
Bayesian Estimation
ÓÄÄe main goal of Bayesian inference is to derive (from data) a posterior distribution over the
latent variables in the model, most notably the parameters of the model. ÓÄÄis posterior can be
subsequently used to probabilistically infer the range of parameters (through Bayesian interval
estimates, in which we make predictive statements such as ‚Äúthe parameter  is in the interval
≈í0:5; 0:56¬ç with probability 0.95‚Äù), compute the parameters‚Äô mean or mode, or compute other
expectations over quantities of interest. All of these are ways to summarize the posterior, instead
of retaining the posterior in its fullest form as a distribution, as described in the previous two
chapters.
In the traditional use of Bayesian statistics, this posterior summarization is done in order
to generate interpretable conclusions about the nature of the problem or data at hand. DiÔ¨Äering
from the traditional use of Bayesian statistics, natural language processing is not usually focused
on summarizing the posterior for this kind of interpretation, but is instead focused on improving
the predictive power of the model for unseen data points. Examples for such predictions are the
syntactic tree of a sentence, the alignment of two sentences or the morphological segmentation
of a word.¬π
ÓÄÄe most basic way to summarize the posterior for the use of an NLP problem is to compute
a point estimate from the posterior. ÓÄÄis means that we identify a single point in the parameter
space (and therefore, a distribution from the model family) to be used for further predictions.
At Ô¨Årst it may seem that such an approach misses the point behind Bayesian inference, which
aims to manage uncertainty about the parameters of the model using full distributions; however,
posterior summarization, such as the posterior mean, often integrates over many values of the pa-
rameters, and therefore, future predictions using this posterior summary rely heavily on the prior
(the posterior takes into account the prior), especially when small amounts of data are available.
Posterior summarization is usually contrasted with the ‚Äúfully Bayesian‚Äù approach, in which
predictions use the full posterior for any prediction. An example of a fully Bayesian approach is the
integration of the posterior over the parameters against the likelihood function to Ô¨Ånd the highest
scoring structure, averaging over all possible parameters. To understand the diÔ¨Äerence between
this fully Bayesian approach and identifying a point estimate, see Section 4.1. While the Bayesian
approach is more ‚Äúcorrect‚Äù probabilistically, from a Bayesian point of view, it is often intractable
¬πÓÄÄere are cases in which the actual values of the parameters are of interest in NLP problems. ÓÄÄe parameters can be used
during the development phase, to determine which features in a model assist the most in improving its predictive power. ÓÄÄe
actual values can also be used to interpret the model and understand the patterns that it has learned. ÓÄÄis information can be
used iteratively to improve the expressive power of the model.
78 4. BAYESIAN ESTIMATION
to follow. On the other hand, posterior summarization, like frequentist point estimates, leads to
lightweight models that can be easily used in future predictions.
ÓÄÄis chapter includes a discussion of several ways in which the posterior can be summa-
rized, and also relates some of these approaches to frequentist estimation. ÓÄÄe chapter includes
two main parts: the Ô¨Årst part appears in Section 4.2, and details the core ways to summarize a
posterior in Bayesian NLP; the second part appears in Section 4.3, and explains the empirical
Bayes approach, in which point estimates are obtained for the hyperparameters, which can often
be used as a substitute for a point estimate for the parameters themselves.
ÓÄÄe techniques described in this chapter stand in contrast to the techniques described in
Chapter 5 (sampling methods) and Chapter 6 (variational inference). ÓÄÄe techniques in these
latter chapters describe ways to fully identify the posterior, or at least identify a means to draw
samples from it. While the techniques in these chapters are often used for fully-Bayesian infer-
ence, they can also be used to identify the posterior and then summarize it using the approaches
described in this chapter.
4.1 LEARNING WITH LATENT VARIABLES: TWO VIEWS
In the unsupervised setting, in which only ‚Äúinput‚Äù examples are available (for the training process),
without examples of the ‚Äúoutput,‚Äù one usually makes a distinction made between two approaches
to learning.
In the Ô¨Årst approach, inference is done on all of the observed data available. ÓÄÄis includes
the data on which we want our Ô¨Ånal predictions to be made. In the second approach, one adheres
to the machine learning tradition of splitting the training data into a training set (and potentially
a development set) and a test set; next, one estimates the model parameters using the training set,
performs some Ô¨Åne tuning using the development set, and Ô¨Ånally decodes unseen examples in the
test set in order to evaluate the predictive power of the model.
ÓÄÄere are examples of following both of these approaches in Bayesian NLP. Naturally, the
Ô¨Årst approach is appropriate only in an unsupervised setting, and not in a supervised case. It is
therefore not surprising that the Ô¨Årst approach is common in Bayesian NLP‚Äîas discussed in
Chapter 2, Bayesian NLP focuses to a large degree on unsupervised learning from data.
ÓÄÄe fully Bayesian approach can be used with a separate training and test set, but this
separation is more common in Bayesian NLP when using Bayesian point estimation. ÓÄÄe fully
Bayesian approach usually requires computationally heavy approximate inference algorithms. ÓÄÄis
setting is not always a good Ô¨Åt for NLP, as NLP systems often require lightweight models with
fast inference on unseen data points. ÓÄÄis is especially true for NLP systems which are deployed
for commercial or other large-scale use. In these circumstances, Bayesian point estimation can
function as a balance between the Bayesian approach and the need for lightweight models.
As mentioned above, Bayesian point estimation is not the only option for combining
Bayesian inference with the train-test traditional machine learning methodology. If the train-
ing set in an unsupervised problem includes the instances x.1/
; : : : ; x.n/
and we are interested in
4.2. BAYESIAN POINT ESTIMATION 79
decoding a new example x0
with a structure z0
, we can infer the full posterior over the parameters
using the training instances:
p.jx.1/
; : : : ; x.n/
/ D
X
z.1/;:::;z.n/
p.; z.1/
; : : : ; z.n/
jx.1/
; : : : ; x.n/
/;
and then proceed by decoding from the approximate posterior:
p.z0
j x.1/
; : : : ; x.n/
; x0
/ 
Z

p.z0
j; x0
/p.jx.1/
; : : : ; x.n/
/d: (4.1)
ÓÄÄis approximation is especially accurate when n is large. In order for the posterior to be
exact, we would need to condition the parameter distribution on x0
as well, i.e.:
p.z0
j x.1/
; : : : ; x.n/
; x0
/ D
Z

p.z0
j ; x0
/p. j x.1/
; : : : ; x.n/
; x0
/d: (4.2)
But with a large n the eÔ¨Äect of x0
on the posterior is negligible, in which case Equation 4.1
describes a good approximation for the right hand-side of Equation 4.2. ÓÄÄe derivation of Equa-
tion 4.2 is a direct result of the independence between input (and output) instances conditioned
on the set of parameters .
Integrating the likelihood in this manner against the posterior is quite complex and com-
putationally ineÔ¨Écient. For this reason, we often resort to approximation methods when inferring
this posterior (such as sampling or variational inference), or alternatively, use Bayesian point es-
timation.
4.2 BAYESIAN POINT ESTIMATION
ÓÄÄe focus of Bayesian point estimation is on summarizing the posterior over the parameters into
a Ô¨Åxed set of parameters. ÓÄÄis kind of estimation often has a strong relationship to frequentist
approaches to estimation, such as maximum likelihood estimation or regularized maximum like-
lihood estimation. ÓÄÄis relationship is most evident in the case of deriving a Bayesian estimate
using Bayesian maximum a posterior (MAP) estimation, as described in the next section.
4.2.1 MAXIMUM A POSTERIORI ESTIMATION
ÓÄÄis section begins by considering MAP estimation in the complete data setting (i.e., in the
supervised setting). Let p./ be some prior for a model with likelihood p.Xj/. ÓÄÄe generative
process is such that  is drawn from the prior, and then x.1/
; : : : ; x.n/
are drawn independently
(given ) with probability p.x.i/
j /. ÓÄÄe random variables that correspond to these observations
80 4. BAYESIAN ESTIMATION
are X.1/
; : : : ; X.n/
. ÓÄÄe MAP estimation chooses the point estimate 
which is the mode of the
posterior:

D arg max

p. j x.1/
; : : : ; x.n/
/ D arg max

p./p.x.1/
; : : : ; x.n/
j /
p.x.1/; : : : ; x.n//
:
ÓÄÄe motivation behind MAP estimation is simple and intuitive: choose the set of param-
eters that are most likely according to the posterior, which takes into account both the prior and
the observed data. Note that p.x.1/
; : : : ; x.n/
/ does not depend on  which is maximized over,
and therefore:

D arg max

p./p.x.1/
; : : : ; x.n/
j /:
In addition, since X.i/
for i 2 f1; : : : ; ng are independent given  and since the log function
is monotone, the MAP estimator corresponds to:

D arg max

log p./ C L./; (4.3)
where
L./ D
n
X
iD1
log p.x.i/
j /:
If p./ is constant (for example, when p./ denotes a uniform distribution over the prob-
ability simplex, a symmetric Dirichlet with hyperparameter 1), then Equation 4.3 recovers the
maximum likelihood (ML) solution. ÓÄÄe function L./ equals the log-likelihood function used
with ML estimation. A uniform prior p./ is considered to be a non-informative prior, and is
discussed more in Section 3.3.
More generally, the term log p./ serves as a penalty term in the objective in Equation 4.3.
ÓÄÄis penalty term makes the objective smaller when  is highly unlikely according to the prior.
Relationship to Minimum Description Length
Minimum Description Length (MDL) refers to an idea in machine learning that is based on
the old principle of Occam‚Äôs Razor. ÓÄÄe MDL principle suggests that a hypothesis (or a set of
parameters, in our case) should be chosen so that it encodes the observed data in the most succinct
manner. ÓÄÄis succinct representation should naturally encode the hypothesis itself.
MAP estimation exactly follows the framework of MDL for probabilistic encoding. If we
negate log terms for the prior and the likelihood in Equation 4.3, the optimization problem turns
4.2. BAYESIAN POINT ESTIMATION 81
from a maximization to a minimization problem. ÓÄÄe term log p./ denotes the number of
natbits it takes to encode the hypothesis according to a certain code. ÓÄÄis code is optimal for
encoding hypotheses (i.e., it minimizes the expected code length) that distribute according to
p./.
Similarly, the log-likelihood term denotes the number of natbits it takes to encode the data,
based on the hypothesis  it conditions on, using a code that minimizes the expected number of
natbits it takes to encode data distributed according to the likelihood for this .
ÓÄÄese two natbit-quantiÔ¨Åed values together compose a measure for the number of natbits
required to represent the hypothesis learned from the data. ÓÄÄey combine our prior beliefs and
our observations from data. We then minimize these values together following principles such as
Occam‚Äôs razor.
When log2 is used instead of the natural logarithm, the unit of measurement changes from
natbits to bits. ÓÄÄe choice of a base for the logarithm does not change the MAP solution.
ÓÄÄe Dirichlet and Additive Smoothing
Sparse counts need to be carefully handled in language data. For example, it is well-known that
distributions over words, or more generally, over n-grams, follow a ZipÔ¨Åan distribution; this
means that there is a heavy tail of rare n-grams, and most of the probability mass is concen-
trated on a relatively small set of n-grams. Since the tail is quite heavy, and includes most of the
word types in the language, it cannot be ignored. Yet, it is hard to estimate the probabilities of
each element in this tail, because single elements from this tail do not occur often in corpora.
Assigning non-zero probability only to n-grams that actually occur in the text that we use to esti-
mate the probabilities can lead to zero probabilities for n-grams that appear in a held-out dataset.
ÓÄÄis can be quite detrimental to any model, as it makes the model brittle and not robust to noise.
To demonstrate this, consider that if any model assigns zero probability to even a single unseen
n-gram that occurs when testing the model, the log-likelihood of the whole data diverges to neg-
ative inÔ¨Ånity.¬≤ More generally, the variance of na√Øve estimates for n-grams that occur infrequently
is very large. We turn now to describe a connection between approaches to solve this sparsity issue
and the Dirichlet distribution.
Consider the case in which  is drawn from a symmetric Dirichlet, with hyperparameter
Àõ > 0. In addition, x.1/
; : : : ; x.n/
are drawn from the multinomial distribution  (each x.i/
2
f0; 1gK
is such that
PK
j D1 x.i/
j D 1 for every i). In Section 3.2.1, we showed that the posterior
p. j x.1/
; : : : ; x.n/
; Àõ/ is also a Dirichlet, with hyperparameters .Àõ; Àõ; : : : ; Àõ/ C
Pn
iD1 x.i/
.
ÓÄÄe density of the Dirichlet distribution has a single maximum point when all hyperpa-
rameters are larger than 1. In this case, if 
is the posterior maximizer, then:
¬≤For this reason, certain language modeling toolkits, such as the SRI language modeling toolkit (Stolcke, 2002), have the option
to ignore unseen words when computing perplexity on unseen data.
82 4. BAYESIAN ESTIMATION

j D
.Àõ 1/ C
Pn
iD1 x.i/
j
K.Àõ 1/ C
PK
j D1
Pn
iD1 x.i/
j
: (4.4)
When Àõ D 1, the Àõ 1 terms in the numerator and the denominator disappear, and we
recover the maximum likelihood estimate‚Äîthe estimate for 
is just composed of the relative
frequency of each event. Indeed, when Àõ D 1, the prior p.jÀõ/ with the Dirichlet is just a uniform
non-informative prior, and therefore we recover the MLE (see previous section and Equation 4.3).
When Àõ > 1, the MAP estimation in Equation 4.4 with Dirichlet-multinomial corre-
sponds to a smoothed maximum likelihood estimation. A pseudo-count, Àõ 1, is added to each ob-
servation. ÓÄÄis type of smoothing, also called additive smoothing, or Laplace-Lidstone smooth-
ing, has been regularly used in data-driven NLP since its early days, because it helps to alleviate
the problem of sparse counts in language data. (With Àõ < 1, there is a discounting eÔ¨Äect, because
Àõ 1 < 0.)
Additive smoothing is especially compelling because it is easy to implement. ÓÄÄe inter-
pretation as a MAP solution has its added value, but it is not the origin of additive smoothing.
Indeed, additive smoothing has been used for n-gram models in the NLP community since the
late 80s, without necessarily referring to its Bayesian interpretation.
Chen and Goodman (1996) describe a thorough investigation of smoothing techniques
for language modeling, and compare additive smoothing to other smoothing techniques. ÓÄÄeir
Ô¨Åndings were that additive smoothing is far from being the optimal solution for an accurate es-
timation of n-gram models. Katz smoothing (Katz, 1987) and interpolation with lower order
estimation of n-gram language models (Jelinek and Mercer, 1980) performed considerably better
on a held-out data set (the reported performance measure is cross-entropy; see Appendix A). Not
surprisingly, smoothing the counts by adding 1 to all of them (as has been argued to be a ‚Äúmorally
correct‚Äù choice by Lidstone (1920) and JeÔ¨Äreys (1961)) did not perform as well as smoothing by
varying the pseudo-counts added to the n-gram counts.¬≥
In spite of its lack of optimality, additive smoothing still remains a basic tool in NLP which
is often tried out after vanilla maximum likelihood estimation. ÓÄÄis is probably due to additive
smoothing‚Äôs eÔ¨Éciency and straightforward implementation as an extension of maximum likeli-
hood estimation. However, it is often the case that in order to achieve state-of-the-art perfor-
mance with maximum likelihood estimation, a more complex smoothing scheme is required, such
as interpolation or incorporation of lower order models.
MAP Estimation and Regularization
ÓÄÄere is a strong connection between maximum a posteriori estimation with certain Bayesian priors
and a frequentist type of regularization, in which an objective function, such as the log-likelihood,
¬≥Modern language models more often use smoothing techniques such as the one by Kneser and Ney (1995), for which a
Bayesian interpretation was discovered relatively recently (under a nonparametric model). See Chapter 7.4.1.
4.2. BAYESIAN POINT ESTIMATION 83
is augmented with a regularization term to avoid overÔ¨Åtting. We now describe this connection
with log-linear models.
Log-linear models are a common type of model for supervised problems in NLP. In the
generative case, a model is deÔ¨Åned on pairs .x; z/ where x is the input to the decoding problem
and z is the structure to be predicted. ÓÄÄe model form is the following:
p.X; Zj/ D
exp.
PK
j D1 j fj .X; Z//
A./
;
where f .x; z/ D .f1.x; z/; : : : ; fK.x; z// is a feature vector that extracts information about the
pair .x; z/ in order to decide on its probability according to the model.
Each function fi .x; z/ maps .x; z/ to R, and is often just a binary function, taking values
in f0; 1g (to indicate the existence or absence of a sub-structure in x and z), or integrals, taking
values in N (to count the number of times a certain sub-structure appears in x and z).
ÓÄÄe function A./ is the partition function, deÔ¨Åned in order to normalize the distribution:
A./ D
X
x
A.; x/; (4.5)
where
A.; x/ D
X
z
exp.
K
X
j D1
j fj .x; z//:
Alternatively, in a discriminative setting, only the predicted structures are modeled, and the
log-linear model is deÔ¨Åned as a conditional model:
p.ZjX; / D
exp.
PK
j D1 j fj .X; Z//
A.; x/
:
In this case, A./ in Equation 4.5 is not needed. ÓÄÄis is important, because A./ is often
intractable to compute because of the summation over all possible x, is not needed. On the other
hand, the function A.; x/ is often tractable to compute for a speciÔ¨Åc x, using algorithms such as
dynamic programming algorithms (Chapter 8).
In both of these cases, the classical approach in NLP for estimating the parameters  is to
maximize a log-likelihood objective with respect to . In the generative case, the objective is:
n
X
iD1
log p.x.i/
; z.i/
j /;
84 4. BAYESIAN ESTIMATION
and in the discriminative case it is:‚Å¥
n
X
iD1
log p.z.i/
j x.i/
; /:
A na√Øve maximization of the likelihood often leads to overÔ¨Åtting the model to the training
data. ÓÄÄe parameter values are not constrained or penalized for being too large; therefore, the log-
likelihood function tends to Ô¨Åt the parameters in such a way that even patterns in the data that are
due to noise or do not represent a general case are taken into account. ÓÄÄis makes the model not
generalize as well to unseen data. With certain low-frequency features that are associated with a
single output, the feature weights might even diverge to inÔ¨Ånity.
Regularization is one solution to alleviate this problem. With L2-regularization,‚Åµ for ex-
ample, the new objective function being optimized is (in the generative case):
n
X
iD1
log p.x.i/
; z.i/
j / C R./; (4.6)
where
R./ D
1
22
0
@
K
X
j D1
2
j
1
A ;
for some Ô¨Åxed  2 R. ÓÄÄe regularized discriminative objective function is deÔ¨Åned analogously,
replacing the log-likelihood with the conditional log-likelihood.
ÓÄÄe intuition behind this kind of regularization is simple. When the parameters become
too large in the objective function (which happens when the objective function also Ô¨Åts the noise
in the training data, leading to overÔ¨Åtting), the regularization term (in absolute value) becomes
large and makes the entire objective much smaller. ÓÄÄerefore, depending on the value of , the
regularization term R./ encourages solutions in which the features are closer to 0.
Even though this type of regularization is based on a frequentist approach to the problem
of estimation, there is a connection between this regularization and Bayesian analysis. When
exponentiating the regularization term and multiplying by a constant (which does not depend on
), this regularization term becomes the value of the density function of the multivariate normal
distribution, deÔ¨Åned over , with zero mean and a diagonal covariance matrix with 2
on the
diagonal.
ÓÄÄis means that maximizing Equation 4.6 corresponds to maximizing:
‚Å¥Modern machine learning makes use of other discriminative learning algorithms for learning linear models similar to that,
most notably max-margin algorithms and the perceptron algorithm.
‚ÅµÓÄÄe L2 norm of a vector x 2 Rd is its Euclidean length:
qPd
iD1 x2
i .
4.2. BAYESIAN POINT ESTIMATION 85
n
X
iD1
log p.x.i/
; z.i/
j / C log p. j 2
/; (4.7)
with p.j2
/ being a multivariate normal prior over the parameters . ÓÄÄe mean value of this
multivariate normal distribution is 0, and its covariance is 2
IKK. Equation 4.7 has exactly
the same structure as in Equation 4.3. ÓÄÄerefore, the L2-regularization corresponds to a MAP
estimation with a Gaussian prior over the parameters.
ÓÄÄere are other alternatives to L2 regularization. Consider, for example, the following prior
on :
p. j / D
K
Y
j D1
p.j j /
p.j j / D
1
2
exp

jj j


: (4.8)
ÓÄÄe distribution over each j in Equation 4.8 is also called the Laplace distribution (its
mean is 0 and its variance is 22
; see Appendix B). ÓÄÄe prior p.j/ coupled with MAP estimation
leads to a maximization problem of the form (ignoring constants):
n
X
iD1
log p.x.i/
; z.i/
j /
1

0
@
K
X
j D1
jj j
1
A :
ÓÄÄis type of regularization is also called L1 regularization, and it is known to encourage
sparse estimates for ‚Äîi.e.,  in which many of the coordinates are exactly 0 (Bishop, 2006). ÓÄÄe
L1 norm is actually used as a relaxation for a regularization term Supp./ which is deÔ¨Åned as:
Supp./ D jfi j i ¬§ 0gj:
Minimizing the support directly in conjunction with the log-likelihood is intractable, so
relaxation such as L1 is used.
MAP Estimation with Latent Variables
When latent variables are introduced to the estimation problem (such as in the case of unsuper-
vised learning), MAP estimation can become much more cumbersome. Assume a joint distribu-
tion that factorizes as follows:
86 4. BAYESIAN ESTIMATION
p.x.1/
; : : : ; x.n/
; z.1/
; : : : ; z.n/
;  j Àõ/ D p.jÀõ/
n
Y
iD1
p.z.i/
j ; Àõ/p.x.i/
j z.i/
; ; Àõ/:
ÓÄÄe latent structures are denoted by the random variables Z.i/
, and the observations are
denoted by the random variables X.i/
. ÓÄÄe posterior has the form:
p.; z.1/
; : : : ; z.n/
j x.1/
; : : : ; x.n/
; Àõ/:
ÓÄÄe most comprehensive way to get a point estimate from this posterior through MAP
estimation is to marginalize Z.i/
and then Ô¨Ånd 
as following:

D arg max

X
z.1/;:::;z.n/
p.; z.1/
; : : : ; z.n/
j x.1/
; : : : ; x.n/
; Àõ/: (4.9)
However, such an estimate often does not have an analytic form, and even computing it nu-
merically can be ineÔ¨Écient. One possible way to avoid this challenge is to change the optimization
problem in Equation 4.9 to:

D arg max

max
z.1/;:::;z.n/
p.; z.1/
; : : : ; z.n/
j x.1/
; : : : ; x.n/
; Àõ/: (4.10)
ÓÄÄis optimization problem, which identiÔ¨Åes the mode of the posterior both with respect
to the parameters and the latent variables, is often more manageable to solve. For example, sim-
ulation methods such as MCMC algorithms can be used together with simulated annealing to
Ô¨Ånd the mode of this posterior. ÓÄÄe idea behind simulated annealing is to draw samples, through
MCMC inference, from the posterior after a transformation so that it puts most of its probability
mass on its mode. ÓÄÄe transformation is gradual, and determined by a ‚Äútemperature schedule,‚Äù
which slowly decreases a temperature parameter. ÓÄÄat particular temperature parameter deter-
mines how peaked the distribution is‚Äîthe lower the temperature is, the more peaked the distri-
bution. Simulated annealing is discussed in detail in Section 5.6. ÓÄÄe replacement of the marginal
maximization problem (Equation 4.9) with the optimization problem in Equation 4.10 is a sig-
niÔ¨Åcant approximation. It tends to work best when the posterior has a peaked form‚Äîi.e., most
of the probability mass of the posterior is concentrated on a few elements in the set of possible
structures to predict.
Another approximation to the optimization problem in Equation 4.9 can be based on vari-
ational approximations. In this case, the posterior is approximated using a distribution q, which
often has a factorized form:
p.; z.1/
; : : : ; z.n/
/  q./ 
n
Y
iD1
q.z.i/
/
!
:
4.2. BAYESIAN POINT ESTIMATION 87
ÓÄÄe distribution q./ is also assumed to have a parametric form, and identifying each con-
stituent of the distribution q (for each predicted structure and the parameters) is done iteratively
using approximate inference methods such as mean-Ô¨Åeld variational inference. ÓÄÄen, the approx-
imate MAP is simply:

D arg max

q./:
where q./ is the marginal approximate posterior distribution over the parameters. A thorough
discussion of variational approximation methods in Bayesian NLP is found in Chapter 6.
4.2.2 POSTERIOR APPROXIMATIONS BASED ON THE MAP SOLUTION
In cases where ‚Äö  RK
, the mode of the posterior can be used to obtain an approximate dis-
tribution of the posterior. ÓÄÄis approximation assumes that the posterior behaves similarly to a
multivariate normal distribution with a mean at the posterior mode (note that the mode and the
mean of the multivariate normal distribution are identical).
Let x be an observation from the likelihood p.x j / with a prior p./. ÓÄÄis normal ap-
proximation to the posterior (also called ‚ÄúLaplace approximation‚Äù) assumes that:
p. j x/  f . j 
; ‚Ä†
/; (4.11)
where
f . j 
; ‚Ä†
/ D
1
.2/ K=2
p
jdet.‚Ä†/j
exp

1
2
. 
/>
.‚Ä†
/ 1
. 
/

;
is the density of the multivariate normal distribution with mean 
(the mode of the posterior)
and covariance matrix ‚Ä†
deÔ¨Åned as inverse of the Hessian of the negated log-posterior at point

:
.‚Ä†
/ 1
i;j D
@2
h
@i @j
.
/;
with h./ D log p. j X D x/. Note that the Hessian must be a positive deÔ¨Ånite matrix to serve
as the covariance matrix of the distribution in Equation 4.11. ÓÄÄis means that the Hessian has to
be a symmetric matrix. A necessary condition is that the second derivatives of the log-posterior
next to the mode are continuous.
ÓÄÄe Laplace approximation is based on a second-order Taylor approximation of the log-
posterior. A second-order Taylor approximation of the log-posterior around point 
yields:
88 4. BAYESIAN ESTIMATION
log p. j X D x/ D h./
 h.
/ . 
/>
rh.
/
1
2
. 
/>
.‚Ä†
/ 1
. 
/
D h.
/
1
2
. 
/>
.‚Ä†
/ 1
. 
/; (4.12)
where Equation 4.12 is true because 
is assumed to be the mode of the posterior, and there-
fore the gradient of h at 
equals 0. Equation 4.12 is proportional to the log-density of the
multivariate normal distribution with mean 
and covariance matrix ‚Ä†
. ÓÄÄerefore, when the
second-order Taylor approximation is accurate, the posterior behaves like a multivariate normal
variable around its mode.
ÓÄÄe Laplace approximation for the posterior has not been widely used in the Bayesian NLP
literature, and is given here for consumption by the interested reader. However, second-order
Taylor approximation of posterior distributions has been used with models for text. For example,
Ahmed and Xing (2007) improve the correlated topic model by using a tighter second-order
approximation for the logistic normal distribution.
As pointed out earlier in this book, it is often the case that  represents a multinomial
distribution. In these cases, when using the Laplace approximation, it is better to change the
parametrization of the prior, so that i is deÔ¨Åned on the real line (Gelman et al., 2003), because
the Laplace approximation is dominated by a Gaussian distribution deÔ¨Åned over the real line.
One transformation that maps .0; 1/ to . 1; 1/ is the logit transformation:
logit.u/ D log
 u
1 u

8u 2 .0; 1/: (4.13)
In Appendix A, there is a discussion of how to re-parametrize distributions using the Ja-
cobian transformation.
4.2.3 DECISION-THEORETIC POINT ESTIMATION
Our discussion in this section is limited to the case where there are no latent variables in the
model. Assume a prior p./ and a likelihood function p.X D xj/. Decision-theoretic Bayesian
analysis assumes the existence of a loss function L. O
.x/; /, which denotes the loss incurred to
the decision maker by estimating  with O
.x/, when observing x. ÓÄÄe analysis then proceeds with
the Bayes risk, which is deÔ¨Åned as:
R. O
/ D
Z

X
x
L. O
.x/; /p.X D xj/p./d:
ÓÄÄis analysis computes the average loss of estimating the parameters using the estimator
function O
.x/, where the average is taken with respect to both the likelihood function and prior
4.2. BAYESIAN POINT ESTIMATION 89
information about the parameters. ÓÄÄe Bayes risk is a natural candidate for minimization in order
to Ô¨Ånd an optimal set of parameters, in which the lowest average loss is incurred. For a complete
discussion of the use of decision theory with Bayesian analysis, see Berger (1985).
Minimizing the Bayes risk can be done by choosing O
.x/ which minimizes the posterior
loss:
E≈íL. O
.x/; /jX D x¬ç D
Z

L. O
.x/; /p.jX D x/d
/
Z

L. O
.x/; /p.X D xj/p./d;
i.e., O
.x/ D arg min0 E≈íL.0
; /jX D x¬ç.
Minimizing this expectation is not necessarily tractable in a general case, but it is often the
case that choosing speciÔ¨Åc loss functions makes it possible to solve this expectation analytically.
For example, if the parameter space is a subset of RK
, and
L. O
.x/; / D jj O
.x/ jj2
2; (4.14)
then the posterior loss minimizer is the mean value of the parameters under the posterior, i.e.:‚Å∂
O
.x/ D arg min
0
E≈íL.0
; /jX D x¬ç D Ep.jXDx/≈í¬ç; (4.15)
or, alternatively, when
L. O
.x/; / D
(
1; if O
.x/ D 
0 otherwise;
then the posterior loss minimizer is the MAP estimator.
If the prior is also conjugate to the likelihood, then the expected value and the mode of the
posteriors may even have an analytic solution.
In Bayesian NLP, it is uncommon to use Bayesian point estimation with non-trivial loss
functions. Most often, either the mean value of the parameters under the posterior is computed, or
MAP estimation is used. Loss functions in NLP are more often deÔ¨Åned directly on the predicted
structure space itself, conditioned on the input. Such loss functions are used, for example, with
minimum Bayes risk decoding in parsing (Goodman, 1996) or machine translation (Kumar and
Byrne, 2004, Tromble et al., 2008).
‚Å∂To see that, consider that for any random variable T , the quantity E≈í.T /2¬ç is minimized with respect to  when
 D E≈íT ¬ç.
90 4. BAYESIAN ESTIMATION
4.2.4 DISCUSSION AND SUMMARY
Bayesian point estimation refers to approaches to summarize the information in the posterior
over the parameters. ÓÄÄe most common approaches to Bayesian point estimation in NLP are
computing the mean value of the posterior parameters and computing the maximum a posteriori
estimation.
Several frequentist approaches, such as L2 regularization and additive smoothing can be
framed as Bayesian point estimation problems with a certain prior distribution. With a rich fre-
quentist theory for L2 regularization and similar methods, these interpretations can serve as an
additional validation for these methods from a Bayesian point of view.
4.3 EMPIRICAL BAYES
As mentioned in Section 3.5, it is often the case that hyperpriors are deÔ¨Åned over the hyperpa-
rameters in order to exploit dependencies in the parameter space. In this case, the posterior over
the parameters for a model without any latent variables to predict (i.e., ‚Äúmodel over X but not
Z‚Äù) is deÔ¨Åned as:
p.jX D x/ D
R
Àõ p.X D xj/p.jÀõ/p.Àõ/dÀõ
R

R
Àõ p.X D xj/p.jÀõ/p.Àõ/dÀõd
;
with p.Àõ/ being a distribution over the hyperparameters. ÓÄÄis fully Bayesian approach places
a second-stage prior on the hyperparameters, and when inferring the posterior, integrating out
Àõ. Empirical Bayes takes a diÔ¨Äerent approach to the problem of encoding information into the
hyperparameters. Instead of using a prior p.Àõ/, in the empirical Bayes setting a Ô¨Åxed value for Àõ
is learned from observed data x. ÓÄÄis hyperparameter, O
Àõ.x/ can either be learned by maximizing
the marginal likelihood p.X D xjÀõ/ or estimated through other estimation techniques. ÓÄÄen,
predictions are made using a posterior of the form p.jX D x; O
Àõ.x//. One can also learn O
Àõ.x/
from a dataset diÔ¨Äerent from the one on which Ô¨Ånal inference is performed.
ÓÄÄis idea of identifying a hyperparameter O
Àõ.x/ based on the observed data is related to
hyperparameter identiÔ¨Åcation with conjugate priors (Section 3.1). Still, there are several key dif-
ferences between hyperparameter identiÔ¨Åcation with conjugate priors and empirical Bayes, as it
is described in this section. First, empirical Bayes does not have to be done with conjugate pri-
ors. Second, identifying O
Àõ.x/ is usually done using a diÔ¨Äerent statistical technique than regular
Bayesian inference (application of Bayes‚Äô rule), as is done with conjugate priors (see below, for
example, about type II maximum likelihood). ÓÄÄird, empirical Bayes is usually just a preliminary
stage to identify a set of hyperparameters, which can then be followed with Bayesian inference,
potentially on a new set of data.
Similarly to the case with hierarchical priors, empirical Bayes is often used when the pa-
rameters of the model are not drawn once for the whole corpus, but are drawn multiple times for
each instance in the corpus (see Section 3.5).
4.3. EMPIRICAL BAYES 91
In this case, there are multiple observations x.1/
; : : : ; x.n/
, each associated with a set of
parameters .i/
for i D f1; : : : ; ng. Empirical Bayes is similar to Bayesian point estimation, only
instead of identifying a single parameter  from the observed data, a single hyperparameter is
identiÔ¨Åed.
ÓÄÄis hyperparameter, O
Àõ.x.1/
; : : : ; x.n/
/, summarizes the information in the learned prior:
p..1/
; : : : ; .n/
j O
Àõ.x.1/
; : : : ; x.n/
// D
n
Y
iD1
p..i/
j O
Àõ.x.1/
; : : : ; x.n/
//:
Although the traditional empirical Bayesian setting proceeds at this point to perform in-
ference (after estimating O
Àõ.x.1/
; : : : ; x.n/
/) with the posterior p.j O
Àõ.x.1/
; : : : ; x.n/
//), it is some-
times preferable in NLP to apply a simple function on O
Àõ.x.1/
; : : : ; x.n/
/ in order to identify
a point estimate for the model. For example, one can Ô¨Ånd the mode of the estimated prior,
p..1/
; : : : ; .n/
j O
Àõ.x.1/
; : : : ; x.n/
// or its mean.
Maximizing marginal likelihood is typically the most common approach to empirical Bayes
in NLP. In that case, the following optimization problem‚Äîor an approximation of it‚Äîis solved:
Àõ.x.1/
; : : : ; x.n/
/ D arg max
Àõ
p.x.1/
; : : : ; x.n/
jÀõ/: (4.16)
ÓÄÄere is an implicit marginalization of a set of parameters .i/
(or a single , if not each
observed datum is associated with parameters) in the formulation in Equation 4.16. In addition,
random variables Z.i/
, the latent structures, are also marginalized out if they are part of the model.
In this setting, empirical Bayes is also referred to as type II maximum likelihood estimation. With
latent variables, maximizing likelihood in this way is often computationally challenging, and al-
gorithms such as variational EM are used. Variational approximations are discussed at length in
Chapter 6.
Finkel and Manning (2009) describe a simple example of the use of empirical Bayes in NLP
and an exploration of its advantages. ÓÄÄey deÔ¨Åne a hierarchical prior for the purpose of domain
adaptation. ÓÄÄeir model is a log-linear model, in which there is a Gaussian prior with a varying
mean over the feature weights (see discussion in Section 4.2.1)‚Äîinstead of a regular L2 regular-
ization that assumes a zero-mean Gaussian prior. Each domain (among K domains) corresponds
to a diÔ¨Äerent mean for the Gaussian prior. In addition, they have a zero-mean Gaussian prior
that is placed on the mean of all of the domain Gaussian priors. Such a hierarchical prior requires
the model to share information between the models if the statistics available are sparse; if there is
enough data for a speciÔ¨Åc domain, it will override this kind of information sharing.
ÓÄÄe space of parameters is RK
, and it parametrizes a conditional random Ô¨Åeld model. ÓÄÄe
hierarchical prior of Finkel and Manning is deÔ¨Åned as follows:
92 4. BAYESIAN ESTIMATION
p.; .1/
; : : : ; .J /
j1; : : : ; J ; / D p.j/
J
Y
iD1
p..j/
jj ; /
!
;
with each p..j/
jj ; / being a multivariate normal variable with covariance matrix 2
j I and mean
, and p.j/ being a multivariate normal variable with mean zero and covariance matrix 2
I.
In an empirical evaluation of their approach, Finkel and Manning tried their prior with
named entity recognition (NER) and dependency parsing. For NER, each domain was repre-
sented by a diÔ¨Äerent NER dataset from the CoNLL 2003 (Tjong Kim Sang and De Meulder,
2003), MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001) shared tasks
datasets. For this problem, their model performed better than just concatenating all of the datasets
and training a single conditional random Ô¨Åeld with that large set of data. ÓÄÄe performance gain
(in F1-measure) ranged from 2.66% to 0.43%, depending on the data set being tested.
For the parsing problem, Finkel and Manning used the OntoNotes data (Hovy et al., 2006),
which includes parse trees from seven diÔ¨Äerent domains. For this problem, the results were more
mixed: in four of the cases the hierarchical model performed better than the rest of the tested
methods, and in three cases, concatenating all of the domains into a single domain performed
better than the rest of the methods being tested.
Finkel and Manning also show that their model is equivalent to the domain adaptation
model of Daume III (2007). In Daume‚Äôs model, the features in the base conditional random Ô¨Åeld
are duplicated for each domain. ÓÄÄen, for each datum in each domain, two sets of features are
used: one feature set associated with the speciÔ¨Åc domain the datum came from, and one feature
set that is used for all of the domains.
4.4 ASYMPTOTIC BEHAVIOR OF THE POSTERIOR
At its core, Bayesian inference begins and ends with the application of Bayes‚Äô rule to invert the
relationship between the parameters and the observed data. However, in the context of posterior
summarization, one can use tools from frequentist analysis to discuss the behavior of the posterior
summary as the size of the sample increases.
ÓÄÄe most notable analysis of this kind is the one that discusses the multivariate normality of
the posterior around the ‚Äútrue‚Äù parameter. ÓÄÄis means that if we have a Bayesian model p.X; / D
p./p.X j /, and a set of samples x.1/
; : : : ; x.n/
where x.i/
are drawn from p.Xj0/ for some 0
in the parameter space, then under some regularity conditions (as Gelman et al. (2003) points out,
these regularity conditions indicate mostly that the log-likelihood is continuous with respect to
, and that 0 is not on the boundary of the parameter space), the posterior distribution acts like
a multivariate normal distribution around 0 as the number of samples increases (i.e., ‚Äún goes to
inÔ¨Ånity‚Äù). ÓÄÄis is shown by developing a Taylor series approximation of the log-posterior around
0. Such an approximation is described in Section 4.2.2. ÓÄÄis general result of the normality of
the posterior is also called the ‚ÄúBayesian central limit theorem.‚Äù
4.5. SUMMARY 93
What happens when x.i/
are sampled from a distribution which does not belong to the
model family (i.e., the above 0 does not exist‚Äîthe model family is ‚Äúincorrect‚Äù)? In this case, the
role of 0 changes from the parameters according to which x.i/
are drawn, to a set of parameters
that minimize some distance between the true distribution and the model family. See Gelman
et al. (2003) for more details, especially Appendix B for proof sketches.
What this type of result generally shows is that the prior has a much more important role
when n is small. As n gets larger, the posterior becomes more and more concentrated around 0.
In frequentist terminology, the posterior mode is a consistent estimator for 0.
4.5 SUMMARY
Bayesian point estimation is especially useful when a summary of the posterior is needed. In
NLP, the most common reason for such a need is to maintain a lightweight model with a Ô¨Åxed
set of parameters. Such a Ô¨Åxed set of parameters enables computationally eÔ¨Écient solutions for
decoding.
Several common smoothing and regularization techniques can be interpreted as Bayesian
point estimation with a speciÔ¨Åc prior. Additive smoothing, for example, can be interpreted as
the mean of a posterior seeded by a Dirichlet prior. L2 regularization can be interpreted as a
maximum a posteriori solution with a Gaussian prior, and L1 regularization can be interpreted as
MAP solution with a Laplace prior.
Empirical Bayes estimation is another technique that is related to Bayesian point estima-
tion. With empirical Bayes, a point estimate for the hyperparameters is identiÔ¨Åed. ÓÄÄis point es-
timate can be subsequently followed with regular Bayesian inference (potentially on new set of
data), or used to summarize the posterior over the parameters to identify a Ô¨Ånal point estimate
for the parameters.
94 4. BAYESIAN ESTIMATION
4.6 EXERCISES
4.1. Show that Equation 4.4 is true. (Hint: the maximizer of the log-posterior is also the
maximizer of the posterior.)
4.2. Let  be a value between ≈í0; 1¬ç drawn from the Beta distribution parametrized by .Àõ; Àá/.
Use the Jacobian transformation (Appendix A) to transform the distribution over  to
the real line with a new random variable  D logit./. ÓÄÄe logit transformation is deÔ¨Åned
in Equation 4.13.
4.3. Show that Equation 4.15 is true for the choice of L. O
.x/; / as it appears in Equa-
tion 4.14.
4.4. Let x.i/
2 Rd
and y.i/
2 R for i 2 f1; : : : ; ng. With least squares ridge regression, our
goal is to Ô¨Ånd a weight vector 
2 Rd
such that:

D arg max

n
X
iD1
.y.i/
  x.i/
/2
!
C 
0
@
d
X
j D1
2
j
1
A ; (4.17)
where  > 0 is a Ô¨Åxed value.
Describe a Bayesian statistical model p.; X/ such that the above 
is the MAP solution
for this model, i.e., 
D arg max p.jx.1/
; : : : ; x.n/
/. Here, x.1/
; : : : ; x.n/
are assumed
to be independently drawn from p.Xj/ (conditioned on a set of parameters drawn be-
fore them). ÓÄÄe Bayesian model will require  to be a hyperparameter. (Hint: You may
use well-known distributions, such as the multivariate normal distribution and others in
designing your model.)
4.5. Is there an analytical solution for the above MAP problem (or in other words, is there an
analytical solution for Equation 4.17)? If so, write it down.
C H A P T E R 5
Sampling Methods
When the posterior cannot be analytically represented, or eÔ¨Éciently computed, we often have
to resort to approximate inference methods. One main thread of approximate inference relies
on the ability to simulate from the posterior in order to draw structures or parameters from the
underlying distribution represented by the posterior. ÓÄÄe samples drawn from this posterior can
be averaged to approximate expectations (or normalization constants). If these samples are close
to the posterior mode, they can be used as the Ô¨Ånal output. In this case, the samples replace the
need to Ô¨Ånd the highest scoring structure according to the model, which is often computationally
diÔ¨Écult to do if one is interested in averaging predictions with respect to the inferred distribution
over the parameters (see Section 4.1).
Monte Carlo (MC) methods provide a general framework ideal for drawing samples from
a target distribution that satisÔ¨Åes certain conditions. While not speciÔ¨Åc to Bayesian statistics, an
especially useful family of MC methods in the Bayesian context is the Markov Chain Monte
Carlo (MCMC) methods. In general, these methods have an advantage in allowing sampling
from a family of distributions that satisfy certain conditions (usually, that a distribution is com-
putable up to a normalization constant). In Bayesian statistics, they are often used for posterior
inference, because posterior distributions for various Bayesian models naturally meet these condi-
tions. MCMC algorithms are especially useful in the Bayesian context for Ô¨Ånding the normaliza-
tion constant of the posterior, marginalizing out variables, computing expectations of summary
statistics and Ô¨Ånding the posterior mode.
It is important to keep in mind that Bayesian inference, at its core, manages uncertainty
regarding the parameters and the remaining latent variables through the use of distributions. ÓÄÄis
means that the goal of Bayesian inference is to eventually Ô¨Ånd the posterior distribution in one
form or another. Monte Carlo methods treat this problem slightly diÔ¨Äerently. Instead of directly
representing the posterior distribution as a member of some (possibly approximate) family of
distributions (such as with variational inference, see Chapter 6), MC methods instead permit
indirect access to this posterior. Access to the posterior comes in the form of being able to draw
from the posterior distribution, without necessarily needing a complete analytic representation
for it.
ÓÄÄe focus of this chapter is to provide an account of the way that Monte Carlo methods are
used in Bayesian NLP. We cover some of the principal Markov chain Monte Carlo methods, and
detail the design choices and the advantages and disadvantages for using them in the context of
Bayesian NLP. We also cover some techniques used to assess the convergence of MCMC methods
96 5. SAMPLING METHODS
to the target distribution. Convergence here means that the MCMC sampler, which is iterative
and outputs a sequence of samples, has Ô¨Ånished its ‚Äúburn-in‚Äù period, during which time it outputs
samples that are not necessarily drawn from the target distribution. When the MCMC sampler
has reached convergence, its output represents samples from the target distribution. It is often
the case that poor assessment of an MCMC method implies that the output returned is invalid,
and does not represent the underlying Bayesian model used.
ÓÄÄis chapter is organized as follows. We begin by providing an overview of MCMC meth-
ods in Section 5.1 and then follow with an account of MCMC in NLP in Section 5.2. We then
start covering several important MCMC sampling algorithms, such as Gibbs sampling (Sec-
tion 5.3), Metropolis-Hastings (Section 5.4) and slice sampling (Section 5.5). We then cover
other topics such as simulated annealing (Section 5.6); the convergence of MCMC algorithms
(Section 5.7); the basic theory behind MCMC algorithms (Section 5.8); non-MCMC sampling
algorithms such as importance sampling (Section 5.9); and Ô¨Ånally, Monte Carlo integration (Sec-
tion 5.10). We conclude with a discussion (Section 5.11) and a summary (Section 5.12).
5.1 MCMC ALGORITHMS: OVERVIEW
ÓÄÄe basic idea of MCMC methods is intuitive. First, a space of states for the random variables
of interest is deÔ¨Åned; these random variables are those for which we want to draw samples. In
Bayesian NLP, these random variables are usually the random variables over which the posterior
is deÔ¨Åned. Each state in the space corresponds to an assignment of values for all variables. Next,
a strategy to explore this space is deÔ¨Åned. Each type of MCMC method (whether it is Gibbs
sampling, the Metropolis-Hastings or another MCMC method) has a diÔ¨Äerent framework for
deÔ¨Åning this strategy. Once the strategy is deÔ¨Åned, the algorithm works by exploring the space
using the strategy until convergence is achieved, and a satisfactory number of samples has been
obtained. ÓÄÄe samples are collected as the state space is being explored.
If the MCMC method is sound, and the framework it oÔ¨Äers is used correctly, there is a
theoretical guarantee that the samples being drawn (i.e., the assignment to the random variables
according to the state being explored) stem from the distribution the sampler was designed for,
such as the posterior. ÓÄÄe samples are not necessarily independent. As a matter of fact, in most
cases with MCMC algorithms, there is a great dependence between states that were explored in
proximity to each other, and thus, between the samples that the algorithm produces. In general,
the farther apart the samples drawn are in the chain, the less correlated they are. ÓÄÄis observation
can be used to generate samples that are closer to being uncorrelated: if S D fy1; : : : ; yM g is a set
of correlated samples that were collected for some large M, then the subset fyi 2 Sji mod m D 0g
for some integer m will have samples with weaker correlation (the larger m is, the weaker the
correlation). Less formally, we pick a subset of the samples drawn, every mth sample, for some
m. ÓÄÄis process is also called ‚Äúthinning.‚Äù
MCMC methods are iterative in nature, with each iteration moving from one state of the
sample space to another state. Being iterative, they require some stopping criterion. ÓÄÄis stop-
5.2. NLP MODEL STRUCTURE FOR MCMC INFERENCE 97
ping criterion is of crucial importance with MCMC algorithms‚Äîchoosing it carelessly could
mean that the algorithm never converges, and therefore that all the samples drawn through the
algorithm‚Äôs execution are not really drawn from the desired posterior. Indeed, in the beginning
period of an MCMC algorithm execution, random samples are drawn that are unrelated to the
true posterior distribution, in a phase called the burn-in phase. It is sometimes possible to initial-
ize the sampler a certain way so that its burn-in period is shorter. More information about the
convergence of MCMC algorithms is discussed in Section 5.7.
It is important to note that MCMC algorithms are often used with Bayesian statistics, but
that they can generally be used whenever there is a need to sample from a distribution, whether it is
a Bayesian posterior or some other distribution that did not arise in a Bayesian context. MCMC
algorithms are often mentioned in the Bayesian context because they are most useful when a
distribution can be computed up to a normalization constant. ÓÄÄis state naturally arises with the
posterior of a Bayesian model, where the posterior is proportional to the joint distribution. While
this joint distribution is usually easy to compute, the normalization constant that is required to
turn this joint distribution into the posterior distribution is often more diÔ¨Écult to compute (see
Chapter 3).
5.2 NLP MODEL STRUCTURE FOR MCMC INFERENCE
Introductory texts about Bayesian statistics usually consider the inference of the parameters with
fully observed data. In NLP, however, most Bayesian statistics are used with latent variables, which
means that the posterior is deÔ¨Åned over parameters and some predicted structure. ÓÄÄis predicted
structure is usually a linguistic structure such as a parse tree, an alignment between a pair of trees
or sentences or sequence denoting part-of-speech tags.
Since Bayesian NLP usually focuses on latent variable models, inference in this chapter
focuses on Bayesian inference with latent variables. ÓÄÄis means that the underlying statistical
model used has the following structure:
p.; X D x; Z D z j Àõ/ D p.jÀõ/p.Z D zj/p.X D xjZ D z; /:
In addition, we consider n identically distributed samples (conditionally independent given
the model parameters), with a prior at the top level (see Section 3.5). ÓÄÄerefore, the joint distri-
bution values over all random variables in this process are deÔ¨Åned as:
p.; x.1/
; : : : ; x.n/
; z.1/
; : : : ; z.n/
j Àõ/ D p. j Àõ/
n
Y
iD1
p.z.i/
j/P.x.i/
jz.i/
; /
!
:
Given that only x.i/
for i 2 f1; : : : ; ng are observed, the posterior then has the form:
98 5. SAMPLING METHODS
p.z.1/
; : : : ; z.n/
; jx.1/
; : : : ; x.n/
/:
MCMC sampling yields a stream of samples from this posterior, which can be used in
various ways, including to Ô¨Ånd a point estimate for the parameters, to draw predicted structures
from the posterior or even to Ô¨Ånd the maximum value of the posterior using simulated annealing
(see Section 5.6). Often, the collapsed setting is of interest, and then samples are drawn from the
posterior over the predicted structures only, integrating out the model parameters:
p.z.1/
; : : : ; z.n/
jx.1/
; : : : ; x.n/
/ D
Z

p.z.1/
; : : : ; z.n/
; jx.1/
; : : : ; x.n/
/d:
In this case, the parameters are nuisance variables, because the inference procedure is not
focused on them. Still, it is often the case that a summary of the parameters (in the style of
Chapter 4) can be inferred from the samples drawn for the latent variables in the model.
5.2.1 PARTITIONING THE LATENT VARIABLES
Perhaps the most important choice when designing an MCMC sampler for a Bayesian NLP
model is about the way in which the (latent) random variables of interest are partitioned. In their
most general form, MCMC methods do not require the partitioning of the latent variables in
the model. In fact, most of the introductory text to MCMC methods describes a single global
state of all of the latent variables in the model (the state is a tuple with assignments for all Z.i/
for i 2 f1; : : : ; ng and ), and a chain that moves between states in this space. ÓÄÄis state is often
represented by a single random variable.
However, in NLP problems, treating the sample space with a single random variable with-
out further reÔ¨Åning it yields challenging inference problems, since this single random variable
would potentially represent a complex combinatorial structure, such as a set of trees or graphs.
ÓÄÄerefore, the set of latent variables is carved up into smaller subsets.
As mentioned earlier, NLP models are usually deÔ¨Åned over discrete structures, and there-
fore the variables Z.i/
commonly denote a structure such as a parse tree, an alignment or a se-
quence. We assume this type of structure for Z.i/
for the rest of this section‚Äîi.e., a discrete
compositional structure.
ÓÄÄere are two common choices for partitioning the latent variables Z.i/
in order to sample
from the posterior:
‚Ä¢ Keep each variable Z.i/
as a single atomic unit. When moving between states, re-sample
a whole Z.i/
structure for some i, possibly more than one structure at a time. ÓÄÄis is one
type of blocked sampling where the atomic unit is a whole predicted structure. It is often the
case that each Z.i/
is sampled using a dynamic programming algorithm. See Section 5.3
for more details.
5.3. GIBBS SAMPLING 99
‚Ä¢ ReÔ¨Åne the predicted structure into a set of random variables, and sample each of them
separately. ÓÄÄis means, for example, that if Z.i/
denotes a dependency tree, it will be reÔ¨Åned
to a set of random variables denoting the existence of edges in the tree. When moving
between states, only one edge (or a small number of edges) is changed at a time. ÓÄÄis is also
often called pointwise sampling. See Section 5.3 for more details.
ÓÄÄe parameters themselves, usually continuous variables, can also be reÔ¨Åned into smaller
constituents. For example, if the parameters are a product of Dirichlets (see Section 8.3.1 for an
example), then each constituent can consist of a single Dirichlet distribution.
In the rest of this chapter, we assume a single random variable U , denoting the la-
tent variables over which we perform the inference. For example, in the non-collapsed setting,
U D .Z; /, with Z denoting the latent structures to be predicted. In the collapsed setting, on
the other hand, U D Z. Here, Z is a tuple of the form .Z.1/
; : : : ; Z.n/
/. (Similarly, we use X
to denote .X.1/
; : : : ; X.n/
/). ÓÄÄe multivariate random variable U is also assumed to have a de-
composable representation, such that U D .U1; : : : ; Up/. ÓÄÄis decomposition partitions .Z; /
(in the non-collapsed setting) into smaller constituents. ÓÄÄe random variable U i denotes the
vector .U1; : : : ; Ui 1; UiC1; : : : ; Up/.
5.3 GIBBS SAMPLING
ÓÄÄe Gibbs sampling algorithm (Geman and Geman, 1984) is one of the most common MCMC
algorithms used in the context of Bayesian NLP. In this setting, Gibbs sampling explores the state
space, sampling ui each time for some i 2 f1; : : : ; pg. ÓÄÄese ui are drawn from the conditional
distributions, p.Ui j U i ; X/. ÓÄÄe full algorithm is given in Algorithm 5.1.
Note that at each step, the ‚Äústate‚Äù of the algorithm is a set of values for U . At each iteration,
the distributions p.Ui j U i / condition on values u i from the current state, and modify the
current state‚Äîby setting a new value for one of the Ui . ÓÄÄe update to the current state of the
algorithm is immediate when a new value is drawn for one of the variables. ÓÄÄe Gibbs algorithm
does not delay the global state update, and each new draw of a random variable is immediately
followed by a global state update. (However, see Section 5.4 for information about using a ‚Äústale‚Äù
state for parallelizing the Gibbs sampler.)
Algorithm 5.1 returns a single sample once the Markov chain has converged. However, once
the Gibbs sampling has converged, a stream of samples can be produced repeatedly by changing
the state according to the conditional distributions and traversing the search space, collecting a
set of samples. All of these samples are produced from the target distribution p.U jX/. While
these samples are not going to be independent of each other, the farther a pair of samples are
from each other, the less correlated they are.
To follow the discussion in Section 5.2.1, the Gibbs sampler can be pointwise or blocked
(Gao and Johnson, 2008). Pointwise sampling implies that the Gibbs sampler alternates between
steps that make very local changes to the state, such as sampling a single part-of-speech tag,
while blocked sampling implies that larger pieces of the structure are sampled at each step. For
100 5. SAMPLING METHODS
.
.
Input: Samplers for the conditionals p.Ui jU i ; X/ of the distribution p.U1; : : : ; UpjX/.
Output: u D .u1; : : : ; up/ drawn from p.U1; : : : ; UpjX/.
.
1: Initialize u1; : : : ; up with some value from their space of allowed values
2: repeat
3: for all i 1 to p do
4: Sample ui from p.ui ju i ; X/
5: end for
6: until Markov chain converged
7: return u1; : : : ; up
Algorithm 5.1: ÓÄÄe Gibbs sampling algorithm, in its ‚Äúsystematic sweep‚Äù form. Part of the input to
the Gibbs algorithm is samplers for the conditional distributions derived from the target distribution.
Such samplers are treated as black-box functions that draw samples from these conditionals (in line
4).
example, a sentence-blocked sampler for part-of-speech tagging could sample the tags for a whole
sentence using the dynamic programming forward-backward algorithm. A blocked sampler can
also sample smaller constituents‚Äîfor example, Ô¨Åve part-of-speech tags at a time‚Äîagain using
the forward-backward algorithm applied to a window of Ô¨Åve part-of-speech tags at a time.
In order to use a Gibbs sampler, one has to be able to draw samples for one variable in the
model conditioned on a Ô¨Åxed value for all others. To achieve this, another MCMC algorithm
can be used to sample from conditional distributions (see Section 5.11); however, it is often the
case in Bayesian NLP models that these conditionals have an analytic form, and therefore are
easy to sample from (while the whole posterior is intractable, and requires MCMC or some other
approximate method).
Example 5.1 Consider the latent Dirichlet allocation model from Chapter 2. We denote the
number of documents it models by N, the size of the vocabulary by V , and the number of words
per document by M (in general, the number of words in a document varies, but for the sake of
simplicity we assume all documents are of the same length). ÓÄÄe full graphical model for LDA
appears in Figure 2.1.
We will use the index i to range over documents, j to range over words in a speciÔ¨Åc docu-
ment, k to range over the possible topics and v to range over the vocabulary. In addition, there are
random variables .i/
2 RK
which are the document topic distributions, Z.i/
j which denote the
topic for the j th word in the ith document, W .i/
j which denote the j th word in the ith document
and Àák 2 RV
which denote the distribution over the vocabulary for the kth topic.
ÓÄÄe joint distribution is factorized as follows:
5.3. GIBBS SAMPLING 101
p.; Àá; Z; W j ; Àõ/ D
K
Y
kD1
p.Àák j /
! 0
@
N
Y
iD1
p..i/
j Àõ/
M
Y
j D1
p.Z.i/
j j .i/
/p.W .i/
j j Àá; Z.i/
j /
1
A :
(5.1)
ÓÄÄe random variables we need to infer are , Àá and Z. One way to break the random
variables into constituents (in conditional distributions) for a Gibbs sampler is the following:
‚Ä¢ p.Àák j ; Àá k; z; w; ; Àõ/ for k 2 f1; : : : ; Kg. ÓÄÄis is the distribution over the parameters of
the LDA model that denote the probabilities over the vocabulary for each topic (conditioned
on all other random variables). We denote by Àá k the set of fÀák0 j k0
¬§ kg.
‚Ä¢ p..i/
j . i/
; Àá; z; w; ; Àõ/ for i 2 f1; : : : ; N g. ÓÄÄis is the distribution over the topic dis-
tribution for the ith document, conditioned on all other random variables in the model. We
denote by . i/
the topic distributions for all documents other than the ith document.
‚Ä¢ p.Z.i/
j j ; z .i;j /; w; ; Àõ/ for i 2 f1; : : : ; N g and j 2 f1; : : : ; Mg. ÓÄÄis is the distribution
over a topic assignment for a speciÔ¨Åc word (j th word) in a speciÔ¨Åc document (ith docu-
ment), conditioned on all other random variables in the model. We denote by z .i;j/ the
set of all topic assignment variables other than Z.i/
j .
At this point, the question remains as to what the form is of each of these distributions,
and how we can sample from them. We begin with p.Àák j ; Àá k; z; w; ; Àõ/. According to
Equation 5.1, the only factors that interact with Àák are denoted on the right-hand side of the
following equation:
p.Àák j ; Àá k; z; w; ; Àõ/ / p.Àák j /
0
@
N
Y
iD1
M
Y
j D1
p.w.i/
j j Àá; z.i/
j /I.z
.i/
j Dk/
1
A
D
V
Y
vD1
Àá 1
k;v
! 0
@
N
Y
iD1
M
Y
j D1
V
Y
vD1
Àá
I.w
.i/
j Dv^z
.i/
j Dk/
k
1
A
D
V
Y
vD1
Àá
1C
PN
iD1
PM
jD1 I.w
.i/
j Dv^z
.i/
j Dk/
k;v
: (5.2)
Denote by nk;v the quantity
PN
iD1 I.w.i/
j D v ^ z.i/
j D k/. In this case, nk;v denotes the
number of times the word v is assigned to topic k in any of the documents based on the current
state of the sampler. ÓÄÄe form of Equation 5.2 is exactly the form of a Dirichlet distribution
with the hyperparameter C nk, where nk is the vector of nk;v ranging over v. ÓÄÄis concludes
the derivation of the conditional distribution, which is required to sample a new set of topic
distributions Àá given the state of the sampler.
102 5. SAMPLING METHODS
Consider p..i/
j . i/
; Àá; z; w; ; Àõ/. Following a similar derivation, we have:
p..i/
j . i/
; Àá; z; w; ; Àõ/ / p..i/
j Àõ/
M
Y
j D1
p.z.i/
j j .i/
/
D
K
Y
kD1
..i/
k
/Àõ 1
M
Y
j D1
K
Y
kD1
..i/
k
/I.z
.i/
j Dk/
D
K
Y
kD1
..i/
k
/Àõ 1C
PM
jD1 I.z
.i/
j Dk/
: (5.3)
Denote by m.i/
k
the quantity
PM
j D1 I.z.i/
j D k/, i.e., the number of times in the ith docu-
ment that a word was assigned to the kth topic. ÓÄÄen, it turns out that from Equation 5.3, .i/
,
conditioned on all other random variables in the model is distributed according to the Dirichlet
distribution, with hyperparameters Àõ C m.i/
where m.i/
is the vector ranging over m.i/
k
for all k.
ÓÄÄe last distribution we have to consider is that of p.Z.i/
j j ; z .i;j/; w; ; Àõ/. Again using
the joint distribution from Equation 5.1, it holds that:
p.Z.i/
j D k j ; z .i;j/; w; ; Àõ/ / p.Z.i/
j D k j .i/
/p.w.i/
j j Àá; z.i/
j / D .i/
k
Àák;w
.i/
j
: (5.4)
Equation 5.4 corresponds to a multinomial distribution proportional to the topic distribu-
tion probability multiplied by the probability of generating the j th word in the document under
the kth topic.
Note that this sampler is a pointwise sampler‚Äîit samples each coordinate of Z.i/
separately.
ÓÄÄis example demonstrates a Gibbs sampler from a Dirichlet-multinomial family with non-trivial
relationships between the multinomials. Such is the relationship in other more complex models
in NLP (PCFGs, HMMs and so on). ÓÄÄe structure of this Gibbs sampler will be similar in these
more complex cases as well. ÓÄÄis is especially true regarding the draw of . In more complex
models, statistics will be collected from the current set of samples for the latent structures, and
combined into hyperparameters for the posterior of the Dirichlet distribution.
In more complex cases, the draws of Z.i/
could potentially rely on dynamic programming
algorithms, for example, to draw a phrase-structure tree conditioned on the parameters (with
PCFGs), or to draw a latent sequence (with HMMs)‚Äîsee Chapter 8.
5.3.1 COLLAPSED GIBBS SAMPLING
It is often the case that inference over the parameters is of no interest. ÓÄÄe main focus is on directly
inferring the predicted structures from the posterior distribution. In this case, the burn-in period
of the Markov chain can be made shorter, if samples are drawn from the marginalized posterior,
i.e.:
5.3. GIBBS SAMPLING 103
.
.
Input: Distribution p.ZjX/.
Output: z.1/
; : : : ; z.n/
drawn from p.Z.1/
; : : : ; Z.n/
jX/.
.
1: Initialize z.1/
; : : : ; z.n/
with some value from their space of allowed values
2: repeat
3: for all i 2 f1; : : : ; ng do
4: Sample z.i/
from p.Z.i/
jz.1/
; : : : ; z.i 1/
; z.iC1/
; : : : ; z.n/
; X/
5: end for
6: until Markov chain converged
7: return z.1/
; : : : ; z.n/
Algorithm 5.2: ÓÄÄe collapsed observation-blocked Gibbs sampling algorithm, in its ‚Äúsystematic
sweep‚Äù form.
p.Z j X/ D
Z

p.; Z j X/d:
Using Gibbs sampling with this marginalized posterior is also called collapsed Gibbs sam-
pling. For the sake of discussion, we Ô¨Årst consider an example of collapsed Gibbs sampling where
we assume the posterior has a certain form without directly deriving it by performing Bayesian
inference with a statistical model. Similar target distributions arise frequently, in one form or
another, as posterior distributions in a Bayesian NLP model.
Example 5.2 Assume the following simple multinomial model:
‚Ä¢ Draw  2 ≈í0; 1¬çK
from a Dirichlet distribution with hyperparameters Àõ D .Àõ1; : : : ; ÀõK/
‚Ä¢ Draw z.i/
from the multinomial distribution  for i D 1; : : : ; n (i.e., z.i/
is a binary vector
of length K with 1 in one coordinate and 0 in all others).
ÓÄÄe joint distribution for this model is:
p.z.1/
; : : : ; z.n/
; jÀõ/ D p.jÀõ/
n
Y
iD1
p.z.j/
j/:
To derive a collapsed Gibbs sampler, the conditional distributions p.Z.i/
jZ. i/
/ are re-
quired. Let ek be the binary vector with 0 in all coordinates, except for the kth coordinate, where
it is 1. ÓÄÄe following holds:
104 5. SAMPLING METHODS
p.Z.i/
D ekjz. i/
; Àõ/ D
Z

p.Z.i/
D ek; jz. i/
; Àõ/d
D
Z

p.jz. i/
; Àõ/p.Z.i/
D ekjz. i/
; ; Àõ/d
D
Z

p.jz. i/
; Àõ/p.Z.i/
D ekj; Àõ/d (5.5)
D
Z

p.jz. i/
; Àõ/kd
D
P
j ¬§i z.j/
k
C Àõk
PK
k0D1
Pn
j ¬§i z.j/
k0 C Àõk0
 (5.6)
D
P
j ¬§i z.j/
k
C Àõk
n 1 C
PK
k0D1 Àõk0
: (5.7)
Equation 5.5 is true because of the conditional independence of Z.i/
given the parameter
. Note that in Equation 5.5 the term p.jz. i/
; Àõ/ is a Dirichlet distribution with the hyperpa-
rameter Àõ C
P
j¬§i z.j/
(see Section 3.2.1), and that in the equation below p.Z.i/
D ekj/ D k.
ÓÄÄerefore, the integral is the mean value of k according to a Dirichlet distribution with hyper-
parameters Àõ C
P
j ¬§i z.j/
, leading to Equation 5.6 (see Appendix B).
Example 5.2 exposes an interesting structure to a Gibbs sampler when the distribution
being sampled is a multinomial with a Dirichlet prior. According to Equation 5.7:
p.Z.i/
D ekjZ. i/
D z. i/
/ D
nk C Àõk
.n 1/ C
PK
k0D1 Àõk0
; (5.8)
with nk D
P
j ¬§i z.j /
k
, which is the total count of event k appearing in z. i/
. Intuitively, the prob-
ability of a latent variable Z.i/
taking a particular value is proportional to the number of times
this value has been assigned to the rest of the latent variables, Z. i/
. Equation 5.8 is essentially
an additively smoothed (see Section 4.2.1) version of the maximum likelihood estimate of the
parameters, when the estimation is based on the values of the variables being conditioned on.
ÓÄÄis kind of structure arises frequently when designing Gibbs samplers for models that have a
Dirichlet-multinomial structure.
ÓÄÄe following example, which is less trivial than the example above, demonstrates this
point.
Example 5.3 Consider the LDA Example 5.1. It is often the case that the topic assignments Z
are the only random variables that are of interest to do inference for.¬π ÓÄÄe parameters Àá and the
topic distributions  can therefore be marginalized in this case when performing Gibbs sampling.
¬πWe follow the derivation of GriÔ¨Éths (2002).
5.3. GIBBS SAMPLING 105
ÓÄÄerefore, we can place our focus on the random variables Z and W . We would like to
draw a value for Z.i/
j conditioned on z .i;j/ and w. ÓÄÄis means that we are interested in the
distribution p.Z.i/
j j z .i;j/; w/. By Bayes‚Äô rule, we have the following:
p.Z.i/
j D k j z .i;j/; w/ / p.w.i/
j j Z.i/
j ; z .i;j/; w .i;j//p.Z.i/
j D k j z .i;j //: (5.9)
For simplicity of notation, we do not explicitly condition on the hyperparameters Àõ and
, though we always assume they exist in the background. We Ô¨Årst tackle the Ô¨Årst term. From
the conditional independence assumptions in the model, it holds that for any k indexing a topic
between 1 and K:
p.w.i/
j j Z.i/
j D k; z .i;j/; w .i;j// D
Z
Àák
p.Àák; w.i/
j j Z.i/
j D k; z .i;j /; w .i;j //dÀák
D
Z
Àák
p.w.i/
j j Z.i/
j D k; Àák/p.Àák j Z.i/
j D k; z .i;j /; w .i;j //dÀák
D
Z
Àák
p.w.i/
j j Z.i/
j D k; Àák/p.Àák j z .i;j/; w .i;j//dÀák: (5.10)
ÓÄÄe last equality holds because Z.i/
j and Àák are conditionally independent when W .i/
j is not
observed. Note also that Àák and Z .i;j/ are a priori also independent of each other, and therefore:
p.Àák j z .i;j/; w .i;j// / p.w .i;j/ j Àák; z .i;j//p.Àák j z .i;j //
D p.w .i;j/ j Àák; z .i;j//p.Àák/
D
V
Y
vD1
N
Y
i0D1
M
Y
j 0D1;.i0;j 0/¬§.i;j/
Àá
I.Z.i;j/Dk^w
.i/
j Dv/
k
p.Àák/
D
V
Y
vD1
Àá
PN
i0D1
PM
j0D1;.i0;j 0/¬§.i;j/ I.Z.i;j/Dk^w
.i/
j Dv/
k
p.Àák/
D
V
Y
vD1
Àá
PN
i0D1
PM
j0D1;.i0;j 0/¬§.i;j/ I.Z.i;j /Dk^w
.i/
j Dv/C 1
k
:
ÓÄÄe above means that the distribution p.Àák j z .i;j/; w .i;j// has the form of a Dirichlet
with parameters C n .i;j /;k, such that n .i;j/;k is a vector of length V , and each coordinate v
equals the number of instances in z .i;j/ and w .i;j/ in which the vth word in the vocabulary was
assigned to topic k (note that we exclude from the count here the j th word in the ith document).
Note that the term p.w.i/
j j Z.i/
j D k; Àák/ in Equation 5.10 is just Àák;w
.i/
j
, and taking it
with the above, means that Equation 5.10 is the mean value of a Dirichlet distribution with
parameters C n .i;j/;k. ÓÄÄis means that:
106 5. SAMPLING METHODS
p.w.i/
j D v j Z.i/
j D k; z .i;j/; w .i;j// D
C ≈ín .i;j /;k¬çv
V C
P
v0 ≈ín .i;j/;k¬çv0
: (5.11)
We Ô¨Ånished tackling the Ô¨Årst term in Equation 5.9. We must still tackle p.Z.i/
j D k j
z .i;j//. First note that by Bayes‚Äô rule and the conditional independence assumptions in the model,
it holds that:
p.Z.i/
j D k j z .i;j// D
Z
.i/
p..i/
; Z.i/
j D k j z .i;j //d.i/
D
Z
.i/
p.Z.i/
j D k j .i/
/p..i/
j z .i;j//d.i/
: (5.12)
A similar derivation as before shows that p..i/
j z .i;j// is a Dirichlet distribution with
parameters Àõ C m .i;j/ where m .i;j/ is a K-length vector such that ≈ím .i;j /¬çk0 is the number of
times in z .i;j/ such that words in the ith document (other than the j th word) were assigned
to topic k0
. ÓÄÄe term p.Z.i/
j D k j .i/
/ is just .i/
k
. Equation 5.12 is therefore again the mean
value of a Dirichlet distribution: it is the kth coordinate of the mean value of a Dirichlet with
parameters Àõ C m .i;j/. ÓÄÄis means that:
p.Z.i/
j D k j z .i;j// D
Àõ C ≈ím .i;j/¬çk
KÀõ C
P
k0 ≈ím .i;j /¬çk0
: (5.13)
Taking Equations 5.9, 5.11 and 5.13, we get that in order to apply Gibbs sampling to LDA
in the collapsed setting, we must sample at each point a single topic assignment for a single word
in a document based on the distribution:
p.Z.i/
j D k j z .i;j/; w/ D
C ≈ín .i;j/;k¬çw
.i/
j
V C
P
v0 ≈ín .i;j/;k¬çv0

Àõ C ≈ím .i;j/¬çk
KÀõ C
P
k0 ≈ím .i;j/¬çk0
:
It should not come as a surprise that the collapsed Gibbs sampler in the example above has
a closed analytic form. ÓÄÄis is a direct result of the Dirichlet distribution being conjugate to the
multinomial distributions that govern the production of Z.i/
and W .i/
. Even though the coupling
of latent variables with the integration of the parameters may lead to non-analytic solutions for
the posterior, the use of conjugate priors still often leads to analytic solutions for the conditional
distributions (and for the Gibbs sampler, as a consequence).
To summarize this section, Algorithm 5.2 gives a collapsed example-blocked Gibbs sam-
pler. It repeatedly draws a latent structure for each example until convergence.
5.3.2 OPERATOR VIEW
ÓÄÄe operator view of Gibbs sampling is often used in NLP in place of explicitly partitioning the
posterior variables into random variables U1; : : : ; Up, as described in Section 5.3. Denote the tar-
get distribution by p.U jX/ and its sample space by . From an implementation perspective, the
5.3. GIBBS SAMPLING 107
.
.
Input: Operators O D ff1; : : : ; fM g and a target distribution p.U j X/.
Output: u drawn from p.U j X/.
.
1: Initialize u to a random value
2: repeat
3: for all i 1 to M do
4: Sample u0
from
q.u0
j u/ D
p.u0
j X/I.u0
2 fi .u//
Z.fi ; u/
(5.14)
5: Set u u0
6: end for
7: until Markov chain converged
8: return u
Algorithm 5.3: ÓÄÄe operator Gibbs sampling algorithm, in its ‚Äúsystematic sweep‚Äù form. ÓÄÄe function
Z.fi ; u/ denotes a normalization constant for the distribution q.u0
/. Note that it could be the case for
the distribution p.U j X/ that its normalization constant is unknown.
great advantage of the operator view is that it allows the modeler to design a correct Gibbs sampler
without committing to a speciÔ¨Åc target distribution. If the design follows a few simple principles,
then the sampler is guaranteed to be correct, no matter the underlying target distribution.
With the operator view, one deÔ¨Ånes a set of operators O D ff1; : : : ; fM g, where each op-
erator fi is a function that maps an element ! 2  to a set of neighbors A  . ÓÄÄe sampler
alternates between the operators, each time sampling one of the neighbors of the current state
proportionally to its probability according to p.U jX/. ÓÄÄis Gibbs algorithm is given in Algo-
rithm 5.3. ÓÄÄere, the notation I.u0
2 A/ is for an indicator function that is 1 if u0
2 A and 0
otherwise. ÓÄÄis indicator function is used to check whether u0
belongs to the neighborhood of
u according to the operator fi . ÓÄÄe term Z.fi ; u/ is a normalization constant that integrates or
sums U over fi ./ in order to ensure that q.U 0
/ is a distribution. For example, if  is a discrete
space, then this means that
Z.fi ; u/ D
X
u0
p.U D u0
jX/I.u0
2 fi .u//:
ÓÄÄe algorithm could also randomly move between the operators, instead of systematically
choosing an operator at each step.
ÓÄÄe operators usually make local changes to the current state in the state space. For example,
if the latent structure of interest is a part-of-speech tagging sequence, then an operator can make
108 5. SAMPLING METHODS
a local change to one of the part-of-speech tags. If the latent structure of interest is a phrase
structure tree, then an operator can locally change a given node in the tree and its neighborhood.
In these cases, the neighborhood returned by the operator is the set of states that are identical to
the input state, other than some local changes.
In order to ensure that the operators do indeed induce a valid Gibbs sampler, there is a need
for the operators to satisfy the following properties:
‚Ä¢ Detailed balance ‚Äì A suÔ¨Écient condition for the detailed balance condition is the follow-
ing: for each operator fi 2 O and each !, detailed balance requires that if !0
2 fi .!/ then
fi .!/ D fi .!0
/. ÓÄÄis ensures the detailed balance condition, meaning that for any u and
u0
, it holds that:
p.u j X/q.u0
j u/ D p.u0
j X/q.u j u0
/; (5.15)
where q.u j u0
/ and q.u0
j u/ are deÔ¨Åned in Equation 5.14. ÓÄÄe reason Equation 5.15 holds
is that the normalization constants for q.u j u0
/ and q.u0
j u/ satisfy Z.fi ; u/ D Z.fi ; u0
/
in case the suÔ¨Écient condition mentioned above is satisÔ¨Åed.
‚Ä¢ Recurrence ‚Äì this implies that it is possible to get from any state in the search space to any
other state. More formally, it means that for any !; !0
2 , there is a sequence of operators
fa1
; : : : ; fa`
with ai 2 f1; : : : ; Mg, such that !0
2 fiM
.fiM 1
.: : : .fi1
.!///, and this chain
is possible with non-zero probability according to Equation 5.14.
Note that the symmetry condition implies that for every ! and fi , it holds that ! 2 fi .!/,
i.e., an operator may not make changes at all to a given state. More generally, detailed balance and
recurrence are formal properties of Markov chains that are important for ensuring the correctness
of a sampler (i.e., that it actually samples from the desired target distribution), and the above two
requirements from the operators are one way to satisfy them for the underlying Markov chain
created by the Gibbs sampler.
ÓÄÄere are several examples in the Bayesian NLP literature that make use of this operator
view for Gibbs sampling, most notably for translation. For example, DeNero et al. (2008) used
Gibbs sampling in this manner to sample phrase alignments for machine translation. ÓÄÄey had
several operators for making local changes to alignments: SWAP, which switches between the
alignments of two pairs of phrases; FLIP, which changes the phrase boundaries; TOGGLE, which
adds alignment links; FLIPTWO, which changes phrase boundaries in both the source and target
language; and MOVE, which moves an aligned boundary either to the left or to the right. Similar
operators have been used by Nakazawa and Kurohashi (2012) to handle the alignment of function
words in machine translation. Ravi and Knight (2011) also used Gibbs operators to estimate the
parameters of IBM Model 3 for translation. For a discussion of the issue of detailed balance with
Gibbs sampling for a synchronous grammar model, see Levenberg et al. (2012).
5.3. GIBBS SAMPLING 109
.
.
Input: Samplers for the conditionals p.Ui jU i ; X/ for the distribution p.U1; : : : ; UpjX/.
Output: u D .u1; : : : ; up/ approximately drawn from the above distribution.
.
1: Initialize u1; : : : ; up with some value from their space of allowed values
2: repeat
3: for all i 1 to p (in parallel over i) do
4: Sample u0
i from p.u0
i ju i ; X/
5: end for
6: for all i 1 to p do
7: ui u0
i
8: end for
9: until Markov chain converged
10: return u1; : : : ; up
Algorithm 5.4: ÓÄÄe parallel Gibbs sampling algorithm.
It is often the case that an operator view of Gibbs sampling yields a sampler that is close
to being a pointwise sampler, because the operators, as mentioned above, make local changes to
the state space. ÓÄÄese operators typically operate on latent structures, and not on the parameters.
If the sampler is explicit, that is the parameters are not marginalized out, then the parameters
can be sampled in an additional Gibbs step that samples from the conditional distribution of the
parameters given the latent structure.
5.3.3 PARALLELIZING THE GIBBS SAMPLER
Each step in the Gibbs sampler can be quite expensive, especially when considering the collapsed
example-blocked setting in NLP. It is therefore beneÔ¨Åcial to parallelize the Gibbs algorithm on
multiple processors (or multiple machines). ÓÄÄe easiest means to parallelize the Gibbs algorithm
would be to draw in parallel from the conditionals‚Äîeach processor can be assigned a single condi-
tional among the p.Ui jU i ; X/ conditionals, and samples for Ui can be drawn on that processor.
Indeed, Geman and Geman (1984) suggested using this parallel Gibbs sampler (also called
the synchronous Gibbs sampler), which is described in more detail in Algorithm 5.4. It turns out
that making this simple change to the Gibbs algorithm‚Äîinstead of making an immediate update
to the state, the state is updated only after all conditionals have been sampled‚Äîactually breaks
the Gibbs algorithm. ÓÄÄe stationary distribution (if it exists) for this sampler is not necessarily
p.U jX/.
However, samplers similar in form have been used in practice (e.g., for the LDA model).
For more information about parallelizing MCMC sampling for LDA, see Newman et al. (2009).
110 5. SAMPLING METHODS
.
.
Input: Distribution p.U jX/ computable up to its normalization constant, proposal
distribution q.U 0
jU /.
Output: u drawn from p.
.
1: Initialize u with some value from its space of allowed values
2: repeat
3: Randomly sample u0
from q.U 0
ju/
4: Calculate an acceptance ratio
Àõ D min

1;
p.u0
jX/q.uju0
/
p.ujX/q.u0ju/

(5.16)
5: Draw a value Àõ0
from a uniform distribution over ≈í0; 1¬ç
6: if Àõ0
< Àõ then
7: u u0
8: end if
9: until Markov chain converged
10: return u
Algorithm 5.5: ÓÄÄe Metropolis-Hastings algorithm.
ÓÄÄe diÔ¨Éculty of parallelizing sampling can be a reason to choose an inference algorithm
which is more amenable to parallelization, such as variational inference. ÓÄÄis is discussed further
in Chapter 6.
Neiswanger et al. (2014) describe another method for parallelizing the Gibbs sampling
algorithm (or any MCMC algorithm, for that matter). With their approach, the data that require
inference is split into subsets, and Gibbs sampling is run separately on each subset to draw samples
from the target distribution. ÓÄÄen, once all of the parallel MCMC chains have completed drawing
samples, the samples are re-combined to get asymptotically exact samples.
5.3.4 SUMMARY
Gibbs sampling assumes a partition of the set of random variables of interest. It is an MCMC
method that draws samples from the target distribution by alternating between steps for drawing
samples from a set of conditional distributions for each of the random variables in each section
of the partition. Among MCMC methods, Gibbs sampling is the most common one in Bayesian
NLP.
5.4. THE METROPOLIS-HASTINGS ALGORITHM 111
5.4 THE METROPOLIS-HASTINGS ALGORITHM
ÓÄÄe Metropolis-Hastings algorithm (MH) is an MCMC sampling algorithm that uses a proposal
distribution to draw samples from the target distribution. Let  be the sample space of the target
distribution p.U jX/ (for this example, U can represent the latent variables in the model). ÓÄÄe
proposal distribution is then a function q.U 0
jU / 2    ! ≈í0; 1¬ç such that each u 2  deÔ¨Ånes
a distribution q.U 0
ju/. It is also assumed that sampling from q.U 0
ju/ for any u 2  is compu-
tationally eÔ¨Écient. ÓÄÄe target distribution is assumed to be computable up to its normalization
constant.
ÓÄÄe Metropolis-Hastings sampler is given in Algorithm 5.5. It begins by initializing the
state of interest with a random value, and then repeatedly samples from the underlying proposal
distribution. Since the proposal distribution can be quite diÔ¨Äerent from the the target distribution,
p.U jX/, there is a correction step following Equation 5.16 that determines whether or not to
accept the sample from the proposal distribution.
Just like the Gibbs sampler, the MH algorithm streams samples. Once the chain has con-
verged, one can continuously produce samples (which are not necessarily independent) by repeat-
ing the loop statements in the algorithm. At each step, u can be considered to be a sample from
the underlying distribution.
We mentioned earlier that the distribution p needs to be computable up to its normaliza-
tion constant. ÓÄÄis is true even though Equation 5.16 makes explicit use of the values of p; the
acceptance ratio always computes ratios between diÔ¨Äerent values of p, and therefore the normal-
ization constant is cancelled.
Accepting the proposed samples using the acceptance ratio pushes the sampler to explore
parts of the space that tend to have a higher probability according to the target distribution. ÓÄÄe
acceptance ratio is proportional to the ratio between the probability of the next state and the prob-
ability of the current state. ÓÄÄe larger the next state probability is, the larger the acceptance ratio
is, and therefore it is more likely to be accepted by the sampler. However, there is an important
correction ratio that is multiplied in: the ratio between the value of the proposed distribution in
the current state and the value of the proposed distribution in the next state. ÓÄÄis correction ratio
controls for the bias that the proposal distribution introduces by having higher probability mass
in certain parts of the state space over others (diÔ¨Äerent than those of the target distribution).
It is important to note that the support of the proposal distribution should subsume (or be
equal to) the support of the target distribution. ÓÄÄis ensures that the underlying Markov chain
is recurrent, and that all sample space will be explored if the sampler is run long enough. ÓÄÄe
additional property of detailed balance (see Section 5.3.2) is also important to ensure that the cor-
rectness of a given MCMC sampler is satisÔ¨Åed through the correction step using the acceptance
ratio.
112 5. SAMPLING METHODS
5.4.1 VARIANTS OF METROPOLIS-HASTINGS
Metropolis et al. (1953) originally developed the Metropolis algorithm, where the proposal dis-
tribution is assumed to be symmetric (i.e., q.U 0
jU / D q.U jU 0
/). In this case, the acceptance ratio
in Equation 5.16 consists only of the ratio between the distribution of interest at the next poten-
tial state and the current state. Hastings (1970) later generalized it to the case of an asymmetric
q, which yielded the Metropolis-Hastings algorithm.
Another speciÔ¨Åc case of the Metropolis-Hastings algorithm is the one in which q.U 0
jU / D
q.U 0
/, i.e., the proposal distribution does not depend on the previous state. In this case, the MH
algorithm reduces to an independence sampler.
An important variant of the Metropolis-Hastings given in Algorithm 5.5 is the component-
wise MH algorithm. ÓÄÄe component-wise MH algorithm is analogous to the Gibbs sampler, in
that it assumes a partition over the variables in the target distribution, and it repeatedly changes
the state of each of these variables using a collection of proposal distributions.
With the component-wise MH algorithm, one deÔ¨Ånes a set of proposal distributions
qi .U 0
jU /, where qi is such that it allocates a non-zero probability mass only to transitions in
the state space that keep U i intact, perhaps changing Ui . More formally, qi .U 0
i jU / > 0 only
if U 0
i D U i . ÓÄÄen, the component-wise MH algorithm alternates randomly or systematically,
each time sampling from qi and using the acceptance ratio:
Àõi D min

1;
p.u0
jX/qi .uju0
/
p.ujX/qi .u0ju/

; (5.17)
to reject or accept the new sample. Each acceptance changes only a single coordinate in U .
ÓÄÄe Gibbs algorithm can be viewed as a special case of the component-wise MH algorithm,
in which
qi .u0
ju/ D
(
p.u0
i ju i ; X/; if u0
i D u i
0 otherwise:
In this case, it holds that Àõi from Equation 5.17 satisÔ¨Åes:
Àõi D min

1;
p.u0
jX/qi .uju0
/
p.ujX/qi .u0ju/

D min

1;
p.u0
jX/p.ui ju0
i ; X/
p.ujX/p.u0
i ju i ; X/

D min

1;
p.u0
i jX/p.u0
i ju0
i ; X/p.ui ju0
i ; X/
p.u i jX/p.ui ju i ; X/p.u0
i ju i ; X/

D min

1;
p.u0
i jX/
p.u i jX/

D 1;
5.5. SLICE SAMPLING 113
where the last equality comes from the fact that qi changes only coordinate i in the state u, and
the transition between the second and third inequality comes from the chain rule applied on
p.ujX/ and p.u0
jX/. Since Àõi D 1 for i 2 f1; : : : ; pg, the MH sampler with the Gibbs proposal
distributions is never going to reject any change to the state. For this reason, no correction step
is needed for the Gibbs sampler, and it is removed.
5.5 SLICE SAMPLING
Slice sampling (Neal, 2003) is an MCMC method that is a speciÔ¨Åc, but interesting case of the
Gibbs sampler. In its most basic form, the slice sampler is designed to draw samples from a
univariate distribution.
To make the discussion concrete, assume that the target distribution is q.Àõ/ where Àõ is a
univariate random variable, that can appear, for example, in a hierarchical model as a hyperpa-
rameter (see Chapter 3). ÓÄÄis means that q.Àõ/ can stand for distributions such as p.Àõj; Z; X/,
or p.ÀõjZ; X/, if the parameters are collapsed.
ÓÄÄe univariate slice sampler relies on the observation that sampling from a distribution can
be done by uniformly sampling a point from the graph of the underlying distribution, and then
projecting this point to the x-axis. Here, ‚Äúgraph‚Äù refers to the area that is bounded by the curve
of the density function. ÓÄÄe x-axis ranges over the values that Àõ can receive, and the y-axis ranges
over the actual density values. Figure 5.1 demonstrates that idea.
ÓÄÄis idea of uniformly sampling the graph is where MCMC comes into play: instead of
directly sampling from the graph, which can be computationally diÔ¨Écult, the slice sampler is an
MCMC sampler for which the stationary distribution is a uniform distribution over the area (or
volume) under the graph of the distribution of interest. Intuitively, the slice sampler is a Gibbs
sampler that moves in straight lines along the x-axis and y-axis in a random walk.
More formally, the slice sampler introduces an auxiliary variable V 2 R to q.Àõ/, and then
deÔ¨Ånes two Gibbs sampling steps for changing the state .v; Àõ/. In the Ô¨Årst step, Àõ (given V D v)
is drawn uniformly from the set fÀõ0
jv  q.Àõ0
/g. ÓÄÄis in essence corresponds to a move along the
x-axis. ÓÄÄe second step is perhaps more intuitive, where we draw v (given Àõ) from a uniform
distribution over the set fvjv  q.Àõ/g, corresponding to a move along the y-axis.
5.5.1 AUXILIARY VARIABLE SAMPLING
Slice sampling is a speciÔ¨Åc case of a general approach to sampling called ‚Äúauxiliary variable sam-
pling.‚Äù ÓÄÄis approach assumes the existence of a variable V , an auxiliary variable, such that to-
gether with U , it induces a joint distribution p.U; V jX/ D p.U jX/p.V jU; X/. One can then
proceed by using one of the MCMC algorithms to sample from p.U; V jX/. ÓÄÄe samples from V
are eventually ignored, and the samples from U are used as samples from the target distribution
p.U jX/.
ÓÄÄe choice of an auxiliary variable is not simple and depends on the model at hand; the
selection should ensure that the MCMC sampler will tend to explore areas with higher probability
114 5. SAMPLING METHODS
0.15
0.10
0.05
0.00
-5 0 5
Density
u
1, u=-1
3, u=2
2
4
Figure 5.1: A demonstration of slice sampling for a univariate variable U . ÓÄÄe density of U is given.
ÓÄÄe purple line (1) denotes the Ô¨Årst sample we begin with, for which u D 1. We then sample a
point across line 1 from a uniform distribution. Once we choose that point, we consider the black
line (2), which intersects line 1 at that point. We uniformly sample a point on line 2, which yields the
second sample, u D 2. ÓÄÄen we sample a random point on the blue line (3) that intersects line 2 at the
new point, u D 2. We continue with that process of choosing points along vertically and horizontally
intersecting lines (the red line, 4, intersects line 3). ÓÄÄis amounts to a random walk on the graph of
the density function of U .
mass. If Gibbs sampling is used in conjunction with the auxiliary variable, one should make sure
that the conditionals p.V jU; X/ and p.U jV; X/ can be sampled from eÔ¨Éciently.
ÓÄÄere is one interesting connection between explicit sampling (when the parameters of the
model are sampled) and auxiliary variable methods. ÓÄÄe parameters themselves can be thought of
as an auxiliary variable for the ‚Äúblocked posterior,‚Äù since they introduce conditional independence
assumptions between the latent structures that one is interested in sampling, and therefore, make
sampling computationally easier.
5.5. SLICE SAMPLING 115
5.5.2 THE USE OF SLICE SAMPLING AND AUXILIARY VARIABLE
SAMPLING IN NLP
Slice sampling for a univariate continuous distribution is especially used in NLP for hyperpa-
rameter inference. For example, Johnson and Goldwater (2009) used slice sampling to sample
the hyperparameters of a Pitman-Yor process, corresponding to the concentration and discount
values. ÓÄÄe slice sampler was used in conjunction with a vague Gamma prior placed on top of
these hyperparameters. With a vague prior placed on the hyperparameters, slice sampling be-
haves almost as a search procedure for Ô¨Ånding the best hyperparameter setting. ÓÄÄis method can
be compared, for example, to empirical Bayes in Section 4.3; other recent examples of using slice
sampling for hyperparameter inference include Lindsey et al. (2012).
One of the thorough uses of auxiliary variable sampling in NLP was done by Blunsom
and Cohn (2010a), who introduced an auxiliary variable sampler for synchronous grammars. ÓÄÄis
sampler obtains synchronous derivations given a pair of sentences. ÓÄÄe goal was to obtain such
derivations faster than a na√Øve dynamic programming algorithm with time complexity, which is
cubic in both the source sentence length and the target sentence length.
ÓÄÄe sampler by Blunsom and Cohn (2010a) introduces an auxiliary continuous variable for
each possible span in a synchronous derivation. ÓÄÄen, when sampling a derivation given this set of
continuous variables, the dynamic programming algorithm prunes away any span in the chart that
has a probability lower than the corresponding value of the auxiliary variable. Blunsom and Cohn
had to introduce a correction step, similar to the one that exists in Metropolis-Hastings, in order
to make their sampler correct. ÓÄÄe two alternative MCMC methods to auxiliary variable sampling
that they discuss in their paper are reported to either mix very slowly (na√Øve Gibbs sampling) or
are computationally ineÔ¨Écient (collapsed Gibbs sampling). See Section 5.7 for more information
about the convergence of MCMC algorithms.
ÓÄÄe slicer sampler of Blunsom and Cohn (2010a) led to a better BLEU score for translating
Chinese to English, and also led to a higher log-likelihood of the model, compared to the na√Øve
Gibbs sampling approach by Blunsom et al. (2009a). ÓÄÄis could be partially attributed to the fact
that the authors discovered that the Gibbs sampler is more sensitive to initialization. For more
information, see Section 5.7. ÓÄÄe authors also discovered that the Gibbs sampler tends to be
trapped in modes of the distribution more often than the auxiliary variable sampler.
Van Gael et al. (2008) designed an inference algorithm for inÔ¨Ånite hidden Markov models
(Section 8.1.1) that combines dynamic programming with slice sampling (they called it a ‚Äúbeam
sampler‚Äù). ÓÄÄey tested their inference algorithm for predicting text from Alice in Wonderland. Still,
their beam sampler did not have better predictive power on this problem than a Gibbs sampling
algorithm. However, for other non-NLP and artiÔ¨Åcial data problems, they discovered that their
beam sampler mixes faster than a Gibbs sampling algorithm.
Bouchard-c√¥t√© et al. (2009) describe an auxiliary-variable sampler with the goal of prun-
ing when running a dynamic programming algorithm, such as the inside-outside algorithm for
PCFGs (Chapter 8). ÓÄÄeir algorithm works by traversing the space of auxiliary variables, which
116 5. SAMPLING METHODS
are binary vectors indexed by spans and syntactic categories in the sentence, where each element
in the vector denotes for each constituent whether it is pruned or not. At each point, once such
binary vector is sampled, and the inside-outside algorithm is run while pruning the constituents
according to the vector (therefore it runs much faster). Finally, all expectations computed from all
of these steps are averaged together to achieve an approximate version of the expectations. ÓÄÄis
process is also related to Monte Carlo integration (Section 5.10).
5.6 SIMULATED ANNEALING
Simulated annealing is a method for biasing an MCMC sampler so that it will gradually focus on
areas in the state space that consist of most of the probability mass of the distribution. It therefore
can be used as a decoding method to Ô¨Ånd the maximum a posteriori solution for a given posterior
distribution.
In its simplest form, simulated annealing explores the state space by starting with a high
temperature, which corresponds to an exploratory phase of searching in the state space; as the
sampler converges, the temperature is turned down, so that the search is more focused on the area
the sampler has reached at that point.
If the target distribution is p.U jX/, then simulated annealing will instead strive to sample
from the distribution:
p1=Tt .U jX/
Z.Tt /
(5.18)
where Z.Tt / is a normalization constant, integrating or summing p1=Tt .U jX/ over U . ÓÄÄe value
Tt corresponds to the temperature, which starts at a high temperature, and slowly decreases to 1 as
the iteration in the sampler, denoted t, increases. If one is interested in using simulated annealing
for optimization, i.e., Ô¨Ånding the posterior mode, then one can even decrease the temperature
to 0, at which point the re-normalized posterior from Equation 5.18 concentrates most of its
probability mass on a single point in the sample space.
For example, with Gibbs sampling, simulated annealing can be done by exponentiating the
conditional distributions by 1=Tt and renormalizing, while increasing t at each iteration of the
Gibbs sampler. For the Metropolis-Hastings algorithm, one needs to change the acceptance ratio
so that it exponentiates p.U jX/ by 1
Tt
. ÓÄÄere will be no need to compute Z.Tt / in this case, since
it is cancelled in the acceptance ratio that appears in Equation 5.16.
5.7 CONVERGENCE OF MCMC ALGORITHMS
Every MCMC algorithm execution goes through a period of ‚Äúburn-in.‚Äù In that phase, the Markov
chain has not stabilized yet (or has not mixed), and the samples drawn are not drawn from the ac-
tual posterior. After the burn-in period, the Markov chain converges to the posterior distribution,
at which point the samples being drawn are (dependent) samples from the posterior.
5.7. CONVERGENCE OF MCMC ALGORITHMS 117
Characterizing the number of iterations before reaching convergence is generally a chal-
lenging problem with MCMC algorithms and speciÔ¨Åcally with Bayesian NLP models. ÓÄÄere is
some theory of convergence that provides the upper bounds on the number of iterations based on
spectral decomposition of the transition matrix (Robert and Casella, 2005)‚Äîsee Section 5.8‚Äîbut
this theory is diÔ¨Écult to apply to practical problems in NLP.
Most of the approaches to testing convergence are empirical, and based on heuristics. Some
of the approaches include:
‚Ä¢ Visual inspection. In the case of a single univariate parameter being sampled, one can
manually inspect a traceplot that plots the value of the sampled parameter vs. the iteration
number of the sampler. If we observe that the chain gets ‚Äústuck‚Äù in a certain range of the
parameters, and then moves to another range, stays there for a while, and continuously
moves between these ranges, this is an indication that the sampler has not mixed.
In NLP, the parameters are clearly multidimensional, and it is not always the case that we
are sampling a continuous variable. In this case, a scalar function of the structure or multi-
dimensional vector being sampled can be calculated at each iteration, and plotted instead.
ÓÄÄis will provide a uni-directional indication about the mixing of the chain: if the pattern
mentioned above is observed, this is an indication that the sampler has not mixed.
One can also plot the mean of some scalar function 1
t
Pt
iD1 f .u.i/
/

against t, where t is
the sampler iteration and u.i/
is the sample drawn at iteration i. ÓÄÄis mean value should
eventually plateau (by the law of large numbers), and if it has not, then that is an indication
the sampler has not mixed. ÓÄÄe scalar function, for example, can be the log-likelihood.
‚Ä¢ Validation on a development set. In parallel to running the MCMC sampler, one could
make predictions on a small annotated development set, if such a set exists for this end. In
the explicit MCMC sampling setting, the parameters can be used to make predictions on
this development set, and the sampler can be stopped when performance stops improving
(performance is measured here according to the evaluation metric relevant to the problem
at hand). If a collapsed setting is used, then one can extract a point estimate based on the
current state of the sampler, and use it again to make predictions on a development set.
Note that with this approach, we are not checking for the convergence of the sampler on
the ‚Äútrue posterior,‚Äù but instead optimize our sampler so that it operates in a part of the
state space that works well with the Ô¨Ånal evaluation metric.
‚Ä¢ Testing autocorrelation. When the target distribution is deÔ¨Åned over real values, one can
use the autocorrelation to test for the diagonsis of an MCMC algorithm. Denote p./ as
the target distribution. ÓÄÄe autocorrelation with lag k is deÔ¨Åned as:
k D
PT k
tD1 .t /.tCk /
PT k
tD1 .t /2
; (5.19)
118 5. SAMPLING METHODS
where  D 1
T
PT
tD1 t , t is the tth sample in the chain and T is the total number of sam-
ples. ÓÄÄe numerator in Equation 5.19 corresponds to an estimated covariance term, and the
denominator to an estimated variance term. Autocorrelation tests rely on the idea that if the
MCMC sampler has reached the stationary distribution, the autocorrelation value should
be small as k increases. ÓÄÄus, an indication of slow mixing or lack of convergence is large
autocorrelation values even for relatively large k.
‚Ä¢ Other tests. ÓÄÄe Geweke test (Geweke, 1992) is a well-known method for checking
whether an MCMC algorithm has converged. It works by splitting a chain of samples into
two parts after an assumed burn-in period; these two parts are then tested to see whether
they are similar to each other. If indeed the chain has reached the stationary state, then these
two parts should be similar. ÓÄÄe test is performed using a modiÔ¨Åcation of the so-called z-test
(Upton and Cook, 2014), and the score used to compare the two parts of the chain is called
the Geweke z-score. Another test that is widely used for MCMC convergence diagnosis is
the Raftery-Lewis test. It is a good Ô¨Åt in cases where the target distribution is deÔ¨Åned over
real values. It works by thresholding all elements in the chain against a certain quantile q,
thus binarizing the chain into a sequence of 1s and 0s. ÓÄÄe test then proceeds by estimating
transition probabilities between these binary values, and uses these transition probabilities
to assess convergence. For more details, see Raftery and Lewis (1992).
A valid criticism of the use of MCMC algorithms in NLP is that the algorithms are often
used without a good veriÔ¨Åcation of chain convergence. Since MCMC algorithms can be quite
expensive in the context of Bayesian NLP, they are often run for a Ô¨Åxed number of iterations
that are limited by the amount of time allotted for an empirical evaluation. ÓÄÄis leads to high
sensitivity of the reported results to starting conditions of the Markov chain, or even worse‚Äîa
result that is based on samples that are not drawn from the true posterior. In this case, the use of
MCMC algorithms is similar to a random search.
It is therefore encouraged to be more careful with Bayesian models, and monitor the con-
vergence of MCMC algorithms. When MCMC algorithms are too slow to converge (an issue
that can arise often with Bayesian NLP models), one should consider switching to a diÔ¨Äerent
approximate inference algorithm such as variational inference (Chapter 6), where convergence
can be more easily assessed.
5.8 MARKOV CHAIN: BASIC THEORY
It is beyond the scope of this book to provide a full account of the theory behind Markov chains
and MCMC methods. For a thorough investigation of Markov chain theory, see Robert and
Casella (2005). Instead, we oÔ¨Äer an informal exposition of the core ideas behind these methods.
ÓÄÄe Ô¨Årst thing to note is that MCMC methods can be thought of as mechanisms to traverse the
sample space, transitioning from one state to another at each iteration. With explicit posterior
5.8. MARKOV CHAIN: BASIC THEORY 119
inference, for example, the search space includes pairs of parameters and latent structures for each
observation: .; z.1/
; : : : ; z.n/
/.
We denote the sample space by , and for simplicity, we assume it is Ô¨Ånite (clearly, it is
not Ô¨Ånite with explicit posterior inference, when the parameters are continuous, or even with
collapsed sampling, where the number of possible latent structures for a given observation is
potentially inÔ¨Ånite). ÓÄÄerefore,  can be enumerated as fs1; : : : ; sN g where N D jj.
A homogeneous Markov chain is determined by a transition matrix T (or a ‚Äútransition ker-
nel‚Äù), such that Tij denotes a probability of transition from si to state sj according to this Markov
chain. ÓÄÄe matrix T , therefore, is a stochastic matrix, with non-negative elements where each
column sums to 1:
N
X
j D1
Tij D 1 8i 2 f1; : : : ; N g: (5.20)
A non-homogeneous Markov chain would have a transition kernel per time step instead of
a single T ; all samplers and algorithms in this chapter are considered to be homogeneous chains.
ÓÄÄere is an important algebraic property of Markov chains with respect to T . Let  be some
distribution over , i.e., a vector such that i  0 and
P
i i D 1. In this case, multiplying T by
 (on the left)‚Äîi.e., computing T ‚Äîyields a distribution as well. To see this, consider that if
w D T then:
N
X
j D1
wj D
N
X
j D1
N
X
iD1
i Tij D
N
X
iD1
i 
0
@
N
X
j D1
Tij
1
A
‚Äû ∆í‚Äö ‚Ä¶
D1
D
N
X
iD1
i D 1;
because of Equation 5.20. ÓÄÄerefore, w is a distribution over the state space . ÓÄÄe distribution
w is not an arbitrary distribution. It is the distribution that results from taking a single step using
the Markov chain starting with the initial distribution  over states. More generally, T k
for an
integer k  0 gives the distribution over  after k steps in the Markov chain. If we let .t/
be the
distribution over  at time step t, then this means that:
.tC1/
D .t/
T D .0/
T tC1
: (5.21)
A key concept in Markov chains is that of a stationary distribution. A stationary distribution
is a distribution such that if we use it to initiate the chain (i.e., use it as .0
/), then the probabil-
ities over the next state will still distribute according to the stationary distribution. Under some
regularity conditions (irreducibility and aperiodicity, see Robert and Casella (2005) for more in-
formation), such a stationary distribution exists and is unique. In addition, under these regularity
conditions, no matter what the initial distribution is, the Markov chain will eventually mix and
arrive at the stationary distribution.
120 5. SAMPLING METHODS
According to Equation 5.21, this means that if  is the stationary distribution, then
T D ; (5.22)
i.e.,  is the eigenvector associated with eigenvalue 1. (ÓÄÄis implicitly means that if T has a
stationary distribution, then it has eigenvalue of 1 to begin with, which is also going to be the
largest eigenvalue.)
ÓÄÄis basic theory serves as the foundation for the proofs of samplers such as the Gibbs
sampler and the Metropolis-Hastings algorithm. In these proofs, two key steps are taken:
‚Ä¢ Proving that the chain induced by the sampler is such that it converges to a stationary
distribution (i.e., satisÔ¨Åes the basic regularity conditions).
‚Ä¢ Proving that the target distribution we are interested in sampling from is the stationary dis-
tribution of the Markov chain induced by the sampler (i.e., that it satisÔ¨Åes Equation 5.22).
5.9 SAMPLING ALGORITHMS NOT IN THE MCMC REALM
Markov chain Monte Carlo methods are not the only way to draw samples from a target distri-
bution such as the posterior. MCMC methods are mostly necessary when it is possible to only
calculate the target distribution up to its normalization constant. When it is possible to evaluate
the target distribution, including the normalization constant, but it is diÔ¨Écult to sample from the
distribution for some algorithmic or computational reason, methods such as rejection sampling
can be useful and more eÔ¨Écient than MCMC methods.
We next describe rejection sampling (also known as the accept-reject algorithm), where
we assume that the target distribution is p.U jX/. ÓÄÄe rejection sampling algorithm assumes the
existence of a proposal distribution q.U / from which sampling is computationally feasible. ÓÄÄis
proposal distribution should satisfy:
p.U jX/  Mq.U /;
for some known M > 0‚Äîsince M is directly used and evaluated in the rejection sampling algo-
rithm. It is also assumed that for any u, one can calculate p.ujX/ (including its normalization
constant). ÓÄÄen, in order to sample from p.U jX/, the procedure in Algorithm 5.6 is followed.
It can be shown that the probability of accepting y in each iteration is 1=M. ÓÄÄerefore,
rejection sampling is not practical when M is very large. ÓÄÄis problem is especially severe with
distributions that are deÔ¨Åned over high dimensional data.¬≤
¬≤ÓÄÄis problem is partially tackled by adaptive rejection sampling. Adaptive rejection sampling is used when multiple samples
u.1/; : : : ; u.m/ are needed from a distribution p.u/. It works by progressively tightening the proposal distribution q.u/
around the target distribution p.u/. See Robert and Casella (2005) for more details. Examples of using adaptive rejection
sampling in Bayesian NLP are quite rare, but see for example Carter et al. (2012) and Dymetman et al. (2012) for its use in
NLP.
5.9. SAMPLING ALGORITHMS NOT IN THE MCMC REALM 121
0.5
0.4
0.3
0.2
0.1
0.0
-5 0 5
Density
u
Reject Region
Distributions
Mq(U)
p(U)
Accept Region
Figure 5.2: ÓÄÄe plots of Mq.U / (the envelope) and p.U jX/ (distribution of interest) for rejection
sampling. Plot adapted from Andrieu et al. (2003).
ÓÄÄe intuition behind rejection sampling is best explained graphically for the univariate case.
Consider Figure 5.2. We see that Mq.u/ serves as an envelope surrounding the target distribution
p.u/.
To proceed with drawing a sample from p.u/, we can repeatedly sample points from this
envelope deÔ¨Åned by Mq.u/, until we happen to hit a point which is within the graph of p.u/.
Indeed, to sample points from the envelope, we sample a point from q.u/; this limits us to a
point on the x-axis that corresponds to a point in the sample space. Now, we can proceed by just
inspecting the line that stretches from the 0 y-axis coordinate to Mq.u/ and draw a uniform
point on that line. If it falls below the graph of p.u/, then the sampler managed to draw a sample
from p.u/. If it does not fall below the graph of p.u/, the process needs to be repeated.
In its general form, the main challenge behind the use of a rejection sampler is Ô¨Ånding a
bounding distribution q.U / with its constant M. However, there is a speciÔ¨Åc case of rejection
sampling in which these quantities can be easily identiÔ¨Åed. Consider the case in which one is in-
terested in sampling from a more restricted subspace of the sample space . Assume the existence
122 5. SAMPLING METHODS
.
.
Input: Distributions p.U jX/ and q.U /, a sampler for a distribution q.U /, and a constant
M.
Output: u, a sample from p.U jX/.
.
1: Set accept to false
2: repeat
3: Set u to be a sample from q.U /
4: Draw Àõ 2 ≈í0; 1¬ç from a uniform distribution.
5: if Àõ  .p.ujX/=Mq.u// then
6: Set accept to true.
7: end if
8: until accept equals true
9: return u
Algorithm 5.6: ÓÄÄe rejection sampling algorithm.
of p.u/, and that the target distribution has the following form:
p0
.u/ D
p.u/I.u 2 A/
Z
;
where A   and I.u 2 A/ is the indicator function that equals 1 if u 2 A and 0 otherwise, and
Z is a normalization constant that integrates or sums p.u/ over A. If p.u/ can be computed for
each u 2  and eÔ¨Éciently sampled from, and the membership query u 2 A can be accomplished
eÔ¨Éciently for any u 2 , then rejection sampling can be used to sample from p0
.u/, with a pro-
posal distribution p.u/. In this case, M D 1, and in order to proceed with sampling from p0
.u/,
one has to sample from p.u/ until u 2 A.
Cohen and Johnson (2013) use rejection sampling in this way in order to restrict Bayesian
estimation of PCFGs to tight PCFGs, i.e., PCFGs for which their normalization constant (sum-
ming over all possible trees according to the grammar) sums to 1. Rejection sampling was used
in conjunction with a Dirichlet prior. ÓÄÄe Dirichlet prior was sampled, and then the resulting
rule probabilities were inspected to see whether they were tight. If they were tight, these rule
probabilities were accepted.
Inverse Transform Sampling: Another non-MCMC method for sampling is inverse transform
sampling (ITS). For a real-valued random variable X, the ITS method assumes that for a given
u 2 R, we can identify the largest x such that F.x/  u, where F is the CDF of X (see Sec-
tion 1.2.1). ÓÄÄen, ITS works by sampling u from a uniform distribution over ≈í0; 1¬ç, and then
returning that largest x.
5.10. MONTE CARLO INTEGRATION 123
ÓÄÄis inverse transform sampling method applies to sampling from multinomial distribu-
tions. In order to sample from such a distribution, we can apply the inverse transform sampling
method on a random variable X that maps each event in the multinomial distribution to a unique
integer between 1 and n, where n is the number of events for the multinomial.
A na√Øve implementation of ITS for the multinomial distribution will require linear time
in n for each sample, where n is the number of events in the multinomial. ÓÄÄere is actually a
simple way to speed this up to O.log n/ per sample from that multinomial, with a preprocessing
step with asymptotic complexity O.n/. Once we map each event to an integer between 1 and n
with a random variable X, we calculate the vector of numbers Àõj D F.j/ D
Pj
iD1 p.X D i/ for
j 2 f1; : : : ; ng and set Àõ0 D 0. Now, in order to use the ITS, we draw a uniform variable u, and
then apply a logarithmic-time binary search on the array represented by the vector .Àõ0; : : : ; Àõn/
to Ô¨Ånd a j such that u 2 ≈íÀõj 1; Àõj ¬ç. We then return the multinomial event associated with index
j .
5.10 MONTE CARLO INTEGRATION
Monte Carlo integration is a method to compute expectations of the form I.f / D Ep.U /≈íf .U /¬ç
for some function f of the variables in a target distribution p.U /. It was one of the original
motivations behind MC sampling methods. MC integration, in its simple form, relies on the
observation that if u.1/
; : : : ; u.M/
is a stream of samples from the target distribution, then this
expectation can be approximated as:
I.f / D Ep.U /≈íf .U /¬ç 
1
M
M
X
iD1
f .u.i/
/: (5.23)
ÓÄÄis approximation is valid because of the law of large numbers, which states that as M !
1, the sum on the right-hand-side of Equation 5.23 will converge to the desired expectation on
the left-hand-side.
Importance Sampling: Importance sampling takes the idea in Equation 5.23, and suggests a way
to approximate the expectation I.f / by sampling using a proposal distribution q.U /. ÓÄÄerefore,
importance sampling can be used when it is not easy to sample from p (but it is possible to
calculate its value). Additionally, when choosing a speciÔ¨Åc proposal distribution under certain
circumstances, importance sampling is more eÔ¨Écient than perfect Monte Carlo integration (i.e.,
when estimating I.f / using samples from p.U /); this means the approximate integral tends to
converge with fewer samples to I.f /.
Importance sampling relies on the following simple identity that holds for any distribution
q.U / such that q.u/ D 0 only if p.u/ D 0:
124 5. SAMPLING METHODS
I.f / D Ep.U /≈íf .U /¬ç D Eq.U /

f .U / 
p.U /
q.U /

: (5.24)
Equation 5.24 is true, because the expectation operator folds a weighted sum/integration
operation using q.U /, which in conjunction with the term p.U /
q.U /
leads to a re-weighting of the
sum/integration operation using p.U /.
ÓÄÄe implication of this is that I.f / can be approximated as:
I.f / 
1
M
M
X
iD1
f .u.i/
/ 
p.u.i/
/
q.u.i//
D O
I.f /;
where u.i/
for i 2 f1; : : : ; Mg are samples from q.U /.
As mentioned above, certain choices of q.U / are preferable over others. One measure for
the eÔ¨Éciency in the use of samples drawn from the proposal distribution is the variance of the
quantity f .U /  p.U /
q.U /
with respect to q.U /. ÓÄÄis variance can be shown to be minimized when
sampling from q
.U / such that:
q
.u/ D
jf .u/jp.u/
R
u jf .u/jp.u/du
;
where integration can be replaced by sum, if u is discrete. q
itself is often hard to calculate or
sample from. ÓÄÄe reason it is optimal is related to the fact that it places large probability mass
where both the magnitude of f and the mass/density of p are large (as opposed to just selecting
a region with high probability mass according to p, but potentially insigniÔ¨Åcant small values of
f ). We want to sample points in the space that balance between being highly probable and also
giving dominant values to f .
Going back to Equation 5.23, the samples drawn from p to estimate the integral can be
generated using any MC method presented in this chapter, including MCMC methods. ÓÄÄis
means, for example, that we can repeatedly draw samples u.i/
from the state space using a Gibbs
sampler, and use them to estimate an integral of a speciÔ¨Åc function that we are interested in.
5.11 DISCUSSION
We will now brieÔ¨Çy discuss some additional topics about sampling methods and their use in NLP.
5.11.1 COMPUTABILITY OF DISTRIBUTION VS. SAMPLING
We have seen that with MCMC that we do not need to be able to compute a probability dis-
tribution for a given assignment for the random variables in order to draw samples from it. For
5.11. DISCUSSION 125
example, with MH sampling, all we need is to be able to compute the probability distribution up
to a multilpicative constant. Still, being able to compute the probability distribution for every ran-
dom variable assignment does not imply that it is easy to sample from it. ÓÄÄis can be shown more
formally, for example, by reducing the problem of Ô¨Ånding a satisfying assignment for a uniquely
satisÔ¨Åable logical formula to the problem of sampling from a distribution that is fully calculable.
More speciÔ¨Åcally, the best well-known algorithm for Ô¨Ånding the unique satisfying assign-
ment for the above problem is exponential in its number of variables. Given such a formula,
one can easily deÔ¨Åne a distribution p.x/ over assignments to the variables such that p.x/ D 1 if
and only if x is the satisfying formula. ÓÄÄe distribution p.x/ can be calculated in linear time in
the number of variables used. However, sampling from p.x/ is equivalent to Ô¨Ånding a uniquely
satisfying Boolean assignment.
5.11.2 NESTED MCMC SAMPLING
MCMC methods can be used in conjunction with each other. For example, one can use the
Metropolis-within-Gibbs sampler, which is a Gibbs sampling algorithm where the sampling from
the conditionals (or at least a few of them) takes a single Metropolis step instead of directly
sampling from the conditional. In this case, there is no need to draw samples at each Gibbs step
until the burn-in period has ended, as it is instead suÔ¨Écient to take a single MH step to show that
the sampler will theoretically converge to draw samples from the target distribution. See Robert
and Casella (2005) for more details. Metropolis-within-Gibbs has been used for various problems
in Bayesian NLP (Johnson et al., 2007b).
5.11.3 RUNTIME OF MCMC SAMPLERS
ÓÄÄe runtime of MCMC methods depends on two factors that have a trade-oÔ¨Ä between them: the
number of iterations it takes for the Markov chain to mix and the runtime of drawing samples
from the proposal distributions or the conditional distributions (in the case of Gibbs sampling).
In general, if possible, it is a good practice to run in a collapsed setting, and not sample variables
that are not of interest for prediction (though there is evidence that samplers in the collapsed
setting can actually run for longer periods of time than an explicit sampler; see, for example, Gao
and Johnson (2008)).
Similarly, MCMC methods should converge in fewer iterations in the blocked setting com-
pared to the pointwise setting. ÓÄÄere is a trade-oÔ¨Ä here, since the blocked setting can make the
sampler more expensive per iteration (requiring, for example, a dynamic programming algorithm
to sample a whole latent structure for a given example), while the total number of iterations will
be smaller than with a pointwise sampler. Empirically, it is often the case that blocked samplers
have a shorter total running time.
126 5. SAMPLING METHODS
Table 5.1: A list of Monte Carlo methods and the components they require in order to operate and
sample from the posterior p.U jX/. ÓÄÄe / symbol denotes that we need to be able to calculate a
quantity only up to a normalization constant.
Sampler Need to Sample From Need to Calculate
Gibbs sampler p(Ui|U-i, X ) none
Gibbs, operator view proportional to p(U|X ) in operator neighborhood {f1, ... fM}
MH sampler q(U '|U) Œ± p(U|X )
Independence sampler q(U) Œ± p(U|X )
Rejection sampler q(U) s.t. p(U|X) ‚â§ Mq(U) M, p(U|X )
Slice sampler q(Œ±|V ) and q(V|Œ±) level sets
Importance sampler q(U) q(U) and p(U|X )
‚àù
‚àù
5.11.4 PARTICLE FILTERING
Particle Ô¨Åltering, sometimes referred to as a sequential Monte Carlo method, is a sampling tech-
nique used to sequentially sample latent states based on observations. With particle Ô¨Åltering,
we assume a sequential model with Z D .Z1; : : : ; Zm/ being a sequence of latent states and
X D .X1; : : : ; Xm/ being a sequence of observations. ÓÄÄe independence assumptions are iden-
tical to the ones we make with hidden Markov models: Xi is independent of all other variables
given Zi , and Zi is independent of all Zj for j < i 1 given Zi 1.
ÓÄÄe model, therefore, has the following structure, which is identical to the structure of a
bigram hidden Markov model (see Section 8.1):
p.Z1; : : : ; Zm; X1; : : : ; Xm/ D p.Z1/p.X1jZ1/
m
Y
iD2
p.Zi jZi 1/p.Xi jZi /: (5.25)
ÓÄÄe goal of particle Ô¨Åltering is to approximate the distribution p.Zi jX1 D x1; : : : ; Xi D
xi /‚Äîi.e., to predict the latent state at position i in the sequence, conditioned on the observations
up to that point. ÓÄÄis means that we are interested in sampling Zi from p.Zi jX1 D x1; : : : ; Xi D
xi / for i 2 f1; : : : mg. Particle Ô¨Åltering approaches this problem through the use of a sequence of
importance sampling steps. ÓÄÄe distribution p is assumed to be known.
First, particle Ô¨Åltering samples M ‚Äúparticles‚Äù from the distribution p.Z1jX1/. ÓÄÄis distri-
bution can be derived using a simple application of Bayes‚Äô rule on p.X1jZ1/ in tandem with the
distribution p.Z1/, both of which are components of the model in Equation 5.25. ÓÄÄis leads to
a set of particles z.i/
1 for i 2 f1; : : : ; Mg.
In the general case, particle Ô¨Åltering samples M particles corresponding to Zj in the j th
step,. ÓÄÄese M particles are used to approximate the distribution p.Zj jX1; : : : ; Xj /‚Äîeach par-
5.12. CONCLUSION AND SUMMARY 127
ticle z.i/
j for i 2 f1; : : : ; Mg is assigned a weight Àáj;i , and the distribution p.Zj jX1; : : : ; Xj / is
approximated as:
p.Zj D zjX1 D x1; : : : ; Xj D xj / 
M
X
iD1
I.z.i/
j D z/Àáj;i : (5.26)
Particle Ô¨Åltering relies on a hierarchical construction of each distribution at each step
j . Bayes‚Äô rule is applied for predicting a state conditioned on previous observations for j 2
f1; : : : ; mg:
p.Zj jX1; : : : ; Xj / /
X
z
p.Zj 1 D zjX1; : : : ; Xj 1/p.Zj jZj 1 D z/p.Xj jZj /:
ÓÄÄe quantity above exactly equals Ep.Zj 1jX1;:::;Xj 1/

p.Zj jZj 1 D z/p.Xj jZj /

. We
can therefore use importance sampling (see Section 5.10) to sample M particles from
p.Zj 1jX1; : : : ; Xj 1/, and for each such draw z, draw z.i/
j from p.Zj jZj 1 D z/, and set its
weight Àáj;i to be proportional to p.Xj D xj jZj D z.i/
j /. ÓÄÄis leads to the approximation in Equa-
tion 5.26, which can be used in the next step of the particle Ô¨Åltering algorithm.
Particle Ô¨Åltering was used by Levy et al. (2009) to describe a model for incremental parsing.
ÓÄÄe motivation is psycholinguistic: the authors were interested in modeling human comprehen-
sion of language. ÓÄÄe authors claim, based on prior work, that there is much evidence that shows
that humans process language incrementally, and therefore it is beneÔ¨Åcial to model the probability
of a partial syntactic derivation conditioned on a preÔ¨Åx of a sentence. In the notation above, par-
tial derivations are modeled whereas the latent random variables Zi and Xi denote the words in a
sentence, and the integer m denotes the length of the sentence. Levy et al.‚Äôs incremental parser was
especially good in modeling the eÔ¨Äect of human memory limitations in sentence comprehension.
Yang and Eisenstein (2013) also developed a sequential Monte Carlo method for normaliz-
ing tweets into English. ÓÄÄe particles they maintain correspond to normalized tweets in English.
ÓÄÄe (non-Bayesian) model they used was composed of a conditional log-linear model that models
the distribution over tweets given an English sentence, and a model over English sentences. ÓÄÄese
two models are multiplied together to get a joint distribution over tweets and English sentences.
5.12 CONCLUSION AND SUMMARY
Monte Carlo methods are an important machinery in Bayesian NLP. Most often they are used to
sample either a set of parameters for the model, which can be followed by point estimation (see
Chapter 4) or to directly sample the structures to be predicted. ÓÄÄe samples are drawn from the
posterior distribution.
An important family of Monte Carlo methods is the Markov chain Monte Carlo meth-
ods, which are based on traversing the sample space using a Markov chain, and converging to a
128 5. SAMPLING METHODS
stationary distribution that is identical to the target distribution such as the posterior distribu-
tion. ÓÄÄey are often used in Bayesian NLP because they only require being able to compute the
posterior distribution up to a normalization constant; this is natural in the Bayesian setting, in
which the posterior is proportional to the joint model distribution. ÓÄÄe joint distribution can be
easily calculated in most cases, but computing the normalization constant for turning it into the
posterior can be intractable. Table 5.1 describes a summary of the sampling algorithms that were
mentioned in this chapter.
In various sections in this chapter (such as in Example 5.1), we gave a detailed account
of how to derive speciÔ¨Åc samplers for speciÔ¨Åc models. We derived the sampler from the basic
principles that need to be followed for the sampler family, such as the Gibbs algorithm. It is im-
portant to note that often NLP researchers do not follow such detailed derivations, and instead
leave much of the derivation to intuition. ÓÄÄis intuition is developed over time, after heavy use
of speciÔ¨Åc families of sampling algorithms; similar intuitions, develop for algorithms such as the
expectation-maximization algorithm. However, such intuition can often also be misleading, es-
pecially with respect to the Ô¨Åner details of the sampling algorithm. For researchers who just begin
to use sampling algorithms, it is highly recommended to derive sampling algorithms from basic
principles, at least until they develop more sound intuition about the sampling algorithms they
use.
5.13. EXERCISES 129
5.13 EXERCISES
5.1. Consider the Bayesian LDA model, with its graphical model in Figure 2.1. Construct a
Gibbs sampler for it, that alternates between sampling the topic distributions .i/
, the
topic parameters Àá and then word topics z.i/
j . ÓÄÄe index i here ranges over N documents.
You can also use generative story 2.1.
5.2. Construct an example of a target distribution to sample from, and a proposal distribution,
such that the Metropolis-Hastings has a very low rejection rate, but actually mixes very
slowly.
5.3. Let T be a transition matrix, such as the one described in Section 5.8. We say that T
satisÔ¨Åes the detailed balance condition with distribution  if:
i Tij D j Tji;
for all i and j. Show that when the detailed balance condition is satisÔ¨Åed,  is the sta-
tionary distribution. Is the reverse true as well?
5.4. ÓÄÄe following two questions prove the correctness of a Gibbs sampler for a simple model.
(ÓÄÄe exercise is based on Section 3 in Casella and George (1992).) Consider a probability
distribution p.X; Y / over two binary random variables X; Y , with the following proba-
bility table:
X
values 0 1
Y
0 p1 p2
1 p3 p4
such that
P4
iD1 pi D 1 and pi  0 for i 2 f1; : : : ; 4g.
Write down two matrices Ayjx and Axjy (in terms of pi ), both of size 2  2 such that:
≈íAyjx¬çij D p.Y D ijX D j/
≈íAxjy¬çij D p.X D ijY D j/
where i and j ranges over f0; 1g.
5.5. Assume that we are interested in sampling from p.X/. Assume that we are sampling
a chain x0 ! y1 ! x1 ! y2 : : : using the matrices Axjy and Ayjx from the previous
question. Compute the transition probability of p.xi jxi 1/, marginalizing yi , and write it
down as a transition matrix Axjx0 . Compute the eigenvector of Axjx0 , which is associated
with eigenvalue 1 (by solving Axjx0 D  with respect to ) and show that  is the
marginal distribution p.X/.
130 5. SAMPLING METHODS
5.6. ÓÄÄe Gibbs sampler is a correct sampler: given the conditional distributions of the target
distribution, running it will converge to the target distribution. For two random variables
X and Y , this means that the conditional distributions p.X j Y / and p.Y j X/ uniquely
identify the joint distribution p.X; Y /. Prove that analytically (i.e., show that p.X; Y /
can be expressed in terms of the conditional distributions). Hint: can you express p.X/
(or p.Y /) in terms of the conditionals?
C H A P T E R 6
Variational Inference
In the previous chapter, we described some of the core algorithms used for drawing samples
from the posterior, or more generally, from a probability distribution. In this chapter, we consider
another approach to approximate inference‚Äîvariational inference.
Variational inference treats the problem of identifying the posterior as an optimization prob-
lem. When this optimization problem is solved, the output is an approximate version of the poste-
rior distribution. ÓÄÄis means that the objective function that variational inference aims to optimize
is a function over a family of distributions. ÓÄÄe reason this is an approximate inference is that this
family of distributions is usually not inclusive of the true posterior, and makes strong assumptions
about the form of the posterior distribution.
ÓÄÄe term ‚Äúvariational‚Äù here refers to concepts from mathematical analysis (such as the cal-
culus of variations) which focus on the maximization and minimization of functionals (map-
pings from a set of functions to real numbers). ÓÄÄis kind of analysis has been used frequently
in physics (e.g., quantum mechanics). Very commonly, it is used in the context of minimizing
energy through a functional that describes the state of physical elements.
Section 6.1 begins the discussion of variational inference in this chapter, by describing the
basic variational bound used in variational inference. We then discuss mean-Ô¨Åeld variational in-
ference, the main type of variational inference used in Bayesian NLP (Sections 6.2‚Äì6.3). We
continue with a discussion of empirical Bayes estimation with variational approximations (Sec-
tion 6.4). In the next section (Section 6.5), we discuss various topics related to variational inference
in Bayesian NLP, covering topics such as initialization of variational inference algorithms, con-
vergence diagnosis, variational inference decoding, the relationship between variational inference
and KL minimization, and Ô¨Ånally, online variational inference. We conclude with a summary
(Section 6.6).
6.1 VARIATIONAL BOUND ON MARGINAL
LOG-LIKELIHOOD
Consider a typical scenario in which the observations are represented by the random variables
X.1/
; : : : ; X.n/
. ÓÄÄese observations are (deterministic or probabilistic) functions of the latent
structure Z.1/
; : : : ; Z.n/
. ÓÄÄese latent structures are the targets for prediction.
On top of the latent structure and the observations, there is a prior p.jÀõ/ over the param-
eters  such that Àõ 2 A is the hyperparameter. ÓÄÄis prior is a top-level prior (Section 3.5), but
132 6. VARIATIONAL INFERENCE
we later address the same scenario when parameters are drawn for each observation and latent
structure. ÓÄÄe joint probability distribution for this typical scenario is:
p.; Z; XjÀõ/
D p.; Z.1/
; : : : ; Z.n/
; X.1/
; : : : ; X.n/
jÀõ/ D p.jÀõ/
n
Y
iD1
p.Z.i/
j/p.X.i/
jZ.i/
; /
!
:
As mentioned in Section 3.1.2, in order to compute the posterior, one needs to compute
the following marginalization constant:
p.x.1/
; : : : ; x.n/
jÀõ/ D
Z

X
z.1/;:::;z.n/
p.jÀõ/
n
Y
iD1
p.z.i/
j/p.x.i/
jz.i/
; /
!
d:
ÓÄÄe computation of this integral-sum combination is intractable, because of the coupling
between the integral and the sum over a potentially exponential space. ÓÄÄis intractability is prob-
lematic even when the chosen prior is a conjugate prior, but choosing a conjugate prior is still
important, because it can make algorithms, such as variational EM, simpler (see Section 3.1.2).
ÓÄÄe computation of this marginal distribution is not just helpful in directly computing
the posterior (when dividing the joint distribution by the marginal distribution). In addition, the
likelihood of the observed data can be derived from the values of this marginal distribution (called
‚Äúthe evidence‚Äù) for Ô¨Åxed values of the observations.
ÓÄÄere is a partial remedy to this issue, if we consider the log-likelihood of the data. ÓÄÄis
remedy is obtained by approximating the log-likelihood, instead of having to compute it indirectly,
which is intractable, as mentioned earlier.
Let q.; Z/ D q.; Z.1/
; : : : ; Z.n/
/ be some distribution over the parameters and the latent
structure. Consider the following inequalities:
log p.XjÀõ/ D
log
0
@
Z

X
z.1/;:::;z.n/
q.; z.1/
; : : : ; z.n/
/ 
p.jÀõ/
Qn
iD1 p.z.i/
j/p.x.i/
jz.i/
; /

q.; z.1/; : : : ; z.n//
!
d
1
A(6.1)

Z

X
z.1/;:::;z.n/
q.; z.1/
; : : : ; z.n/
/  log
p.jÀõ/
Qn
iD1 p.z.i/
j/p.x.i/
jz.i/
; /

q.; z.1/; : : : ; z.n//
!
d (6.2)
D Eq
"
log
p.jÀõ/
Qn
iD1 p.Z.i/
j/p.x.i/
jZ.i/
; /

q.; Z/
!Àá
Àá
Àá
Àá
Àá
Àõ
#
(6.3)
D F.q; x.1/
; : : : ; x.n/
jÀõ/:
6.1. VARIATIONAL BOUND ON MARGINAL LOG-LIKELIHOOD 133
Equation 6.1 is the result of multiplying and dividing p.XjÀõ/ by the distribution q.; Z/,
then marginalizing over the parameters and latent variables‚Äîafter expressing p.XjÀõ/ through
the full joint distribution with a sum over the latent variables. ÓÄÄis by itself has no eÔ¨Äect on the
log-likelihood. Equation 6.2 is the result of applying Jensen‚Äôs inequality (see Appendix A), which
switches between the order of the sum and integration in the application of the log function. Last,
Equation 6.3 is the result of folding the sum and integration into an expectation with respect to
q.; Z/. ÓÄÄe bound F is sometimes referred to as ‚ÄúELBO,‚Äù short for ‚Äúevidence lower bound.‚Äù
Consider what happens when the distribution q actually equals the posterior, i.e.,
q.; Z/ D p.; ZjX; Àõ/:
In that case, it can be shown, using of the deÔ¨Ånition of conditional probability, that the
inequality in Equation 6.1 becomes an equality, and
log p.XjÀõ/ D F.q; x.1/
; : : : ; x.n/
jÀõ/:
Since, in that case, the bound F.q; x.1/
; : : : ; x.n/
jÀõ/ is tight, this means that the poste-
rior distribution maximizes the lower bound, and F.q; x.1/
; : : : ; x.n/
jÀõ/ equals the marginal log-
likelihood. Because Ô¨Ånding the posterior for Bayesian NLP problems, in the general case, is in-
tractable, it follows that optimizing F.q; x.1/
; : : : ; x.n/
jÀõ/ with respect to q, in the general case,
is intractable (if we could do it, we would have been able to Ô¨Ånd the true posterior).
ÓÄÄis is where variational inference plays an important role in removing the intractability
of this optimization problem. Variational inference implies that this optimization problem is still
being solved, but with a compromise: we maximize the bound with respect to a certain family of
distributions Q. ÓÄÄe family Q is chosen so that, at the very least, Ô¨Ånding a local maximum for the
following maximization problem is tractable:
max
q2Q
F.q; x.1/
; : : : ; x.n/
jÀõ/: (6.4)
Since the true posterior usually does not belong to Q, this is an approximate method.
Clearly, the closer that one of the distributions in Q is to the true posterior (or, the more ex-
pressive the distributions in Q are), the more accurate this approximate solution is.
ÓÄÄe approximate posterior in the solution is not only due to the restriction inherent in
choosing Q. Another reason is that even with ‚Äútractable‚Äù Q, the above optimization problem
is non-convex, and therefore, there is inherent diÔ¨Éculty in Ô¨Ånding the global maximum for it.
Mean-Ô¨Åeld variational inference is one algorithm that treats this issue by applying coordinate
ascent on a factorized approximate posterior family (Section 6.3).¬π ÓÄÄe maximization problem,
however, stays non-convex.
¬πCoordinate ascent is a method for maximizing the value of a real-valued function f .y1; : : : ; yn/. It works by iterating through
the diÔ¨Äerent arguments of f , at each point maximizing f with respect to a speciÔ¨Åc variable yi while holding yj for j ¬§ i
Ô¨Åxed with values from the previous steps. ÓÄÄe value of the assignment to yi at each step is guaranteed to increase the value of
f , and under some circumstances, converge to the maximizer of f .
134 6. VARIATIONAL INFERENCE
6.2 MEAN-FIELD APPROXIMATION
Mean-Ô¨Åeld approximation deÔ¨Ånes an approximate posterior family which has a factorized form.
Just as in the case of Gibbs sampling (Chapter 5), mean-Ô¨Åeld variational inference requires a
partition of the latent variables in the model (the most common choice for such a partition is a
separation of the parameters from the latent structures being predicted).
Once the latent variables Z.1/
; : : : ; Z.n/
and  are carved up into p random variables,
U1; : : : ; Up, the factorized form of the approximate posterior is such that it assumes indepen-
dence between each of Ui . More speciÔ¨Åcally, each member q of the family Q is assumed to have
the form:
q.U1; : : : ; Up/ D
p
Y
iD1
q.Ui /: (6.5)
See Section 5.2.1 for more discussion about the ways to partition the latent variables of a
Bayesian model.
One of the most natural approximate posterior families Q is one that decouples between
the parameters  and the random variables of each of the latent structures Z D .Z.1/
; : : : ; Z.n/
/.
With a top-level prior placement, a typical approximate posterior has the form:
q.; Z/ D q./q.Z/:
ÓÄÄerefore, Q is the set of all distributions such that  and the latent structures Z.i/
(for i D
1; : : : ; n) are all independent of each other. ÓÄÄis approximation for the posterior family belongs
to the family of mean-Ô¨Åeld approximations.
When a new set of parameters is drawn for each example (Section 3.5), leading to a set
of parameters .1/
; : : : ; .n/
, it is more typical to use a mean-Ô¨Åeld approximation that assumes
independence between each of .i/
and Z.i/
such that the approximate posterior has the form:
n
Y
iD1
q.z.i/
/
!

n
Y
iD1
q..i/
/
!
: (6.6)
ÓÄÄe approximation in Equation 6.6 is the most na√Øve mean-Ô¨Åeld approximation, where
all variables in the model are assumed to be independent of each other (parameters and latent
structures).
It is important to note, however, that this na√Øve mean-Ô¨Åeld approximation in this case is
not very useful by itself. When considering the bound in Equation 6.3, with this factorization
of the posterior family, we actually end up with n separate optimization sub-problems for each
observation, and these sub-problems do not interact with each other. Interaction between these
sub-problems, however, can be introduced by following a variational expectation-maximization
algorithm, that estimates joint hyperparameters for all .i/
, in the style of empirical Bayes. For a
6.3. MEAN-FIELD VARIATIONAL INFERENCE ALGORITHM 135
discussion of empirical Bayes, see Section 4.3. For more details about variational EM, see Sec-
tion 6.4.
ÓÄÄe factorized posterior family q does not have to follow the above na√Øve factorization, and
more structured approximations can certainly be used. ÓÄÄe way that the posterior is factorized
depends on the modeler choice for carving up the variables on which the inference is done. In
addition, each factor in q can either be parametric (by deÔ¨Åning ‚Äúvariational parameters‚Äù that con-
trol this factor) or can be left as nonparametric (see Section 1.5.1 for the diÔ¨Äerence between a
parametric model family and a nonparametric one). In many cases it can be shown that even when
a factor is left as nonparametric (to obtain a tighter approximation, without conÔ¨Åning the factor
to a special form), the factor that gives the tightest approximation actually has a parametric form.
See Section 6.3.1.
Mean-Ô¨Åeld methods actually originate in statistical physics. ÓÄÄe main motivation behind
them in statistical physics (or statistical mechanics) is to reduce the complexity of stochastically
modeling interactions between many physical elements by considering much simpler models.
Mean-Ô¨Åeld methods are now often used in machine learning, especially with inference for graph-
ical models (Wainwright and Jordan, 2008).
6.3 MEAN-FIELD VARIATIONAL INFERENCE
ALGORITHM
Finding the optimal solution, q (for Equation 6.3), even with a more constrained set of
distributions Q, can be computationally intractable in the general case. ÓÄÄe main problem is that
the functional F often leads to a non-convex optimization problem. ÓÄÄe optimization tools cur-
rently available for non-convex functions are quite limited for the problems that arise in Bayesian
NLP, and in most cases, these optimization algorithms come with a guarantee to converge to a
local maximum of the optimization problem in Equation 6.4.
Computational intractability here can be formally stated: for many problems (such as the
case with PCFGs, Cohen and Smith (2010a)), it can be shown that an algorithm for Ô¨Ånding the
global maximum of F can be used to solve an NP-hard decision problem.
ÓÄÄe family of variational inference algorithms Ô¨Ånds a local maximum for the optimization
problem in Equation 6.4. It does so by choosing an approximate posterior family such that the
computation of the evidence lower bound can be performed eÔ¨Éciently. For example, the family
of approximate posteriors can be made parametric so that both the evidence lower bound and
its gradient are computed eÔ¨Éciently. In that case, gradient-based optimization algorithms can be
used to Ô¨Ånd a local maximum for the bound.
Most commonly in Bayesian NLP, variational inference is used in the context of the mean-
Ô¨Åeld approximation mentioned in Section 6.2 with a coordinate ascent algorithm, where the ‚Äúco-
ordinates‚Äù being optimized, each in their own turn, correspond to each factor that appears in
the factorization in Equation 6.5. Algorithm 6.1 provides the skeleton for the coordinate ascent
mean-Ô¨Åeld variational inference algorithm.
136 6. VARIATIONAL INFERENCE
.
.
Input: Observed data x.1/
; : : : ; x.n/
, a partition of the latent variables into U1; : : : ; Up and
a set of possible distributions for U1; : : : ; Up: Q1; : : : ; Qp.
Output: Factorized approximate posterior q.U1; : : : ; Up/.
.
1: Initialize q
.Ui / from Qi for i D 1; : : : ; p
q
.U1; : : : ; Up/
Qp
iD1 q
.Ui /

2: repeat
3: for i 2 f1; : : : ; pg do
4: Set Q
D fq
.U1/g  fq
.Ui 1/g  : : :  Qi  fq
.UiC1/g  : : :  fq
.Up/g
5: q
.Ui / the factor q.Ui / in
arg max
q2Q
F.q; x.1/
; : : : ; x.n/
jÀõ/ (6.7)
6: end for
7: q
.U1; : : : ; Up/
Qp
iD1 q
.Ui /

8: until the bound F.q
; x.1/
; : : : ; x.n/
jÀõ/ converged
9: return q
Algorithm 6.1: ÓÄÄe mean-Ô¨Åeld variational inference algorithm. Its input are observations, a partition
of the random variables that the inference is done on, and a set of distribution families, one set per
element in the partition. ÓÄÄe algorithm then iterates (with iterator i) through the diÔ¨Äerent elements
in the partition, each time maximizing the variational bound for the observations with respect to Qi ,
while holding q
.Uj / for j ¬§ i Ô¨Åxed.
ÓÄÄe optimization problem in Equation 6.7 is not always easy to solve, but fortunately, the
solution has a rather general formula. It can be shown that q
.Ui / maximizing Equation 6.7
equals:
q
.Ui / D
exp Eq i
≈ílog p.X; U1; : : : ; Up/¬ç

Zi
; (6.8)
where q i is a distribution over U i deÔ¨Åned as:
q i .U1; : : : ; Ui 1; UiC1; : : : ; Up/ D
Y
j ¬§i
q.Uj /;
and Zi is a normalization constant that integrates or sums the numerator in Equation 6.8
with respect to Ui . For example, if Ui is discrete, then Zi D
P
u Eq i
≈ílog p.X; U1; : : : ; Ui D
6.3. MEAN-FIELD VARIATIONAL INFERENCE ALGORITHM 137
u; : : : ; Up/¬ç/¬ç. ÓÄÄis general derivation appears in detail in Bishop (2006), but in Section 6.3.1 we
derive a speciÔ¨Åc case of this formulation for the Dirichlet-Multinomial family.
ÓÄÄe following decisions need to be made by the modeler when actually implementing Al-
gorithm 6.1.
‚Ä¢ Partitioning of the latent variables ÓÄÄis issue is discussed at length in Chapter 5 and in
Section 6.2. ÓÄÄe decision that the modeler has to make with respect to this issue is how to
carve up the random variables into a set of random variables that have minimal interaction,
or that oÔ¨Äer some computational tractability for maximizing the variational bound with
respect to each of members of this set of random variables.
‚Ä¢ Choosing the parametrization of each factor (Qi ) Determining the parametrization of
each of the factors requires a balance between richness of the parametrization (so we are
able to get a tighter bound) and tractability (see below). It is often the case that even when
Qi is left nonparametric (or when it includes the set of all possible distributions over the
sample space of Ui ), the solution to the coordinate ascent step is actually a distribution from
a parametric family. Identifying this parametric family can be done as part of the derivation
of the variational EM algorithm, so that the variational distributions can be represented
computationally (and optimized with respect to their parameters, also referred to as ‚Äúvaria-
tional parameters‚Äù).
‚Ä¢ Optimizing the bound at each step of the coordinate ascent At each step of the mean-
Ô¨Åeld variational inference algorithm, we have to Ô¨Ånd the factor in q that maximizes the
variational bound while maintaining all other factors Ô¨Åxed according to their values from the
previous iterations. If the parametrization of each factor is chosen carefully, then sometimes
closed-form solutions for these mini-maximization problems are available (this is especially
true when the prior is conjugate to the likelihood). It is also often the case that a nested
optimization problem needs to be solved using optimization techniques such as gradient
descent or Newton‚Äôs method. Unfortunately, sometimes the nested optimization problem
itself is a non-convex optimization problem.
Recent work, such as the work by Kucukelbir et al. (2016), tries to minimize the decision
making which is not strictly related to modeling and data collection. ÓÄÄe work by Kucukelbir et
al. proposes an automatic variational inference algorithm, which uses automatic diÔ¨Äerentation,
integrated into the Stan programming language Carpenter et al. (2015).
6.3.1 DIRICHLET-MULTINOMIAL VARIATIONAL INFERENCE
ÓÄÄe following example demonstrates the decisions that the modeler has to make when deriving
a variational inference algorithm. We derive in this section a mean-Ô¨Åeld variational inference
algorithm which is commonly used for Dirichlet-Multinomial models.
Consider the case where the likelihood is a collection of multinomials, which is often the
case with NLP models such as probabilistic context-free grammars and hidden Markov models.
138 6. VARIATIONAL INFERENCE
In such a case, the model family is parametrized by  D .1
; : : : ; K
/ such that each k
is in the
probability simplex of dimension Nk 1 for some natural number Nk:
k
i  0 8k 2 f1; : : : ; Kg; 8i 2 f1; : : : ; Nkg
Nk
X
iD1
k
i D 1 8k 2 f1; : : : ; Kg:
For example, in the case of PCFGs, K would be the number of nonterminals in the gram-
mar, Nk would be the number of rules for each nonterminal and k
i would correspond to the
probability of the ith rule for the kth nonterminal. See Section 8.2 for more details about this
formulation. Let f k
i .x; z/ be a function that counts the times that event i from multinomial k
Ô¨Åres in .x; z/.
ÓÄÄe most common choice of a conjugate prior for this model is a product-of-Dirichlet
distribution, such that:
p.jÀõ/ /
K
Y
kD1
Nk
Y
iD1
.k
i /Àõk
i 1
;
with Àõ D .Àõ1
; : : : ; ÀõK
/ and Àõk
2 RNk such that Àõk
i  0 for all i and k.
Assuming a top-level prior, and with X.1/
; : : : ; X.n/
being the observed random variables
and Z.1/
; : : : ; Z.n/
being the latent structures, the likelihood is:
n
Y
j D1
p.x.j/
; z.j/
j/ D
n
Y
j D1
K
Y
kD1
Nk
Y
iD1
.k
i /f k
i .x.j /;z.j //
D
K
Y
kD1
Nk
Y
iD1
.k
i /
Pn
jD1 f k
i .x.j /;z.j //
;
with f k
i .x; z/ being the count of event i from multinomial k in the pair .x; z/. We denote in
short:
fk;i D
n
X
j D1
f k
i .x.j/
; z.j/
/:
We will be interested in mean-Ô¨Åeld variational inference such that q remains nonparametric
(for now), factorized into q./ and q.Z/. ÓÄÄat, in essence, is the tightest approximation one can
use, while assuming independent parameters and latent structures for the approximate posterior
6.3. MEAN-FIELD VARIATIONAL INFERENCE ALGORITHM 139
family. In that case, the functional F.q; x.1/
; : : : ; x.n/
jÀõ/, following Equation 6.3 (which gives
the bound on the marginal log-likelihood), looks like the following:
F.q; x.1/
; : : : ; x.n/
jÀõ/
D Eq
2
4log
0
@p.jÀõ/ 
K
Y
kD1
Nk
Y
iD1
.k
i /fk;i
1
A
3
5 Eq≈ílog q./¬ç Eq≈ílog q.Z/¬ç
D
K
X
kD1
Nk
X
iD1
Eq≈í.fk;i C Àõk
i 1/  log.k
i /¬ç C H.q.// C H.q.Z//;
where H.q.// denotes the entropy of the distribution q./, and H.q.Z// denotes the entropy
of the distribution q.Z/ (for the deÔ¨Ånition of entropy, see Appendix A).
If we consider Algorithm 6.1 for this case, then we iterate between stages: (a) assuming q./
is Ô¨Åxed, we optimize the bound in the above equation with respect to q.Z/ and (b) assuming q.Z/
is Ô¨Åxed, we optimize the bound in the above equation with respect to q./.
Assume the case where q./ is Ô¨Åxed. In that case, fk;i depends only on the latent assign-
ments z.1/
; : : : ; z.n/
and not on the parameters, and therefore it holds that:
F.q; x.1/
; : : : ; x.n/
jÀõ/ D
K
X
kD1
Nk
X
iD1
Eq≈í.fk;i C Àõk
i 1/  k
i ¬ç C H.q.Z// C const
D
K
X
kD1
Nk
X
iD1
Eq≈í k
i fk;i log A. /¬ç C H.q.Z// C const; (6.9)
with having the same vector structure like  and Àõ such that
k
i D Eq./≈ílog.k
i /¬ç;
and
log A. / D
X
z.1/
: : :
X
z.n/
exp
0
@
K
X
kD1
Nk
X
iD1
k
i fk;i
1
A :
Note that the term log A. / can be added to Equation 6.9 because it does not depend on the
latent structures, since we sum them out in this term. It does, however, depend on q./, but it is
assumed to be Ô¨Åxed. If we carefully consider Equation 6.9, we note that it denotes the negated KL-
divergence (Appendix A) between q.Z/ and a log-linear model over Z with suÔ¨Écient statistics
fk;i and parameters k
i . ÓÄÄerefore, when q./ is Ô¨Åxed, the functional F is maximized when
we choose q.Z/ to be a log-linear distribution with the suÔ¨Écient statistics fk;i and parameters
k
i D Eq./≈ílog.k
i /¬ç.
140 6. VARIATIONAL INFERENCE
ÓÄÄe meaning of this is that even though we a priori left q.Z/ to be in a nonparametric fam-
ily, we discovered that the tightest solution for it resides in a parametric family, and this family
has a very similar form to the likelihood (the main diÔ¨Äerence between the approximate posterior
family and the likelihood is that with the approximate posterior family we also require normal-
ization through log Z. / because does not necessarily represent a collection of multinomial
distributions).
What about the opposite case, i.e., when q.Z/ is Ô¨Åxed and q./ needs to be inferred? In
that case, it holds that:
F.q; x.1/
; : : : ; x.n/
jÀõ/ /
K
X
kD1
Nk
X
iD1

Eq≈ífk;i ¬ç C Àõk
i 1

 Eq./≈ílog k
i ¬ç H.q.//: (6.10)
If we carefully consider the equation above, we see that it is proportional to the KL-
divergence between q./ and a product of Dirichlet distributions (of the same form as the prior
family) with hyperparameters Àá D .Àá1
; : : : ; ÀáK
/ such that Àák
i D Eq≈ífk;i ¬ç C Àõk
i . ÓÄÄis is again a
case where we leave q./ nonparametric, and we discover that the tightest solution has a para-
metric form. In fact, not only is it parametric, it also has the same form as the prior family.
ÓÄÄe Ô¨Ånal variational inference algorithm looks like this:
‚Ä¢ Initialize in some way Àá D .Àá1
; : : : ; ÀáK
/.
‚Ä¢ Repeat until convergence:
 Compute q.z.1/
; : : : ; z.n/
/ as the log-linear model mentioned above with parameters
k
i D Eq./≈ílog.k
i /jÀá¬ç.
 Compute q./ as a product of Dirichlet distributions with hyperparameters Àák
i D
Eq≈ífk;i j ¬ç C Àõk
i .
Consider the computation of Eq./≈ílog.k
i /jÀá¬ç and Eq≈ífk;i j ¬ç. It is known that for a given
Dirichlet distribution, the expected log value of a single parameter can be expressed using the
digamma function, meaning that:
Eq./≈ílog.k
i /jÀá¬ç D ‚Ä∞.Àák
i / ‚Ä∞.
Nk
X
iD1
Àák
i /;
with ‚Ä∞ representing the digamma function. ÓÄÄe digamma function cannot be expressed analyti-
cally, but there are numerical recipes for Ô¨Ånding its value for a given parameter. See Appendix B
for more details about the digamma function and its relationship to the Dirichlet distribution.
On the other hand, computing Eq≈ífk;i j ¬ç can be done using an algorithm that heavily
depends on the structure of the likelihood function. For PCFGs, for example, this expectation can
6.3. MEAN-FIELD VARIATIONAL INFERENCE ALGORITHM 141
be computed using the inside-outside algorithm. For HMMs, it can be done using the forward-
backward algorithm. See Chapter 8 for more details. Note that this expectation is computed for
each observed example separately, i.e., we calculate Eq≈íf k
i .x.j/
; z.j/
/j ¬ç for j 2 f1; : : : ; ng and
then aggregate all of these counts to get Eq≈ífk;i j ¬ç.
Whenever we are simply interested in the posterior over q.Z/, the above two update steps
collapse to the following update rule for the variational parameters of q.Z/:
. k
i /new
‚Ä∞.Eq≈ífk;i j old
¬ç C Àõk
i / ‚Ä∞.
Nk
X
iD1
Eq≈ífk;i j old
¬ç C Àõk
i /: (6.11)
Note that for these updates, the variational parameters k
i need to be initialized Ô¨Årst. Most
often in the Bayesian NLP literature, when variational inference is used, the Ô¨Ånal update rule
forms such as the one above are described. ÓÄÄe log-linear model parametrized by k
i can be re-
parameterized using a new set of parameters k
i D exp. k
i / for all k and i. In this case, the update
becomes:
.k
i /new exp ‚Ä∞.Eq≈ífk;i jold
¬ç C Àõk
i /

exp

‚Ä∞.
PNk
iD1 Eq≈ífk;i jold¬ç C Àõk
i /
: (6.12)
Note that now we have a similar update to the EM algorithm, where we compute expected
counts, and in the M-step, we normalize them. ÓÄÄe main diÔ¨Äerence is that the counts are passed
through the Ô¨Ålter of the exp-digamma function, exp.‚Ä∞.x//. Figure 6.1 plots the exp-digamma
function and compares it against the function x 0:5. We can see that as x becomes larger, the
two functions get closer to each other. ÓÄÄe main diÔ¨Äerence between the two functions is that at
values smaller than 0:5, for which the exp-digamma function returns positive values which are
very close to 0, while x 0:5 returns negative values. ÓÄÄerefore, one way to interpret the update
Equation 6.12 is as the truncation of low expected counts during the E-step (lower than 0:5).
Higher counts are also subtracted a value of around 0:5, and the higher the count is in the E-step,
the less inÔ¨Çuential this decrease will be on the corresponding  parameter.
6.3.2 CONNECTION TO THE EXPECTATION-MAXIMIZATION
ALGORITHM
ÓÄÄe variational inference algorithm in the previous section, in spirit, resembles the expectation-
maximization (EM) algorithm of Dempster et al. (1977), which is set up in the frequentist setting.
ÓÄÄe goal of the EM algorithm is to estimate the parameters of a given model from incomplete
data.
ÓÄÄe EM algorithm iterates between two steps: the E-step, in which the posterior over the
latent structures is computed, and the M-step, in which a new set of parameters is computed,
142 6. VARIATIONAL INFERENCE
1.5
1.0
0.5
0.0
0.5
0.0 1.0 1.5 2.0
f(x)
x
Figure 6.1: A plot of the function f .x/ D exp.‚Ä∞.x//, the exp-digamma function (in the middle in
black) compared to the functions f .x/ D x (at the top in blue) and f .x/ D x 0:5 (at the bottom in
red). Adapted from Johnson (2007b).
until the marginal log-likelihood converges. It can be shown that the EM algorithm Ô¨Ånds a local
maximum of the marginal log-likelihood function. ÓÄÄe M-step is performed by maximizing the
expected log-likelihood of all variables in the model. ÓÄÄe expectation is taken with respect to a
product distribution: the product of the empirical distribution over the observed data and the pos-
terior induced in the E-step. For more detailed information about the expectation-maximization
algorithm, see Appendix A.
ÓÄÄere is actually a deeper connection between the EM algorithm and the variational in-
ference algorithm presented in Algorithm 6.1. ÓÄÄe variational inference algorithm reduces to the
EM algorithm when the inputs to the variational inference algorithm and the prior in the model
are chosen carefully.
Consider the case where the set of latent variables is partitioned into two random variables:
in terms of Algorithm 6.1, U1 corresponds to a random variable over the parameters, and U2
corresponds to a variable over the set of all latent structures in the model (usually, it would be
Z.1/
; : : : ; Z.n/
). Hence, the posterior has the form q.; Z/ D q./q.Z/.
6.4. EMPIRICAL BAYES WITH VARIATIONAL INFERENCE 143
Consider also Q1 to represent the set of all distributions that place their whole probability
mass on a single point on the parameter space. ÓÄÄis means that Q1 includes the set of all the
distributions q.j/ (parameterized by  2 ‚Äö) such that
q.j/ D
(
1; if  D 
0 otherwise;
Q2, on the other hand, remains nonparametric, and just includes the set of all possible
distributions over the latent structures. Last, the prior chosen in the model is chosen to be p./ D
c (for some constant c) for all  2 ‚Äö, i.e., a uniform non-informative prior (possibly improper).
ÓÄÄe functional F now in essence depends on the assignment  (selecting q.j/) and the
q.Z/. We will express this functional as:
F.q.Z/; ; x.1/
; : : : ; x.n/
/ D Eq.Z/
"
log
p.jÀõ/p.Z; X D .x.1/
; : : : ; x.n/
/j/
q.Z/
!#
:
If we assume a non-informative constant prior, then maximizing the bound with respect to
q.Z/ and  can be done while ignoring the prior:
F.q.Z/; ; x.1/
; : : : ; x.n/
/ / Eq.Z/
"
log
p.Z; X D .x.1/
; : : : ; x.n/
/j/
q.Z/
!#
:
ÓÄÄis functional is exactly the same bound that the expectation-maximization algorithm
maximizes. Maximizing the right-hand side with respect to q.Z/ while keeping  Ô¨Åxed yields
the posterior q.Z/ D p.ZjX D .x.1/
; : : : ; x.n/
/; /, which in turn yields the E-step in the EM
algorithm. On the other hand, maximizing the right-hand side with respect to  yields the M-
step‚Äîdoing so maximizes the bound with respect to the parameters, which keeps q.Z/ Ô¨Åxed.
See Appendix A for a derivation of the EM algorithm.
6.4 EMPIRICAL BAYES WITH VARIATIONAL INFERENCE
In the empirical Bayes setting (Section 4.3), parameters are drawn for each observed instance.
ÓÄÄere, the typical approach to mean-Ô¨Åeld variational inference would be to use an approximate
posterior family such that all latent structures and all parameter sets are independent of each other
(see Equation 6.6).
ÓÄÄe variational inference algorithm (Algorithm 6.1) in this case actually separately solves
each pair of problems for each instance i, Ô¨Ånding the posterior q..i/
/ and q.Z.i/
/. ÓÄÄerefore,
when using this kind of mean-Ô¨Åeld approximation, we require an additional estimation step,
which integrates all the solutions for these sub-problems into a re-estimation step of the prior.
144 6. VARIATIONAL INFERENCE
.
.
Input: Observed data x.1/
; : : : ; x.n/
, the bound F.q
; x.1/
; : : : ; x.n/
jÀõ/.
Output: Factorized approximate posteriors q..i/
/ and q.Z.i/
/ for i 2 f1; : : : ; ng and an
estimated hyperparameter Àõ.
.
1: Initialize Àõ0
2: repeat
3: Maximize F.q
; x.1/
; : : : ; x.n/
jÀõ0
/ with respect to q
4: using Algorithm 6.1 with factorization as in Equation 6.6
5: Àõ0
arg maxÀõ0 F.q
; x.1/
; : : : ; x.n/
jÀõ0
/
6: until the bound F.q
; x.1/
; : : : ; x.n/
jÀõ0
/ converges
7: return .Àõ0
; q
/
Algorithm 6.2: ÓÄÄe mean-Ô¨Åeld variational expectation-maximization algorithm (empirical Bayes).
ÓÄÄis is the main idea behind the variational EM algorithm. Variational EM is actually an
expectation-maximization algorithm, in which the hyperparameters for a prior family are esti-
mated based on data, and in which the E-step is an approximate E-step that Ô¨Ånds a posterior
based on a variational inference algorithm, such as the one introduced in Algorithm 6.1. ÓÄÄe
approximate posterior is identiÔ¨Åed over Z and , while the M-step maximizes the marginal log-
likelihood with respect to the hyperparameters.
ÓÄÄe variational EM algorithm, with mean-Ô¨Åeld variational inference for the E-step, is given
in Algorithm 6.2.
6.5 DISCUSSION
We turn now to a discussion about important issues regarding the variational inference algorithms
presented in this chapter‚Äîissues that have a crucial eÔ¨Äect on the performance of these algorithms,
but do not have well-formed theory.
6.5.1 INITIALIZATION OF THE INFERENCE ALGORITHMS
ÓÄÄe need to properly initialize the variational parameters in variational inference is a problem-
atic issue, mostly because it does not have a well-established theory, yet it has been shown that
initialization may greatly aÔ¨Äect the results when using variational inference.
For example, with the Dirichlet-multinomial family mean-Ô¨Åeld variational inference algo-
rithm in Section 6.3.1, one has to decide how to initialize Àá (or alternatively, if the algorithm is
started on the second step in the loop instead of the Ô¨Årst step, when computing the expectations
of the features fk;i , one would have to decide how to initialize the parameters of the log-linear
model q.z.1/
; : : : ; z.n/
/).
6.5. DISCUSSION 145
ÓÄÄe variational bound that F represents, for a general model, is a non-convex function
(with respect to the approximate posterior q), and therefore Algorithm 6.1 does not have any
guarantees in terms of converging to the global maximum of the variational bound. One approach
for tackling this issue is quite similar to the solution that is often used for the EM algorithm in
the form of random restarts. ÓÄÄe variational inference algorithm is run repetitively from diÔ¨Äerent
starting points, and we eventually choose the run that gives the maximal value to the variational
bound.
ÓÄÄis method will not necessarily lead to optimal results with respect to the evaluation metric
used. ÓÄÄe aim of maximizing the variational bound is to obtain higher log-likelihood for the
observed data. Log-likelihood here is used as a proxy for the underlying evaluation metric, such
as the parsing evaluation metric (Black et al., 1991) or part-of-speech accuracy. However, being
just a proxy, it does not fully correlate with the evaluation metric. Even if we were able to globally
maximize the log-likelihood, this problem would persist.
For this reason, random restarts, which aim at maximizing the variational bound, are some-
times replaced with a more speciÔ¨Åc initialization technique which is based on some intuition that
the modeler has about the relationship between the data and the model. For example, a common
technique for unsupervised dependency parsing, is to initialize EM (Klein and Manning, 2004)
or variational inference (Cohen et al., 2009) with parameters that tend to prefer attachments for
words that are close, in their position in the text, to each other. ÓÄÄis is a very useful bias for
unsupervised dependency parsing in general (Eisner and Smith, 2005, Spitkovsky et al., 2010).
Other initialization techniques include initialization based on a simpler model, sometimes
a model which induces a concave log-likelihood function (Gimpel and Smith, 2012). Many of
the techniques used to initialize EM for various models can also be used eÔ¨Äectively for variational
inference.
6.5.2 CONVERGENCE DIAGNOSIS
Checking for convergence of the variational inference algorithm (or more speciÔ¨Åcally, the bound
F.q
; x.1/
; : : : ; x.n
jÀõ/ in Algorithm 6.1 or Algorithm 6.2) is relatively easy, since all quantities
in the bound F are computable. However, it is important to note that unlike EM, variational
inference does not guarantee an increase in the log-likelihood of the data after each iteration.
While both EM and variational inference are coordinate ascent algorithms, EM Ô¨Ånds a local
maximum for the log-likelihood function, and variational inference Ô¨Ånds a local maximum for
the variational bound only. (Both algorithms use a similar bounding technique, based on Jensen‚Äôs
inequality, but the EM‚Äôs bound for the log-likelihood is tight because no assumptions are made
for the approximate posterior family. ÓÄÄis means that the bound for EM equals the log-likelihood
at its maximal value.)
146 6. VARIATIONAL INFERENCE
6.5.3 THE USE OF VARIATIONAL INFERENCE FOR DECODING
ÓÄÄere are several ways to use the output of variational inference and variational EM in order to
actually predict or estimate parameters. In the non-empirical-Bayes variational inference setting,
once q./ is estimated, this posterior can be summarized as a point estimate following the tech-
niques in Chapter 4. Afterward, decoding can proceed using this point estimate. In addition, one
can follow maximum a posteriori decoding directly using q.Z/ and identify
.z.1/
; : : : ; z.n/
/ D arg max
.z.1/;:::;z.n//
q.Z D .z.1/
; : : : ; z.n/
//:
A similar route can be followed in the empirical Bayesian setting, decoding z.i/
by com-
puting arg maxz q.Z.i/
D z/.
With variational EM, the hyperparameters Àõ that are being eventually estimated can be
used to get a summary for the parameter‚Äôs point estimate. For example, given these hyperpa-
rameters Àõ, one can use the mean value of the posterior over the parameters as a point estimate


D E≈íjÀõ¬ç D
Z

p.jÀõ/d;
or alternatively, 
D arg max p.jÀõ/ (corresponding to maximum a posteriori estimate). See
Chapter 4 for a discussion. If the hyperparameters Àõ have the same structure as the parameters
(i.e., for each hyperparameter in the ith coordinate, Àõi , maps directly to a parameter i ), then
the hyperparameters themselves can be used as a point estimate. ÓÄÄe hyperparameters may not
adhere, perhaps, to constraints on the parameter space (i.e., it could be the case that Àõ ‚Ä¶ ‚Äö), but
they often do yield weights, which can be used in decoding the underlying model.
Cohen and Smith (2010b) used this technique, and estimated the hyperparameters of a
collection of logistic normal distributions for grammar induction. ÓÄÄe Gaussian means were even-
tually used as parameters for a weighted grammar they used in decoding.
ÓÄÄe above approach is especially useful when there is a clear distinction between a training
set and a test set, and the Ô¨Ånal performance measures are reported on the test set, as opposed to
a setting in which inference is done on all of the observed data.
When this split between a training and test set exists, one can use a diÔ¨Äerent approach to the
problem of decoding with variational EM. Using the hyperparameters estimated from the training
data, an extra variational inference step can be taken on the test set, thus identifying the posterior
over latent structures for each of the training examples (using mean-Ô¨Åeld variational inference).
Based on these results, it is possible to follow the same route mentioned in the beginning of this
section, Ô¨Ånding the highest scoring structure according to each of the posteriors and using these
as the predicted structure.
6.5. DISCUSSION 147
6.5.4 VARIATIONAL INFERENCE AS KL DIVERGENCE MINIMIZATION
Consider Equation 6.3 again, restated below:
log p.XjÀõ/ D
D Eq
"
log
p.jÀõ/
Qn
iD1 p.Z.i/
j/p.x.i/
jZ.i/
; /

q.; Z/
!Àá
Àá
Àá
Àá
Àá
Àõ
#
D F.q; x.1/
; : : : ; x.n/
jÀõ/:
ÓÄÄe bound F actually denotes the Kullback-Leibler (KL) divergence (see Appendix A)
between q and the posterior. As mentioned in the beginning of this chapter, Ô¨Ånding an approx-
imate posterior is done by minimizing F. ÓÄÄerefore, minimization of the bound F corresponds
to Ô¨Ånding a posterior q from the family of posteriors Q which minimizes KL.q; p/.
KL divergence is not a symmetric function, and unfortunately, this minimization of the
KL divergence is done in the ‚Äúreverse direction‚Äù from what is desirable. In most, ‚Äúmore correct,‚Äù
KL divergence minimization problems (such as maximum likelihood estimation), the free dis-
tribution that is optimized should represent the second argument to the KL divergence, while
the ‚Äútrue‚Äù distribution (the true posterior, in the case of variational inference), should represent
the Ô¨Årst argument. In the reverse direction, minq KL.q; p/, one could Ô¨Ånd solutions that are not
necessarily meaningful. Still, with this approach, the KL divergence would get its minimum when
p D q (and then it would be 0), which is a desirable property.
A discussion regarding KL divergence minimization direction for variational inference,
with graphical models, is given by Koller and Friedman (2009).
6.5.5 ONLINE VARIATIONAL INFERENCE
Standard variational inference and variational EM algorithms work in a batch mode. ÓÄÄis means
that the available learning data is pre-determined, statistics are then computed from all datapoints
(E-step), and Ô¨Ånally an update to the parameters is made (M-step). Expectation-maximization
works in a similar way.
An alternative to these batch algorithms is online algorithms. With online algorithms, each
datapoint is Ô¨Årst processed, then followed by an update to the parameters. ÓÄÄe motivation behind
online algorithms is a scenario in which an ‚ÄúinÔ¨Ånite‚Äù stream of datapoints is fed to an algorithm,
and the inference algorithm needs to update its internal state. ÓÄÄis internal state can be used to
make predictions until more data in the inÔ¨Ånite stream arrives. ÓÄÄis setting is especially appeal-
ing for large-scale data and real-world applications, in which a statistical model is continuously
updated as more data is being presented to it (for example, from the web).
Variational inference and variational EM can be converted into an online algorithm as well,
relying on ideas from the literature regarding online EM (Capp√© and Moulines, 2009, Liang and
Klein, 2009, Neal and Hinton, 1998, Sato and Ishii, 2000). ÓÄÄe idea behind this conversion is
148 6. VARIATIONAL INFERENCE
to make an update to the parameters once the posterior of an example is computed. ÓÄÄe current
parameters are then interpolated (with some mixture coeÔ¨Écient  2 ≈í0; 1¬ç) using the statistics
computed for the example.
Batch algorithms and online algorithms, as described above are two extremes in a wide
spectrum of approaches. As a middle ground, one can use a so-called ‚Äúmini-batch‚Äù online algo-
rithm, in which several examples are processed at a time using the current set of parameters, and
only then followed by an update to the parameters.
Online variational inference that relies on such ideas has been used, for example, for the
LDA model (HoÔ¨Äman et al., 2010), as well as for unsupervised learning of syntax (Kwiatkowski
et al., 2012a). It has also been used for nonparametric models, such as the hierarchical Dirichlet
process (Wang et al., 2011). Such models are described in Chapter 7.
6.6 SUMMARY
Variational inference is just one of the workhorses used in Bayesian NLP inference. ÓÄÄe most
common variant of variational inference used in NLP is that of mean-Ô¨Åeld variational inference,
the main variant discussed in this chapter.
Variational expectation-maximization can be used in the empirical Bayes setting, using a
variational inference sub-routine for the E-step, while maximizing the variational bound with
respect to the hyperparameters in the M-step.
6.7. EXERCISES 149
6.7 EXERCISES
6.1. Consider the model in Example 5.1. Write down a mean-Ô¨Åeld variational inference al-
gorithm to infer p.jx.1/
; : : : ; x.n/
/.
6.2. Consider again the model in Example 5.1, only now change it such that there are multiple
parameter draws, and .1/
; : : : ; .n/
drawn for each example. Write down a mean-Ô¨Åeld
variational inference algorithm to infer p..1/
; : : : ; .n/
jx.1/
; : : : ; x.n/
/.
6.3. Show that Equation 6.8 is true. Also, show that Equation 6.10 is true.
6.4. Let 1; : : : ; K represent a set of K parameters, where K is Ô¨Åxed. In addition, let p.Xji /
represent a Ô¨Åxed distribution for a random variable X over sample space  such that
jj < 1. Assume that p.xji / ¬§ p.xjj / for any x 2  and i ¬§ j. DeÔ¨Åne the following
model, parametrized by :
p.Xj; 1; : : : ; K/ D
K
X
kD1
kp.Xjk/;
where
PK
kD1 k D 1 and k  0. ÓÄÄis is a mixture model, with mixture components that
are Ô¨Åxed.
What is the log-likelihood for n observations, x.1/
; : : : ; x.n/
, with respect to the param-
eters ? Under what conditions is the log-likelihood convex, if at all (with respect to
)?
6.5. Now assume that there is a symmetric Dirichlet prior over , hyperparametrized by Àõ >
0. Compute the marginal log-likelihood n observations, x.1/
; : : : ; x.n/
, integrating out
. Is this a convex function with respect to Àõ?
C H A P T E R 7
Nonparametric Priors
Consider a simple mixture model that deÔ¨Ånes a distribution over a Ô¨Åxed set of words. Each draw
from that mixture model corresponds to a draw of a cluster index (corresponding to a mixture
component) followed by a draw from a cluster-speciÔ¨Åc distribution over words. Each distribution
associated with a given cluster can be deÔ¨Åned so that it captures speciÔ¨Åc distributional properties
of the words in the vocabulary, or identiÔ¨Åes a speciÔ¨Åc category for the word. If the categories
are not pre-deÔ¨Åned, then the modeler is confronted with the problem of choosing the number
of clusters in the mixture model. On one hand, if there is not a suÔ¨Éciently large number of
components, it will be diÔ¨Écult to represent the range of possible categories; indeed, words that
are quite dissimiliar according to the desired categorization may end up in the same cluster. ÓÄÄe
opposite will happen if there are too many components in the model: much of the slack in the
number of clusters will be used to represent the noise in the data, and create overly Ô¨Åne-grained
clusters that should otherwise be merged together.
Ideally, we would like the number of clusters to grow as we increase the size of the vocabu-
lary and the observed text in the data. ÓÄÄis Ô¨Çexibility can be attained with nonparametric Bayesian
modeling. ÓÄÄe size of a nonparametric Bayesian model can potentially grow to be quite large (it
is unbounded), as a function of the number of data points n; but for any set of n data points, the
number of components inferred will always be Ô¨Ånite.
ÓÄÄe more general approach to nonparametric Bayesian modeling is to use a nonparametric
prior, often a stochastic process (roughly referring to a set of random variables indexed by an
inÔ¨Ånite, linearly ordered set, such as the integers), which provides a direct distribution over a
set of functions or a set of distributions, instead of a distribution over a set of parameters. A
typical example of this in Bayesian NLP would be the Dirichlet process. ÓÄÄe Dirichlet process
is a stochastic process that is often used as a nonparametric prior to deÔ¨Åne a distribution over
distributions. Each distribution drawn from the Dirichlet process can later be used to eventually
draw the observed data. For those who are completely unfamiliar with nonparametric Bayesian
modeling this may seem a bit vague, but a much more thorough discussion of the Dirichlet process
follows later in this chapter.¬π
Nonparametric priors are often generalizations of parametric priors, in which the number of
parameters is taken to inÔ¨Ånity for the speciÔ¨Åc parametric family. For example, the nonparametric
GriÔ¨Éths-Engen-McCloskey distribution can be thought of as a multinomial with an inÔ¨Ånite
number of components. ÓÄÄe Dirichlet process is the limit of a series of Dirichlet distributions.
¬πFor a description of a very direct relation between the k-means clustering algorithm and the Dirichlet process, see Kulis and
Jordan (2011).
152 7. NONPARAMETRIC PRIORS
A Gaussian process is a generalization of the multivariate Gaussian distribution, only instead of
draws from the Gaussian process being vectors indexed by a Ô¨Ånite set of coordinates, they are
indexed by a continuous value (which can be interpreted, for example, as a temporal axis).
ÓÄÄe area of Bayesian nonparametrics in the Statistics and Machine Learning literature is an
evolving and highly active area of research. New models, inference algorithms and applications are
frequently being developed in this area. Traditional parametric modeling is in a more stable state
compared to Bayesian nonparametrics, especially in natural language processing. It is therefore
diÔ¨Écult to comprhensively review the rich, cutting-edge literature on this topic; instead, the goal
of this chapter is to serve as a preliminary peek into the core technical ideas behind Bayesian
nonparametrics in NLP.
ÓÄÄe Dirichlet process plays a pivotal role in Bayesian nonparametrics for NLP, similar to the
pivotal role that the Dirichlet distribution plays in parametric Bayesian NLP modeling. ÓÄÄerefore,
this chapter focuses on the Dirichlet process, and as such has the following organization. We begin
by introducing the Dirichet process and its various representations in Section 7.1. We then show
how the Dirichlet process can be used in an nonparametric mixture model in Section 7.2. We
next show in Section 7.3 how hierarchical models can be constructed with the Dirichlet process
as the foundation for solving issues such as the selection of the number of topics for the latent
Dirichlet allocation model. Finally, we discuss the Pitman-Yor process, for which the Dirichlet
process is a speciÔ¨Åc case (Section 7.4), and then follow with a brief discussion of other stochastic
processes that are used in Bayesian nonparametrics.
7.1 THE DIRICHLET PROCESS: THREE VIEWS
ÓÄÄe Dirichlet process (Ferguson, 1973) deÔ¨Ånes a distribution over distributions. ÓÄÄe parameters
controlling a Dirichlet process are a concentration parameter (also called a strength parameter) s,
and a base distribution G0, which functions as the ‚Äúmean‚Äù of the Dirichlet process, in a manner
that will be described below. We denote the support of G0 by ‚Äö. We use ‚Äö because often G0 is a
function G0W ‚Äö ! ≈í0; 1¬ç such that ‚Äö deÔ¨Ånes some parameter space for a parametric model. Such
is the case, for example, with the Dirichet process mixture model (see Section 7.2).
Each draw G  DP.G0; s/ from such a Dirichlet process is a distribution with its support
being a discrete subset of the support of G0. Each draw G from the Dirichlet process satisÔ¨Åes the
following condition: for every Ô¨Ånite partition A1; : : : ; Ar of ‚Äö, with each Ai being measurable, it
holds that the random vector .G.A1/; : : : ; G.Ar // distributes as follows:
.G.A1/; : : : ; G.Ar //  Dirichlet.sG0.A1/; : : : ; sG0.Ar //:
ÓÄÄis property is both necessary and suÔ¨Écient, and actually serves as one of several equivalent
deÔ¨Ånitions of the Dirichlet process. ÓÄÄis deÔ¨Ånition also results in the fact that any draw G from
the Dirichlet process is actually a discrete distribution‚Äîi.e., any draw G is such that G./ D 0
for all  2 ‚Äö n ‚Äö0, where ‚Äö0 is a countable set. ÓÄÄe base distribution G0 functions as the ‚Äúmean,‚Äù
where the expected value E≈íG.A/¬ç (where expectation is taken with respect to G) equals G0.A/
7.1. THE DIRICHLET PROCESS: THREE VIEWS 153
.
.
Full stick .
.
1 Àá1
.
.
Àá1
.
.
1 Àá1 Àá2
.
.
Àá2
.
.
1 Àá1 Àá2 Àá3
.
.
Àá3
.
Figure 7.1: A graphical depiction of the stick breaking process. ÓÄÄe rectangles on the right (in blue)
are the actual probabilities associated with each element in the inÔ¨Ånite multinomial. ÓÄÄe process of
breaking the stick repeats ad inÔ¨Ånitum, leading to an inÔ¨Ånite vector of Àái variables. At each step, the
left part of the stick is broken into two pieces.
for any measurable set A. ÓÄÄe concentration parameter, on the other hand, controls the variance
in the Dirichlet process as follows:
Var.G.A// D
G0.A/.1 G0.A//
s C 1
:
ÓÄÄe larger s is, the closer draws G from the Dirichlet process to G0.
ÓÄÄe mathematical deÔ¨Ånition of the Dirichlet process above, as fundamental as it may be,
is not constructive.¬≤ In the next two sections we provide two other perspectives on the Dirichlet
process that are more constructive, and therefore are also more amenable to use in Bayesian NLP
with approximate inference algorithms.
7.1.1 THE STICK-BREAKING PROCESS
ÓÄÄe stick-breaking process (Figure 7.1) is a constructive representation of the Dirichlet process
due to Sethuraman (1994). To deÔ¨Åne the stick-breaking process, we Ô¨Årst need to deÔ¨Åne the
GriÔ¨Éths-Engen-McCloskey (GEM) distribution. Let k  Beta.1; s/ for k 2 f1; 2; : : :g, i.e., a
sequence of i.i.d. draws from the Beta distribution with hyperparameters .1; s/ for s > 0. (ÓÄÄe
Beta distribution deÔ¨Ånes a probability distribution over the interval ≈í0; 1¬ç.) DeÔ¨Åne
Àák D k
k 1
Y
j D1
.1 j /: (7.1)
In this case, the inÔ¨Ånite vector .Àá1; Àá2; : : :/ is said to be drawn from the GEM distribution
with concentration parameter s.
Draws from the GEM distribution can be thought of as draws of ‚ÄúinÔ¨Ånite multinomials,‚Äù
because the following is satisÔ¨Åed for every draw Àá from the GEM distribution:
¬≤By that we mean that it does not describe the Dirichlet process in a way amenable to model speciÔ¨Åcation, or inference.
154 7. NONPARAMETRIC PRIORS
.
.
Hyperparameters: G0, s.
Variables: Àái  0, i for i 2 f1; 2; : : :g.
Output: Distribution G over a discrete subset of the sample space of G0.
.
‚Ä¢ Draw Àá  GEM.s/.
1:
‚Ä¢ Draw 1; 2; : : :  G0.
2:
‚Ä¢ ÓÄÄe distribution G is deÔ¨Åned as:
3:
G./ D
1
X
kD1
ÀákI. D k/: (7.4)
Generative Story 7.1: ÓÄÄe generative story for drawing a distribution from the Dirichlet process.
Àák  0 8k 2 f1; 2; : : :g; (7.2)
1
X
kD1
Àák D 1: (7.3)
Since the components of this inÔ¨Ånite multinomial sum to one, the components must decay
quickly, so that the tail
P1
iDm Àái goes to 0 as m goes to inÔ¨Ånity. ÓÄÄis is guaranteed by the iterative
process through which a unit ‚Äústick‚Äù is broken to pieces, each time further breaking the residual
part of the stick (Equation 7.1).
Based on the stick-breaking representation, a draw of a distribution G  DP.G0; s/ from
the Dirichet process can be represented using generative story 7.1. First, an inÔ¨Ånite non-negative
vector that sums to 1 is drawn from the GEM distribution (line 1). ÓÄÄis corresponds to an inÔ¨Ånite
multinomial over ‚Äúatoms,‚Äù which are drawn next from the base distribution (line 2). Each atom
is associated with an index in the inÔ¨Ånite vector (line 3). ÓÄÄis means that every draw from the
Dirichlet process has the structure in Equation 7.4 for some atoms k and coeÔ¨Écients Àá.
ÓÄÄe stick-breaking process also demonstrates again the role of the s parameter. ÓÄÄe larger
s is, the less rapidly the parts of the stick decay to zero (in their length, taken with respect to the
unit length). ÓÄÄis is evident in Equation 7.1, when considering the fact that k  Beta.1; s/. ÓÄÄe
larger s is, the smaller each k is compared to 1 k, and therefore, more of the probability mass
is preserved for other pieces in the stick.
7.1. THE DIRICHLET PROCESS: THREE VIEWS 155
ÓÄÄe stick-breaking process also demonstrates that any draw from the Dirichlet process is
a distribution deÔ¨Åned over a discrete (or Ô¨Ånite) support. ÓÄÄe distribution in Equation 7.4 assigns
positive weight to a discrete subset of the sample space of G0.
7.1.2 THE CHINESE RESTAURANT PROCESS
As implied by the stick-breaking representation in Section 7.1.1, one way to view the Dirichlet
process is as draws of inÔ¨Ånite multinomial distributions over a countable set of atoms, drawn from
the base distribution G0. ÓÄÄe Chinese Restaurant Process (CRP) representation of the Dirichlet
process describes how to draw samples from these multinomials.
ÓÄÄe CRP by itself deÔ¨Ånes a prior over partitions. Similarly to the Dirichlet process, it is
controlled by a concentration hyperparameter s. Each y.i/
drawn from the CRP is associated with
an integer index (denoting a cluster assignment). ÓÄÄe CRP then deÔ¨Ånes a distribution for drawing
y.i/
conditioned on the draws y.1/
; : : : ; y.i 1/
. ÓÄÄe Ô¨Årst draw is assigned with index 1. Now, let
y.1/
; : : : ; y.i 1/
be draws from the CRP and deÔ¨Åne y
i D maxj i 1 y.j /
(i.e., y
i is the cluster
with largest index that was assigned to the Ô¨Årst i 1 instances in the samples from the CRP).
ÓÄÄen, in order to draw y.i/
conditioned on y.1/
; : : : ; y.i 1/
we use the following distribution:
p.Y .i/
D rjy.1/
; : : : ; y.i 1/
; s/ D
8
ÀÜ
ÀÜ
ÀÜ
ÀÜ
<
ÀÜ
ÀÜ
ÀÜ
:ÃÇ
Pi 1
j D1 I.y.j/
D r/
i 1 C s
; if r  y
i
s
i 1 C s
; if r D y
i C 1:
(7.5)
ÓÄÄis distribution deÔ¨Ånition, together with the chain rule, immediately leads to the following
distribution over assignments of each y.i/
to an integer:
p.y.1/
; : : : ; y.n/
js/ D p.y.1/
js/
n
Y
iD2
p.y.i/
jy.1/
; : : : ; y.i 1/
; s/
‚Äû ∆í‚Äö ‚Ä¶
Equation 7.5
;
with p.y.1/
js/ D 0 for any value other than y.1/
D 1.
In the CRP metaphor, when a new ‚Äúcustomer,‚Äù indexed by i, enters the restaurant, it sits at a
table with other customers with a probability proportional to the number of the current occupants
at that table, or starts a new table with a probability proportional to s. Here, the role of s is evident
again‚Äîthe larger s is, the more ‚Äúnew tables‚Äù are opened. In the CRP metaphor, y.i/
denotes the
table assignment for the ith customer, and y
i denotes the number of ‚Äúoccupied tables‚Äù before the
ith customer is seated. Figure 7.2 depicts this posterior distribution in a graphical manner.
It is important to note that the set of random variables Y .i/
for i 2 f1; : : : ; ng in the Chinese
restaurant process is a set of exchangeable random variables (see also Section 1.3.3). ÓÄÄis means
that the joint distribution over Y .1/
; : : : ; Y .n/
is the same as the distribution over a permutation
of these random variables. In the CRP metaphor, the order in which the customers enter the
restaurant does not matter.
156 7. NONPARAMETRIC PRIORS
.
.
Hyperparameters: G0, s.
Variables: Y .i/
, i for i 2 f1; : : : ; ng.
Output: .i/
for i 2 f1; : : : ; ng drawn from G  DP.G0; s/.
.
‚Ä¢ Draw y.1/
; : : : ; y.n/
 CRP.s/.
1:
‚Ä¢ Draw 1; : : : ; y
n
 G0.
2:
‚Ä¢ Set each .i/
to y.i/ for i 2 f1; : : : ; ng.
3:
Generative Story 7.2: ÓÄÄe Dirichlet process represented using the Chinese restaurant process.
.
.
Table 1
.
    
. Table 2
.
 
. Table 3
.
  
. (new table)
.
Figure 7.2: A graphical depiction of the posterior distribution of the Chinese restaurant process.
Each black circle is a ‚Äúcustomer‚Äù that sat next to one of the tables. In the picture, 3 tables are open in
the restaurant with 10 customers. For Àõ D 1:5, there is a probability of
5
5 C 2 C 3 C 1:5
D
5
11:5
for
a new customer to go to the Ô¨Årst table, probability of
2
11:5
to go the second table,
3
11:5
for the third
table and
1:5
11:5
to go to a new table.
ÓÄÄis view of the Dirichlet process demonstrates the way that the number of parameters
grows as more data is available (see beginning of this chapter). ÓÄÄe larger the number of samples
is, the more ‚Äútables‚Äù are open, and therefore, the larger the number of parameters used when
performing inference.
ÓÄÄe Chinese restaurant process induces a distribution over partitions of Y .i/
, and therefore
it is only a function of the counts of customers that are seated next to a certain table. More formally,
the CRP distribution is a function of the integer count vector N of length m with m D y
n (the
total number of tables used for n customers) and Nk D
Pn
iD1 I.Y .i/
D k/ (the count of customers
for each table k 2 f1; : : : ; mg) and is deÔ¨Åned as follows:
p.Njs/ D
sm
Qm
kD1.Nk 1/≈†

Qn 1
iD0 .i C s/
:
7.2. DIRICHLET PROCESS MIXTURES 157
.
.
Hyperparameters: G0, s.
Latent variables: .i/
for i 2 f1; : : : ; ng.
Observed variables: X.i/
for i 2 f1; : : : ; ng.
Output: x.1/
; : : : ; x.n/
for i 2 f1; : : : ; ng drawn from the Dirichlet process mixture model.
.
‚Ä¢ Draw G  DP.G0; s/.
1:
‚Ä¢ Draw .1/
; : : : ; .n/
 G.
2:
‚Ä¢ Draw x.i/
 p.X.i/
j.i/
/ for i 2 f1; : : : ; ng.
3:
Generative Story 7.3: ÓÄÄe generative story for the Dirichlet process mixture model. ÓÄÄe distribution
G0 is the base distribution such that its sample space is a set of parameters for each of the mixture
components.
With the CRP process, one can use a generative story 7.2 to deÔ¨Åne a representation for the
Dirichlet process. An equivalent procedure for generating .1/
; : : : ; .n/
from G  DP.G0; s/ is
to draw a partition from the CRP, assigning each ‚Äútable‚Äù in the partition a draw from G0 and
then setting .i/
to be that draw from G0 according to the table the ith customer is seated in.
ÓÄÄe CRP has a strong connection to the GEM distribution. Let  be a draw from the
GEM distribution with concentration parameter s. Let U1; : : : ; UN be a set of random variables
that take integer values such that p.Ui D k/ D k. ÓÄÄese random variables induce a partition
over f1; : : : ; N g, where each set in the partition consists of all Ui that take the same value. As
such, for a given N, the GEM distribution induces a partition over f1; : : : ; N g. ÓÄÄe distribution
over partitions that the GEM distribution induces is identical to the distribution over partitions
that the CRP induces (with N customers) with concentration parameter s.
7.2 DIRICHLET PROCESS MIXTURES
Dirichlet process mixture models (DPMMs) are a generalization of the Ô¨Ånite mixture model. Just
like a Ô¨Ånite mixture model, the Dirichlet process mixture associates each of the observations with
a cluster. ÓÄÄe number of clusters, as expected, is potentially unbounded‚Äîthe more data observed,
the more clusters will be inferred. Each cluster is associated with a parameter, drawn from the
base distribution, which is deÔ¨Åned over the space ‚Äö. ÓÄÄere is a probability distribution p.Xj/,
where  2 ‚Äö. Generative story 7.3 deÔ¨Ånes the generative process of the Dirichlet process mixture
model.
Note the similarity to a regular non-Bayesian parametric mixture model. If G were a Ô¨Åxed
distribution over a Ô¨Ånite number of elements (K) in ‚Äö (instead of a countable set), then the last
two steps in the above generative process would correspond to a (non-Bayesian) mixture model.
158 7. NONPARAMETRIC PRIORS
In this case, placing a prior (which can be parametric) on G would turn the model into a Bayesian
Ô¨Ånite mixture model.
7.2.1 INFERENCE WITH DIRICHLET PROCESS MIXTURES
In this section, we describe the two common methods for inference with DPMMs: MCMC and
variational.
MCMC Inference for DPMMs
One way to deÔ¨Åne posterior inference for DPM is to identify the posterior distribution
p..1/
; : : : ; .n/
jX; G0; s/. ÓÄÄe most straightforward way to do so is to use a Gibbs sampler
that samples from the conditional distributions p..i/
j. i/
; X; G0; s/, for i 2 f1; : : : ; ng (Neal,
2000).
ÓÄÄe Ô¨Årst observation we make is that .i/
is independent of X. i/
conditioned on . i/
.
ÓÄÄerefore, we seek conditional distributions of the form p..i/
j. i/
; x.i/
; G0; s/. It holds for this
conditional that:¬≥
p..i/
j. i/
; x.i/
; G0; s/
/ p..i/
; x.i/
j. i/
; G0; s/
D
1
n 1 C s
X
j ¬§i
I..j/
D .i/
/p.x.i/
j.j/
/ C
s
n 1 C s
G0..i/
/p.x.i/
j.i/
/
D
1
n 1 C s
X
j ¬§i
I..j/
D .i/
/p.x.i/
j.j/
/ C
s
n 1 C s
Z

G0./p.x.i/
j/d

p..i/
jx.i/
/; (7.6)
where p.jX/ is the posterior distribution over the parameter space for the distribution p.; X/ D
G0./p.Xj/, i.e., p.jX/ / G0./p.Xj/. ÓÄÄe transition to Equation 7.6 can be justiÔ¨Åed by
the following:
G0..i/
/p.x.i/
j.i/
/ D G0..i/
/
p..i/
jx.i/
/p.x.i/
/
G0..i//
D p..i/
jx.i/
/p.x.i/
/;
and
p.x.i/
/ D
Z

G0./p.x.i/
j /d: (7.7)
¬≥ÓÄÄis can be shown by exchangeability, and assuming that the ith sample is the last one that was drawn.
7.2. DIRICHLET PROCESS MIXTURES 159
ÓÄÄis is where the importance of the conjugacy of G0 to the likelihood becomes apparent.
If indeed there is such a conjugacy, the constant in Equation 7.7 can be easily computed, and the
posterior is easy to calculate as well. (ÓÄÄis conjugacy is not necessary for the use of the Dirichlet
process mixture model, but it makes it more tractable.)
ÓÄÄese observations yield a simple posterior inference mechanism for .1/
; : : : ; .n/
: for each
i, given . i/
, do the following:
‚Ä¢ With probability proportional to p.x.i/
j.j/
/ for j ¬§ i, set .i/
to .j/
‚Ä¢ With probability proportional to s
R
 G0./p.x.i/
j/d

set .i/
to a draw from the dis-
tribution p.jx.i/
/.
ÓÄÄis sampler is a direct result of Equation 7.6. ÓÄÄe probability of .i/
given the relevant
information can be viewed as a composition of two types of events: an event that assigns .i/
an
existing .j/
for j ¬§ i, and an event that draws a new .i/
.
ÓÄÄe above two steps are iterated until convergence. While the sampler above is a perfectly
correct Gibbs sampler (Escobar, 1994, Escobar and West, 1995), it tends to mix quite slowly. ÓÄÄe
reason is that each .i/
is sampled separately‚Äîlocal changes are made to the seating arrangements,
and therefore it is hard to Ô¨Ånd a global seating arrangement with high probability. Neal (2000)
describes another Gibbs algorithm that solves this issue. ÓÄÄis algorithm samples the assignment
to tables, and changes atoms for all customers sitting at a given table at once. He also suggests a
speciÔ¨Åcation of this algorithm to the conjugate DPM case, where G0 is conjugate to the likelihood
from which x.i/
are sampled. For a thorough investigation of other scenarios in which Gibbs
sampling or other MCMC algorithms can be used for Dirichlet process mixture models, see
Neal (2000).
Variational Inference for DPMMs
ÓÄÄe DPM can be represented using the stick-breaking process, which enables the development of
a variational inference algorithm for it (Blei and Jordan, 2004). ÓÄÄe stick-breaking representation
for DPM is displayed in generative story 7.4.
ÓÄÄe variational inference algorithm of Blei and Jordan treats this model with inÔ¨Ånite com-
ponents by using variational distributions that are Ô¨Ånite, and more speciÔ¨Åcally, correspond to a
truncated stick-breaking distribution.
ÓÄÄe variational distribution for the DPMM needs to account for the latent variables 
(derived from Àá), i and z.i/
. ÓÄÄe main point behind Blei and Jordan‚Äôs variational inference al-
gorithm is to use a truncated stick approximation for the  distribution. Since the  distribution
comes from a GEM distribution, it is constructed from i for i 2 f1; 2; : : :g, which is drawn from
the Beta distribution, as explained in Section 7.1.1. ÓÄÄerefore, to deÔ¨Åne a variational distribution
over , it is suÔ¨Écient to deÔ¨Åne a variational distribution over 1; 2; : : :.
Blei and Jordan suggest using a mean-Ô¨Åeld approximation for i , such that q.i / for i 2
f1; : : : ; K 1g is a Beta distribution (just like the true distribution for i ) and:
160 7. NONPARAMETRIC PRIORS
.
.
Hyperparameters: G0, s.
Latent variables: , j for j 2 f1; : : :g, Z.i/
for i 2 f1; : : : ; ng.
Observed variables: X.i/
for i 2 f1; : : : ; ng.
Output: x.i/
; : : : ; x.n/
generated from the Dirichlet process mixture model.
.
‚Ä¢ Draw   GEM.s/.
1:
‚Ä¢ Draw 1; 2; : : :  G0.
2:
‚Ä¢ Draw z.i/
  for i 2 f1; : : : ; ng such that z.i/
is an integer.
3:
‚Ä¢ Draw x.i/
 p.x.i/
jz.i/ / for i 2 f1; : : : ; ng.
4:
Generative Story 7.4: ÓÄÄe generative story for the DPMM using the stick-breaking process. ÓÄÄe
distribution G0 is the base distribution such that its sample space is a set of parameters for each of the
mixture components.
q.K/ D
(
1 K D 1
0 K ¬§ 1:
Since q.K/ puts its whole probability mass on K being 1, there is no need to deÔ¨Åne
variational distributions for i for i > K. Such q.K/ implies that i D 0 for any i > K according
to the variational distribution over i . ÓÄÄe variational stick is truncated at point K.
Other Inference Algorithms for DPMMs
A rather unique way of performing inference with DPMMs was developed by Daume (2007).
Daum√© was interested in identifying the cluster assignment for a DPM that maximizes the prob-
ability according to the model‚Äîthe maximum a posteriori (MAP) assignment. His algorithm is
a search algorithm that keeps a queue with possible partial clusterings and values attached to
them (tables, dishes and customers associated with them). If the function that attaches values to
these clusterings (the scoring function) satisÔ¨Åes some properties (i.e., it is admissible, meaning it
always overestimates the probability of the best clustering that agrees with a partial clustering in
the queue), and if the beam size used during the search is 1, then the search algorithm is guar-
anteed to Ô¨Ånd the MAP assignment for the clustering. Daum√© also provides a few admissible
scoring functions; he tested his algorithm on a character recognition problem and the clustering
of documents from the NIPS conference.
7.3. THE HIERARCHICAL DIRICHLET PROCESS 161
.
.
Constants: K and n.
Hyperparameters: G0 base measure deÔ¨Åned over ‚Äö, s, a model family F.Xj/ for  2 ‚Äö.
Latent variables: Z.1/
; : : : ; Z.n/
.
Observed variables: X.1/
; : : : ; X.n/
.
Output: A set of n points drawn from a mixture model.
.
‚Ä¢ Generate 1; : : : ; K  G0.
1:
‚Ä¢ Draw   Dirichlet.Àõ=K/ (from a symmetric Dirichlet).
2:
‚Ä¢ For j 2 f1; : : : ; ng ranging over examples:
3:
 Draw z.j/
from  for j 2 f1; : : : ; ng.
4:
 Draw x.j/
from F.Xjzj
/ for j 2 f1; : : : ; ng.
5:
Generative Story 7.5: An approximation of the Dirichlet process mixture model using a Ô¨Ånite mix-
ture model.
7.2.2 DIRICHLET PROCESS MIXTURE AS A LIMIT OF MIXTURE MODELS
Another DPM construction exists that provides more intuition about its structure. Consider gen-
erative story 7.5. It can be shown that as K becomes large, this model is a good approximation
of the Dirichlet process. It can also be shown that as K becomes large, the number of compo-
nents actually used for a Ô¨Åxed number of datapoints, n, becomes independent of K (approximately
O.Àõ log n/‚Äîsee also the exercises).
7.3 THE HIERARCHICAL DIRICHLET PROCESS
ÓÄÄe distributions drawn from a Dirichlet process, as mentioned in Section 7.1.1, have a countable
(or Ô¨Ånite) support. ÓÄÄe Dirichlet process selects a countable set of atoms from the base distribution,
and then places weights on these atoms.
It is often desirable to create some hierarchy over this set of atoms drawn from a Dirichlet
process. For example, in the spirit of the latent Dirichlet allocation model, a corpus of documents
(modeled as bags of words) can be represented by an inÔ¨Ånite, countable set of topics corresponding
to multinomials over the vocabulary (see Section 2.2); each document might have a diÔ¨Äerent set
of probabilities assigned to each of these topics.
ÓÄÄis means that the set of atoms‚Äîcorresponding to multinomial distributions over the
vocabulary‚Äîshould be shared across all of the documents. However, the distribution of selecting
162 7. NONPARAMETRIC PRIORS
each of these multinomials (corresponding to topic distribution in LDA) should be diÔ¨Äerent for
each document, similar to the LDA model.
Note that the number of topics assumed to exist in the corpora is inÔ¨Ånite, but still countable.
ÓÄÄis is a reasonable modeling assumption if one thinks of each topic as a concept in the real world.
While clearly the number of such concepts may grow over time, it should remain countable.
However, any reasonable prior distribution over topics, i.e., over vocabulary multinomials,
is a continuous distribution (consider, for example, the Dirichlet distribution). ÓÄÄerefore, if we
draw for each document a distribution over possible topics, G.j/
 DP.G0; s/, j 2 f1; : : : ; ng,
there almost surely will be no atoms at the intersection of the support of G.j/
and G.j 0/
. ÓÄÄe
support of each of these inÔ¨Ånite multinomials is countable, and therefore has negligible cardinality
compared to the cardinality of the continuous space from which the support is being drawn. ÓÄÄis
issue is problematic because it violates the countable-topic-world assumption mentioned earlier.
A solution to this problem (presented in more generality, and described also for other problems
outside of NLP) is suggested by Teh et al. (2006).
ÓÄÄe authors suggest having a hierarchical prior where G0  DP.H; s/‚Äîthis means that
the base distribution G0 for the Dirichlet process is itself a draw from the Dirichlet process. Now
G0 has a countable support, and therefore atom sharing will occur between the distributions G.j/
.
With this model, a draw of G0 represents a draw of the set of all possible topics that could emerge
in the text, and G.j/
, for each document, re-weights the probability of these topics.
ÓÄÄe full hierarchical Dirichlet process for the bag-of-word LDA-style model is the fol-
lowing. Assume H is some prior distribution over topics, which can now be continuous. For
example, H can be a Dirichlet distribution over the probability simplex of a dimension the size
of the vocabulary.
ÓÄÄen, in order to generate n documents, each of length `j , j 2 f1; : : : ; ng, we follow gen-
erative story 7.6. ÓÄÄe observed random variables are X.j/
, where X.j/
is a vector of length `j ,
with X.j/
i being the ith word in document j .
Inference with the HDP ÓÄÄe original paper by Teh et al. (2006) suggested to perform inference
with the HDP using MCMC sampling (Chapter 5). ÓÄÄe authors suggest three sampling schemes
for the hierarchical Dirichlet process:
‚Ä¢ Sampling using the Chinese restaurant franchise representation: ÓÄÄe HDP can be described
in terms similar to that of the Chinese restaurant process. Instead of having a single restau-
rant, there would be multiple restaurants, each representing a draw from a base distribution,
which is by itself a draw from a Dirichlet process. With this sampling scheme, the state
space consists of indices for tables assigned to each customer i in restaurant j , and indices
for dishes serves at table t in restaurant j .
‚Ä¢ Sampling using an augmented representation: In the sampling scheme mentioned above,
the base distribution of the Ô¨Årst DP in the HDP hierarchy (G0) is assumed to be integrated
out, which can complicate matters for certain models, such as HMMs, where there are
7.4. THE PITMAN-YOR PROCESS 163
.
.
Constants: `j for j 2 f1; : : : ; ng a sequence of lengths for n documents.
Hyperparameters: H, s, s
.
Latent variables: G0, G.j /
, .j/
, G0.
Observed variables: X.j/
.
Output: A set of n documents. ÓÄÄe jth document has `j words, x.j/
; : : : ; x.j/
`j
.
.
‚Ä¢ Generate G0  DP.H; s/.
1:
‚Ä¢ For j 2 f1; : : : ; ng ranging over documents:
2:
 Draw G.j/
 DP.G0; s
/ denoting the topic distribution for document j
3:
 For i 2 f1; : : : ; `j g ranging over words in the document:
4:
 Draw .j/
i  Gj , representing a topic.
5:
 Draw x.j/
i  Multinomial..j/
i /, a word in the document.
6:
Generative Story 7.6: ÓÄÄe generative story for an LDA-style model using the Dirichlet process.
additional dependencies between the Dirichlet processes. ÓÄÄerefore, Teh also describes an
MCMC sampling scheme in which G0 is not integrated out. ÓÄÄis sampler resembles the
previous one, only now the state space also includes bookkeeping for G0.
‚Ä¢ Sampling using direct assignment: In the previous two samplers, the state space is such
that assignments of customers to dishes is indirect, through table dish assignment. ÓÄÄis can
make bookkeeping complicated as well. Teh suggests another sampler, in which customers
are assigned to dishes directly from the Ô¨Årst draw of the HDP, G0. ÓÄÄis means that the state
space is now assignments to a dish (from the draw of G0) for each customer i in restaurant
j and a count of the number of customers with a certain dish k in restaurant j .
Alternatives to MCMC sampling for the HDP have also been suggested. For example,
Wang et al. (2011) developed an online variational inference algorithm for the HDP. Bryant
and Sudderth (2012) developed another variational inference algorithm for the HDP, based on
a split-merge technique. Teh et al. (2008) extended a collapsed variational inference algorithm,
originally designed for the LDA model, to the HDP model.
7.4 THE PITMAN-YOR PROCESS
ÓÄÄe Pitman-Yor process (Pitman and Yor, 1997), sometimes also called the ‚Äútwo-parameter
Poisson-Dirichlet process‚Äù is closely related to Dirichlet process, and it also deÔ¨Ånes a distribution
164 7. NONPARAMETRIC PRIORS
over distributions. ÓÄÄe Pitman-Yor process uses two real-valued parameters‚Äîa strength param-
eter s that plays the same role as in the CRP, and a discount parameter d 2 ≈í0; 1¬ç. In addition, it
also makes use of a base distribution G0.
ÓÄÄe generative process for generating a random distribution from the Pitman-Yor process
is almost identical to the generative process described in Section 7.1.2. ÓÄÄis means that for n
observations, one draws a partition over the integers between 1 to n, and each cluster in the
partition is assigned an atom from G0.
ÓÄÄe diÔ¨Äerence between the Dirichlet process and the Pitman-Yor process is that the
Pitman-Yor process makes use of a generalization of the Chinese restaurant process, and it mod-
iÔ¨Åes Equation 7.5 such that:
p.Y .i/
D rjy.1/
; : : : ; y.i 1/
; s; d/ D
8
ÀÜ
ÀÜ
<
ÀÜ
:ÃÇ
Pi 1
j D1 I.y.j/
D r/

d
i 1 C s
; if r  y
i
s C y
i d
i 1 C s
; if r D y
i C 1
:
ÓÄÄe discount parameter d plays a role in controlling the expected number of tables that will
be generated for n ‚Äúcustomers.‚Äù With d D 0, the Pitman-Yor process reduces to the Dirichlet
process. With larger d, more tables are expected to be used for n customers. For a more detailed
discussion, see Section 7.4.2.
ÓÄÄis modiÔ¨Åed Chinese restaurant process again induces a distribution that is a function of
the integer count vector N of length m with m D y
n and Nk D
Pn
iD1 I.Y .i/
D k/. It is deÔ¨Åned
as follows:
p.Njs; d/ D
Qm
kD1

.d.k 1/ C s/ 
QNk 1
j D1 .j d/

Qn 1
iD0 .i C s/
: (7.8)
ÓÄÄe Pitman-Yor process also has a stick-breaking representation, which is very similar to the
stick-breaking representation of the Dirichlet process. More speciÔ¨Åcally, a PY with a base distri-
bution G0, a strength parameter s and a discount parameter d, will follow the same generative pro-
cess in Section 7.1.1 for the Dirichlet process, only k are now drawn from Beta.1 d; s C kd/
(Pitman and Yor, 1997). ÓÄÄis again shows that when d D 0, the Pitman-Yor process reduces to
the Dirichlet process.
ÓÄÄe Pitman-Yor process can be used to construct a hierarchical Pitman-Yor process, similar
to the way that the Dirichlet process can be used to create an HDP (Section 7.3). ÓÄÄe hierarchy
is constructed from Pitman-Yor processes, instead of Dirichlet processes‚Äîwith an additional
discount parameter. Such a hierchical PY process was used, for example, for dependency parsing
by Wallach et al. (2008).
7.4. THE PITMAN-YOR PROCESS 165
7.4.1 PITMAN-YOR PROCESS FOR LANGUAGE MODELING
Language modeling is one of the most basic and earliest problems in natural language processing.
ÓÄÄe goal of a language model is to assign a probability to an utterance, a sentence or some text.
Language models are used, for example, to ensure the output of a machine translation system is
coherent, or speech recognition to assess the plausbility of an utterance, as it is decoded from a
given speech signal.
If x1    xm is a string, corresponding to a sentence over some vocabulary, then one can
imagine a model that progressively generates word by word, each time conditioning on the words
generated up to that point. ÓÄÄe words being conditioned on are ‚Äúthe context‚Äù in which the new
word is being generated. Mathematically, this is a simple use of the chain rule, that deÔ¨Ånes the
probability of x1    xm as:
p.x1    xm/ D p.x1/
n
Y
iD2
p.xi jx1    xi 1/: (7.9)
ÓÄÄe most common and successful language models are n-gram models, which simply make
a Markovian assumption that the context necessary to generate xi is the n 1 words that pre-
ceeded xi . For example, for a bigram model, where n D 2, the probability in Equation 7.9 would
be formulated as:
p.x1    xm/ D p.x1/
n
Y
iD2
p.xi jxi 1/:
Naturally, a Bayesian language model would place a prior over the probability distribu-
tions that generate a new word given its context. Teh (2006b) uses the Pitman-Yor process as
a prior over these distributions. In the terminlogy of the paper, let an n-gram distribution such
as p.wjw1    wn 1/ be Gw1wn 1
.w/ and let .w1    wr / D w2    wr , i.e.,  is a function that
takes a sequence of words and removes the ‚Äúearliest‚Äù word. ÓÄÄen, Teh deÔ¨Ånes a hierarchical prior
over the n-gram distributions Gw1wr (r  n 1) as:
G;  PY.d0; a0; G0/ base case
Gw1wr  PY.dr ; ar ; G.w1wr // recursive case for 1  r  n 1:
Here, G0 is the base distribution such that G0.w/ D 1=V where V is the vocabulary size
(i.e., it is a uniform distribution over the vocabulary). In addition, there is a uniform prior on the
discount parameters, and a Gamma.1; 1/ prior over all concentration parameters.
Because of the hierarchical structure of this prior, a word that has been drawn for a certain
context w1    wr is more likely to be drawn again for all other contexts that share an identical
suÔ¨Éx to w1    wr .
166 7. NONPARAMETRIC PRIORS
ÓÄÄis hierarchical deÔ¨Ånition of a prior is reminscent of back-oÔ¨Ä smoothing or interpolation
with lower order models, both of which are common smoothing techniques for n-gram language
model estimations (Rosenfeld, 2000). As a matter of fact, Teh (2006a) shows that his hierarchical
prior can be viewed as a generalization of the interpolated Kneser-Ney estimator for n-gram
language models (Chen and Goodman, 1996, Kneser and Ney, 1995).
Teh designs an inference scheme for predicting the probability of a word given the context
in which it appears (i.e., the words that came before it), and proceeds to compute the perplexity
of text based on his language model. ÓÄÄe inference algorithm is a Gibbs sampler that uses the
Chinese restaurant process representation.
Teh reports that the Gibbs implementation of the Pitman-Yor language model (PYLM)
performs better than interpolated Kneser-Ney, but worse than ‚ÄúmodiÔ¨Åed Kneser-Ney,‚Äù (MKN)
which is an improved variant of Kneser-Ney, by Chen and Goodman (1996). Teh hypothesized
that the reason PYLM does worse than MKN is that the inferred estimates of the hyperparame-
ters are not optimal for prediction. He overcomes this limitation by designing a cross-validation
scheme for Ô¨Ånding these hyperparameter estimates for PYLM. (MKN‚Äôs parameters are also opti-
mized using cross-validation.) Indeed, the perplexity result he reports using this cross-validation
scheme with PYLM, compared to MKN, is lower, and is overall the best among all of the base-
lines that he compares (such as interpolated Kneser-Ney and a hierarchical Dirichlet model, for
diÔ¨Äerent n-gram sizes).
Noji et al. (2013) used the Pitman-Yor process for combining an n-gram language model
with a topic model. ÓÄÄeir work builds and extends the model of Wallach (2006), which also
combines an n-gram language model with a topic model, but in the parametric setting (with
Dirichlet priors).
7.4.2 POWER-LAW BEHAVIOR OF THE PITMAN-YOR PROCESS
ÓÄÄe PY process reduces to a Dirichlet process when the discount parameter is 0. In this case,
it can be shown that the expected number of tables, with the CRP representation, is approxi-
mately s log n, with n being the number of customers (usually corresponding to the number of
observations), and s being the concentration parameter for the Dirichlet process.
With d > 0, the number of expected tables actually changes in a critical manner: from a
logarithmic number of tables, the expected number of tables becomes snd
. ÓÄÄis means that the
expected number of tables follows a power-law, so that the number of expected tables as a function
of n is proportional to a power of n.
ÓÄÄis power-law behavior Ô¨Åts natural languge modeling very well. For example, with a
Pitman-Yor unigram language model, each table corresponds to a word type, and n corresponds to
word tokens. ÓÄÄerefore, the expected number of tables gives the expected number of word types
in a corpus of size n. As Zipf (1932) argued, this setting Ô¨Åts a power-law behavior.
Figure 7.3 demonstrates the behavior of the Dirichlet process vs. the Pitman-Yor process
when d ¬§ 0. First, it shows that i , the length of the i part of the stick (in the stick-breaking
7.5. DISCUSSION 167
0.08
0.04
0.00
40
50
20
30
0
10
Ãüi
0 10 20 30 40 50 0 20 40 60 80 100
Number
of
Tables
n
i
Figure 7.3: Left plot: Average of i as a function of i according to 5,000 samples from a Pitman-Yor
process prior. ÓÄÄe value of the concentration parameter s is 10 in all cases. ÓÄÄe value of the discount
parameters are d D 0 (black; Dirichlet process), d D 0:1 (red), d D 0:5 (blue) and d D 0:8 (purple).
Right plot: the average of number of tables obtained as a function of the number of customers n over
5,000 samples from the Chinese restaurant process. ÓÄÄe concentration and the discount parameters
are identical to the left plot.
process representation of DP and PYP) has an exponential decay when d D 0 (i.e., when we use
the Dirichlet process) vs. a heavier tail when we use the Pitman-Yor process. With d ¬§ 0, it
can be shown that i / .i/
1
d . ÓÄÄe Ô¨Ågure also shows the average number of tables opened when
sampling from the Chinese restaurant process for DP and PYP. ÓÄÄe number of tables grows
logarithmically when using d D 0, but grows faster when d > 0.
7.5 DISCUSSION
ÓÄÄis chapter has a large focus on the Dirichlet process, its derivatives and its diÔ¨Äerent representa-
tions. While the Dirichlet process plays an important role in Bayesian nonparametric NLP, there
are other stochastic processes used as nonparametric Bayesian priors.
ÓÄÄe machine learning literature has produced many other nonparametric Bayesian priors
for solving various problems. In this section we provide an overview of some of these other non-
parametric models. We do not describe the posterior inference algorithms with these priors, since
they depend on the speciÔ¨Åcs of the model in which they are used. ÓÄÄe reader should consult with
the literature in order to understand how to do statistical inference with any of these priors. In-
deed, the goal of this section is to inspire the reader to Ô¨Ånd a good use for the described priors for
natural language processing problems, as these priors have not often been used in NLP.
168 7. NONPARAMETRIC PRIORS
7.5.1 GAUSSIAN PROCESSES
Gaussian processes (GPs) is a stochastic process commonly used to model temporal and/or spatial
observed data. A Gaussian process is a set of random variables fXt jt 2 T g that are indexed by
some set T (T , for example, can denote time), such that each Ô¨Ånite subset of this set of random
variable distributes according to the multivariate normal distribution.
ÓÄÄe Gaussian process, therefore, is deÔ¨Åned using an expectation function W T ! R and
a covariance function Cov.t; t0
/ that denotes the covariance between Xt and Xt0 . Every sub-
vector of the random variables, X D .Xt1
; : : : ; Xtn / is normally distributed, such that E≈íX¬ç D
..t1/; : : : ; .tn// and covariance matrix ‚Ä† such that ‚Ä†ij D Cov.ti ; tj /.
When instantiating all values of Xt to a certain value, one can think of the Gaussian process
as a function: each t 2 T is mapped to a real-value. ÓÄÄis is the main inspiration for using Gaussian
processes in the Bayesian setting: they are used to deÔ¨Åne priors over real-valued functions. Each
draw from a Gaussian process yields a speciÔ¨Åc function that maps the values of T to real-values.
ÓÄÄe ‚Äúmean function‚Äù of this prior is . For more detailed information about the use of Gaussian
processes in machine learning, see Rasmussen and Williams (2006).
Gaussian processes are not heavily used in NLP. ÓÄÄere is some work that uses these pro-
cesses for classiÔ¨Åcation and sequence labeling; for example, the work of Altun et al. (2004) uses
a Gaussian process prior over a compatibility function that measures the match of an observation
to a certain label. Altun et al. test their model on the problem of named entity recognition, and
show error reduction compared to a conditional random Ô¨Åeld model.
More recent work that makes use of GPs in N LP focuses on regression problems. For
example, Preo≈£iuc-Pietro and Cohn (2013) use Gaussian processes to model temporal changes in
Twitter data. ÓÄÄe goal is to predict trends in time (in Twitter), as indicated by the surrogate of
the volume of a hashtag. ÓÄÄe regression function f .t/ is assumed to be drawn from a Gaussian
process prior, where t denotes the time and f .t/ denotes the volume (i.e., each Xt in the process
denotes a random value for f .t/). ÓÄÄe authors also made use of the predictions in an additional
classiÔ¨Åcation step to predict the hashtags of given tweets.
7.5.2 THE INDIAN BUFFET PROCESS
ÓÄÄe Indian buÔ¨Äet process (IBP) describes a stochastic process for drawing inÔ¨Ånite binary matrices
(GriÔ¨Éths and Ghahramani, 2005). It also uses a restaurant metaphor, similar to the CRP, only
customers can have multiple dishes based on their popularity with other customers. ÓÄÄe IBP is
controlled by a hyperparameter Àõ.
ÓÄÄe metaphor for the IBP is the following. ÓÄÄere is a restaurant with an inÔ¨Ånite number of
dishes, ordered in a long buÔ¨Äet-style line. ÓÄÄe Ô¨Årst customer enters, and takes a serving from the
Ô¨Årst r1 dishes, where r1 is drawn from Poisson.Àõ/. When the ith customer enters, she samples
a dish according to its popularity, serving herself a dish k with probability mk=i, where mk is
the number of customers that previously served themselves dish k. After reaching the end of
7.5. DISCUSSION 169
all previously sampled dishes, she serves herself ri new dishes, where ri is drawn according to
Poisson.Àõ=i/.
ÓÄÄe description of the above process implies that the IBP deÔ¨Ånes a distribution over inÔ¨Ånite
binary matrices. Each draw M of such a binary matrix (where the columns and rows are indexed
with a natural number, corresponding to the customer index) has Mik D 1 if customer i served
himself the kth dish, and 0 otherwise.
7.5.3 NESTED CHINESE RESTAURANT PROCESS
ÓÄÄe nested Chinese restaurant process (Blei et al., 2010) gives a prior of inÔ¨Ånitely branching trees
with an inÔ¨Ånite depth. ÓÄÄe nested CRP assumes the existence of an inÔ¨Ånite number of Chinese
restaurants, each with an inÔ¨Ånite number of tables. ÓÄÄere is a main ‚Äúroot‚Äù restaurant, and each
table points to exactly one other restaurant. Each table in each of the other restaurants also points
to another restaurant. Each restaurant is referenced exactly once, and therefore the restaurants
have a tree structure, where the nodes are the referenced restaurants (or referencing tables) and
edges denote that one restaurant references the other.
ÓÄÄe nested CRP induces a distribution over paths in this inÔ¨Ånite tree‚Äîeach draw from the
nested CRP is such a path. We assume M tourists, coming to a city with the Chinese restaurants
described above, starting at the root restaurant, choosing a table, and then moving to the next
restaurant as referenced by that table. ÓÄÄis is repeated ad-inÔ¨Ånitum. With M tourists, there will
be M inÔ¨Ånite paths in the tree, which can be described as a subtree of the inÔ¨Ånite tree with a
branching factor of at most M.
Blei et al. (2010) use the nested CRP to describe a hierarchical LDA model. ÓÄÄe inÔ¨Ånite
tree over all tables is assumed to be a hierarchical topic distribution. ÓÄÄis means that each node
in the tree is associated with a distribution over the vocabulary of words in the documents. ÓÄÄe
generative story uses a draw from the nested CRP (of an inÔ¨Ånite path) instead of the usual topic
distribution. ÓÄÄen, to draw the words in the document, we Ô¨Årst draw a path from a nested CRP,
and then for each word we Ô¨Årst draw a level in that inÔ¨Ånite path, and then the word from the
vocabulary distribution associated with the node in that level of the path.
7.5.4 DISTANCE-DEPENDENT CHINESE RESTAURANT PROCESS
As mentioned in Section 7.1.2, the Chinese restaurant process deÔ¨Ånes a probability distribution
over partitions. ÓÄÄe metaphor is that customers are arranged in seats next to tables, where the
ith customer is seated next to a table with a probability proportional to the number of customers
already sitting next to the table. Some probability mass is allocated for seating a customer next to
a new, empty table.
ÓÄÄe key observation that Blei and Frazier (2011) made is that the CRP deÔ¨Ånes the seating
arrangement based on the tables, more speciÔ¨Åcally, according to the number of people seated at a
table. ÓÄÄey suggest an alternative view, in which a customer is seated with another customer.
170 7. NONPARAMETRIC PRIORS
More formally, let f be a ‚Äúdecay function‚Äù that needs to be positive, non-increasing with
f .1/ D 0. In addition, let D be an n  n matrix such that Dij denotes a distance between cus-
tomer i and customer j . We assume that Dij  0. ÓÄÄen, if Y .i/
for i 2 f1; : : : ; ng is a random
variable denoting who customer i sits with, the distance-based CRP assumes that:
p.Y .i/
D jjD; s/ /
(
f .Dij/; if i ¬§ j
s; if i D j;
where s plays a similar role to a concentration parameter.
ÓÄÄe seating arrangement of each customer next to another customer induces a seating ar-
rangement next to tables, similar to the CRP, by assigning a table to all customers who are con-
nected to each other through some path.
Note that the process that Blei and Fraizer suggest is not sequential. ÓÄÄis means that cus-
tomer 1 can pick customer 2 and customer 2 can pick customer 1, and more generally, that there
can be cycles in the arrangement. ÓÄÄe distance-based CRP is not sequential in the sense of the
original CRP. In addition, the order in which the customers enter the restaurant does matter,
and the property of exchangeability is broken with the distance-dependent CRP (see also Sec-
tion 7.1.2).
However, it is possible to recover the traditional sequential CRP (with concentration pa-
rameter s) by assuming that Dij D 1 for j > i, Dij D 1 for j < i, f .D/ D 1=d with f .1/ D 0.
In this case, customers can only join customers that have a lower index. ÓÄÄe total probability of
joining a certain table will be proportional to the sum of the distances between a customer and all
other customers sitting at that table, which in the above formulation will just be the total number
of customers sitting next to that table.
ÓÄÄere has been relatively little use of distance-dependent CRPs in NLP, but examples of
such use include the use of distance-dependent CRP for the induction of part-of-speech classes
(Sirts et al., 2014). Sirts et al. use a model that treats words as customers entering a restaurant,
and at each point, the seating of a word at a table depends on its similarity to the other words
at the table with respect to distributional and morphological features. ÓÄÄe tables represent the
part-of-speech classes.
Another example for the use of distance-dependent CRPs in NLP is by Titov and Kle-
mentiev (2012). In this example, the distance-dependent CRP is used to induce semantic roles
in an unsupervised manner.
7.5.5 SEQUENCE MEMOIZERS
Sequence memoizers (Wood et al., 2009) are hierarchical nonparametric Bayesian models that
deÔ¨Åne non-Markovian sequence models. ÓÄÄeir idea is similar to the one that appears in Sec-
tion 7.4.1. A sequence of distributions over predicted tokens at each step are drawn from the
Pitman-Yor process. To predict the kth token given context x1    xk 1, the distribution Gx1xk 1
7.6. SUMMARY 171
is drawn from a Pitman-Yor process with base distribution Gx2xk 1
. ÓÄÄe distribution Gx2xk 1
,
in turn, is drawn from a Pitman-Yor distribution with base distribution Gx3xk 1
and so on.
Wood et al. describe a technique to make posterior inference with sequence memoizers
eÔ¨Écient. To do so, they use a speciÔ¨Åc sub-family of the Pitman-Yor process, in which the con-
centration parameter is 0. ÓÄÄis enables marginalizing out the distributions Gxi xk 1
for i > 1,
and constructing the Ô¨Ånal predictive distribution Gx1xk 1
. It also enables an eÔ¨Écient represen-
tation of the sequence memoizer, which grows linearly with the sequence length.
Sequence memoizers were mostly used for language modeling, as a replacement for the
usual n-gram Markovian models. ÓÄÄey were further improved by Gasthaus and Teh (2010), in-
troducing a richer hyperparameter setting, and a representation that is memory eÔ¨Écient. Inference
algorithms for this improved model are also described by Gasthaus and Teh (2010). ÓÄÄe richer hy-
perparameter setting allows the modeler to use a non-zero concentration value for the underlying
Pitman-Yor process. ÓÄÄey were also used by Shareghi et al. (2015) for structured prediction.
7.6 SUMMARY
ÓÄÄis chapter gave a glimpse into the use of Bayesian nonparametrics in the NLP literature. Some
of the important concepts that were covered include the following.
‚Ä¢ ÓÄÄe Dirichlet process, which serves as an important building block in many Bayesian non-
parametric models for NLP. ÓÄÄree equivalent constructions of the Dirichlet process were
given.
‚Ä¢ Dirichlet process mixtures, which are a generalization of a Ô¨Ånite mixture model.
‚Ä¢ ÓÄÄe Hierarchical Dirichlet process, in which several Dirichlet processes are nested together
in the model, in order to be able to ‚Äúshare atoms‚Äù between the various parts of the model.
‚Ä¢ ÓÄÄe Pitman-Yor process, which is a generalization of the Dirichlet process.
‚Ä¢ Some discussion of other Bayesian nonparametric models and priors.
In Chapter 8, other examples of models and uses of Bayesian nonparametrics are given,
such as the HDP probabilistic context-free grammar and adaptor grammars.
172 7. NONPARAMETRIC PRIORS
7.7 EXERCISES
7.1. Show that the Chinese restaurant process describes a joint distribution over an exchange-
able set of random variables.
7.2. Let Zn be a random variable denoting the total number of tables open after n customers
were seated in a Chinese restaurant process arrangement (with concentration Àõ). Show
that as n becomes large, E≈íZn¬ç  Àõ log n.
7.3. Consider Equations 7.2‚Äì7.3. Show that indeed a draw Àá from the GEM distribution
satisÔ¨Åes them. You will need to use the deÔ¨Ånition of Àá from Equation 7.1.
7.4. In Section 7.2.1, a Gibbs sampler is given to sample .1/
; .n/
from the Dirichlet process
mixture model. Write down the sampler for the case in which G0 is a Dirichlet dis-
tribution over the p 1 dimensional probability simplex, and p.Xj/ is a multinomial
distribution over p elements.
7.5. Alice derived the sampler in the previous question, but discovered later that a better
choice of G0 is a renormalized Dirichlet distribution, in which none of the coordinates in
  G0 can be larger than 1=2 (assume p > 2). Based on rejection sampling (Section 5.9),
derive a Gibbs sampler for such G0. Consult with Section 3.1.4 as needed.
C H A P T E R 8
Bayesian Grammar Models
One of the most successful applications of the Bayesian approach to NLP is probabilistic models
derived from grammar formalisms. ÓÄÄese probabilistic grammars play an important role in the
modeling toolkit of NLP researchers, with applications pervasive in all areas, most notably, the
computational analysis of language at the morphosyntactic level.
In order to apply Bayesian statistics to inference with probabilistic grammars, we must
Ô¨Årst place a prior over the parameters of the grammar. Probabilistic context-free grammars, for
example, can have a Dirichlet prior on their rule multinomial probabilities. More complex models
can also be used. For example, one can use nonparametric priors such as the Dirichlet process
with probabilistic context-free grammars (PCFGs) to break the independence assumptions that
PCFGs make.
Most of the work done with probabilistic grammars in the Bayesian setting is done in an
unsupervised setting, where only strings are available to the learner, and the goal is to infer a
posterior over derivation trees, or even to induce the structure of the grammar. Still, there are a
few exceptions, where Bayesian learning is used with grammars in a supervised setting‚Äîsome of
these exceptions are mentioned in this chapter.
Most of this chapter is devoted to a speciÔ¨Åc grammar model, probabilistic context-free
grammars, and its use in the Bayesian setting, both parametric and nonparametric. ÓÄÄere are
several reasons for this focus.
‚Ä¢ Probabilistic context-free grammars are the simplest cases of grammar formalisms in which
the rewrite rules have a context-free format of the form a ! Àõ where a is a ‚Äúnontermi-
nal‚Äù and Àõ is some object that can replace a in a partial derivation. Such a rule structure
is common with many other formalisms that have a ‚Äúcontext-free backbone.‚Äù¬π Examples
include linear context-free rewriting systems‚Äîwhich are now being revisited in NLP and
generalize many grammar formalisms used in NLP (Kallmeyer and Maier, 2010, Vijay-
Shanker et al., 1987)‚Äîcombinatory categorial grammars (Steedman and Baldridge, 2011),
and even formalisms where the language is not a set of strings, such as graph rewriting
grammars (Rozenberg and Ehrig, 1999). ÓÄÄey also generalize sequence models, such as
hidden Markov models.
¬πIf a grammar has a ‚Äúcontext-free backbone,‚Äù it does not necessarily mean that the language it generates is context-free. It just
means that its production rules do not require context on the left-hand side.
174 8. BAYESIAN GRAMMAR MODELS
‚Ä¢ ÓÄÄere is a large body of work on probabilistic context-free grammars and their derivatives.
ÓÄÄis means that being well-acquainted with this grammar formalism is important for any
NLP researcher.
‚Ä¢ PCFGs oÔ¨Äer a generic way to derive and communicate about statistical models in NLP.
ÓÄÄey oÔ¨Äer a sweet spot between tractability and expressivity, and as such, their use is not
limited to just syntactic parsing. Many models in NLP can be captured in terms of PCFGs.
In Section 8.1 we cover the use of hidden Markov models, a fundamental sequence label-
ing model that is used in NLP and outside of it. In Section 8.2 we provide a general overview of
PCFGs and set up notation for the rest of this chapter. In Section 8.3 we begin the discussion
of the use of PCFGs with the Bayesian approach. We then move to nonparametric modeling
of grammars, covering adaptor grammars in Section 8.4, and the hierarchical Dirichlet process
PCFGs in Section 8.5. We then discuss dependency grammars in Section 8.6, synchronous gram-
mars in Section 8.7 and multilingual learning in Section 8.8, and conclude with some suggestions
for further reading in Section 8.9.
8.1 BAYESIAN HIDDEN MARKOV MODELS
Hidden Markov models are important models that are used in many NLP applications for se-
quence modeling. HMMs are usually not thought of as ‚Äúgrammar models,‚Äù but they are actually
a special case of probabilistic context-free grammars. ÓÄÄis is clariÔ¨Åed later in Section 8.2.3. A
hidden Markov model is represented by a tuple H D .T ; N; Àò; / where:
‚Ä¢ T is a Ô¨Ånite set of symbols called the emission symbols.
‚Ä¢ N is a Ô¨Ånite set of symbols called the states, such that T \ N D ;.
‚Ä¢ A special state symbol Àò 2 T called ‚Äúthe stop state‚Äù or ‚Äúthe sink state.‚Äù
‚Ä¢  is a vector of parameters such that it deÔ¨Ånes for every s; s0
2 N and every o 2 T the
following non-negative parameters:
 Initial state probabilities: s. It holds that
P
s2N s D 1.
 Emission probabilities: ojs for all s 2 N n fÀòg and o 2 T . It holds that
P
o2T ojs D
1.
 Transition probabilities: s0js for all s 2 N n fÀòg and s0
2 N. It holds that
P
s2N sjs0 D 1.
Hidden Markov models deÔ¨Åne a probability distribution over pairs .x; z/ such that x D
x1    xm is a string over T , and z D z1    zm is a sequence of states from N such that zi ¬§ Àò.
ÓÄÄis distribution is deÔ¨Åned as follows:
8.1. BAYESIAN HIDDEN MARKOV MODELS 175
.
.
z1
. z2
. zm 1
. zm
.
x1
.
x2
.
xm 1
.
xm
Figure 8.1: A hidden Markov model depicted graphically as a chain structure. ÓÄÄe shaded nodes
correspond to observations and the unshaded nodes correspond to the latent states. ÓÄÄe sequence is of
length m, with latent nodes and observations indexed 3 through m 2 reprsented by the dashed line.
p.x; zj/ D z1
x1jz1
m
Y
iD2
zi jzi 1
xi jzi
!
Àòjzm :
HMMs have basic inference algorithms called the forward and backward algorithms. ÓÄÄese
are dynamic programming algorithms that can be used to compute feature expectations given an
observation sequence. For example, they can compute the expected number of times that a cer-
tain emission ojs Ô¨Åres in the sequence, or alternatively, this pair of algorithms can compute the
expected number of times a certain transition sjs0
Ô¨Åres. For a complete description of these algo-
rithms, see Rabiner (1989). ÓÄÄese algorithms are an analog to the inside and outside algorithms
for PCFGs (see Section 8.2.2).
A graphical depiction of a hidden Markov model is given in Figure 8.1. ÓÄÄe chain structure
graphically denotes the independence assumptions in the HMM. Given a Ô¨Åxed set of parameters
for the HMM and a state zi , the observation xi is conditionally independent of the rest of the
nodes in the chain. In addition, given a Ô¨Åxed set of parameters for the HMM and a state zi 1,
the state zi is conditionally independent of all previous states zj , j < i 1.
Hidden Markov models can be deÔ¨Åned with higher order‚Äîin which case the probability
distribution over a given state depends on more states than just the previous one. A trigram
HMM, for example, has the probability over a given state depend on the two previous states.
ÓÄÄe algorithms for inference with trigram HMMs (or higher order HMMs) are similar to the
inference algorithms that are used with vanilla bigram HMMs.
8.1.1 HIDDEN MARKOV MODELS WITH AN INFINITE STATE SPACE
With an HMM, the problem of selecting the number of latent states to use (when the states do
not correspond to a known set of states, such as with supervised part-of-speech tagging) can be
solved using the usual set of tools for controlling for model complexity. For example, with held-
176 8. BAYESIAN GRAMMAR MODELS
.
.
Constants: `, s0, s1
Latent variables: Àá, 0, i , i for i 2 f1; : : :g, Zj for j 2 f1; : : : ; `g
Observed variables: Xj for j 2 f1; : : : ; `g
.
‚Ä¢ Generate a base inÔ¨Ånite multinomial Àá  GEM.s0/
1:
‚Ä¢ Generate i  DP.s1; Àá/ for i 2 f0; 1; : : :g
2:
‚Ä¢ Generate i  G0 for i 2 f1; : : :g
3:
‚Ä¢ Generate z1  0
4:
‚Ä¢ Generate x1  F.jz1
/
5:
‚Ä¢ For j 2 f2; : : : ; `g
6:
 Generate zj  zj 1
7:
 Generate xj  F.jzj
/
8:
Generative Story 8.1: ÓÄÄe generative story of the inÔ¨Ånite hidden Markov model.
out data validation, one can increasingly add more states, each time performing inference on the
data, and checking the behavior of the log-likelihood function on the held-out data.
Another way to overcome the requirement to predeÔ¨Åne the number of latent states in an
HMM is the use of nonparametric modeling, and more speciÔ¨Åcally, the hierarchical Dirichlet
process (Section 7.3). ÓÄÄis allows the modeler to deÔ¨Åne an inÔ¨Ånite state space (with a countable
number of latent states). During inference with sequence observations, the underlying number
of states that is used for explaining the data will grow with the amount of data available.
Hierarchical Dirichlet process hidden Markov models (HDP-HMM) are a rather intuitive
extension, which combines HMMs with the HDP. Here we provide a speciÔ¨Åcation of the HDP-
HMM in the general form that Teh (2006b) gave, but also see the inÔ¨Ånite HMM model by Beal
et al. (2002).
ÓÄÄe Ô¨Årst model component that Teh assumes is a parametric family F.j/ (for  2 ‚Äö) for
generating observations and a base distribution G0 used as a prior over the space of parameters ‚Äö.
For example,  could be a multinomial distribution over a set of observation symbols T , and G0
could be a Dirichlet distribution over the .jT j 1/th probability simplex. In order to generate a
sequence of observations of length `, we use generative story 8.1.
ÓÄÄe generative process assumes a discrete state space, and works by Ô¨Årst generating a base
distribution Àá, which is an inÔ¨Ånite multinomial distribution over the state space. It then draws
8.2. PROBABILISTIC CONTEXT-FREE GRAMMARS 177
S
S
NP-SBJ
DT
ÓÄÄe
NN
governor
VP
MD
could
RB
n‚Äôt
VP
VB
make
NP
PRP
it
,
,
IN
so
S
NP-SBJ
DT
the
NN
lieutenant
VP
VBD
welcomed
NP
DT
the
NNS
guests
.
.
Figure 8.2: An example of a phrase-structure tree in English inspired by the Penn treebank (Marcus
et al., 1993).
another inÔ¨Ånite multinomial distribution for each state. ÓÄÄat inÔ¨Ånite multinomial corresponds to
a transition distribution from the state it is indexed by to other states (so that each event in the
inÔ¨Ånite multinomial is also mapped to one of the states). ÓÄÄen the emission parameters are drawn
for each state using G0, which is a base distribution over parameter space ‚Äö. In order to actually
generate the sequences, the generative process proceeds by following the usual Markovian process
while using the transition and emission distributions. Inference with this inÔ¨Ånite HMM model
is akin to inference with the Hierarchical Dirichlet process (Section 7.3). For the full details, see
Teh (2006b). See also Van Gael et al. (2008).
8.2 PROBABILISTIC CONTEXT-FREE GRAMMARS
One of the most basic, generic model families that is available in NLP is probabilistic context-
free grammars (PCFGs). Probabilistic context-free grammars augment context-free grammars
(CFGs) with a probabilistic interpretation. CFGs are a grammar formalism that provides a mech-
anism to deÔ¨Åne context-free languages (the language of a grammar is the set of strings that can
be generated by the grammar). ÓÄÄey also associate each string in such a language with one or
more grammar derivations in the form of a phrase-structure tree, such as the one in Figure 8.2.
ÓÄÄis phrase-structure tree is a labeled tree, with labels on nodes denoting the syntactic category
of the substring they span in their yield (NP for noun phrase, VP for verb phrase, and so on).
ÓÄÄe yield, read from left to right, represents the sentence for which this derivation was created.
A collection of such trees in a speciÔ¨Åc language is referred to as a ‚Äútreebank.‚Äù
More formally, a context-free grammar is a tuple G D .T ; N; S; R/ where:
‚Ä¢ T is a Ô¨Ånite set of symbols called the terminal symbols. ÓÄÄese include the symbols that appear
at the yield of the phrase-structure trees the CFG generates.
178 8. BAYESIAN GRAMMAR MODELS
‚Ä¢ N is a Ô¨Ånite set of nonterminal symbols, which label nodes in the phrase-structure tree. We
require that T \ N D ;.
‚Ä¢ R is a Ô¨Ånite set of production rules. Each element r 2 R has the form of a ! Àõ, with a 2 N
and Àõ 2 .T [ N/
. We denote by Ra the set fr 2 R j r D a ! Àõg, i.e., the rules associated
with nonterminal a 2 N on the left-hand side of the rule.
‚Ä¢ S is a designated start symbol which always appears at the top of the phrase-structure trees.
In their most general form, CFGs can also have rules of the form a ! " where " is the
‚Äúempty word.‚Äù Of special interest in natural language processing are CFGs which are in Chomsky
normal form. With Chomsky normal form grammars, only productions of the form a ! t (for
a 2 N and t 2 T ) or a ! b c (for a; b; c 2 N) are allowed in the grammar. ÓÄÄe original deÔ¨Ånition
of Chomsky normal form also permits a rule S ! ", but in most uses of CFGs in Bayesian NLP,
this rule is not added to the grammar. For simpliÔ¨Åcation, we will not introduce " rules to the
grammars we discuss. (Still, " rules are useful for modeling speciÔ¨Åc components of linguistic
theories, such as empty categories or ‚Äúgaps.‚Äù)
Chomsky normal form is useful in NLP because it usually has simple algorithms for basic
inference, such as the CKY algorithm (Cocke and Schwartz, 1970, Kasami, 1965, Younger, 1967).
ÓÄÄe simplicity of CNF does not come at the expense of its expressive power‚Äîit can be shown
that any context-free grammar can be reduced to a CNF form that generates equivalent grammar
derivations and the same string language. ÓÄÄerefore, our focus is on CFGs in CNF form.¬≤
A probabilistic context-free grammar attaches a set of parameters  to a CFG G (these
are also called rule probabilities). Here,  is a set of jNj vectors. Each a for a 2 N is a vector of
length jRaj in the probability simplex. Each coordinate in a corresponds to a parameter a!b c or
a parameter a!t . With probabilistic context-free grammars, the following needs to be satisÔ¨Åed:
a!b c  0 a ! b c 2 R (8.1)
a!t  0 a ! t 2 R (8.2)
X
b;cWa!b c2R
a!b c C
X
tWa!t2R
a!t D 1 8a 2 N: (8.3)
ÓÄÄis means that each nonterminal is associated with a multinomial distribution over the
possible rules for that nonterminal. ÓÄÄe parameters  deÔ¨Åne a probability distribution p.Zj/
over phrase-structure trees. Assume z is composed of the sequence of rules r1; : : : ; rm where
ri D ai ! bi ci or ri D ai ! xi . ÓÄÄe distribution P.Zj/ is deÔ¨Åned as:
p.Z D .r1; : : : ; rm/j/ D
m
Y
iD1
ri
!
: (8.4)
¬≤Note that it is not always possible to convert a PCFG to a PCFG in CNF form while retaining the same distribution over
derivations and strings, and approximations could be required. See also Abney et al. (1999).
8.2. PROBABILISTIC CONTEXT-FREE GRAMMARS 179
Not every assignment of rule probabilities that satisÔ¨Åes the above constraints (Equa-
tions 8.1‚Äì8.3) yields a valid probability distribution over the space of trees. Certain rule prob-
abilities lead to PCFGs that are ‚Äúinconsistent‚Äù‚Äîthis means that these PCFGs allocate non-zero
probability to trees of inÔ¨Ånite length. For example, for the grammar with rules S ! S S (with
probability 0.8) and S ! buffalo (with probability 0.2), there is a non-zero probability to generate
trees with an inÔ¨Ånite yield. For a comprehensive discussion of this issue, see Chi (1999).
In most cases in the Bayesian NLP literature, the underlying symbolic grammar in a PCFG
is assumed to be known. It can be a hand-crafted grammar that is compatible with a speciÔ¨Åc NLP
problem, or it can be a grammar that is learned by reading the rules that appear in the parse trees
in a treebank.
With PCFGs, the observations are typically the yield of the sentence appearing in a deriva-
tion z. ÓÄÄis means that PCFGs deÔ¨Åne an additional random variable, X, which ranges over strings
over T . It holds that X D yield.Z/ where yield.z/ is a deterministic function that returns the
string in the yield of z. For example, letting the derivation in Figure 8.2 be z, it holds that yield.z/
is the string ‚ÄúÓÄÄe governor could n‚Äôt make it, so the lieutenant welcomed the guests.‚Äù
ÓÄÄe distribution p.Zjx; / is the conditional distribution for all possible derivation trees
that have the yield x. Since X is a deterministic function of Z, Equation 8.4 deÔ¨Ånes the distri-
bution p.X; Zj/ as follows:
p.x; zj/ D
(
p.zj/ yield.z/ D x
0 yield.z/ ¬§ x:
In this chapter, the words in string x will often be denoted as x1    xm where m is the
length of x, and each xi is a symbol from the terminal symbols T .
Another important class of models related to PCFGs is that of weighted context-free gram-
mars. With weighted CFGs, a!b c and a!t have no sum-to-1 constraints (Equation 8.2). ÓÄÄey
can be of arbitrary non-negative weight. Weighted PCFGs again induce distribution p.Zj/, by
deÔ¨Åning:
p.zj/ D
Qm
iD1 ri
A./
; (8.5)
where A./ is a normalization constant that equals:
A./ D
X
z
m
Y
iD1
ri
:
For consistent PCFGs, A./ D 1. For any assignment of rule probabilities (i.e., assignment
of weights that satisfy Equations 8.1‚Äì8.2), it holds that A./  1. For weighted CFGs, it is not
always the case that A./ is a Ô¨Ånite real number. ÓÄÄe function A./ can also diverge to inÔ¨Ånity
180 8. BAYESIAN GRAMMAR MODELS
for certain grammars with certain weight settings, since the sum is potentially over an inÔ¨Ånite
number of derivations with diÔ¨Äerent yields.
8.2.1 PCFGS AS A COLLECTION OF MULTINOMIALS
It is common to factorize a set of parameters in a Bayesian NLP generative model into a set of
multinomial distributions. In general,  is a vector that consists of K subvectors, each of length
Nk for k 2 f1; : : : ; Kg. ÓÄÄis means that the following is satisÔ¨Åed:
k;k0  0 8k 2 f1; : : : ; Kg and k0
2 f1; : : : ; Nkg
Nk
X
k0D1
k;k0 D 1 8k 2 f1; : : : ; Kg:
With PCFGs, K equals the number of nonterminals in the grammar, and Nk is the size of
Ra for the nonterminal a associated with index k 2 f1; : : : ; Kg. We denote by k;k0 the event k0
in the kth multinomial.
With this kind of abstract model, each multinomial event corresponds to some piece of
structure. With PCFGs, these pieces of structures are production rules. Let fk;k0 .x; z/ be the
number of times that production rule k0
for nonterminal k 2 f1; : : : ; Kg Ô¨Åres in the pair of string
and phrase-structure tree .x; z/. ÓÄÄe probabilistic model deÔ¨Åned over pairs of strings and phrase
structure trees is:
p.x; zj/ D
K
Y
kD1
Nk
Y
k0D1

fk;k0 .x;z/
k;k0 : (8.6)
If we consider x D .x.1/
; : : : ; x.n/
/ and z D .z.1/
; : : : ; z.n/
/ being generated from the like-
lihood in Equation 8.6, independently (given the parameters), then it can be shown the likelihood
of these data is:
p.x; zj/ D
K
Y
kD1
Nk
Y
k0D1

Pn
iD1 fk;k0 .x.i/;z.i//
k;k0 :
8.2.2 BASIC INFERENCE ALGORITHMS FOR PCFGS
Bayesian inference over PCFGs often requires computing marginal quantities for a given sen-
tence, such as the probability of a certain nonterminal spanning a substring in a sentence (called
the ‚Äúinside probability‚Äù) or the total probability of a string according to a given PCFG.
8.2. PROBABILISTIC CONTEXT-FREE GRAMMARS 181
.
.
Input: A probabilistic context-free grammar G with weights , a string x D x1    xm, a
nonterminal a 2 N, a pair of endpoints .j; j 0
/, inside probability chart ‚Äúin.‚Äù
Output: A sampled tree, headed by nonterminal a spanning words xj through xj 0 , based
on the inside-outside chart.
.
1: if j D j0
then
2: z a tree with a root a and the word xj below a
3: return z
4: end if
5: for all rules a ! b c 2 R ranging over b; c 2 N do
6: for q j to j0
1 do
7: Let sampleMult.a ! b c; q/ be a!b c  in.b; j; q/  in.c; q C 1; j 0
/
8: end for
9: end for
10: Normalize s: Let sampleMult.a ! b c; q/ be
sampleMult.a ! b c; q/
P
a!b c;q sampleMult.a ! b c; q/
11: Sample from the multinomial sampleMult an event .a ! b c; q/.
12: zleft SamplePCFG(G, , x, b, .j; q/, in)
13: zright SamplePCFG(G, , x, c, .q C 1; j 0
/, in)
14: z a
zleft zright
15: return z
Algorithm 8.1: A recursive algorithm SamplePCFG for sampling from a probabilistic context-free
grammar (in Chomsky normal form) conditioned on a Ô¨Åxed string and Ô¨Åxed parameters.
ÓÄÄe inference algorithms for identifying these quantities are usually based on dynamic pro-
gramming. It is often the case in the Bayesian setting that these dynamic programming algorithms
are used with weighted grammars, as previously mentioned, i.e., the parameters assigned to the
rules in the grammar do not have to satisfy Equation 8.2. ÓÄÄese kinds of weighted grammars
appear, for example, with variational updates rules such as in Equation 6.11. More discussion of
this appears in Section 8.3.3.
ÓÄÄe Ô¨Årst basic inference algorithm we tackle in this section is the inside algorithm. ÓÄÄe
inside algorithm comes to compute, for a given sentence x1    xm, its total probability (or weight)
according to a PCFG (in CNF form), i.e.:
182 8. BAYESIAN GRAMMAR MODELS
in.S; 1; m/ D
X
zWyield.z/Dx1xm
p.zj/:
Here, p.Zj/ is a PCFG distribution parametrized by weights  with start symbol S. ÓÄÄe
use of the notation in.S; 1; m/, with the arguments 1 and m, is deliberate: this quantity can be
calculated for other nonterminals and other spans in the sentence. Generally, it holds that:
in.a; i; j/ D
X
z2A.a;i;j/
p.zj/;
where A.a; i; j/ D fzjyield.z/ D xi    xj ; h.z/ D ag. ÓÄÄis means that the inside probability of
a nonterminal a for span .i; j/ is the total probability of generating the string xi    xj , starting
from nonterminal a. ÓÄÄe function h.z/ returns the root of the derivation z. Note that in this
formulation we allow for the root to be arbitrary nonterminal, not just S. ÓÄÄe probability p.zj/
is deÔ¨Åned as usual, as the product of all rules in the derivation.
ÓÄÄe inside quantities can be computed through a recursive procedure, using quantities of
the same form. ÓÄÄe following is the recursive deÔ¨Ånition:
in.a; i; i/ D a!xi
a 2 N; a ! xi 2 R; 1  i  m
in.a; i; j / D
j
X
kDi
X
a!b c2R
a!b c  in.b; i; k/  in.c; k C 1; j/ a 2 N; 1  i < j  m:
ÓÄÄe intermediate quantity in.a; i; j/ is to be interpreted as the total weight of all trees
in which the nonterminal a spans words xi    xj at positions i through j . ÓÄÄere are various
execution models for computing the recursive equations above. One simple way is to use bottom-
up dynamic programming, in which the chart elements in.a; i; j/ are computed starting with
those that have small width of j i C 1 and ending with the Ô¨Ånal element in.S; 1; n/ which has
a width of n. One can also use an agenda algorithm, such as the one that was developed by Eisner
et al. (2005)‚Äîsee also Smith (2011).
Another important quantity of interest is the outside probability which is computed using
the outside algorithm. ÓÄÄe outside quantity calculates the probability of generating an ‚Äúouter‚Äù
part of the derivation. More formally, we deÔ¨Åne out.a; i; j/ for any i < j indices in a given string
and a a nonterminal as:
8.2. PROBABILISTIC CONTEXT-FREE GRAMMARS 183
out.a; i; j/ D
X
z2B.a;i;j/
p.zj/;
where B.a; i; j / D fzjyield.z/ D x1    xi 1yxjC1    xn; y 2 T 
; head of y in z is ag. ÓÄÄis
means that the outside probability is the total probability of generating the partial derivation
x1    xi 1axj C1    xn where a is the nonterminal not being fully rewritten yet to a string.
Similarly to the inside probability, the outside probability can also be computed recursively as
follows (this time top-down):
out.S; 1; n/ D 1
out.a; 1; n/ D 0 a 2 N; a ¬§ S
out.a; i; j/ D
j 1
X
kD1
X
b!c a2R
b!c a  in.c; k; i 1/  out.b; k; j/
C
n
X
kDjC1
X
b!a c2R
b!a c  in.c; j C 1; k/  out.b; i; k/ a 2 N; 1  i < j  n:
ÓÄÄe most important use of the inside and outside algorithms is to compute feature expec-
tations of nonterminals spanning certain positions for a given sentence. More formally, if the
following indicator is deÔ¨Åned:
I.ha; i; j i 2 z/ D
(
1 if a spans words i through j in z
0 otherwise;
then the inside and outside probabilities assist in computing:
E≈íI.ha; i; ji 2 Z/jx1    xm¬ç D
P
z;yield.z/Dx1xm
p.zj/I.ha; i; ji 2 z/
p.x1    xmj/
;
because it can be shown that:
E≈íI.ha; i; ji 2 Z/jx1    xm¬ç D
in.a; i; j/  out.a; i; j/
in.S; 1; n/
:
Similarly, expectations of the form E≈íI.ha ! b c; i; k; j i 2 Z/jx1    xm¬ç can also be com-
puted. Here, I.ha ! b c; i; k; j i 2 z/ is 1 if the rule a ! b c is used in z such that a spans words
i to j and below it b spans words i to k and c spans words k C 1 to j. It can be shown that:
E≈íI.ha ! b c; i; k; j i 2 Z/jx1    xm¬ç D
a!b c  in.b; i; k/  in.c; k C 1; j/  out.a; i; j/
in.S; 1; n/
:
(8.7)
184 8. BAYESIAN GRAMMAR MODELS
Last, expectations of the form E≈íI.ha ! xi ; ii 2 Z/jx1    xm¬ç can be similarly deÔ¨Åned as:
E≈íI.ha ! xi ; ii 2 Z/jx1    xm¬ç D
a!xi
 out.a; i; i/
in.S; 1; n/
: (8.8)
ÓÄÄe inside probabilities have an important role in PCFG sampling algorithms as well. ÓÄÄe
inside probabilities are used in a sampling algorithm that samples a single tree z from the dis-
tribution p.Zjx1    xm; /. ÓÄÄe sampling algorithm is given in Algorithm 8.1. ÓÄÄe algorithm
assumes the computation (for a Ô¨Åxed ) of the inside chart of the relevant sentence is available. It
then proceeds by recursively sampling a left child and right child for a given node, based on the
inside chart.
ÓÄÄe above pair of inside and outside algorithms are used with PCFGs in Chomsky normal
form. ÓÄÄere are generalizations to these algorithms that actually work for arbitrary grammars
(without " rules). For example, the Earley algorithm can be used to compute feature expectations
of the form above (Earley, 1970).
ÓÄÄe Computational Complexity of Basic Inference with PCFGs
ÓÄÄe inside and the outside algorithms for Chomsky normal form grammars have an asymptotic
complexity of O.Gm3
/, where m is the length of the string and G is the size of the grammar
(the total length of all of its production rules). ÓÄÄis means that in the worst case, the asymptotic
complexity could be cubic in the number of nonterminals, O.N3
m3
/, with N being the number of
nonterminals, since the size of a grammar can be cubic in the number of nonterminals. Using the
‚Äúfolding trick,‚Äù an optimization technique for improving the complexity of dynamic programming
algorithms (Burstall and Darlington, 1977, Johnson, 2007a), the complexity of the inside and
outside algorithms can be further reduced to O.N 2
m3
C N3
m2
/.
It is important to note that once the inside and outside charts have been computed for a
given string, conditioned on a speciÔ¨Åc set of parameters, we can eÔ¨Éciently draw as many trees as
we want based on this chart and Algorithm 8.1, without re-computing the inside-outside chart.
ÓÄÄe worst-case asymptotic complexity of Algorithm 8.1, as described in the Ô¨Ågure, is linear in
the grammar size and is quadratic in the length of the string. (Note that the running time is
random, and balanced trees will be sampled faster than unbalanced trees.) ÓÄÄe algorithm can be
sped up by computing lines 6‚Äì8 just once in an outer loop, prior to the execution of the sampling
algorithm. Finally, the algorithm can be sped up further by using a logarithmic-time sampling
algorithm for multinomial sampling (see Section 5.9 for more information). ÓÄÄese modiÔ¨Åcations
will be important only if multiple trees are sampled using the same inside chart.
8.2.3 HIDDEN MARKOV MODELS AS PCFGS
HMMs can be captured using a right-branching PCFG. More speciÔ¨Åcally, using the notation in
Section 8.1, deÔ¨Åne a PCFG such that:
‚Ä¢ ÓÄÄe nonterminal symbols are ..N n Àò/  f0; 1g/ [ fSg.
8.3. BAYESIAN PROBABILISTIC CONTEXT-FREE GRAMMARS 185
‚Ä¢ ÓÄÄe terminal symbols are T .
‚Ä¢ For each pair of states s; s0
2 N n fÀòg there is a rule .s; 0/ ! .s; 1/.s0
; 0/ with probability
s0js.
‚Ä¢ For each state s 2 N n fÀòg there is a rule .s; 0/ ! .s; 1/ with probability Àòjs.
‚Ä¢ For each pair of a state and an observation symbol, s 2 N n Àò, o 2 T , there is a rule .s; 1/ !
o with probability ojs.
‚Ä¢ For each state s 2 N n fÀòg there is a rule S ! .s; 0/ with probability s.
ÓÄÄis PCFG induces a distribution over derivations that are equivalent to sequences of states.
To produce the state sequence from a given derivation, one must traverse the derivation from the
top symbol S, and then visit the nonterminal nodes in the tree by always picking the right child
from a given node.
Basic inference with PCFGs can be used to compute feature expectations for HMMs as
well. In fact, the inside and outside dynamic programming algorithms can be viewed as general-
izations of the forward and backward algorithms. Running the inside and outside algorithms on
sequential structures, however, leads to computational complexity that is cubic in the length of
the sequence, while the forward and backward algorithms are linear in the length of the sequence
(and quadratic in the number of states). ÓÄÄe forward and backward algorithms are linear in the
length of the sequence because they exploit the linear structure of the sequence‚Äîthey do not re-
quire computing an inside chart with two endpoints, denoting a span in the string, but instead can
maintain a dynamic programming chart with a single endpoint (the other endpoint is assumed to
be either the last word or the Ô¨Årst word).
8.3 BAYESIAN PROBABILISTIC CONTEXT-FREE
GRAMMARS
ÓÄÄe next natural step, once PCFGs are deÔ¨Åned, is to introduce them in a Bayesian context. ÓÄÄis
is the focus of this section.
8.3.1 PRIORS ON PCFGS
A conjugate product-of-Dirichlets distribution is a natural choice as a prior for the parameters of
PCFGs, which can be viewed as a collection of multinomials. ÓÄÄis prior over  is deÔ¨Åned as:
p.jÀõ/ /
Y
a2N
0
@
Y
a!b c2R.a/

.Àõa!b c 1/
a!b c
1
A 
0
@
Y
a!t2R.a/
.Àõa!t 1/
a!t
1
A : (8.9)
186 8. BAYESIAN GRAMMAR MODELS
Here, Àõ is a vector of hyperparameters that decomposes the same way that  does. Each
Àõa!b c is non-negative. See Chapter 3 for a deÔ¨Ånition of the missing normalization constant in
Equation 8.9.
ÓÄÄere is also room for exploring diÔ¨Äerent granularities for the hyperparameters Àõ. For ex-
ample, instead of having a hyperparameter associated with a rule in the grammar, there can be
a symmetric Dirichlet deÔ¨Åned per nonterminal a 2 N by using a single hyperparameter Àõa for
each a 2 N, or perhaps even a single hyperparameter Àõ for all rules in the grammar.
ÓÄÄe distribution in Equation 8.9 is conjugate to the distribution deÔ¨Åned by a PCFG in
Equation 8.4. Let us assume the complete data scenario, in which derivations from the grammar,
z.1/
; : : : ; z.n/
, are observed. Denote the yields of these derivations by x.1/
; : : : ; x.n/
. ÓÄÄe posterior,
p.jÀõ; z.1/
; : : : ; z.n/
; x.1/
; : : : ; x.n/
/;
is a product-of-Dirichlet distribution with hyperparameters Àõ C
Pn
j D1 f .x.j/
; z.j/
/ where f is a
function that returns a vector indexed by grammar rules such that fa!b c.x; z/ counts the number
of times rule a ! b c appears in .x; z/ and fa!t .x; z/ counts the number of times rule a ! t
appears in .x; z/.
As mentioned earlier, not all assignments of rule probabilities to a PCFG (or more gen-
erally, a multinomial generative distribution) lead to a consistent PCFG. ÓÄÄis means that the
Dirichlet prior in Equation 8.9 potentially assigns a non-zero probability mass to inconsistent
PCFGs. ÓÄÄis issue is largely ignored in the Bayesian NLP literature, perhaps because it makes
little empirical diÔ¨Äerence.
8.3.2 MONTE CARLO INFERENCE WITH BAYESIAN PCFGS
Section 8.2.2 gives a basic algorithm for sampling from a PCFG given its parameters . As
pointed out by Johnson et al. (2007a), this algorithm can be used in a rather straightforward
way for an explicit sentence-blocked Gibbs sampler with a product-of-Dirichlets prior. Johnson
et al. explore MCMC inference with such a model, where x.1/
; : : : ; x.n/
is a set of observed sen-
tences, and the target prediction is a set of derivations z.1/
; : : : ; z.n/
. ÓÄÄe symbolic grammar itself
is assumed to be known. Johnson et al. use a top-level product-of-Dirichlets prior for the PCFG
model, as speciÔ¨Åed in Equation 8.9 for CNF grammars.
ÓÄÄis Gibbs sampler that Johnson et al. design alternates between sampling a derivation z.i/
from p.Z.i/
jx.i/
; / and then sampling  from p.jz.1/
; : : : ; z.n/
; x.1/
; : : : ; x.n/
; Àõ/. ÓÄÄe latter
distribution is also a Dirichlet distribution, because of the conjugacy of the product-of-Dirichlet
distribution to the PCFG likelihood. See Section 8.3.1 and Chapter 5 for more detail. ÓÄÄe sam-
ples from p.Z.i/
jx.i/
; / are drawn using Algorithm 8.1.
Using the terminology from Chapter 5, this Gibbs sampler is an explicit sentence-blocked
sampler. As such, it tends to converge slowly and also requires re-parsing the entire corpus of input
8.3. BAYESIAN PROBABILISTIC CONTEXT-FREE GRAMMARS 187
strings before making updates to . Johnson et al. designed a Metropolis-Hastings algorithm (or
Metropolis-within-Gibbs, to be more exact) that tackles these two issues.
ÓÄÄis sampler is collapsed, and samples directly from the posterior p.ZjX; Àõ/, marginaliz-
ing out . A sentence-blocked Gibbs sampler in the collapsed setting uses the following condi-
tionals:
p.z.i/
jx.1/
; : : : ; x.n/
; z. i/
; Àõ/ D
p.x.i/
jz.i/
/p.z.i/
jz. i/
; Àõ/
p.x.i/jz. i/; Àõ/
: (8.10)
ÓÄÄe distribution p.X.i/
jZ.i/
/ is just a deterministic distribution that places its whole proba-
bility mass on the string yield.Z.i/
/. ÓÄÄe quantity p.z.i/
jz. i/
; Àõ/ can also be computed by relying
on the conjugacy of the prior to the PCFG likelihood (see exercises). However, there is no known
eÔ¨Écient way to compute p.x.i/
jz. i/
; Àõ/. ÓÄÄis means that the conditional distribution can only
be computed up to a normalization constant, making it a perfect candidate for MCMC sampling.
ÓÄÄerefore, Johnson et al. approach the problem of sampling from the conditional distribu-
tion in Equation 8.10 by sampling from a proposal distribution and then making a Metropolis-
Hastings correction step. ÓÄÄeir algorithm is given in Algorithm 8.2.
ÓÄÄere is no need to re-calculate 0
a!Àá
from scratch after each tree draw. One can keep one
global count, together with the current state of the sampler (which consists of a tree per sentence
in the corpus), and then just subtract the counts of a current tree, and add back the counts of a
newly drawn tree.
Inferring sparse grammars Johnson et al. report that Bayesian inference with PCFGs and a
Dirichlet prior does not give a radically diÔ¨Äerent result than a plain EM algorithm without
Bayesian inference. ÓÄÄey tested their Bayesian inference with a simple grammar for analyzing
the morphology of one of the Bantu languages, Sesotho.
ÓÄÄey discovered that their MCMC inference was not very sensitive to the hyperparameters
of the Dirichlet distribution (in terms of the F1-measure of morphological segmentation and
exact segmentation), except for when Àõ < 0:01, in which case performance was low. On the other
hand, small Àõ values (but larger than 0:01) lead to relatively sparse posterior over . ÓÄÄerefore,
small values of Àõ can be used to estimate a sparse , leading to an interpretable model, which
has a small number of grammar rules that are actually active. ÓÄÄe performance of their model
sharply peaks as Àõ is decreased to a value around 0:01. ÓÄÄis peak is followed by a slow decrease in
performance, as Àõ is decreased further to signiÔ¨Åcantly smaller values.
8.3.3 VARIATIONAL INFERENCE WITH BAYESIAN PCFGS
Mean-Ô¨Åeld variational inference for PCFGs (with product-of-Dirichlet prior) can be viewed as a
special case of the variational inference algorithm given for the Dirichlet-Multinomial family in
Section 6.3.1. ÓÄÄe part that must be specialized for PCFGs is the one that calculates the feature
expectations Eq≈ífk;i j old
¬ç in Equation 6.11.
188 8. BAYESIAN GRAMMAR MODELS
.
.
Input: A PCFG, a vector Àõ ranging over rules in the PCFG, a set of strings from the
language of the grammar x.1/
; : : : ; x.n/
.
Output: z D .z.1/
; : : : ; z.n/
/ trees from the posterior deÔ¨Åned by the grammar with a
Dirichlet prior with hyperparameters Àõ.
.
1: Initialize the trees z.1/
; : : : ; z.n/
randomly
2: repeat
3: for i ! 1 to n do
4: Calculate for each rule a ! Àá 2 R
0
a!Àá D
P
j ¬§i fa!Àá .x.j/
; z.j/
/ C Àõa!Àá
P
ÀáWa!Àá2R
P
j ¬§i fa!Àá .x.j/; z.j// C Àõa!Àá
:
5: Draw z from the PCFG distribution p.ZjX D x.i/
; 0
/.
6: Set z.i/
to be z with probability
min
(
1;
p.Z.i/
D zjx.i/
; z. i/
; Àõ/p.z.i/
jx.i/
; 0
/
p.Z.i/ D z.i/jx.i/; z. i/; Àõ/p.Z.i/ D zjx.i/; 0/
)
7: end for
8: until reaching convergence
9: return z.1/
; : : : ; z.n/
Algorithm 8.2: An algorithm for sampling from the posterior of a PCFG with Dirichlet prior:
p.zjx; Àõ/ D
R
 p.z; jx; Àõ/d.
ÓÄÄese feature expectations can be computed using Equation 8.7 and Equation 8.8. More
speciÔ¨Åcally, to compute the expectation of feature fk;i where k represents nonterminal a and i
represents rule a ! b c, one would have to sum expectations of the form E≈íI.ha ! b c; i; k; j i 2
Z/jx1    xm¬ç over all possible i; k; j . Expectations for rules of the form a ! xi can be computed
directly using Equation 8.8.
ÓÄÄe conjugacy of the product-of-Dirichlet distribution to the PCFG likelihood function
makes the derivation of a mean-Ô¨Åeld variational inference algorithm more straightforward, but the
product-of-Dirichlet is not the only possible prior for PCFG variational inference. For example,
Cohen et al. (2009) use a product of logistic normal distributions as a prior over a PCFG grammar.
For inference, they used a Ô¨Årst-order Taylor approximation for the normalization constant of the
logistic normal prior‚Äîa normalization constant for which computing the expectation for the
variational bound is not trivial.
8.4. ADAPTOR GRAMMARS 189
8.4 ADAPTOR GRAMMARS
Probabilistic context-free grammars make strong independence assumptions: the probability of a
partial derivation below a given node is conditionally independent of everything rewritten above
that node, if the identity of the nonterminal at that node is known. ÓÄÄese strong independence
assumptions enable the simple basic inference mechanisms for deriving, for example, feature ex-
pectations, as described in Section 8.2.2.
Still, these independence assumptions can be too strong for modeling language. ÓÄÄis has
been noted in the parsing literature, especially for treebank-driven parsing in a supervised setting;
myriad solutions have been suggested to overcome this issue, and adaptor grammars also address
that issue.
Adaptor grammars are a type of syntactic model, and they are most suitable for a learning
scenario in which only strings are available (without examples of phrase-structure trees, i.e., in an
unsupervised setting). Adaptor grammars deÔ¨Åne distributions over phrase-structure trees, simi-
larly to PCFGs. ÓÄÄe distribution over phrase-structure trees they deÔ¨Åne is based on a PCFG. For
a given phrase-structure tree, denote by Subtrees.z/ the tuple .z0
1; : : : ; z0
m/ where z0
i denotes the
subtree of the ith immediate child of z. In addition, for a given phrase-structure tree z, denote
by h.z/ 2 N, its root nonterminal and by r.z/ 2 R, the rule that appears at the top of the tree.
(ÓÄÄe left-hand side of r.z/ is always h.z/.)
We will assume the existence of a PCFG with the notation from Section 8.2. ÓÄÄen, adaptor
grammars deÔ¨Åne the following set of statistical relationships on distributions Ha and Ga for
a 2 N, where both Ga and Ha are distributions over phrase-structure trees with root a 2 N:
8a 2 N W Ga such that Ga.z/ D a!Àá
m
Y
iD1
Hh.z0
i /.z0
i / (8.11)
where h.z/ D a and r.z/ D a ! Àá and Subtrees.z/ D .z0
1; : : : ; z0
m/
8a 2 N W Ha such that Ha  Ca.Ga/: (8.12)
Here, Ca is an adaptor, which deÔ¨Ånes a distribution over a set of distributions. Each dis-
tribution in this set is deÔ¨Åned over phrase-structure trees. ÓÄÄe distribution Ga serves as the ‚Äúbase
distribution‚Äù for the adaptor. As such, each distribution in the set of distributions mentioned
above, on average, bears some similarity to Ga. In the most general form of adaptor grammars,
the actual adaptor is left unspeciÔ¨Åed. ÓÄÄis means that any distribution over phrase-structure tree
distributions (that is based on Ga) can be used there. If Ca places all of its probability mass on Ga
(this means that Equation 8.12 is replaced with Ha D Ga), then what remains from the above
statistical relationships is the deÔ¨Ånition of a regular PCFG.
ÓÄÄe Ô¨Ånal distribution over phrase-structure trees from which we draw full trees is HS . ÓÄÄe
key idea with adaptor grammars is to choose Ca, so that we break the independence assump-
tions that PCFGs have, which can be too strong for modeling language. Using a Pitman-Yor
190 8. BAYESIAN GRAMMAR MODELS
.
.
Constants: Context-free grammar, adaptors Ca for a 2 N
Hyperparameters: Àõ > 0, sa > 0 strength parameters and discount parameters da, a 2 A
Latent variables: a for a 2 N, Ha for a 2 A, Z.1/
; : : : ; Z.n/
grammar derivations
Observed variables: X.1/
; : : : ; X.n/
strings
.
‚Ä¢ Generate PCFG parameters   Dirichlet.Àõ/ for the underlying CFG (see Equa-
tion 8.9).
1:
2:
‚Ä¢ Generate HS from the following PYAG Equations 8.11‚Äì8.12 with Ca being a
Pitman-Yor process with strength parameter sa and discount parameter da for a 2 A
(for a 2 N n A, Ca is the probabilistic identity mapping).
3:
4:
5:
‚Ä¢ For i 2 f1; : : : ; ng, generate z.i/
 HS .
6:
‚Ä¢ For i 2 f1; : : : ; ng, set x.i/
D yield.z.i/
/.
7:
Generative Story 8.2: ÓÄÄe generative story of the Pitman-Yor adaptor grammar.
process for the adaptors, Ca serves as an example for breaking these independence assumptions.
In Section 8.4.1, this use of the Pitman-Yor process is described.
An adaptor grammar makes a distinction between the set of the ‚Äúadapted non-terminals‚Äù
(denoted A) and the set of non-adapted non-terminals (which are N n A). For the non-adapted
non-terminals, Ca refers only to the probabilistic identity mapping, that maps Ga to a distribution
that is set on Ga with probability 1.
8.4.1 PITMAN-YOR ADAPTOR GRAMMARS
A Pitman-Yor Adaptor Grammar (PYAG) is a statistical model that deÔ¨Ånes a distribution over
phrase-structure tree distributions that satisfy the relationships in Equations 8.11‚Äì8.12 while
using a Pitman-Yor process for Ca with strength parameters sa and discount parameters da for
a 2 N. ÓÄÄe set of adapted nonterminals A is assumed to be ‚Äúnon-recursive‚Äù in the sense that a
nonterminal a 2 A can never appear in a derivation with an ancestor being an a as well. If these
cases are not avoided, a PYAG can be ill-deÔ¨Åned (see exercises).
A draw from a PYAG deÔ¨Ånes a distribution HS .Z/ over phrase-structure trees. ÓÄÄe support
of HS .Z/ (i.e., the phrase-structure trees with non-zero probability) is subsumed by the tree
language of the underlying context-free grammar. ÓÄÄe generative model of a PYAG for generating
z.1/
; : : : ; z.n/
, a set of phrase-structure trees, is given in generative story 8.2.
Without loss of generality, we can actually assume that all nonterminals are adapted, i.e.,
A D N, since the Pitman-Yor process with a concentration parameter da D 1 reduces to the
identity function. Note the added step of generating PCFG parameters from the Dirichlet dis-
8.4. ADAPTOR GRAMMARS 191
tribution in generative story 8.2. ÓÄÄis step appears in the original formulation of PYAG from
Johnson et al. (2007b), to make the model fully Bayesian. ÓÄÄe distribution HS is itself a random
variable, and therefore can be marginalized out. In this manner, one can reveal the distribution
that a PYAG induces directly on phrase-structure trees, similarly to the way PCFGs do.
It is possible to write down an analytic expression that speciÔ¨Åes the distribution that an
adaptor grammar deÔ¨Ånes on Z (a set of n phrase-structure trees). However, understanding adaptor
grammars in terms of the Chinese restaurant process is more revealing and easier to grasp, so we
begin Ô¨Årst by describing the generative process for generating a phrase-structure tree z.i/
based
on the previous trees z.1/
; : : : ; z.i 1/
.
Once the PCFG parameters  are drawn, phrase-structure trees can be consecutively gen-
erated from
p.Z.i/
jz.1/
; : : : ; z.i 1/
; s; d; /: (8.13)
Trees from Equation 8.13 are generated top-down beginning with the start symbol S. Any
non-adapted nonterminal a 2 N n A is expanded by drawing a rule from Ra. ÓÄÄere are two ways
to expand a 2 A.
‚Ä¢ With probability .nz da/=.na C sa/ we expand a to subtree z (a tree rooted at a with a
yield in T 
), where nz is the number of times the tree z was previously generated as a subtree
in z.1/
; : : : ; z.i 1/
and na is the total number of subtrees (tokens) previously generated with
their root being a.
‚Ä¢ With probability .sa C kada/=.na C sa/, a is expanded as in a PCFG by a draw from a
over Ra, where ka is the number of subtrees (types) previously generated with root a in
z.1/
; : : : ; z.i 1/
.
ÓÄÄe counts nz, na and ka are all functions of the previously generated phrase-structure trees
z.1/
; : : : ; z.n/
.
ÓÄÄe state of an adaptor grammar, i.e., an assignment to all latent structures, can be described
using a set of analyses. Assume that we use an adaptor grammar to draw x.1/
; : : : ; x.n/
and their
corresponding phrase-structure trees z.1/
; : : : ; z.n/
. In addition, denote by z.a/ the list of subtrees
that were generated by the adaptor grammar and are headed by nonterminal a. ÓÄÄis means that
z.a/ D ..z.a//.1/
; : : : ; .z.a//.ka/
/ with:
ka
X
iD1
nz.a/.i/ D na:
Adaptor grammars can be viewed as deÔ¨Åning a distribution over a set of analyses. An analysis
u is a pair .z; `/ where z is a phrase structure tree and ` is a function. Let Nodes.z/ be the set
of nodes in the phrase-structure z. We deÔ¨Åne ` 2 Nodes.z/ ! N to be the following. For any
192 8. BAYESIAN GRAMMAR MODELS
q 2 Nodes.z/, l.q/, let a be the nonterminal at node q. ÓÄÄen `.q/ is the index in z.a/ of the
subtree q dominates. If q dominates the subtree z0
, then this means that z0
D z.a/.`.q//
.
Note that z.a/ for a 2 N is a deterministic function of u.1/
; : : : ; u.n/
. ÓÄÄerefore, the dis-
tribution that an adaptor grammar deÔ¨Ånes over analyses can now be readily deÔ¨Åned in terms of
u D .u.1/
; : : : ; u.n/
/ where the phrase structure in u.i/
corresponds to the tree z.i/
. More specif-
ically, it holds:
p.ujs; d; Àõ/ D
Y
a2N

B.Àõa C f .z.a//
B.Àõa/

 PY.m.a/jsa; da/; (8.14)
with f .z.a// being a vector indexed by the rules in the grammar that have a on the left-hand side,
and fa!Àá .z.a// denoting the total count of a ! Àá in all subtrees in the list z.a/. In addition,
m.a/ is a vector of the same length as z.a/, such that mi .a/ D nz.a/.i/ . ÓÄÄerefore, m.a/ is a
vector of integers, and the term PY.m.a/jsa; da/ is computed according to the distribution of the
Pitman-Yor process, deÔ¨Åned in Equation 7.8 and repeated here:
PY.m.a/jsa; da/ D
Qka
kD1

.da.k 1/ C sa/ 
Qmk.a/ 1
j D1 .j da/

Qna 1
iD0 .i C sa/
:
ÓÄÄe function B.y/ is deÔ¨Åned for a vector of integers y as (see also Equation 2.3):
B.y/ D
¬Ä.
Pjyj
iD1 yi /
Qjyj
iD1 ¬Ä.yi /
:
8.4.2 STICK-BREAKING VIEW OF PYAG
Analogously to the stick-breaking process representation for the Pitman-Yor process (or the
Dirichlet process), there is a stick-breaking representation for adaptor grammars (Cohen et al.,
2010).
ÓÄÄis stick-breaking process for describing adaptor grammars was developed to enable vari-
ational inference. ÓÄÄis variational inference algorithm is based on the truncated stick-breaking
variational inference algorithm for the Dirichlet process that was developed by Blei and Jordan
(2004). See the next section for more details.
ÓÄÄe generative process that the stick-breaking process for adaptor grammars follows is de-
scribed in generative story 8.3.
8.4.3 INFERENCE WITH PYAG
ÓÄÄis section discusses the two main approaches for the following inference schemes with adaptor
grammars: sampling and variational inference.
8.4. ADAPTOR GRAMMARS 193
.
.
Constants: Context-free grammar
Hyperparameters: Àõ > 0, sa > 0 strength parameters and discount parameters da, a 2 A
Latent variables: a for a 2 A inÔ¨Ånite multinomials, a for a 2 N, Z.1/
; : : : ; Z.n/
grammar derivations, Za;i for a 2 A and i 2 N
Observed variables: X.1/
; : : : ; X.n/
strings
.
‚Ä¢ For each a 2 A, draw a  Dirichlet.Àõa/.
1:
‚Ä¢ For all a 2 A, deÔ¨Åne Ga as follows:
2:
 Draw œÄa j sa; da  GEM.sa; da/.
3:
 For i 2 f1; : : :g, grow a tree za;i as follows:
4:
 Draw a ! b1 : : : bm from Ra.
5:
 za;i D a
b1    bn
6:
 While yield.za;i / has nonterminals:
7:
 Choose an unexpanded nonterminal b from the yield of za;i .
8:
 If b 2 A, expand b according to Gb (deÔ¨Åned on previous iterations of
step 2).
9:
10:
 If b 2 N n A, expand b with a rule from Rb according to
Multinomial.B/.
11:
12:
 For i 2 f1; : : :g, deÔ¨Åne Ga.za;i / D a;i
13:
‚Ä¢ For i 2 f1; : : : ; ng draw zi as follows:
14:
 If S 2 A, draw z.i/
j GS  GS .
15:
 If S ‚Ä¶ A, draw z.i/
as in line 4.
16:
‚Ä¢ Set x.i/
D yield.z.i/
/ for i 2 f1; : : : ; ng.
17:
Generative Story 8.3: ÓÄÄe generative story for adaptor grammars with the stick-breaking represen-
tation.
MCMC Inference Consider the distribution deÔ¨Åned over analyses as deÔ¨Åned in Equation 8.14.
ÓÄÄe inference usually considered with adaptor grammars is one such that it infers parse trees
(phrase-structure trees) for a given set of strings x D .x.1/
; : : : ; x.n/
/.
194 8. BAYESIAN GRAMMAR MODELS
ÓÄÄe distribution over phrase structure trees can be derived from the distribution over
analyses by marginalizing out the phrase-structure trees. More speciÔ¨Åcally, we are interested in
p.Zjx; s; d; Àõ/. However, computing this posterior is intractable.
In order to perform inference, Johnson et al. (2007b) suggest using a compononent-wise
Metropolis-Hastings algorithm. ÓÄÄey Ô¨Årst specify how to create a static PCFG, a snapshot of
the adaptor grammar, based on a speciÔ¨Åc state of the adaptor grammar. ÓÄÄis snapshot grammar
includes all rules in the underlying context-free grammar and rules that rewrite a nonterminal
directly to a string, corresponding to subtrees that appear in the history derivation vectors z.a/
for a 2 N. All of these rules are assigned probabilities according to the following estimate:
0
a!Àá D

kada C sa
na C da

fa!Àá .z.a// C Àõa!Àá
ka C
P
a!Àá2Ra
Àõa!Àá
!
C
X
iWyield.z.a/.i//DÀá
nz.a/.i/ sa
na C da
: (8.15)
ÓÄÄe Ô¨Årst two multiplied terms are responsible for selecting a grammar rule from the under-
lying context-free grammar. ÓÄÄe term to the right of the sum is the MAP estimate of the rules
added to the snapshot grammar, in the form of a nonterminal rewriting to a string.
ÓÄÄis snapshot grammar is created, and then used with a Metropolis-Hastings algorithm,
such that the proposal distribution is based on the snapshot grammar with the 0
estimates from
Equation 8.15 (i.e., we use the snapshot grammar to deÔ¨Åne a distribution over the analyses, condi-
tioning on the strings in the corpus). ÓÄÄe target distribution is the one speciÔ¨Åed in Equation 8.14.
Note that the real target distribution needs a normalization constant, corresponding to the prob-
ability of the strings themselves according to the adaptor grammar, but this constant is canceled
in the MH algorithm, when calculating the probability of rejecting or accepting the update.
Sampling with their MH algorithm is component-wise‚Äîeach analysis u.i/
is sampled
based on the snapshot grammar. At this point, the acceptance ratio is calculated, and if the MH
sampler decides to accept this sample for u.i/
, the state of the sampler is updated, and the snapshot
grammar is re-calculated.
Variational Inference Cohen et al. (2010) describe a variational inference algorithm based on
the stick-breaking representation for adaptor grammars described in Section 8.4.2. ÓÄÄe main idea
in their variational inference algorithm is similar to the idea that is used for variational inference
algorithms with the Dirichlet process mixture (see Section 7.2). Each nonparametric stick for each
adapted nonterminal (and the corresponding strength and concentration parameters) is associated
with a variational distribution, which is a truncated stick‚Äîi.e., it is a distribution that follows a
Ô¨Ånite version of the GEM distribution (see Equation 7.1).
One major advantage of the truncated stick-breaking variational inference algorithm for
adaptor grammars over MCMC inference is that its E-step can be parallelized. On the other
hand, its major disadvantage is the need to select a Ô¨Åxed subset of strings for each adapted non-
terminal to which the variational distributions may assign non-zero probability to. A speciÔ¨Åc
subset of strings, for a speciÔ¨Åc adapted nonterminal is selected from the set of strings that can be
8.4. ADAPTOR GRAMMARS 195
.
.
Constants: n, number of samples
Hyperparameters: Àõ  0; Àõtype
 0; Àõemission
 0; Àõbinary
 0 concentration parameters
Parameters: Àá, 
type
k
, emission
k
, 
binary
k
‚Äîdistributions over a discrete of Ô¨Ånite set
Latent variables: Z.1/
; : : : ; Z.n/
Observed variables: X.1/
; : : : ; X.n/
.
‚Ä¢ Draw Àá, an inÔ¨Ånite column vector, from the GEM distribution with hyperparameter
Àõ. ÓÄÄe inÔ¨Ånite vector Àá indices correspond to nonterminal symbols in the grammar.
1:
2:
‚Ä¢ For each grammar symbol k 2 f1; 2; : : :g
3:
 Draw 
type
k
 Dirichlet.Àõtype
/.
4:
 Draw emission
k
 Dirichlet.Àõemission
/.
5:
 Draw 
binary
k
 DP.Àõbinary
; ÀáÀá>
/.
6:
‚Ä¢ For i 2 f1; : : : ; ng draw a tree z.i/
(and a string x.i/
) as follows:
7:
 While the yield of z.i/
is not all terminal symbols:
8:
 Pick an unexpanded node in z.i/
. Denote its nonterminal by k.
9:
 Draw a rule type from 
type
k
10:
 If the rule type is ‚Äúemission,‚Äù expand this node by drawing a rule from
emission
k
.
11:
12:
 If the rule type is ‚Äúbinary,‚Äù expand this node by drawing a rule from 
binary
k
.
13:
 Set x.i/
to be the yield of the fully expanded z.i/
.
14:
Generative Story 8.4: ÓÄÄe generative story for the HDP-PCFG model.
the yield of a subtree dominated by that nonterminal. Cohen et al. use heuristics in order to select
such subset for each adapted nonterminal.
Online and Hybrid Methods Zhai et al. (2014) have developed an inference algorithm for adap-
tor grammars that combines MCMC inference and variational inference. ÓÄÄe inference algorithm
in this case is an online algorithm. ÓÄÄe training data is processed in mini-batches. At each mini-
batch processing step, the algorithm updates the posterior so that it reÔ¨Çects the information in
the new data. During this update, MCMC inference is used to estimate the suÔ¨Écient statistics
that are needed in order to make an update to the current posterior.
196 8. BAYESIAN GRAMMAR MODELS
ÓÄÄe main motivation behind such an inference algorithm, and more generally online algo-
rithms, is the ability to process an ongoing stream of data, without the need to either iteratively
go through the data several times, or keep it all in memory.
8.5 HIERARCHICAL DIRICHLET PROCESS PCFGS
(HDP-PCFGS)
With parametric priors on PCFGs, such as the one described in Section 8.3.1, the symbolic
grammar is Ô¨Åxed, and as a consequence, the number of nonterminals in the grammar is Ô¨Åxed
as well. Liang et al. (2007) introduced a PCFG model that overcomes this limitation by using
nonparametric Bayesian modeling. ÓÄÄeir goal is to automatically decide, through Bayesian non-
parameteric modeling, on the number of nonterminals that is needed to accurately represent the
data.
ÓÄÄeir model, which is based on the hierarchical Dirichlet process (see Chapter 7), lets the
number of syntactic categories grow as more trees are observed, and the prior over the grammar
includes an inÔ¨Ånite (countable) number of nonterminals. ÓÄÄe parameters of the model, , are
an inÔ¨Ånite vector with the following subvectors (k in the following varies over an inÔ¨Ånite set of
nonterminals).
‚Ä¢ 
type
k
for k 2 f1; : : :g - for each k, a multinomial of Ô¨Åxed length, which gives the distribution
over ‚Äúrule types‚Äù that are available in the grammar for nonterminal k. Since Liang et al.‚Äôs
experiment with grammars in Chomsky normal form, the available rule types are ‚Äúemission‚Äù
and ‚Äúbinary‚Äù for preterminal rules and binary rules. ÓÄÄerefore, the size of this multinomial
for HDP-PCFGs is 2.
‚Ä¢ emission
k
for k 2 f1; : : :g - for each k, this is a multinomial distribution over the terminal
symbols in the grammar. It corresponds to rule probabilities for rewriting a nonterminal to
a word.
‚Ä¢ 
binary
k
for k 2 f1; : : :g - this is an inÔ¨Ånite multinomial distribution over pairs of nontermi-
nals. It can therefore also be seen as a doubly-inÔ¨Ånite matrix indexed by pairs of nontermi-
nals. Each 
binary
k;k1;k2
gives the probability of a binary rule that rewrites a nonterminal k to the
pair of nonterminals k1 and k2 on the right-hand side.
ÓÄÄe generative story of their model (HDP-PCFG) is given in generative story 8.4. Note the
prior distribution for binary rule probabilities, which is denoted by DP.Àõbinary
; ÀáÀá>
/. ÓÄÄe vector Àá
is an inÔ¨Ånite vector, with all positive elements that sum to one. ÓÄÄerefore, ÀáÀá>
is a doubly-inÔ¨Ånite
matrix with coordinate k; k0
being ÀákÀák0 , such that the sum over all elements in this matrix is 1
as well. It can be viewed as a distribution over the right-hand sides of binary rules. A draw from
the DP prior that uses ÀáÀá>
as a base distribution is a doubly-inÔ¨Ånite matrix of the same form (in
which all elements sum to 1), and this matrix corresponds to all possible binary rule expansions
of pairs of nonterminals from the countable set of nonterminals available in Àá.
8.5. HIERARCHICAL DIRICHLET PROCESS PCFGS (HDP-PCFGS) 197
ÓÄÄe generative process above is actually missing an essential detail about the generation of
the top root symbol. Liang et al. did not address this issue in their model description, but it can
be easily overcome by adding an additional initial step that draws a root nonterminal for the tree
from a distribution generated from DP.Àõroot
; Àá/.
ÓÄÄe HDP-PCFG model is a hierarchical Dirichlet process model because of the Dirichlet
process draw of binary rules on top of the draw of Àá from the GEM distribution. ÓÄÄe draw of Àá
provides the basic set of nonterminals and is a building block in constructing the rule probabilities
for binary rules.
ÓÄÄis model demonstrates again the importance of ‚Äúatom sharing‚Äù with the hierarchical
Dirichlet process (see Section 7.3). ÓÄÄe top draw from the GEM distribution of a countable set
of atoms ensures that there is a non-zero probability for the nonterminals to be shared in the
binary rules.
Liang et al. developed an inference algorithm for their HDP-PCFG model that is based on
the truncated stick-breaking representation that Blei and Jordan (2004) developed for the goal of
performing inference with a Dirichlet process mixture model. Liang‚Äôs et al. algorithm is based on
mean-Ô¨Åeld variational inference, such that the binary rule parameters, the emission parameters,
the rule type parameters (for a Ô¨Ånite, truncated, subset of symbol k 2 f1; : : : Kg), the distribution
over Z.i/
and the distribution over Àá are factorized: each component above has its own variational
distribution.
8.5.1 EXTENSIONS TO THE HDP-PCFG MODEL
PCFGs, by themselves, are rather weak models for modeling language, or its syntax, to be more
precise. Reading a PCFG grammar oÔ¨Ä a treebank and using it as it is (using any kind of reasonable
estimation technique, Bayesian or frequentist), leads to rather poor results.
Indeed, it has been noted in the literature that PCFGs by themselves can be too weak to
model syntax in natural language. One of the biggest problems is that the syntactic categories that
appear in various treebanks, by themselves, do not provide suÔ¨Écient contextual information on
their own for the derivation steps. ÓÄÄis is true whenever a grammar is extracted from a treebank
by considering a node and its immediate children. To remedy this, one can reÔ¨Åne the syntactic
categories in this extracted grammar with latent states (Matsuzaki et al., 2005, Prescher, 2005).
ÓÄÄis means that each nonterminal in the grammar is indexed with an integer that denotes its
state‚Äîand this state is never observed.
In the frequentist use of L-PCFGs, the goal of the learner is to estimate the parameters of
this latent-variable PCFG, without ever observing the latent states. ÓÄÄe statistical parsing model
itself is still a PCFG, but now the data is incomplete, because it does not include the additional
information about the latent states of each syntactic node. ÓÄÄis changes the expressive power of
a vanilla PCFG model extracted from the treebank, since now the probability of each derivation
is a sum-of-products of possible derivations‚Äîthe derivations now include latent states, and the
sum is taken with respect to all possible combinations of latent states for that derivation. With a
198 8. BAYESIAN GRAMMAR MODELS
vanilla PCFG, the probability of a derivation is just the product of the rules that appear in that
derivation.
However, the problem of choosing the number of latent states associated with each non-
terminal, though, is not trivial. Previous work only attempted to use a Ô¨Åxed number of states, and
later estimation algorithms for latent-variable PCFGs used techniques such as coarse-to-Ô¨Åne EM
(Petrov et al., 2006) and other automatic splitting techniques (Dreyer and Eisner, 2006), thresh-
old singular values using spectral methods (Cohen and Collins, 2014, Cohen et al., 2013) and
other estimation algorithms based on the method of moments (Cohen et al., 2014).
In the Bayesian context, Liang et al. extended the HDP-PCFG model to a grammar re-
Ô¨Ånement model, where Bayesian nonparametrics help choose the number of latent states to reÔ¨Åne
the nonterminals in a PCFG. Instead of having an inÔ¨Ånite set of nonterminals that are never ob-
served, they designed a model like the HDP-PCFG extending a Ô¨Åxed set of known nonterminals
and binary rules, such that an inÔ¨Ånite set of atoms reÔ¨Ånes these nonterminals and rules.
Let N be a Ô¨Åxed set of nonterminals. ÓÄÄen, the reÔ¨Ånement extension of the HDP-PCFG
draws distributions of the following form.
‚Ä¢ For each nonterminal a 2 N, an inÔ¨Ånite multinomial Àáa is drawn from a GEM distribution.
‚Ä¢ For each nonterminal a 2 N and k 2 f1; 2; : : :g, an inÔ¨Ånite multinomial emission
a;k
is drawn.
ÓÄÄe index k ranges over the nonterminal reÔ¨Ånements.
‚Ä¢ For each rule a ! b c and k 2 f1; 2; : : :g, a doubly-inÔ¨Ånite matrix 
binary
a!b c;k
is drawn from
a Dirichlet process with ÀábÀá>
c being the base distribution.
ÓÄÄe parameters above are very similar to the parameters drawn for the vanilla HDP-PCFG,
only they are indexed with nonterminals from the Ô¨Åxed set, or a rule from the Ô¨Åxed set of rules.
In addition, the authors also add parameters for unary rules.
8.6 DEPENDENCY GRAMMARS
Dependency grammar (Tesni√®re, 1959, Tesni√®re et al., 2015) refers to linguistic theories that de-
scribe syntax using directed trees (in the graph theoretic sense). In these trees, words are vertices
and edges that denote syntactic relationships. For the use of dependency grammar and depen-
dency parsing in natural language processing, see K√ºbler et al. (2009).
8.6.1 STATE-SPLIT NONPARAMETRIC DEPENDENCY MODELS
Finkel et al. (2007) devised several nonparametric Bayesian models, based on the hierarchical
Dirichlet process, for dependency trees. At the core of their models is the idea to generate a
dependency tree with the nodes being latent states originating in a set of atoms from a Dirichlet
process. Each such atom is a distribution over words, or observations. Once a latent state has been
generated, it then generates its observations.
8.6. DEPENDENCY GRAMMARS 199
The King welcomed the guests .
5 1 7 3 2 1
Figure8.3: A dependency tree with latent states. ÓÄÄe Ô¨Årst line denotes the latent states and the second
line denotes the words generated from these latent states.
Let Z denote a random variable that ranges over dependency trees, such that zi denotes
the state of node i in the tree. ÓÄÄese states are taken from some discrete set. In addition, let X
denote the observations generated by each state, ranging over some vocabulary. ÓÄÄen, xi denotes
the observation generated by state zi . ÓÄÄe list of children of a node i is denoted by c.i/, and
Zc.i/ denotes a vector of latent states ranging over the children of node i. A graphical example of
this formulation of dependency trees is given in Figure 8.3. In this Ô¨Ågure, Zc.3/ D .1; 2; 1/ and
c.2/ D .1/. If i is a leaf node, then c.i/ D ;, therefore, c.1/ D c.4/ D c.6/ D ;. (ÓÄÄe nodes are
numbered according to their order in the sentence, i.e., the node 1 is ‚ÄúÓÄÄe,‚Äù node 2 is ‚Äúking,‚Äù node
3 is ‚Äúwelcomed‚Äù and so on.)
Finkel et al. suggests three progressively advanced models for generating z.1/
; : : : ; z.n/
and
their corresponding x.1/
; : : : ; x.n/
given the model parameters. ÓÄÄe models that Finkel et al. sug-
gest assume knowledge of the actual tree structure. ÓÄÄe goal of their models is to populate this tree
structure with latent states and observations. ÓÄÄeir models are based on a probabilistic decompo-
sition of a top-down generation of the latent states, each time generating the children states of a
node conditioned on the parent state. If node 1 is the root of Z, a dependency tree with m nodes,
then:
p.z; x/ D p.z1/ 
0
@
Y
iWc.i/¬§;
p.zc.i/jzi /
1
A 
m
Y
iD1
p.xi jzi /
!
: (8.16)
With their Ô¨Årst model, ‚Äúindependent children,‚Äù it holds that:
p.zc.i/jzi / D
Y
j 2c.i/
p.zj jzi /: (8.17)
ÓÄÄe independent children model is not realistic for modeling natural language, because it
assumes independence assumptions that are too strong (all siblings are conditionally independent
given the parent). For this reason, Finkel et al. suggest two additional models. In their second
model, ‚ÄúMarkov children,‚Äù a child node is assumed to be conditionally independent of the rest of
the children given the parent and its sibling. More speciÔ¨Åcally, if c.i/ D .j1; : : : ; jr /, then:
200 8. BAYESIAN GRAMMAR MODELS
p.zc.i/jzi / D p.zj1
jzi / 
r
Y
kD2
p.zjk
jzjk 1
; zi /
!
: (8.18)
Finkel et al. do not specify in which order they generate the children, which is necessary to
know to complete the model. ÓÄÄeir last model, ‚Äúsimultaeneous children,‚Äù assumes that all children
are generated as one block of nodes. ÓÄÄis means that p.zc.i/jzi / is not decomposed.
ÓÄÄe main idea in Finkel‚Äôs et al. model is to use nonparametric distributions for Zi . ÓÄÄe
latent states are assumed to obtain an integer value f1; 2; : : :g. ÓÄÄe prior over the latent state
distributions is constructed using the hierarchical Dirichlet process (see Section 7.3).
For the independent children model, the generative process is the following. First, a basic
distribution over the integers is drawn from GEM.s0/, where s0 is a concentration parameter.
ÓÄÄen, for each k 2 f1; 2; : : :g, a distribution k  DP.; s1/ is drawn. ÓÄÄe distributions k (for
k  2) are used for the conditional distributions p.Zj jZi / in Equation 8.17‚Äîi.e., p.zj jZi D
k/ D k;zj
. (Note that 1 is used for the distribution p.Z1/ as it appears in Equation 8.16.)
In addition, for generating the observations X, a multinomial distribution k is generated from
a Dirichlet distribution. ÓÄÄen the observation distributions in Equation 8.16 are set such that
p.xi jZi D k/ D k;xi
.
For their simultaneous children model, Finkel et al. draw a distribution over the latent states
for all children from DP.s2; G0/, where G0 is deÔ¨Åned to be the independent children distribution
from 8.17 drawn from the prior described above. ÓÄÄis draw deÔ¨Ånes p.Zc.i/jZi / for each node
i in the dependency tree. According to Finkel et al., the use of the independent children distri-
bution as a base distribution for the simultaneous children distribution promotes consistency‚Äîif
a certain sequence of children states has high probability, then similar sequences of latent states
(i.e., sequences that overlap with the high probability sequence) will also have high probability.
ÓÄÄe prior over the latent state distributions for the Markov children model (Equation 8.18)
makes similar use of the hierarchical Dirichlet process. With this model, k` are generated, cor-
responding to distributions over latent states conditioning on a pair of latent states, a parent and
a sibling, with the parent being assigned to latent state k and the sibling being assigned to latent
state `. ÓÄÄe observations are handled identically to the independent children model.
8.7 SYNCHRONOUS GRAMMARS
ÓÄÄe term ‚Äúsynchronous grammars‚Äù broadly refers to grammars that deÔ¨Åne multiple-string lan-
guages (languages which are sets of tuples of strings), most commonly two languages. Usually
this means that such grammars are deÔ¨Åned over two sets of vocabularies, T1 and T2 (each for a
diÔ¨Äerent language, for example French and English), and their grammar rules generate a syn-
chronous derivation: a derivation that can be decomposed into two parse trees over two strings in
the languages. DiÔ¨Äerent parts of the parse trees are aligned to each other. Naturally, there are
probabilistic extensions to synchronous grammars.
8.8. MULTILINGUAL LEARNING 201
ÓÄÄe most common type of probabilistic synchronous grammars in NLP is synchronous
PCFGs. ÓÄÄe rules in such a grammar have a Ô¨Åxed nonterminal set N, and rules of the form a !
hÀõ; Àái where a 2 N and Àõ 2 .N [ T1/
and Àá 2 .N [ T2/
. ÓÄÄe right-hand side Àõ corresponds
to one language while Àá corresponds to the other. In addition, there is an alignment function for
each rule that maps between nonterminals in Àõ and nonterminals in Àá. DiÔ¨Äerent restrictions on Àõ
and Àá coupled with restrictions on the alignments yield diÔ¨Äerent families of synchronous PCFGs.
For example, with inversion-transduction grammars (Wu, 1997), Àõ D Àõ1    Àõm is a string over
the nonterminals and vocabulary, and Àá is either the same string with elements from T1 in Àõ
replaced with elements from T2 or the Àõ reversed (Àõ reversed is Àõm  Àõ1) with elements from T1
replaced with elements from T2. ÓÄÄe identical nonterminals in Àõ and Àá are aligned to each other.
ÓÄÄe natural application for synchronous grammars is machine translation. Synchronous
grammars are typically used for syntax-based machine translation, in which the nonterminals carry
some syntactic interpretation (such as denoting noun phrases or verb phrases), and hierarchical
phrase-based translation, in which the nonterminals usually do not have a syntactic interpretation.
For more information about synchronous grammars and their relation to machine translation, see
Williams et al. (2016). Much of the work about synchronous grammars in a Bayesian context was
done with the aim of learning the grammar rules themselves (synchronous grammar induction)
for machine translation, usually through nonparametric Bayesian modeling. ÓÄÄis includes work by
Blunsom et al. (2009b), Blunsom et al. (2009a), Neubig et al. (2011), Sankaran et al. (2011) and
Levenberg et al. (2012). Yamangil and Shieber (2010) made use of synchronous grammar induc-
tion algorithms for the problem of sentence compression. Earlier work that learns the grammar
rules of a synchronous grammar (for phrase-based machine translation) without necessarily using
nonparametric modeling includes work by Zhang et al. (2008).
ÓÄÄere are extensions of other Bayesian grammars to the synchronous setting. For example,
Huang et al. (2011) extended adaptor grammars (Section 8.4) to synchronous adaptor grammars,
and used them to solve a transliteration problem. ÓÄÄe grammar they used maps syllables in one
language to the other, with certain nonterminals being adapted to capture groups of syllables that
are transliterated to other groups of syllables in the target language.
8.8 MULTILINGUAL LEARNING
ÓÄÄis chapter focuses on structured models where the underlying backbone model is a well-known
formalism or grammar, such as a hidden Markov model or a probabilistic context-free grammar.
However, over the years, many researchers have developed creative generative models to solve
speciÔ¨Åc problems in natural language processing. ÓÄÄe basic building blocks for these models are a
set of probability distributions such as the multinomial distribution (Chapter 3) or the Dirichlet
distribution. ÓÄÄese blocks are assembled together, just like Lego pieces, into one model. In this
section, we provide an example for one such thread of models aimed at multilingual learning.
Multilingual learning is a broad umbrella term for the use of language data in multiple
languages to estimate models for each of the languages that can be used to solve problems in NLP,
202 8. BAYESIAN GRAMMAR MODELS
.
.
Hyperparameters: Àõ0; Àõ1; Àõ0
0; Àõ0
1; Àõ! > 0
Constants: N and N0
, lengths of sentences
Parameters: t 2 RjV j
, t 2 RjT j
for t 2 T , 0
t 2 RjV 0j
, 0
t 2 RjT 0j
for t 2 T 0
, !
Latent variables: y1; : : : ; yN 2 T , y0
1; : : : ; y0
N0 2 T 0
POS tags for two languages
Observed variables: x1; : : : ; xN 2 V , x0
1; : : : ; x0
N0 2 V 0
, words for Ô¨Årst and second lan-
guage, a alignment between words
.
‚Ä¢ (First language) Draw transition multinomial distribution over tags t for each t 2 T
from a Dirichlet distribution with hyperparameter Àõ0.
1:
2:
‚Ä¢ (First language) Draw emission multinomial distribution over vocabulary t for each
t 2 T from a Dirichlet distribution with hyperparameter Àõ1.
3:
4:
‚Ä¢ (Second language) Draw transition multinomial distribution over tags 0
t for each
t 2 T 0
from a Dirichlet distribution with hyperparameter Àõ0
0.
5:
6:
‚Ä¢ (Second language) Draw emission multinomial distribution over vocabulary 0
t for
each t 2 T 0
from a Dirichlet distribution with hyperparameter Àõ0
1.
7:
8:
‚Ä¢ Draw a multinomial distribution ! over T  T 0
from a Dirichlet distribution
9:
‚Ä¢ DeÔ¨Åne the distribution for four tags y; y0 2 T and y0
; y0
0 2 T 0
10:
p.y; y0
jy0; y0
0/ / y0;y0
y0
0;y0 !y;y0 :
‚Ä¢ Let a0 D fi j :9j.i; j/ 2 ag be the unaligned indices in a for the Ô¨Årst language
11:
‚Ä¢ Let a0
0 D fj j :9i.i; j/ 2 ag be the unaligned indices in a for the second language
12:
‚Ä¢ Draw tags y1; : : : ; yN and y0
1; : : : ; y0
N0 from the distribution
13:
p.y1; : : : ; yN ; y0
1; : : : ; y0
N0 / D
Y
i2a0
yi 1;yi
Y
j 2a0
0
y0
j 1;y0
j
Y
.i;j/2a
p.yi ; y0
j j yi 1; y0
j 1/:
‚Ä¢ (First language) For each i 2 ≈íN¬ç, emit a word xi from the multinomial yi
.
14:
‚Ä¢ (Second language) For each j 2 ≈íN0
¬ç, emit a word x0
j from the multinomial y0
j
.
15:
Generative Story 8.5: Continues.
8.8. MULTILINGUAL LEARNING 203
GenerativeStory8.5: Continued. ÓÄÄe generative story for the bilingual part-of-speech tagging model
of Snyder et al. ÓÄÄe alignment a is a set of pairs of indices that align a word in one language to a word in
the other language. ÓÄÄe alignment does not have to be fully speciÔ¨Åed (i.e., words in both languages can
be unaligned). Each word is aligned to at most one word. ÓÄÄe set of words (vocabulary) in each language
is V and V 0
, respectively. ÓÄÄe set of part-of-speech tags in each language is T and T 0
, respectively. ÓÄÄe
generative story describes a draw for a single pair of sentences. ÓÄÄe parameters t , t , 0
t and 0
t are
also latent variables, and integrated out during inference.
such as parsing or part-of-speech tagging. Multilingual learning usually exploits some weak or
strong alignment between the corpora in the diÔ¨Äerent languages.
8.8.1 PART-OF-SPEECH TAGGING
One of the early uses of Bayesian learning in the context of multilingual learning was introduced
by Snyder et al. (2008). ÓÄÄe model Snyder et al. introduced learns bilingual part-of-speech tag
models in an unsupervised manner.
ÓÄÄe generative story for the bilingual POS tagging model is described in generative
story 8.5. As expected in a Bayesian context, the generative story starts by drawing the parame-
ters of the model from a prior distribution‚Äîthe emission and transition parameters. ÓÄÄe emission
parameters are multinomial distributions over the set of words in each language that condition
on a speciÔ¨Åc POS tag. ÓÄÄe transition parameters generate new tags based on previous ones. ÓÄÄe
tag generation is based on an alignment between the sentences in the two languages.¬≥ ÓÄÄe main
novelty that the model of Snyder et al. introduces is based on the idea that the POS tags of two
aligned words provide information about each other. Whenever two words are aligned in the two
sentences, their POS tags are coupled through a ‚Äúcoupling distribution‚Äù !.
In principle, this model can also be used in a frequentist setting, and estimated with an
algorithm such as the expectation-maximization algorithm (Appendix A). However, there are
several advantages to using a Bayesian model in this setup. First, all parameters for the diÔ¨Äerent
languages are drawn from the same prior distribution. ÓÄÄis means that this prior distribution ties
all of these languages together through one universal distribution. ÓÄÄis prior distribution itself is
parametrized, and this hyperparameter can be thought of as describing a property of all languages
used in the data (such as the level of the sparsity in the transition and emission parameters).
ÓÄÄe inference mechanism that Snyder et al. used to sample POS tags and words based on
the POS tags is reminiscent of hidden Markov models (HMMs). ÓÄÄe model of Snyder et al. relies
on an alignment to generate pairs of tags together, but the information these tags condition on is
the same type of information bigram HMMs use‚Äîthe previous tags before them in the sequence.
¬≥ÓÄÄe alignments are assumed to be Ô¨Åxed and observed. ÓÄÄe alignments are generated using the machine translation tool
GIZA++ (Och and Ney, 2003).
204 8. BAYESIAN GRAMMAR MODELS
Snyder et al. (2009b) further extended their model to introduce a fully multilingual model:
a model for POS tagging that models more than two languages. ÓÄÄey do so by introducing a new
ingredient into their model‚Äîsuperlingual tags which are coarse tags that are presupposed to be
common to all languages, but are latent. ÓÄÄese superlingual tags are generated from a nonpara-
metric model (Chapter 7).
8.8.2 GRAMMAR INDUCTION
ÓÄÄe problem of grammar induction in NLP is an umbrella term for the unsupervised learning
of syntax in language in various forms. It can refer to the learning of an actual grammar with its
rules (such as a context-free grammar), the estimation of the parameters of an existing grammar
(production probabilities) from strings only, or to the induction of parse trees from a set of strings,
not necessarily with an explicit grammar formalism.
In the context of multilingual learning, Snyder et al. (2009a) described a Bayesian model
for multilingual grammar induction. ÓÄÄeir model relies on a tree-edit model that aligns a pair
of trees (corresponding to parse trees) for two sentences in two languages (translations of each
other). ÓÄÄis tree-edit model (also known as unordered tree alignment by Jiang et al. (1995)) helps
to capture cross-lingual syntactic regularities.
ÓÄÄe generative story for their model is a generalization of the constituent-context model
(CCM) of Klein and Manning (2004). ÓÄÄe main novelty is the multilingual (or bilingual, to
be more precise) setup. Once the model parameters are drawn, the model works by drawing,
a pair of aligned parse trees, one for each language, from a uniform distribution. Next begins
the phase of sentence-pair generation, in which constituents, distituents, constituent contexts and
distituent contexts are drawn. ÓÄÄese elements are substrings of the observed part-of-speech tag
sequences observed in the data (on which the grammar induction process is based) in various
forms. Constituents, for example, are substrings that are dominated by some node in a parse tree.
Any substring that is not a constituent is a distituent.
ÓÄÄe model that Snyder et al. introduce inherits, in a Bayesian context, a property that the
CCM model has: the model overgenerates the observed data. ÓÄÄis happens because constituents,
distituents and contexts all consist of overlapping strings, and as such diÔ¨Äerent parts of the ob-
served data are generated multiple times.
ÓÄÄe model of Snyder et al. introduces another novelty when compared to the CCM model.
For all aligned nodes in the pair of parse trees generated at the beginning, it generates a Giza
score. ÓÄÄis is a score that generates another part of the observed data (other than the POS tags
sequences in the two languages) that is based on word alignments between the paired sentences
created using GIZA++ (Och and Ney, 2003). Let a and b be a pair of aligned nodes in the two
generated parse trees. Let m be the number of pairs of words (one word for each language) that
are aligned according to the GIZA++ alignments and are dominated by the nodes a and b in each
tree. In addition, let n be the number of pairs of words in the sentence where one of the words is
aligned by a or b, but the other is not dominated by b or a. ÓÄÄen the Giza score is m n.
8.9. FURTHER READING 205
ÓÄÄe Giza scores matches how well a pair of substrings is aligned according to GIZA++.
ÓÄÄe higher the score is of a given pair of nodes, the more likely they should be aligned. As such,
the Giza score generation component in the model is a way to ensure that posterior inference will
look for trees that are aligned in a way that matches the Giza score.
8.9 FURTHER READING
ÓÄÄe topics that are presented in this chapter are by no means an exhaustive representation of the
rich literature that has evolved for grammar learning and estimation in a Bayesian framework.
Here we provide some additional highlights of the literature.
Context-free grammars are central to NLP and to syntactic models of language in linguis-
tics and the theory of formal languages. It has long been argued that their expressive power is too
limited for representing the syntax of natural languages. ÓÄÄe goal of modeling language through a
grammar formalism is to identify a grammar that generates a language that is as close as possible
to natural language. ÓÄÄis means that, on the one hand, it should generate any sentence possible
in a language; on the other hand, it should not overgenerate‚Äîotherwise it is a not a good model
for natural language. (A grammar that just generates V 
for some vocabulary V of English words
is easy to describe using a few rules, but clearly will overgenerate as a model for English.)
ÓÄÄe most prominent property that is present in many languages but that is diÔ¨Écult to rep-
resent using context-free grammars is that of crossing or intercalating dependencies, often arising
from free word order. ÓÄÄis property appears in language in diÔ¨Äerent ways, and to accommodate
it, alternative grammar formalisms have been suggested, most notably those often referred to as
‚Äúmildly context-sensitive‚Äù (MCS) grammars.
ÓÄÄese formalisms are so-called because if their grammars were represented using production
rules like in CFGs, this would require that the left-hand side include some additional context (it
would not be just a single nonterminal), without using the full expressive power of constext-
sensitive rules. Typically, such rules engender inÔ¨Ånite sets of nonterminal symbols, or an inÔ¨Ånite
set of rules (and therefore the languages they generate are no longer context-free).
Two most common examples of ‚Äúnear context-free‚Äù MCS grammar formalisms that have
been applied in Bayesian NLP are tree adjoining grammars (Joshi and Schabes, 1997, TAG) and
combinatory categorial grammars (Steedman, 2000, CCG). ÓÄÄese two formalisms are weakly
equivalent (Weir, 1988). ÓÄÄis means that any language that can be generated using a TAG can
also be represented using a CCG and vice versa. However, they do not have the same strong
generative capacity. Each formalism represents a diÔ¨Äerent set of derivations, and it is not always
possible to create one-to-one mappings between the derivations of one to the derivations of the
other, even when two such grammars generate the same language.
MCS grammars have analogues to the CKY and the inside-outside algorithm, but with
increased computational complexity. For the near context-free TAG and CCG, the complexity
of the parsing algorithm is O.n6
/, where n is the length of the sentence.
206 8. BAYESIAN GRAMMAR MODELS
ÓÄÄe concepts that are relevant to the use of Bayesian statistics with these grammars are
similar to those already described in this chapter for CFG. ÓÄÄe parameters in the grammar are
represented as a set of multinomials, and inference proceeds from there. Some previous studies
that used such grammars in a Bayesian context include the use of CCG for Bayesian grammar
induction (Bisk and Hockenmaier, 2013, Huang et al., 2012, Kwiatkowski et al., 2012b), the use
of TAG for parsing with Bayesian nonparametric methods (Yamangil and Shieber, 2013) and
others.
In principle, the concepts that are relevant to the use of Bayesian statistics with these gram-
mars are similar to those already described in this chapter. ÓÄÄe parameters in the grammar are
represented as a set of multinomials, and inference proceeds from there. Some previous studies
that used such grammars in a Bayesian context include the use of CCG for Bayesian grammar
induction (Bisk and Hockenmaier, 2013, Huang et al., 2012), the use of TAG for parsing with
Bayesian nonparametric methods (Yamangil and Shieber, 2013) and others. Formalisms in the
form of automata have also been used in Bayesian NLP, such as for semantic parsing Jones et al.
(2012).
In addition, Cohn et al. (2010) developed a Gibbs sampling using a Bayesian nonparamet-
ric model to learn tree substitution grammars (TSG). Tree substitution grammars are another
type of formalism with a context-free backbone, in which a context-free rule can substitute a
nonterminal in a partial derivation with a whole subtree. ÓÄÄe yield of these subtrees consists of
either nonterminals or terminals. ÓÄÄe nonparametric model of Cohn et al. controls the size of
the fragments that are learned in the TSG. ÓÄÄe expressive power of TSG does not necessarily
supersede that of a CFG, but it can be a better model for language since it can generate frequent
sub-structures without direct compositionality as in CFGs, leading to better generalization for
language data. A similar nonparametric model for inducing TSGs was developed by Post and
Gildea (2009) and Post and Gildea (2013).
It is also important to note that the use of grammar models in NLP with Bayesian statistics
is not limited to the unsupervised setting. Shindo et al. (2012), for example, developed a symbol
reÔ¨Ånement tree substitution model with a Bayesian component, and reported state-of-the-art
parsing results with their model. Here, symbol reÔ¨Ånement refers to latent states that reÔ¨Åne the
syntactic categories that appear in the treebank, in the style of Matsuzaki et al. (2005), Prescher
(2005) and Petrov et al. (2006). See also Section 8.5.1.
Finally, if we are willing to step away from the more ‚Äútraditional‚Äù uses of Bayesian statistics
with grammars, then we can Ô¨Ånd cases in which Bayesian approaches are used to perform inference
about the underlying structure of the grammar. Stolcke and Omohundro (1994), for example,
show how to use a ‚ÄúBayesian model merging‚Äù procedure to learn the structure of a grammar.
ÓÄÄeir idea is based on the idea of model merging (Omohundro, 1992). Model merging works by
building an initial model for each data point available, and then merging these models iteratively,
by coalescing substructures from the current set of models available. ÓÄÄe ‚Äúmerge‚Äù operations are
applied according to the Ô¨Åtness score of the new model after the merging operation. For their
8.10. SUMMARY 207
grammar learning, Stolcke and Omohundro used the posterior distribution of a Bayesian model
given the data. For this, they used a prior that decomposes the structure from the parameters. ÓÄÄe
probability of a structure is inversely proportional to its exponentiated size, and therefore higher
for simpler models.
8.10 SUMMARY
Probabilistic grammars are the most commonly used generic model family in NLP. As such,
given the fact that most probabilistic grammars are generative models, this enabled a large focus
of Bayesian NLP to be on development of Bayesian models and inference algorithms for these
grammars. ÓÄÄe focus of this chapter was on probabilistic context-free grammars and their use
with Bayesian analysis, both parametric and nonparametric. Consequently, we covered the basics
of inference with PCFGs, their use with their Dirichlet priors and nonparametric models such
as adaptor grammars and the hierarchical Dirichlet process PCFGs.
208 8. BAYESIAN GRAMMAR MODELS
8.11 EXERCISES
8.1. Consider the exponential model family which is discussed in Section 3.4. Show that
the model deÔ¨Åned by Equation 8.5 is an exponential model, and deÔ¨Åne the diÔ¨Äerent
components of the exponential model based on the notation from Section 8.2.
8.2. Consider the context-free grammar with rules S ! S a, S ! S b and S ! c where S is
a nonterminal and a; b; c are terminal symbols. Can you Ô¨Ånd the probability assignment
to its rules such that the grammar is not consistent (or not ‚Äútight‚Äù)? If not, show that
such an assignment does not exist. (See also Section 8.3.1.)
8.3. In Equations 8.7‚Äì8.8 we show how to compute feature expectations for nonterminals
spanning certain positions in a string, and similarly for a rule. ÓÄÄese two can be thought
of as features of ‚Äúheight‚Äù 1 and height 2, respectively. Write the equations of computing
expectations for features of height 3.
8.4. Show that the prior in Equation 8.9 is conjugate to the PCFG distribution in Equa-
tion 8.5. Identify the missing normalization constant of the posterior.
8.5. Consider the context-free grammar with rules S ! S a and S ! a. In Section 8.4.1 we
mentioned that a PYAG could not adapt the nonterminal S with this CFG as the base
grammar. Can you explain why a PYAG, in the form of generative story 8.2 could be
ill-deÔ¨Åned if we were allowed to adapt S?
Closing Remarks
Bayesian NLP is a relatively new area in NLP, that has emerged in the early 2000s, and
has only recently matured to its current state. Its future still remains to be seen. Dennis Gabor, a
Nobel prize-winning physicist once said (in a paraphrase) ‚Äúwe cannot predict the future but we
can invent it.‚Äù ÓÄÄis applies to Bayesian NLP too, I believe. ÓÄÄere are a few key areas in which
Bayesian NLP could be further strengthened.
‚Ä¢ Applications for nonparametric models‚ÄîNonparametric modeling is one of the basic
building blocks of Bayesian statistics, especially given the recent developments in Bayesian
analysis that machine learning has gone through. Various nonparametric models have been
suggested, some of them in a general form, and some in a more problem-speciÔ¨Åc form. Still,
in Bayesian NLP, there is an immense focus on the Dirichlet process and its derivatives,
with a few exceptions. Finding applications that can exploit nonparametric models to their
fullest extent has the potential to make the NLP literature much richer in that respect.
‚Ä¢ Construction of richer priors‚ÄîÓÄÄe ability to use a prior distribution in Bayesian modeling
stands at the core of such modeling. Priors can encapsulate knowledge and expert opinion
about the problem at hand. ÓÄÄis is especially true for language, where the potential for
encoding prior linguistic knowledge is immense. Yet, the set of priors that have been used in
NLP thus far is rather limited (mostly focusing on the Dirichlet distribution). Constructing
and using new priors, for example, through prior elicitation of linguistic knowledge, has
great potential.
ÓÄÄis means that we would have to step away from the traditional deÔ¨Ånition of conjugacy,
as deÔ¨Åned by RaiÔ¨Äa and Schlaifer (1961), who were concerned with the convenience of
Ô¨Ånding the posterior distribution with a speciÔ¨Åc prior and likelihood. Conjugacy has become
interchangeable with ‚Äúhaving a closed-form solution for the posterior‚Äù (though it is not its
original deÔ¨Ånition), and perhaps it is time to think in terms of computational conjugacy‚Äîi.e.,
conjugacy between the prior and the likelihood that is computationally tractable, but does
not necessarily lead to a simple mathematical formulation of the posterior. Computational
conjugacy would mean that the posterior, as a blackbox perhaps, can be eÔ¨Éciently accessed
for various tasks (where eÔ¨Éciency is measured in computational complexity terms), and that
blackbox can be repeatedly updated with new data.
‚Ä¢ ConÔ¨Çuence with other machine learning techniques‚ÄîIn the recent years, neural networks
have become an important tool in the NLP machine learning toolkit. Yet, very little work
has been done in NLP to connect Bayesian learning with these neural networks, although
210 CLOSING REMARKS
previous work connecting the two exists in the machine learning literature. Bayesian learn-
ing can be used to control the complexity of the structure of a neural network, and it can
also be used for placing a prior on the parameter weights.
‚Ä¢ Scaling up Bayesian inference in NLP‚ÄîIn the past decade, the scale of text resources that
NLP researchers have been working with has grown tremendously. One of the criticisms
of Bayesian analysis in machine learning and NLP is that Bayesian inference does not scale
(computationally) to large datasets in the ‚ÄúBig Data‚Äù age. Methods such as MCMC infer-
ence are slow to converge and process on a much smaller scale than we are now used to.
Still, in the recent years, researchers in the Statistics and machine learning community have
made progress in scalable Bayesian inference algorithms, for example, by creating stochastic
versions of MCMC methods and variational inference methods. ÓÄÄis knowledge has not
yet transferred to the NLP community in full form, and in order to conduct inference with
large datasets in a Bayesian context in NLP, this might be necessary. For a discussion of
Bayesian inference in the Big Data age, see Jordan (2011) and Welling et al. (2014).
Basic Concepts
A.1 BASIC CONCEPTS IN INFORMATION THEORY
ÓÄÄis section deÔ¨Ånes some basic concepts in information theory, such as entropy, cross entropy and
KL divergence. For a full introduction to information theory, see Cover and ÓÄÄomas (2012).
A.1.1 ENTROPY AND CROSS ENTROPY
ÓÄÄe entropy of a discrete random variable X with distribution p and sample space  is deÔ¨Åned
as:
H.p/ D
X
x2
p.X D x/ log p.X D x/:
Entropy is always non-negative. If the entropy is 0, then the random variable is a con-
stant value with probability 1. ÓÄÄe larger the entropy is, the closer the uncertainty in the random
variable, or in a sense, the random variable distribution is closer to a uniform distribution. (ÓÄÄe
convention is to use 0 for 0 log 0 terms, which are otherwise deÔ¨Åned. ÓÄÄe limit of p log p as p ! 0
is indeed 0.)
When log2 is used instead of log, the entropy provides the expected number of bits required
to encode the random variable as follows. Each value x is assigned with a code that consists
of log2 p.X D x/ bits. ÓÄÄe motivation behind this can be demonstrated through the notion of
cross-entropy. ÓÄÄe cross entropy H.p; q/ between two distributions for a given random variable
is deÔ¨Åned as:
H.p; q/ D
X
x
p.X D x/ log q.X D x/:
When log2 is used, the cross entropy gives the expected number of bits used to encode
random samples from p when using for each x 2  a code of length log2 q.X D x/. ÓÄÄe cross
entropy is minimized, minq H.p; q/ when p D q. In this case, H.p; q/ D H.p/. ÓÄÄerefore, en-
coding random samples from p using a code that assigns each x 2  log2 p.X D x/ bits is opti-
mal in this sense.
212 A. BASIC CONCEPTS
When using the natural logarithm for calculating entropy, entropy is measured in ‚Äúnatbits.‚Äù
Entropies calculated with a diÔ¨Äerent base for the logarithm change by a multiplicative factor, as
loga b D
logc a
logc b
for any a; b; c > 0.
ÓÄÄe notion of entropy can be naturally extended to continuous random variables as well (as
can be the notion of cross entropy). If  is a random variable with density p./ taking values in
R, then the entropy H./ is deÔ¨Åned as:
H./ D
Z 1
1
p./ log p./d:
ÓÄÄe entropy of a continuous random variable is also called ‚ÄúdiÔ¨Äerential entropy.‚Äù ÓÄÄere are
several diÔ¨Äerences between the entropy of discrete and continuous random variables: the entropy
of a continuous random variable may be negative or diverge to inÔ¨Ånity, and it also does not stay
invariant under a change of variable, unlike discrete random variable entropy.
A.1.2 KULLBACK-LEIBLER DIVERGENCE
ÓÄÄe Kullback-Leibler divergence (KL divergence) between two discrete distributions p and q is
deÔ¨Åned as:
KL.p; q/ D
X
x
p.X D x/ log

p.X D x/
q.X D x/

D
X
x
p.X D x/ log p.X D x/
X
x
p.X D x/ log q.X D x/
D H.p; q/ H.p/:
KL divergence is a measure of dissimilarity between two distributions. ÓÄÄe larger the KL
divergence is, the more dissimilar p and q are. KL divergence is always non-negative, and equals
zero only when p D q. KL divergence asymmetric in the general case, KL.p; q/ ¬§ KL.q; p/.
Similarly to entropy, KL divergence can be generalized to the continuous case.
A.2 OTHER BASIC CONCEPTS
ÓÄÄere are three concepts we mention in this book: Jensen‚Äôs inequality, transformations of con-
tinuous random variables and the expectation-maximization algorithm‚Äîwhich we will review
here.
A.2.1 JENSEN‚ÄôS INEQUALITY
In the context of probability theory, Jensen‚Äôs inequality states that if f is a convex function for a
real-valued random variable X, then
A.2. OTHER BASIC CONCEPTS 213
f .E≈íX¬ç/  E≈íf .X/¬ç:
As such, it immediately holds that for a concave function g, g.E≈íX¬ç/  E≈íg.X/¬ç (i.e., the
negation of a convex function is concave). Jensen‚Äôs inequality is used to derive the evidence lower
bound for variational inference (Chapter 6). ÓÄÄe function g that is used is g.x/ D log x, with
Jensen‚Äôs inequality being applied on the marginal log-likelihood. See Section 6.1 for more details.
A.2.2 TRANSFORMATION OF CONTINUOUS RANDOM VARIABLES
Sometimes the parametrization of a distribution is not appropriate for the application at hand. For
example, in Section 4.2.2, we discuss the use of Laplace approximation for a posterior over a multi-
nomial distribution. Since the posterior is deÔ¨Åned over the probability simplex, but the Laplace
approximation gives an approximate posterior that is deÔ¨Åned over Rd
for some d, it is perhaps
better to Ô¨Årst transform the probability simplex random variables so that they become unbounded,
with each coordinate of the transformed multivariate random variable spanning ≈í 1; 1¬ç. ÓÄÄis
can be done using the logit function.
ÓÄÄe question that needs to be asked at this point is how to calculate the probability dis-
tribution of the newly transformed random variables, since this is required for following up with
the Laplace approximation and to generally manipulate the random variables in the new space.
ÓÄÄis can be done using the Jacobian transformation.
ÓÄÄe Jacobian transformation works as follows. Suppose we are given a PDF for a multi-
variate random variable in Rd
. ÓÄÄis PDF is f ./ for  2   Rd
. In addition, suppose that we
are given a function rW  ! Rd
such that r is diÔ¨Äerentiable and bijective. Let s be the inverse of
r, i.e., s./ D r 1
./.  D r./ deÔ¨Ånes a new multivariate random variable. Its density is going
to be g./ D f .s.//jdet.J./j where J W f ./ ! Rd
 Rd
is deÔ¨Åned as:
≈íJ./¬çij D
@si
@j
./
where si W  ! R deÔ¨Åned as si ./ D ≈ís./¬çi (the ith coordinate of s). J is also called ‚Äúthe Jaco-
bian‚Äù (in this case of s).
ÓÄÄis transformation is also often used to compute integrals (whether in probability theory
or outside of it), following a change of variables in the integral. It is often the case that following
such a change, the integrals are easier to compute, or are reduced to well-known integrals that
have analytic solutions.
A.2.3 THE EXPECTATION-MAXIMIZATION ALGORITHM
In Chapter 6 there is a thorough discussion of variational inference and the variational EM algo-
rithm. ÓÄÄis section completes it by giving some information about the expectation-maximization
algorithm for estimating the parameters in a classic frequentist setting with incomplete data.
214 A. BASIC CONCEPTS
ÓÄÄe general scenario is as follows. We have a model p.X; Zj/ where X is an observed
random variable and Z is a latent random variable. We have n observations x.1/
; : : : ; x.n/
, sampled
from p.Xj
/ D
P
z p.X; zj
/, and our goal is to identify the true parameter 
. One way to
do this is by trying to maximize the marginal log-likelihood L./ with respect to :
L./ D
n
X
iD1
log p.x.i/
j/ D
n
X
iD1
log
X
z
p.x.i/
; zj/
!
:
Generally, this function is not convex, and has multiple global maxima. It is often also
computationally diÔ¨Écult to Ô¨Ånd its global maximum. ÓÄÄe EM algorithm is a coordinate ascent
algorithm that iteratively creates a sequence of parameters 1; 2; : : : such that L.i /  L.i 1/
and that eventually converges to a local maximum of L./.
Note Ô¨Årst that L./ can also be expressed in the following manner:
L./ D
n
X
iD1
log Eqi .Z/
"
p.x.i/
; zj/
qi .z/
#!
;
for any set of Ô¨Åxed distributions q1.Z/; : : : ; qn.Z/ over the latent variables with a support
that subsumes the support of p for Z (to see this, just unfold the expectation under qi .Z/, and
consider that the term qi .z/ in the numerator and the denominator will cancel). Jensen‚Äôs inequality
tells us we can deÔ¨Åne the following bound B.jq1; : : : ; qn/  L./ for any  and qi as above:
B.jq1; : : : ; qn/ D
n
X
iD1
Eqi .Z/
"
log
p.x.i/
; zj/
qi .z/
!#
:
It can be shown that for any , B.jq1; : : : ; qn/ D L./ when qi .Z/ D p.Zjx.i/
; /. ÓÄÄe
EM algorithm capitalizes on this observation, and iteratively maximizes the lower bound B by
alternating between maximizing B with respect to  and maximizing the bound with respect to
qi . ÓÄÄerefore, the EM algorithm works as follows.
‚Ä¢ Initialize 1 with some value.
‚Ä¢ Repeat until B.jq1; : : : ; qn/ converges (or for a Ô¨Åxed number of iterations):
 (E-Step:) Compute qi .Zjx.i/
; 1/ for i 2 f1; : : : ; ng and identify the bound
B.jq1; : : : ; qn/.
 (M-Step:) iC1 arg max B.jq1; : : : ; qn/.
Note that the EM algorithm is not the only option to maximize the lower bound
B.jq1; : : : ; qn/. Other optimization techniques can also be used, usually reaching a local maxi-
mum as well.
Distribution Catalog
ÓÄÄis appendix gives some basic information about the various distributions that are mentioned in
this book.
B.1 THE MULTINOMIAL DISTRIBUTION
‚Ñ¶
Notes.
‚Ä¢ ÓÄÄe Dirichlet distribution is conjugate to the multinomial distribution.
‚Ä¢ When n D 1, the distribution is a ‚Äúcategorical distribution‚Äù over binary vectors that sum to
1. With the categorical distribution,  can be any set of k objects. Sometimes the categorical
distribution is referred to as a ‚Äúmultinomial distribution,‚Äù since the categorical distribution
is a speciÔ¨Åc case of the multinomial distribution when  are the binary vectors mentioned
above.
‚Ä¢ A distribution over a Ô¨Ånite set A D fa1; : : : ; ad g with a probability i associated with each
ai may also often be referred to as a multinomial distribution.
216 B. DISTRIBUTION CATALOG
B.2 THE DIRICHLET DISTRIBUTION
‚Ñ¶ ‚Ñ¶
Notes.
‚Ä¢ B.Àõ/ is the Beta function, deÔ¨Åned as:
B.Àõ/ D
Qd
iD1 ¬Ä.Àõi /
¬Ä.
Pd
iD1 Àõi /
;
where ¬Ä.x/ is the Gamma function (Weisstein, 2014).
‚Ä¢ .x/ is the digamma function, which is the Ô¨Årst derivative of the log-Gamma function:
.x/ D
d
dx
log ¬Ä.x/:
It does not have an analytic form, and can be approximated using numerical recipes, or
through series expansion approximation.¬π (Chapter 3).
‚Ä¢ When d D 2, then the Dirichlet distribution can be viewed as deÔ¨Åning a distribution over
≈í0; 1¬ç (since 2 D 1 1). In that case, it is called the Beta distribution (see Section 2.2.1).
‚Ä¢ ÓÄÄe symmetric Dirichlet is a Dirichlet distribution in which the hyperparameters Àõi are all
the same.
‚Ä¢ If Àõi D 1 for all i 2 f1; : : : ; dg, the Dirichlet distribution becomes a uniform distribution
over the probability simplex. (Its density is constant.)
¬πhttp://web.science.mq.edu.au/~mjohnson/code/digamma.c
B.3. THE POISSON DISTRIBUTION 217
B.3 THE POISSON DISTRIBUTION
‚Ñ¶
Notes.
‚Ä¢ A conjugate prior for  is the Gamma distribution.
‚Ä¢ If X1; : : : ; Xn are independent Poisson random variables with rates 1; : : : ; n, then
p.X1; : : : ; Xn j
Pn
iD1 Xi D K/ is a multinomial distribution with parameters K and i D
i
Pn
iD1 i
(Section B.1).
B.4 THE GAMMA DISTRIBUTION
‚Ñ¶ ‚Ñ¶
–ì
–ì
Notes.
‚Ä¢ Often used as a vague prior for hyperparameters in hierarchical Bayesian models.
‚Ä¢ Another common parametrization for it is using two parameters, shape Àõ and ‚Äúrate‚Äù Àá where
Àá D
1

.
‚Ä¢ If Xi  Gamma.Àõi ; 1/ for Àõ1; : : : ; ÀõK are independently distributed, then
.
X1
PK
iD1 Xi
; : : : ;
XK
PK
iD1 Xi
/ is distributed according to the Dirichlet distribution with
parameters .Àõ1; : : : ; ÀõK/. See also Section 3.2.1.
218 B. DISTRIBUTION CATALOG
B.5 THE MULTIVARIATE NORMAL DISTRIBUTION
‚Ñ¶ ‚Ñ¶
‚àë
‚àë
‚àë
‚àë
‚àë
Notes.
‚Ä¢ ÓÄÄe PDF is symmetric around the mean.
‚Ä¢ ÓÄÄere is no closed form for the cummulative distribution function.
‚Ä¢ ÓÄÄe multivariate normal distribution is conjugate to itself, when considering the mean pa-
rameters.
B.6 THE LAPLACE DISTRIBUTION
‚Ñ¶ ‚Ñ¶
Notes.
‚Ä¢ Can be used as a Bayesian interpretation for L1 regularization (Section 4.2.1).
B.7. THE LOGISTIC NORMAL DISTRIBUTION 219
B.7 THE LOGISTIC NORMAL DISTRIBUTION
‚Ñ¶ ‚Ñ¶
‚àë
‚àë
‚àë
Notes.
‚Ä¢ ÓÄÄe PDF is deÔ¨Åned as:
f ./ D
1
p
.2/d det.‚Ä†/
 .
d
Y
iD1
i / 1
exp

1
2
.log. d =d / />
‚Ä† 1
log. d =d / /

:
ÓÄÄe term log. d =d / 2 Rd 1
is deÔ¨Åned as:
≈ílog. d =d /¬çi D log.i =d / 8i 2 f1; : : : ; d 1g:
‚Ä¢ A draw from the logistic normal distribution is equivalent to drawing a real-valued vector
 2 Rd 1
from the multivariate normal distribution (see below) with parameters .; ‚Ä†/
and then setting:
i D
exp.i /
1 C
Pd 1
j D1 exp.j /
8i 2 f1; : : : ; d 1g;
d D
1
1 C
Pd 1
j D1 exp.j /
:
220 B. DISTRIBUTION CATALOG
B.8 THE INVERSE WISHART DISTRIBUTION
‚Ñ¶ ‚Ñ¶
Œ®
Œ®
Œ®
Œ®
Œ®
Œ®iiŒ®jj
Œ®ij
Œìp
Notes.
‚Ä¢ ÓÄÄe function tr.A/ for a matrix A 2 Rpp
is deÔ¨Åned as the trace: the sum of all diagonal
elements of A,
Pp
iD1 Aii.
‚Ä¢ If A is drawn from the Wishart distribution, then A 1
is drawn from the inverse Wishart
distribution.
‚Ä¢ ÓÄÄe inverse Wishart is a conjugate prior for the covariance matrix parameter of a multivari-
ate normal distribution.
Bibliography
Abney, S., McAllester, D., and Pereira, F. (1999). Relating probabilistic grammars and automata.
In Proc. of the 37th Annual Meeting of the Association for Computational Linguistics, pages 542‚Äì
549, College Park, MD. DOI: 10.3115/1034678.1034759. 178
Ahmed, A. and Xing, E. P. (2007). On tight approximate inference of the logistic normal topic
admixture model. In Proc. of the 11th International Conference on ArtiÔ¨Åcal Intelligence and Statis-
tics. Omnipress. 88
Aitchison, J. (1986). ÓÅêe Statistical Analysis of Compositional Data. Chapman and Hall, London.
DOI: 10.1007/978-94-009-4109-0. 58, 59, 61, 62, 64
Altun, Y., Hofmann, T., and Smola, A. J. (2004). Gaussian process classiÔ¨Åcation for segment-
ing and annotating sequences. In Proc. of the 21st International Conference on Machine Learn-
ing (ICML 2004), pages 25‚Äì32, New York, Max-Planck-Gesellschaft, ACM Press. DOI:
10.1145/1015330.1015433. 168
Andrieu, C., De Freitas, N., Doucet, A., and Jordan, M. I. (2003). An introduction to MCMC
for machine learning. Machine Learning, 50(1-2), pages 5‚Äì43. 121
Ash, R. B. and Dol√©ans-Dade, C. A. (2000). Probability and measure theory. Access online via
Elsevier. 2, 10
Barnett, V. (1999). Comparative Statistical Inference. Wiley. DOI: 10.1002/9780470316955. xxiv
Beal, M. J., Ghahramani, Z., and Rasmussen, C. E. (2002). ÓÄÄe inÔ¨Ånite hidden Markov model.
In Machine Learning, pages 29‚Äì245. MIT Press. 176
Bejan, C., Titsworth, M., Hickl, A., and Harabagiu, S. (2009). Nonparametric Bayesian models
for unsupervised event coreference resolution. In Bengio, Y., Schuurmans, D., LaÔ¨Äerty, J.,
Williams, C., and Culotta, A., Eds., Advances in Neural Information Processing Systems 22,
pages 73‚Äì81. Curran Associates, Inc. 27
Berger, A. L., Pietra, V. J. D., and Pietra, S. A. D. (1996). A maximum entropy approach to
natural language processing. Computational Linguistics, 22(1), pages 39‚Äì71. 27
Berger, J. O. (1985). Statistical Decision ÓÅêeory and Bayesian Analysis. Springer. DOI:
10.1007/978-1-4757-4286-2. 41, 71, 89
222 BIBLIOGRAPHY
Bertsekas, D. P. and Tsitsiklis, J. N. (2002). Introduction to Probability, vol. 1. Athena ScientiÔ¨Åc
Belmont, MA. 1
Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Springer. 85, 137
Bisk, Y. and Hockenmaier, J. (2013). An HDP model for inducing combinatory categorial gram-
mars. Transactions of the Association for Computational Linguistics, 1, pages 75‚Äì88. 206
Black, E., Abney, S., Flickenger, D., Gdaniec, C., Grishman, R., Harrison, P., Hindle, D., In-
gria, R., Jelinek, F., Klavans, J., Liberman, M., Marcus, M., Roukos, S., Santorini, B., and
Strzalkowski, T. (1991). A procedure for quantitatively comparing the syntactic coverage
of English grammars. In Proc. of DARPA Workshop on Speech and Natural Language. DOI:
10.3115/112405.112467. 145
Blei, D. M. and Frazier, P. I. (2011). Distance dependent chinese restaurant processes. Journal
of Machine Learning Research, 12, pages 2461‚Äì2488. 169
Blei, D. M., GriÔ¨Éths, T. L., and Jordan, M. I. (2010). ÓÄÄe nested chinese restaurant process and
Bayesian nonparametric inference of topic hierarchies. Journal of the ACM (JACM), 57(2),
page 7. DOI: 10.1145/1667053.1667056. 169
Blei, D. M. and Jordan, M. I. (2004). Variational methods for the Dirichlet process. In Proc. of
the 21st International Conference on Machine Learning. DOI: 10.1145/1015330.1015439. 159,
192, 197
Blei, D. M. and LaÔ¨Äerty, J. D. (2006). Correlated topic models. In Weiss, Y., Sch√∂lkopf, B.,
and Platt, J., Eds., Advances in Neural Information Processing Systems 18, pages 147‚Äì154. MIT
Press. 61
Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine
Learning Research, 3, pages 993‚Äì1022. 27, 29, 31
Blunsom, P. and Cohn, T. (2010a). Inducing synchronous grammars with slice sampling. In
Human Language Technologies: ÓÅêe 2010 Annual Conference of the North American Chapter of the
Association for Computational Linguistics, pages 238‚Äì241, Los Angeles, CA. 27, 115
Blunsom, P. and Cohn, T. (2010b). Unsupervised induction of tree substitution grammars for
dependency parsing. In Proc. of the 2010 Conference on Empirical Methods in Natural Language
Processing, pages 1204‚Äì1213, Cambridge, MA. Association for Computational Linguistics. 27
Blunsom, P., Cohn, T., Dyer, C., and Osborne, M. (2009a). A Gibbs sampler for phrasal
synchronous grammar induction. In Proc. of the Joint Conference of the 47th Annual Meet-
ing of the ACL and the 4th International Joint Conference on Natural Language Processing of the
AFNLP, pages 782‚Äì790, Suntec, Singapore. Association for Computational Linguistics. DOI:
10.3115/1690219.1690256. 115, 201
BIBLIOGRAPHY 223
Blunsom, P., Cohn, T., and Osborne, M. (2009b). Bayesian synchronous grammar induction. In
Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L., Eds., Advances in Neural Information
Processing Systems 21, pages 161‚Äì168. Curran Associates, Inc. 201
B√∂rschinger, B. and Johnson, M. (2014). Exploring the role of stress in Bayesian word segmenta-
tion using adaptor grammars. Transactions of the Association for Computational Linguistics, 2(1),
pages 93‚Äì104. 29
Bouchard-c√¥t√©, A., Petrov, S., and Klein, D. (2009). Randomized pruning: EÔ¨Éciently calculating
expectations in large dynamic programs. In Bengio, Y., Schuurmans, D., LaÔ¨Äerty, J., Williams,
C., and Culotta, A., Eds., Advances in Neural Information Processing Systems 22, pages 144‚Äì152.
Curran Associates, Inc. 115
Bryant, M. and Sudderth, E. B. (2012). Truly nonparametric online variational inference for
hierarchical Dirichlet processes. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K.,
Eds., Advances in Neural Information Processing Systems 25, pages 2699‚Äì2707. Curran Asso-
ciates, Inc. 163
Burstall, R. M. and Darlington, J. (1977). A transformation system for developing recursive
programs. Journal of the ACM, 24(1), pages 44‚Äì67. DOI: 10.1145/321992.321996. 184
Capp√©, O. and Moulines, E. (2009). On-line expectation‚Äìmaximization algorithm for latent
data models. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 71(3),
pages 593‚Äì613. DOI: 10.1111/j.1467-9868.2009.00698.x. 147
Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data Analysis. CRC
Press. DOI: 10.1201/9781420057669. 52
Carpenter, B., Gelman, A., HoÔ¨Äman, M., Lee, D., Goodrich, B., Betancourt, M., Brubaker,
M. A., Guo, J., Li, P., and Riddell, A. (2015). Stan: a probabilistic programming language.
Journal of Statistical Software. 137
Carter, S., Dymetman, M., and Bouchard, G. (2012). Exact sampling and decoding in high-
order hidden Markov models. In Proc. of the 2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational Natural Language Learning, pages 1125‚Äì1134,
Jeju Island, Korea. Association for Computational Linguistics. 120
Casella, G. and Berger, R. L. (2002). Statistical Inference. Duxbury PaciÔ¨Åc Grove, CA. DOI:
10.2307/2532634. 13
Casella, G. and George, E. I. (1992). Explaining the Gibbs sampler. ÓÅêe American Statistician,
46(3), pages 167‚Äì174. DOI: 10.2307/2685208. 129
224 BIBLIOGRAPHY
Chang, J., Gerrish, S., Wang, C., Boyd-graber, J. L., and Blei, D. M. (2009). Reading tea leaves:
How humans interpret topic models. In Bengio, Y., Schuurmans, D., LaÔ¨Äerty, J., Williams,
C., and Culotta, A., Eds., Advances in Neural Information Processing Systems 22, pages 288‚Äì296.
Curran Associates, Inc. 38
Chen, H., Branavan, S., Barzilay, R., Karger, D. R., et al. (2009). Content modeling using latent
permutations. Journal of ArtiÔ¨Åcial Intelligence Research, 36(1), pages 129‚Äì163. 27
Chen, S. F. and Goodman, J. (1996). An empirical study of smoothing techniques for language
modeling. In Proc. of the 34th Annual Meeting of the Association of Computational Linguistics,
pages 310‚Äì318, Stroudsburg, PA. DOI: 10.3115/981863.981904. 82, 166
Chi, Z. (1999). Statistical properties of probabilistic context-free grammars. Computational
Linguistics, 25(1), pages 131‚Äì160. 179
Chinchor, N. (2001). Message understanding conference (muc) 7, ldc2001t02. 92
Chinchor, N. and Sundheim, B. (2003). Message understanding conference (muc) 6, ldc2003t13.
92
Cocke, J. and Schwartz, J. T. (1970). Programming languages and their compilers: Preliminary
notes. Technical report, Courant Institute of Mathematical Sciences, New York University.
178
Cohen, S. and Smith, N. A. (2009). Shared logistic normal distributions for soft parameter
tying in unsupervised grammar induction. In Proc. of Human Language Technologies: ÓÅêe 2009
Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 74‚Äì82, Boulder, CO. DOI: 10.3115/1620754.1620766. 62
Cohen, S. and Smith, N. A. (2010a). Viterbi training for PCFGs: Hardness results and com-
petitiveness of uniform initialization. In Proc. of the 48th Annual Meeting of the Association for
Computational Linguistics, pages 1502‚Äì1511, Uppsala, Sweden. 135
Cohen, S. B., Blei, D. M., and Smith, N. A. (2010). Variational inference for adaptor grammars.
In Human Language Technologies: ÓÅêe 2010 Annual Conference of the North American Chapter of
the Association for Computational Linguistics, pages 564‚Äì572, Los Angeles, CA. 192, 194
Cohen, S. B. and Collins, M. (2014). A provably correct learning algorithm for latent-variable
PCFGs. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1052‚Äì1061, Baltimore, MD. DOI: 10.3115/v1/p14-1099. 198
Cohen, S. B., Gimpel, K., and Smith, N. A. (2009). Logistic normal priors for unsupervised
probabilistic grammar induction. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L.,
Eds., Advances in Neural Information Processing Systems 21, pages 321‚Äì328. Curran Associates,
Inc. 62, 145, 188
BIBLIOGRAPHY 225
Cohen, S. B. and Johnson, M. (2013). ÓÄÄe eÔ¨Äect of non-tightness on Bayesian estimation of
PCFGs. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 1033‚Äì1041, SoÔ¨Åa, Bulgaria. 52, 122
Cohen, S. B. and Smith, N. A. (2010b). Covariance in unsupervised learning of probabilistic
grammars. Journal of Machine Learning Research, 11, pages 3017‚Äì3051. 62, 146
Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., and Ungar, L. (2013). Experiments with
spectral learning of latent-variable PCFGs. In Proc. of the 2013 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Language Technologies,
pages 148‚Äì157, Atlanta, GA. 198
Cohen, S. B., Stratos, K., Collins, M., Foster, D. P., and Ungar, L. (2014). Spectral learning
of latent-variable PCFGs: Algorithms and sample complexity. Journal of Machine Learning
Research, 15, pages 2399‚Äì2449. 198
Cohn, T., Blunsom, P., and Goldwater, S. (2010). Inducing tree-substitution grammars. ÓÅêe
Journal of Machine Learning Research, 11, pages 3053‚Äì3096. 206
Cover, T. M. and ÓÄÄomas, J. A. (2012). Elements of Information ÓÅêeory. John Wiley & Sons. 211
Cox, R. T. (1946). Probability, frequency and reasonable expectation. American Journal of Physics,
14(1), pages 1‚Äì13. DOI: 10.1119/1.1990764. 66
Daume, H. (2007). Fast search for Dirichlet process mixture models. In Meila, M. and Shen, X.,
Eds., Proc. of the 11th International Conference on ArtiÔ¨Åcial Intelligence and Statistics (AISTATS-
07), vol. 2, pages 83‚Äì90. Journal of Machine Learning Research‚ÄîProceedings Track. 160
Daume III, H. (2007). Frustratingly easy domain adaptation. In Proc. of the 45th Annual Meeting
of the Association of Computational Linguistics, pages 256‚Äì263, Prague, Czech Republic. 92
Daume III, H. (2009). Non-parametric Bayesian areal linguistics. In Proc. of Human Language
Technologies: ÓÅêe 2009 Annual Conference of the North American Chapter of the Association for
Computational Linguistics, pages 593‚Äì601, Boulder, CO. DOI: 10.3115/1620754.1620841. 27
Daume III, H. and Campbell, L. (2007). A Bayesian model for discovering typological im-
plications. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,
pages 65‚Äì72, Prague, Czech Republic. 27
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood from incomplete
data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1), pages 1‚Äì38.
141
226 BIBLIOGRAPHY
DeNero, J., Bouchard-C√¥t√©, A., and Klein, D. (2008). Sampling alignment structure under a
Bayesian translation model. In Proc. of the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 314‚Äì323, Honolulu, HI. Association for Computational Linguis-
tics. DOI: 10.3115/1613715.1613758. 108
Doyle, G. and Levy, R. (2013). Combining multiple information types in Bayesian word seg-
mentation. In Proc. of the 2013 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 117‚Äì126, Atlanta, GA. 29
Dreyer, M. and Eisner, J. (2006). Better informed training of latent syntactic features. In Proc. of
the 2006 Conference on Empirical Methods in Natural Language Processing, pages 317‚Äì326, Syd-
ney, Australia. Association for Computational Linguistics. DOI: 10.3115/1610075.1610120.
198
Dreyer, M. and Eisner, J. (2011). Discovering morphological paradigms from plain text using
a Dirichlet process mixture model. In Proc. of the Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 616‚Äì627, Edinburgh. Supplementary material (9 pages)
also available. 27
Dymetman, M., Bouchard, G., and Carter, S. (2012). Optimization and sampling for nlp from
a uniÔ¨Åed viewpoint. In Proc. of the 1st International Workshop on Optimization Techniques for
Human Language Technology, pages 79‚Äì94, Mumbai, India. ÓÄÄe COLING 2012 Organizing
Committee. 120
Earley, J. (1970). An eÔ¨Écient context-free parsing algorithm. Communications of the ACM, 13(2),
pages 94‚Äì102. DOI: 10.1145/357980.358005. 184
Eisenstein, J., Ahmed, A., and Xing, E. (2011). Sparse additive generative models of text. In
Getoor, L. and ScheÔ¨Äer, T., Eds., Proc. of the 28th International Conference on Machine Learning
(ICML-11), pages 1041‚Äì1048, New York, NY, ACM. 68
Eisenstein, J. and Barzilay, R. (2008). Bayesian unsupervised topic segmentation. In Proc. of the
2008 Conference on Empirical Methods in Natural Language Processing, pages 334‚Äì343, Hon-
olulu, HI. Association for Computational Linguistics. DOI: 10.3115/1613715.1613760. 26
Eisner, J. (2002). Transformational priors over grammars. In Proc. of the ACL-02 Conference
on Empirical Methods in Natural Language Processing, vol. 10, pages 63‚Äì70. Association for
Computational Linguistics. DOI: 10.3115/1118693.1118702. 73
Eisner, J., Goldlust, E., and Smith, N. A. (2005). Compiling comp ling: Weighted dy-
namic programming and the dyna language. In Proc. of Human Language Technology Con-
ference and Conference on Empirical Methods in Natural Language Processing, pages 281‚Äì290,
Vancouver, British Columbia, Canada. Association for Computational Linguistics. DOI:
10.3115/1220575.1220611. 182
BIBLIOGRAPHY 227
Eisner, J. and Smith, N. A. (2005). Parsing with soft and hard constraints on dependency length.
In Proc. of the 9th International Workshop on Parsing Technology, pages 30‚Äì41, Vancouver, British
Columbia. Association for Computational Linguistics. DOI: 10.3115/1654494.1654498. 145
Elsner, M., Goldwater, S., Feldman, N., and Wood, F. (2013). A joint learning model of word
segmentation, lexical acquisition, and phonetic variability. In Proc. of the 2013 Conference on
Empirical Methods in Natural Language Processing, pages 42‚Äì54, Seattle, WA. Association for
Computational Linguistics. 29
Escobar, M. D. (1994). Estimating normal means with a Dirichlet process prior. Journal of the
American Statistical Association, 89(425), pages 268‚Äì277. DOI: 10.2307/2291223. 159
Escobar, M. D. and West, M. (1995). Bayesian density estimation and inference using
mixtures. Journal of the American Statistical Association, 90(430), pages 577‚Äì588. DOI:
10.1080/01621459.1995.10476550. 159
Feinberg, S. E. (2011). Bayesian models and methods in public policy and government settings.
Statistical Science, 26(2), pages 212‚Äì226. DOI: 10.1214/10-sts331. 43
Ferguson, T. S. (1973). A Bayesian analysis of some nonparametric problems. ÓÅêe Annals of
Statistics, 1(2), pages 209‚Äì230. DOI: 10.1214/aos/1176342360. 152
Finetti, B. d. (1980). Foresight; its logical laws, its subjective sources. In Kyberg, H.E. and
Smokler, H.E., Eds., Studies in Subjective Probability, pages 99‚Äì158. 8
Finkel, J. R., Grenager, T., and Manning, C. D. (2007). ÓÄÄe inÔ¨Ånite tree. In Proc. of the 45th
Annual Meeting of the Association of Computational Linguistics, pages 272‚Äì279, Prague, Czech
Republic. 198
Finkel, J. R. and Manning, C. D. (2009). Hierarchical Bayesian domain adaptation. In
Proc. of Human Language Technologies: ÓÅêe 2009 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 602‚Äì610, Boulder, CO. DOI:
10.3115/1620754.1620842. 91
Frank, S., Feldman, N. H., and Goldwater, S. (2014). Weak semantic context helps phonetic
learning in a model of infant language acquisition. In Proc. of the 52nd Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages 1073‚Äì1083, Baltimore,
MD. DOI: 10.3115/v1/p14-1101. 29
Frank, S., Keller, F., and Goldwater, S. (2013). Exploring the utility of joint morphological and
syntactic learning from child-directed speech. In Proc. of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages 30‚Äì41, Seattle, WA. Association for Computa-
tional Linguistics. 29
228 BIBLIOGRAPHY
Fullwood, M. and O‚ÄôDonnell, T. (2013). Learning non-concatenative morphology. In Proc. of the
4th Annual Workshop on Cognitive Modeling and Computational Linguistics (CMCL), pages 21‚Äì
27, SoÔ¨Åa, Bulgaria. Association for Computational Linguistics. 29
Gao, J. and Johnson, M. (2008). A comparison of Bayesian estimators for unsupervised Hidden
Markov Model POS taggers. In Proc. of the 2008 Conference on Empirical Methods in Natural
Language Processing, pages 344‚Äì352, Honolulu, HI. Association for Computational Linguis-
tics. DOI: 10.3115/1613715.1613761. 99, 125
Gasthaus, J. and Teh, Y. W. (2010). Improvements to the sequence memoizer. In LaÔ¨Äerty, J.,
Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A., Eds., Advances in Neural Informa-
tion Processing Systems 23, pages 685‚Äì693. Curran Associates, Inc. 171
Gelman, A., Carlin, J. B., Stern, H. B., and Rubin, D. B. (2003). Bayesian Data Analysis, 2nd
ed., Chapman and Hall/CRC Texts in Statistical Science. 88, 92, 93
Gelman, A. and Shalizi, C. R. (2013). Philosophy and the practice of Bayesian statistics. British
Journal of Mathematical and Statistical Psychology, 66(1), pages 8‚Äì38. DOI: 10.1111/j.2044-
8317.2011.02037.x. 23
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the Bayesian
restoration of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6),
pages 721‚Äì741. DOI: 10.1109/tpami.1984.4767596. 99, 109
Geweke, J. (1992). Evaluating the accuracy of sampling-based approaches to the calculation of
posterior moments. Bayesian Statistics, 4, pages 169‚Äì193. 118
Gimpel, K. and Smith, N. A. (2012). Concavity and initialization for unsupervised dependency
parsing. In Proc. of the 2012 Conference of the North American Chapter of the Association for
Computational Linguistics: Human Language Technologies, pages 577‚Äì581, Montr√©al, Canada.
145
Goldwater, S. and GriÔ¨Éths, T. (2007). A fully Bayesian approach to unsupervised part-of-speech
tagging. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,
pages 744‚Äì751, Prague, Czech Republic. 26, 56, 66
Goldwater, S., GriÔ¨Éths, T., and Johnson, M. (2009). A Bayesian framework for word
segmentation: Exploring the eÔ¨Äects of context. Cognition, 112(1), pages 21‚Äì54. DOI:
10.1016/j.cognition.2009.03.008. 29
Goldwater, S., GriÔ¨Éths, T. L., and Johnson, M. (2006). Contextual dependencies in unsuper-
vised word segmentation. In Proc. of the 21st International Conference on Computational Lin-
guistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 673‚Äì680,
Sydney, Australia. DOI: 10.3115/1220175.1220260. 29
BIBLIOGRAPHY 229
Goodman, J. (1996). Parsing algorithms and metrics. In Proc. of the 34th Annual Meet-
ing of the Association for Computational Linguistics, pages 177‚Äì183, Santa Cruz, CA. DOI:
10.3115/981863.981887. 89
GriÔ¨Éths, T. (2002). Gibbs sampling in the generative model of Latent Dirichlet Allocation.
Technical report, Stanford University. 104
GriÔ¨Éths, T. and Ghahramani, Z. (2005). InÔ¨Ånite latent feature models and the indian buÔ¨Äet
process. Gatsby Computational Neuroscience Unit, Technical Report, 1. 168
GriÔ¨Éths, T. L., Chater, N., Kemp, C., Perfors, A., and Tenenbaum, J. B. (2010). Probabilis-
tic models of cognition: exploring representations and inductive biases. Trends in Cognitive
Sciences, 14(8), pages 357‚Äì364. DOI: 10.1016/j.tics.2010.05.004. 29
GriÔ¨Éths, T. L., Kemp, C., and Tenenbaum, J. B. (2008). Bayesian models of cognition. In Sun,
R., Ed., Cambridge Handbook of Computational Cognitive Modeling, pages 59‚Äì100. Cambridge
University Press, Cambridge. 29
Haghighi, A. and Klein, D. (2007). Unsupervised coreference resolution in a nonparametric
Bayesian model. In Proc. 45th Annual Meeting of the ACL, pages 848‚Äì855, Prague, Czech
Republic. Association for Computational Linguistics. 27
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains and their appli-
cations. Biometrika, 57(1), pages 97‚Äì109. DOI: 10.2307/2334940. 112
HoÔ¨Äman, M., Bach, F. R., and Blei, D. M. (2010). Online learning for latent Dirichlet allocation.
In LaÔ¨Äerty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A., Eds., Advances in
Neural Information Processing Systems 23, pages 856‚Äì864. Curran Associates, Inc. 148
Hofmann, T. (1999a). Probabilistic latent semantic analysis. In Proc. of Uncertainty in ArtiÔ¨Åcial
Intelligence, pages 289‚Äì296. 57
Hofmann, T. (1999b). Probabilistic latent semantic indexing. In Proc. of the 22nd Annual Interna-
tional ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR‚Äô99,
pages 50‚Äì57, New York, NY. DOI: 10.1145/312624.312649. 31
Hovy, E., Marcus, M., Palmer, M., Ramshaw, L., and Weischedel, R. (2006). Ontonotes: ÓÄÄe
90% solution. In Proc. of the Human Language Technology Conference of the NAACL, Companion
Volume: Short Papers, pages 57‚Äì60, New York, NY. Association for Computational Linguistics.
92
Huang, Y., Zhang, M., and Tan, C. L. (2011). Nonparametric Bayesian machine transliteration
with synchronous adaptor grammars. In Proc. of the 49th Annual Meeting of the Association for
Computational Linguistics: Human Language Technologies, pages 534‚Äì539, Portland, OR. 201
230 BIBLIOGRAPHY
Huang, Y., Zhang, M., and Tan, C.-L. (2012). Improved combinatory categorial grammar induc-
tion with boundary words and Bayesian inference. In Proc. of COLING 2012, pages 1257‚Äì1274,
Mumbai, India. ÓÄÄe COLING 2012 Organizing Committee. 206
Jaynes, E. T. (2003). Probability ÓÅêeory: ÓÅêe Logic of Science. Cambridge University Press. DOI:
10.1017/cbo9780511790423. xxiv, 23, 66
JeÔ¨Äreys, H. (1961). ÓÅêeory of Probability. Oxford University. DOI: 10.1063/1.3050814. 67, 68,
82
Jelinek, F. and Mercer, R. L. (1980). Interpolated estimation of Markov source parameters from
sparse data. In Proc. of Workshop on Pattern Recognition in Practice, Amsterdam, ÓÄÄe Nether-
lands. 82
Jiang, T., Wang, L., and Zhang, K. (1995). Alignment of trees‚Äîan alternative to tree edit.
ÓÅêeoretical Computer Science, 143(1), pages 137‚Äì148. DOI: 10.1016/0304-3975(95)80029-9.
204
Johnson, M. (2007a). Transforming projective bilexical dependency grammars into eÔ¨Éciently-
parsable CFGs with unfold-fold. In Proc. of the 45th Annual Meeting of the Association of Com-
putational Linguistics, pages 168‚Äì175, Prague, Czech Republic. 184
Johnson, M. (2007b). Why doesn‚Äôt EM Ô¨Ånd good HMM POS-taggers? In Proc. of the 2007 Joint
Conference on Empirical Methods in Natural Language Processing and Computational Natural
Language Learning (EMNLP-CoNLL), pages 296‚Äì305, Prague, Czech Republic. Association
for Computational Linguistics. 142
Johnson, M. (2008). Using adaptor grammars to identify synergies in the unsupervised acquisition
of linguistic structure. In Proc. of ACL-08: HLT, pages 398‚Äì406, Columbus, OH. Association
for Computational Linguistics. 29
Johnson, M., Christophe, A., Dupoux, E., and Demuth, K. (2014). Modelling function words
improves unsupervised word segmentation. In Proc. of the 52nd Annual Meeting of the Associ-
ation for Computational Linguistics (Volume 1: Long Papers), pages 282‚Äì292, Baltimore, MD.
DOI: 10.3115/v1/p14-1027. 29
Johnson, M., Demuth, K., Jones, B., and Black, M. J. (2010). Synergies in learning words and
their referents. In LaÔ¨Äerty, J., Williams, C., Shawe-Taylor, J., Zemel, R., and Culotta, A., Eds.,
Advances in Neural Information Processing Systems 23, pages 1018‚Äì1026. Curran Associates, Inc.
29
Johnson, M. and Goldwater, S. (2009). Improving nonparameteric Bayesian inference: experi-
ments on unsupervised word segmentation with adaptor grammars. In Proc. of Human Lan-
guage Technologies: ÓÅêe 2009 Annual Conference of the North American Chapter of the Association
BIBLIOGRAPHY 231
for Computational Linguistics, pages 317‚Äì325, Boulder, CO. DOI: 10.3115/1620754.1620800.
115
Johnson, M., GriÔ¨Éths, T., and Goldwater, S. (2007a). Bayesian inference for PCFGs via Markov
chain Monte Carlo. In Human Language Technologies 2007: ÓÅêe Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,
pages 139‚Äì146, Rochester, NY. 26, 186
Johnson, M., GriÔ¨Éths, T. L., and Goldwater, S. (2007b). Adaptor grammars: A framework
for specifying compositional nonparametric Bayesian models. In Sch√∂lkopf, B., Platt, J., and
HoÔ¨Äman, T., Eds., Advances in Neural Information Processing Systems 19, pages 641‚Äì648. MIT
Press. 27, 125, 191, 194
Jones, B., Johnson, M., and Goldwater, S. (2012). Semantic parsing with Bayesian tree transduc-
ers. In Proc. of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), pages 488‚Äì496, Jeju Island, Korea. 206
Jordan, M. I. (2011). Message from the president: ÓÄÄe era of big data. International Society for
Bayesian Analysis (ISBA) Bulletin, 18(2), pages 1‚Äì3. 210
Joshi, A. K. and Schabes, Y. (1997). Tree-adjoining grammars. In Handbook of Formal Languages,
pages 69‚Äì123. Springer. DOI: 10.1007/978-3-642-59126-6_2. 205
Joshi, M., Das, D., Gimpel, K., and Smith, N. A. (2010). Movie reviews and revenues: An
experiment in text regression. In Human Language Technologies: ÓÅêe 2010 Annual Conference
of the North American Chapter of the Association for Computational Linguistics, pages 293‚Äì296,
Los Angeles, CA. 40
Kallmeyer, L. and Maier, W. (2010). Data-driven parsing with probabilistic linear context-free
rewriting systems. In Proc. of the 23rd International Conference on Computational Linguistics
(Coling 2010), pages 537‚Äì545, Beijing, China. Coling 2010 Organizing Committee. DOI:
10.1162/coli_a_00136. 173
Kasami, T. (1965). An eÔ¨Écient recognition and syntax-analysis algorithm for context-free lan-
guages. Technical Report AFCRL-65-758, Air Force Cambridge Research Lab. 178
Katz, S. M. (1987). Estimation of probabilities from sparse data for the language model com-
ponent of a speech recognizer. In IEEE Transactions on Acoustics, Speech and Signal Processing,
pages 400‚Äì401. DOI: 10.1109/tassp.1987.1165125. 82
Klein, D. and Manning, C. (2004). Corpus-based induction of syntactic structure: Mod-
els of dependency and constituency. In Proc. of the 42nd Meeting of the Association for
Computational Linguistics (ACL‚Äô04), Main Volume, pages 478‚Äì485, Barcelona, Spain. DOI:
10.3115/1218955.1219016. 62, 145, 204
232 BIBLIOGRAPHY
Kneser, R. and Ney, H. (1995). Improved backing-oÔ¨Ä for m-gram language modeling. In Proc. of
the IEEE International Conference on Acoustics, Speech and Signal Processing, vol. I, pages 181‚Äì
184, Detroit, MI. IEEE Inc. DOI: 10.1109/icassp.1995.479394. 82, 166
Koller, D. and Friedman, N. (2009). Probabilistic Graphical Models: Principles and Techniques.
MIT press. 19, 147
K√ºbler, S., McDonald, R., and Nivre, J. (2009). Dependency Parsing. Synthe-
sis Lectures on Human Language Technologies. Morgan & Claypool. DOI:
10.2200/s00169ed1v01y200901hlt002. 198
Kucukelbir, A., Tran, D., Ranganath, R., Gelman, A., and Blei, D. M. (2016). Automatic dif-
ferentiation variational inference. arXiv preprint arXiv:1603.00788. 137
Kulis, B. and Jordan, M. I. (2011). Revisiting k-means: New algorithms via Bayesian nonpara-
metrics. arXiv preprint arXiv:1111.0352. 151
Kumar, S. and Byrne, W. (2004). Minimum bayes-risk decoding for statistical machine transla-
tion. In Susan Dumais, D. M. and Roukos, S., Eds., HLT-NAACL 2004: Main Proceedings,
pages 169‚Äì176, Boston, MA. Association for Computational Linguistics. 89
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., and Steedman, M. (2012a). A probabilistic
model of syntactic and semantic acquisition from child-directed utterances and their mean-
ings. In Proc. of the 13th Conference of the European Chapter of the Association for Computational
Linguistics, pages 234‚Äì244, Avignon, France. 148
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., and Steedman, M. (2012b). A probabilistic
model of syntactic and semantic acquisition from child-directed utterances and their mean-
ings. In Proc. of the 13th Conference of the European Chapter of the Association for Computational
Linguistics, pages 234‚Äì244, Avignon, France. 206
Levenberg, A., Dyer, C., and Blunsom, P. (2012). A Bayesian model for learning scfgs with dis-
contiguous rules. In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language
Processing and Computational Natural Language Learning, pages 223‚Äì232, Jeju Island, Korea.
Association for Computational Linguistics. 108, 201
Levy, R. P., Reali, F., and GriÔ¨Éths, T. L. (2009). Modeling the eÔ¨Äects of memory on human
online sentence processing with particle Ô¨Ålters. In Koller, D., Schuurmans, D., Bengio, Y., and
Bottou, L., Eds., Advances in Neural Information Processing Systems 21, pages 937‚Äì944. Curran
Associates, Inc. 127
Liang, P. and Klein, D. (2009). Online EM for unsupervised models. In Proc. of Human Lan-
guage Technologies: ÓÅêe 2009 Annual Conference of the North American Chapter of the Association
for Computational Linguistics, pages 611‚Äì619, Boulder, CO. DOI: 10.3115/1620754.1620843.
147
BIBLIOGRAPHY 233
Liang, P., Petrov, S., Jordan, M., and Klein, D. (2007). ÓÄÄe inÔ¨Ånite PCFG using hierarchi-
cal Dirichlet processes. In Proc. of the 2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),
pages 688‚Äì697, Prague, Czech Republic. Association for Computational Linguistics. 196
Lidstone, G. J. (1920). Note on the general case of the Bayes-Laplace formula for the inductive
or posteriori probabilities. Transactions of the Faculty of Actuaries, 8(182). 82
Lin, C.-C., Wang, Y.-C., and Tsai, R. T.-H. (2009). Modeling the relationship among linguistic
typological features with hierarchical Dirichlet process. In Proc. of the 23rd PaciÔ¨Åc Asia Confer-
ence on Language, Information and Computation, pages 741‚Äì747, Hong Kong. City University
of Hong Kong. 27
Lindsey, R., Headden, W., and Stipicevic, M. (2012). A phrase-discovering topic model using
hierarchical Pitman-Yor processes. In Proc. of the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational Natural Language Learning, pages 214‚Äì222,
Jeju Island, Korea. Association for Computational Linguistics. 115
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated
corpus of English: ÓÄÄe Penn treebank. Computational Linguistics, 19(2), pages 313‚Äì330. 177
Matsuzaki, T., Miyao, Y., and Tsujii, J. (2005). Probabilistic CFG with latent annotations.
In Proc. of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL‚Äô05),
pages 75‚Äì82, Ann Arbor, MI. DOI: 10.3115/1219840.1219850. 197, 206
McGrayne, S. B. (2011). ÓÅêe ÓÅêeory that Would not Die: How Bayes‚Äô Rule Cracked the Enigma Code,
Hunted Down Russian Submarines, and Emerged Triumphant from Two Centuries of Controversy.
Yale University Press. xxiv
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. (1953).
Equation of state calculations by fast computing machines. Journal of Chemical Physics, 21,
pages 1087‚Äì1092. DOI: 10.1063/1.1699114. 112
Mimno, D., Wallach, H., and McCallum, A. (2008). Gibbs sampling for logistic normal topic
models with graph-based priors. In NIPS Workshop on Analyzing Graphs. 61
Mimno, D., Wallach, H., Talley, E., Leenders, M., and McCallum, A. (2011). Optimizing
semantic coherence in topic models. In Proc. of the 2011 Conference on Empirical Methods
in Natural Language Processing, pages 262‚Äì272, Edinburgh, Scotland, UK. Association for
Computational Linguistics. 38
Minka, T. (1999). ÓÄÄe Dirichlet-tree distribution. Technical report, Justsystem Pittsburgh Re-
search Center. 54
234 BIBLIOGRAPHY
Minka, T. (2000). Bayesian linear regression. Technical report, Massachusetts Institute of Tech-
nology. 41
Murphy, K. P. (2012). Machine Learning: A Probabilistic Perspective. MIT press. 17
Nakazawa, T. and Kurohashi, S. (2012). Alignment by bilingual generation and monolingual
derivation. In Proc. of COLING 2012, pages 1963‚Äì1978, Mumbai, India. ÓÄÄe COLING 2012
Organizing Committee. 108
Neal, R. M. (2000). Markov chain sampling methods for Dirichlet process mixture models.
Journal of computational and graphical statistics, 9(2), pages 249‚Äì265. DOI: 10.2307/1390653.
158, 159
Neal, R. M. (2003). Slice sampling. Annals of Statistics, 31, pages 705‚Äì767. DOI:
10.1214/aos/1056562461. 113
Neal, R. M. and Hinton, G. E. (1998). A view of the EM algorithm that justiÔ¨Åes incremental,
sparse, and other variants. In Learning in Graphical Models, pages 355‚Äì368. Springer. DOI:
10.1007/978-94-011-5014-9_12. 147
Neiswanger, W., Wang, C., and Xing, E. P. (2014). Asymptotically exact, embarrassingly parallel
MCMC. In Proc. of the 30th Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, UAI, pages 623‚Äì
632, Quebec City, Quebec, Canada. AUAI Press. 110
Neubig, G., Watanabe, T., Sumita, E., Mori, S., and Kawahara, T. (2011). An unsupervised
model for joint phrase alignment and extraction. In Proc. of the 49th Annual Meeting of the
Association for Computational Linguistics: Human Language Technologies, pages 632‚Äì641, Port-
land, OR. 201
Newman, D., Asuncion, A., Smyth, P., and Welling, M. (2009). Distributed algorithms for topic
models. Journal of Machine Learning Research, 10, pages 1801‚Äì1828. 109
Newman, D., Lau, J. H., Grieser, K., and Baldwin, T. (2010). Automatic evaluation of topic
coherence. In Human Language Technologies: ÓÅêe 2010 Annual Conference of the North American
Chapter of the Association for Computational Linguistics, pages 100‚Äì108, Los Angeles, CA. 38
Noji, H., Mochihashi, D., and Miyao, Y. (2013). Improvements to the Bayesian topic n-gram
models. In Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 1180‚Äì1190, Seattle, WA. Association for Computational Linguistics. 166
Och, F. J. and Ney, H. (2003). A systematic comparison of various statistical alignment models.
Computational Linguistics, 29(1), pages 19‚Äì51. DOI: 10.1162/089120103321337421. 203, 204
Omohundro, S. M. (1992). Best-Ô¨Årst Model Merging for Dynamic Learning and Recognition. In-
ternational Computer Science Institute. 206
BIBLIOGRAPHY 235
O‚ÄôNeill, B. (2009). Exchangeability, correlation, and Bayes‚Äô eÔ¨Äect. International Statistical Re-
view, 77(2), pages 241‚Äì250. DOI: 10.1111/j.1751-5823.2008.00059.x. 9
Pajak, B., Bicknell, K., and Levy, R. (2013). A model of generalization in distributional learning
of phonetic categories. In Demberg, V. and Levy, R., Eds., Proc. of the 4th Workshop on Cog-
nitive Modeling and Computational Linguistics, pages 11‚Äì20, SoÔ¨Åa, Bulgaria. Association for
Computational Linguistics. 29
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. Mor-
gan Kaufmann, San Mateo, CA. 18
Perfors, A., Tenenbaum, J. B., GriÔ¨Éths, T. L., and Xu, F. (2011). A tutorial introduction
to Bayesian models of cognitive development. Cognition, 120(3), pages 302‚Äì321. DOI:
10.1016/j.cognition.2010.11.015. 29
Petrov, S., Barrett, L., ÓÄÄibaux, R., and Klein, D. (2006). Learning accurate, compact, and inter-
pretable tree annotation. In Proc. of the 21st International Conference on Computational Linguis-
tics and 44th Annual Meeting of the Association for Computational Linguistics, pages 433‚Äì440,
Sydney, Australia. DOI: 10.3115/1220175.1220230. 198, 206
Pitman, J. and Yor, M. (1997). ÓÄÄe two-parameter Poisson-Dirichlet distribution de-
rived from a stable subordinator. ÓÅêe Annals of Probability, 25(2), pages 855‚Äì900. DOI:
10.1214/aop/1024404422. 163, 164
Post, M. and Gildea, D. (2009). Bayesian learning of a tree substitution grammar. In Proc. of the
ACL-IJCNLP 2009 Conference Short Papers, pages 45‚Äì48, Suntec, Singapore. Association for
Computational Linguistics. DOI: 10.3115/1667583.1667599. 206
Post, M. and Gildea, D. (2013). Bayesian tree substitution grammars as a usage-based approach.
Language and Speech, 56, pages 291‚Äì308. DOI: 10.1177/0023830913484901. 206
Preo≈£iuc-Pietro, D. and Cohn, T. (2013). A temporal model of text periodicities using gaussian
processes. In Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing,
pages 977‚Äì988, Seattle, WA. Association for Computational Linguistics. 168
Prescher, D. (2005). Head-driven PCFGs with latent-head statistics. In Proc. of the 9th Interna-
tional Workshop on Parsing Technology, pages 115‚Äì124, Vancouver, British Columbia. Associa-
tion for Computational Linguistics. DOI: 10.3115/1654494.1654506. 197, 206
Rabiner, L. R. (1989). A tutorial on hidden Markov models and selected applications in speech
recognition. Proc. of the IEEE, 77(2), pages 257‚Äì286. DOI: 10.1109/5.18626. 175
Raftery, A. E. and Lewis, S. M. (1992). Practical Markov chain Monte Carlo: Comment: One
long run with diagnostics: Implementation strategies for Markov chain Monte Carlo. Statistical
Science, 7(4), pages 493‚Äì497. 118
236 BIBLIOGRAPHY
RaiÔ¨Äa, H. and Schlaifer, R. (1961). Applied Statistical Decision ÓÅêeory. Wiley-Interscience. 52,
209
Rasmussen, C. E. and Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT
Press. DOI: 10.1007/978-3-540-28650-9_4. 168
Ravi, S. and Knight, K. (2011). Deciphering foreign language. In Proc. of the 49th Annual Meeting
of the Association for Computational Linguistics: Human Language Technologies, pages 12‚Äì21,
Portland, OR. 108
Robert, C. P. and Casella, G. (2005). Monte Carlo Statistical Methods. Springer. DOI:
10.1007/978-1-4757-3071-5. 117, 118, 119, 120, 125
Rosenfeld, R. (2000). Two decades of statistical language modeling: Where do we go from here?
Proc. of the IEEE, 88(8), pages 1270‚Äì1278. DOI: 10.1109/5.880083. 166
Rozenberg, G. and Ehrig, H. (1999). Handbook of Graph Grammars and Computing by Graph
Transformation, vol. 1. World scientiÔ¨Åc Singapore. DOI: 10.1142/9789812384720. 173
Sankaran, B., HaÔ¨Äari, G., and Sarkar, A. (2011). Bayesian extraction of minimal scfg rules for
hierarchical phrase-based translation. In Proc. of the 6th Workshop on Statistical Machine Trans-
lation, pages 533‚Äì541, Edinburgh, Scotland. Association for Computational Linguistics. 201
Sato, M.-A. and Ishii, S. (2000). On-line EM algorithm for the normalized Gaussian network.
Neural Computation, 12(2), pages 407‚Äì432. DOI: 10.1162/089976600300015853. 147
Sethuraman, J. (1994). A constructive deÔ¨Ånition of Dirichlet priors. Statistica Sinica, 4,
pages 639‚Äì650. 153
Shareghi, E., HaÔ¨Äari, G., Cohn, T., and Nicholson, A. (2015). Structured prediction of sequences
and trees using inÔ¨Ånite contexts. In Machine Learning and Knowledge Discovery in Databases,
pages 373‚Äì389. Springer. DOI: 10.1007/978-3-319-23525-7_23. 171
Shindo, H., Miyao, Y., Fujino, A., and Nagata, M. (2012). Bayesian symbol-reÔ¨Åned tree substi-
tution grammars for syntactic parsing. In Proc. of the 50th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), pages 440‚Äì448, Jeju Island, Korea. 27, 206
Sirts, K., Eisenstein, J., Elsner, M., and Goldwater, S. (2014). Pos induction with distributional
and morphological information using a distance-dependent chinese restaurant process. In Proc.
of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short
Papers), pages 265‚Äì271, Baltimore, MD. DOI: 10.3115/v1/p14-2044. 170
Smith, N. A. (2011). Linguistic Structure Prediction. Synthesis Lectures on Human Language
Technologies. Morgan & Claypool. DOI: 10.2200/s00361ed1v01y201105hlt013. 182
BIBLIOGRAPHY 237
Snyder, B. and Barzilay, R. (2008). Unsupervised multilingual learning for morphological seg-
mentation. In Proc. of ACL-08: HLT, pages 737‚Äì745, Columbus, OH. Association for Com-
putational Linguistics. 27
Snyder, B., Naseem, T., and Barzilay, R. (2009a). Unsupervised multilingual grammar induction.
In Proc. of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International
Joint Conference on Natural Language Processing of the AFNLP, pages 73‚Äì81, Suntec, Singapore.
Association for Computational Linguistics. DOI: 10.3115/1687878.1687890. 204
Snyder, B., Naseem, T., Eisenstein, J., and Barzilay, R. (2008). Unsupervised multilingual learning
for POS tagging. In Proc. of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 1041‚Äì1050, Honolulu, HI. Association for Computational Linguistics. DOI:
10.3115/1613715.1613851. 27, 203
Snyder, B., Naseem, T., Eisenstein, J., and Barzilay, R. (2009b). Adding more languages im-
proves unsupervised multilingual part-of-speech tagging: a Bayesian non-parametric approach.
In Proc. of Human Language Technologies: ÓÅêe 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics, pages 83‚Äì91, Boulder, CO. DOI:
10.3115/1620754.1620767. 203
Spitkovsky, V. I., Alshawi, H., and Jurafsky, D. (2010). From baby steps to leapfrog: How ‚Äúless
is more‚Äù in unsupervised dependency parsing. In Human Language Technologies: ÓÅêe 2010
Annual Conference of the North American Chapter of the Association for Computational Linguistics,
pages 751‚Äì759, Los Angeles, CA. 145
Steedman, M. (2000). ÓÅêe Syntactic Process, vol. 35. MIT Press. 205
Steedman, M. and Baldridge, J. (2011). Combinatory categorial grammar. In Borsley, R. and
Borjars, K. Eds. Non-Transformational Syntax Oxford, pages 181‚Äì224. 173
Steyvers, M. and GriÔ¨Éths, T. (2007). Probabilistic topic models. Handbook of Latent Semantic
Analysis, 427(7), pages 424‚Äì440. DOI: 10.4324/9780203936399.ch21. 33
Stolcke, A. (2002). SRILM-an extensible language modeling toolkit. In Proc. International
Conference on Spoken Language Processing, pages 901‚Äì904, Denver, CO. International Speech
Communication Association (ISCA). 81
Stolcke, A. and Omohundro, S. (1994). Inducing probabilistic grammars by Bayesian
model merging. In Grammatical Inference and Applications, pages 106‚Äì118. Springer. DOI:
10.1007/3-540-58473-0_141. 73, 206
Synnaeve, G., Dautriche, I., B√∂rschinger, B., Johnson, M., and Dupoux, E. (2014). Unsupervised
word segmentation in context. In Proc. of COLING 2014, the 25th International Conference on
Computational Linguistics: Technical Papers, pages 2326‚Äì2334, Dublin, Ireland. Dublin City
University and Association for Computational Linguistics. 29
238 BIBLIOGRAPHY
Teh, Y. W. (2006a). A Bayesian interpretation of interpolated Kneser-Ney. Technical report.
166
Teh, Y. W. (2006b). A hierarchical Bayesian language model based on Pitman-Yor processes. In
Proc. of the 21st International Conference on Computational Linguistics and 44th Annual Meet-
ing of the Association for Computational Linguistics, pages 985‚Äì992, Sydney, Australia. DOI:
10.3115/1220175.1220299. 165, 176, 177
Teh, Y. W., Jordan, M. I., Beal, M. J., and Blei, D. M. (2006). Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Association, 101(476), pages 1566‚Äì1581. DOI:
10.1198/016214506000000302. 162
Teh, Y. W., Kurihara, K., and Welling, M. (2008). Collapsed variational inference for hdp. In
Platt, J., Koller, D., Singer, Y., and Roweis, S., Eds., Advances in Neural Information Processing
Systems 20, pages 1481‚Äì1488. Curran Associates, Inc. 163
Tenenbaum, J. B., Kemp, C., GriÔ¨Éths, T. L., and Goodman, N. D. (2011). How to grow a mind:
Statistics, structure, and abstraction. Science, 331(6022), pages 1279‚Äì1285. DOI: 10.1126/sci-
ence.1192788. 29
Tesni√®re, L. (1959). √âl√©ment de Syntaxe Structurale. Klincksieck. 198
Tesni√®re, L., Osborne, T. J., and Kahane, S. (2015). Elements of Structural Syntax. John Benjamins
Publishing Company. DOI: 10.1075/z.185. 198
Titov, I. and Klementiev, A. (2012). A Bayesian approach to unsupervised semantic role induc-
tion. In Proc. of the 13th Conference of the European Chapter of the Association for Computational
Linguistics, pages 12‚Äì22, Avignon, France. 170
Tjong Kim Sang, E. F. and De Meulder, F. (2003). Introduction to the CoNLL-2003 shared task:
Language-independent named entity recognition. In Daelemans, W. and Osborne, M., Eds.,
Proc. of the 7th Conference on Natural Language Learning at HLT-NAACL 2003, pages 142‚Äì147.
DOI: 10.3115/1119176. 92
Toutanova, K. and Johnson, M. (2008). A Bayesian LDA-based model for semi-supervised part-
of-speech tagging. In Platt, J., Koller, D., Singer, Y., and Roweis, S., Eds., Advances in Neural
Information Processing Systems 20, pages 1521‚Äì1528. Curran Associates, Inc. 56
Tromble, R., Kumar, S., Och, F., and Macherey, W. (2008). Lattice Minimum Bayes-Risk de-
coding for statistical machine translation. In Proc. of the 2008 Conference on Empirical Methods
in Natural Language Processing, pages 620‚Äì629, Honolulu, HI. Association for Computational
Linguistics. DOI: 10.3115/1613715.1613792. 89
Upton, G. and Cook, I. (2014). A Dictionary of Statistics, 3rd ed., Oxford University Press. DOI:
10.1093/acref/9780199679188.001.0001. 118
BIBLIOGRAPHY 239
Van Gael, J., Saatci, Y., Teh, Y. W., and Ghahramani, Z. (2008). Beam sampling for the inÔ¨Å-
nite hidden Markov model. In Proc. of the 25th international conference on Machine learning,
pages 1088‚Äì1095. ACM Press. DOI: 10.1145/1390156.1390293. 115, 177
Vijay-Shanker, K., Weir, D. J., and Joshi, A. K. (1987). Characterizing structural descrip-
tions produced by various grammatical formalisms. In Proc. of the 25th Annual Meet-
ing of the Association for Computational Linguistics, pages 104‚Äì111, Stanford, CA. DOI:
10.3115/981175.981190. 173
Wainwright, M. and Jordan, M. (2008). Graphical models, exponential families, and varia-
tional inference. Foundations and Trends in Machine Learning, 1(1‚Äì2), pages 1‚Äì305. DOI:
10.1561/2200000001. 135
Wallach, H., Sutton, C., and McCallum, A. (2008). Bayesian modeling of dependency trees using
hierarchical Pitman-Yor priors. In ICML Workshop on Prior Knowledge for Text and Language
Processing, pages 15‚Äì20, Helsinki, Finland. ACM. 164
Wallach, H. M. (2006). Topic modeling: beyond bag-of-words. In Proc. of the 23rd Interna-
tional Conference on Machine Learning, pages 977‚Äì984, Pittsburgh, PA. ACM Press. DOI:
10.1145/1143844.1143967. 166
Wang, C., Paisley, J. W., and Blei, D. M. (2011). Online variational inference for the hierarchical
Dirichlet process. In International Conference on ArtiÔ¨Åcial Intelligence and Statistics, pages 752‚Äì
760. 148, 163
Weir, D. (1988). Characterizing Mildly Context-Sensitive Grammar Formalisms. Ph.D. thesis,
Department of Computer and Information Science, University of Pennsylvania. Available as
Technical Report MS-CIS-88-74. 205
Weisstein, E. W. (2014). Gamma function. from MathWorld‚Äìa Wolfram web resource. http:
//mathworld.wolfram.com/GammaFunction.html, Last visited on 11/11/2014. 216
Welling, M., Teh, Y. W., Andrieu, C., Kominiarczuk, J., Meeds, T., Shahbaba, B., and Vollmer,
S. (2014). Bayesian inference with big data: a snapshot from a workshop. International Society
for Bayesian Analysis (ISBA) Bulletin, 21(4), pages 8‚Äì11. 210
Williams, P., Sennrich, R., Koehn, P., and Post, M. (2016). Syntax-based Statistical Machine
Translation. Synthesis Lectures on Human Language Technologies. Morgan & Claypool.
201
Wood, F., Archambeau, C., Gasthaus, J., James, L., and Teh, Y. W. (2009). A stochastic memo-
izer for sequence data. In Proc. of the 26th Annual International Conference on Machine Learning,
pages 1129‚Äì1136. ACM. DOI: 10.1145/1553374.1553518. 170
240 BIBLIOGRAPHY
Wu, D. (1997). Stochastic inversion transduction grammars and bilingual parsing of parallel
corpora. Computational Linguistics, 23(3), pages 377‚Äì403. 201
Yamamoto, M. and Sadamitsu, K. (2005). Dirichlet mixtures in text modeling. Technical Report
CS-TR-05-1, University of Tsukuba. 51
Yamangil, E. and Shieber, S. M. (2010). Bayesian synchronous tree-substitution grammar in-
duction and its application to sentence compression. In Proc. of the 48th Annual Meeting of the
Association for Computational Linguistics, pages 937‚Äì947, Uppsala, Sweden. 201
Yamangil, E. and Shieber, S. M. (2013). Nonparametric Bayesian inference and eÔ¨Écient parsing
for tree-adjoining grammars. In Proc. of the 51st Annual Meeting of the Association for Compu-
tational Linguistics (Volume 2: Short Papers), pages 597‚Äì603, SoÔ¨Åa, Bulgaria. 206
Yang, R. and Berger, J. O. (1998). A Catalog of Noninformative Priors. 68
Yang, Y. and Eisenstein, J. (2013). A log-linear model for unsupervised text normalization. In
Proc. of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 61‚Äì72,
Seattle, WA. Association for Computational Linguistics. 127
Younger, D. H. (1967). Recognition and parsing of context-free languages in time n3
. Information
and Control, 10(2). DOI: 10.1016/s0019-9958(67)80007-x. 178
Zhai, K., Boyd-Graber, J., and Cohen, S. (2014). Online adaptor grammars with hybrid inference.
Transactions of the Association for Computational Linguistics, 2, pages 465‚Äì476. 195
Zhai, K. and Boyd-graber, J. L. (2013). Online latent Dirichlet allocation with inÔ¨Ånite vocab-
ulary. In Dasgupta, S. and Mcallester, D., Eds., Proc. of the 30th International Conference on
Machine Learning (ICML-13), vol. 28(1), pages 561‚Äì569. JMLR Workshop and Conference
Proceedings. 30
Zhang, H., Quirk, C., Moore, R. C., and Gildea, D. (2008). Bayesian learning of non-
compositional phrases with synchronous parsing. In Proc. of ACL-08: HLT, pages 97‚Äì105,
Columbus, OH. Association for Computational Linguistics. 201
Zipf, G. K. (1932). Selective Studies and the Principle of Relative Frequency in Language. Harvard
University Press. DOI: 10.4159/harvard.9780674434929. 166
Index
adaptor grammars, 189
MCMC inference, 193
Pitman-Yor prior, 190
stick-breaking view, 192
variational inference, 194
autocorrelation, 117
Bayes risk, 89
Bayes‚Äô rule, 7
Bayesian
objectivism, 22
subjectivism, 22
Bayesian philosophy, 22
chain rule, 6
Chinese restaurant process, 155, 172
Chomsky normal form, 178
computational conjugacy, 209
conditional distribution, 5
conjugacy, 33
context-free grammar, 208
synchronous, 200
context-sensitive grammars, 205
cross entropy, 211
cumulative distribution function, 4
de Finetti‚Äôs theorem, 8
dependency grammars, 198
digamma function, 140
directed graphical models, 17
plate notation, 18
Dirichlet process, 152
hierarchical, 161
mixture, 157
MCMC inference, 158
search inference, 160
variational inference, 159
PCFGs, 196
distance-dependent Chinese restaurant pro-
cess, 169
distribution
Bernoulli, 11, 54
Beta, 35
categorical, 53, 215
conditional, 5
Dirichlet, 32, 54, 216
Gamma, 217
Gamma and Dirichlet, 57
GEM, 153
inverse Wishart, 220
joint, 4
Laplace, 218
logistic normal, 59, 219
marginal, 4
multinomial, 33, 53, 215
multivariate normal, 44, 218
normal, 40
Poisson, 217
prior, 12, 28, 43
symmetric Dirichlet, 35
target, 95
uniform, 66
entropy, 211
estimation
decision-theoretic, 88
Dirichlet, 81
empirical Bayes, 90, 143
latent variables, 85
244 INDEX
maximum a posteriori, 26, 79
maximum likelihood, 13
regularization, 82
smoothing, 81
exchangeability, 8
exp-digamma function, 141
expectation-maximization, 141, 213
factorized posterior, 133
frequentist, 12, 22, 28
Fubini‚Äôs theoreom, 10
Gaussian mixture model, 15
Gaussian process, 168
GEM distribution, 153
generative story, 15
Gibbs sampling, 99
collapsed, 102
operators, 107
grammar induction, 201, 204
hidden Markov models, 174, 203
inÔ¨Ånite state space, 175
PCFG representation, 184
hyperparameters, 33
inclusion-exclusion principle, 24
independence assumptions, 16
Indian buÔ¨Äet process, 168
inference, 12, 38
approximate, 95
Markov chain Monte Carlo, 39, 95
variational, 39, 131, 133
convergence diagnosis, 145
decoding, 146
Dirichlet process mixtures, 159
Dirichlet-Multinomial, 137
mean-Ô¨Åeld, 134, 149
online, 147
inside algorithm, 183
inverse transform sampling, 122
joint distribution, 4
Kullback-Leibler divergence, 147, 212
language modeling, 82, 165
large-scale MCMC, 210
latent Dirichlet allocation, 19, 25, 29
collapsed Gibbs sampler, 104
Gibbs sampler, 100
independence assumptions, 31
latent-variable learning, 21
latent-variable PCFG, 197
learning
semi-supervised, 19
supervised, 19, 27
unsupervised, 19, 27
likelihood, 22
linear regression, 39
log-likelihood, 22
marginal, 19
machine translation, 200
marginal distribution, 4
marginalization of random variables, 4
Markov chains, 118
maximum likelihood estimate, 13
MCMC
detailed balance, 108
recurrence, 108
search space, 96
mean-Ô¨Åeld variational inference, 134
Metropolis-Hastings sampling, 111
mildly CFGs, 205
minimum description length, 80
model
bag of words, 30
discriminative, 14, 27
exponential, 69, 208
generative, 14, 27
grammar, 173
INDEX 245
graphical, 17
hierarchical, 71
latent Dirichlet allocation, 129
latent variables, 48, 78
log-linear, 83
mixture, 49, 151
mixture of Gaussians, 15
nonparametric, 12
parametric, 12
probabilistic-context free grammars, 177
structure, 97
unigram, 30
model merging, 206
Monte Carlo integration, 123
multilingual learning, 201
grammar induction, 204
part-of-speech tagging, 203
multinomial collection, 180
nested Chinese restaurant process, 169
normalization constant, 5, 47
observations, 7, 14, 19
online variational inference, 147
outside algorithm, 183
particle Ô¨Åltering, 126
PCFG inference, 180
feature expectations, 182
inside algorithm, 181
outside algorithm, 182, 183
PCFGs
MCMC inference, 186
tightness, 51, 186
phrase-structure tree, 177
Pitman-Yor process, 163
language modeling, 165
power-law behavior, 166
plate notation for graphical models, 18
posterior
asymptotics, 92
Laplace‚Äôs approximation, 87
point estimate, 77
summarization, 77
power-law behavior, 166
prior
conjugate, 44, 51
improper, 66
JeÔ¨Äreys, 67
non-informative, 65
nonparametric, 151
PCFGs, 185
sparsity, 37
structural, 72
probability
distribution, 1
event, 1
measure, 1
probability density function, 3
probability mass function, 3
probability simplex, 34, 53
random variable
transformation, 213
random variables, 2
conditional independence, 8
continuous, 3
covariance, 11
discrete, 3
exchangeable, 155
expectation, 9
independence, 5, 7
latent, 14, 16, 19, 28, 98
moments, 11
multivariate, 3
variance, 11
regression
Ridge, 94
rejection sampling, 120
246 INDEX
sample space, 1
discrete, 2, 4
sampling
auxiliary variable, 113
blocked, 98
component-wise, 112
convergence, 116
Gibbs, 99
collapsed, 102
operators, 106
parallelization, 109
inverse transform, 122
Metropolis-Hastings, 111
nested MCMC, 125
pointwise, 99
rejection, 120
slice, 113
sequence memoizers, 170
sequential Monte Carlo, 126
simulated annealing, 116
slice sampling, 113
smoothing, 26
sparse grammars, 187
sparsity, 37, 56
statistical model, 11
stick-breaking process, 153
suÔ¨Écient statistics, 69
supervised learning, 21
synchronous grammars, 200
text regression, 39
topic modeling, 29, 100
transductive learning, 20
tree substitution grammars, 206
treebank, 177
unsupervised learning, 20
variational bound, 131
variational inference, 133
PCFGs, 187
weighted CFG, 179
