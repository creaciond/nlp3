Assessing Spoken Language Ability:
A Many-Facet Rasch Analysis
Sahbi Hidri
Abstract Assessing speaking in a useful way has been attended with some con-
cerns, such as scoring subjectivity and test bias. Most often, scoring the speaking
performance might result in some unfairness that could possibly emanate from
these two issues. This, in turn, could harm the life of test-takers and many other
stakeholders. This article investigated the assessment of speaking among learners of
English in an ESP program in an EFL context. To this end, 213 test-takers were
assessed on their speaking ability by 12 raters using a five-rubric scale (task
achievement, fluency, grammar, vocabulary and phonology). Each candidate was
assessed by two raters, totaling six pairs of two raters each). The speaking exam
included four parts only of which two parts were graded, thus excluding the opening
and closing sections. All the exam questions were pre-formulated and teachers were
instructed to stick to the frame of the exam. The results of the study showed that
generally the test-takers’ speaking ability was scored more leniently than harshly
and that raters were biased towards the speaking rubrics, which indicated a fuzzy
idea about such rubrics. Assuredly, the different statistical tests of the FACETS
showed that the speaking exam was neither valid nor reliable. The study had
relevant implications for rater training in how to score speaking in an objective
way and for recommending writing a list of test specifications (specs) to design
useful and fair speaking exams in similar-related contexts.
Keywords Speaking • Rubrics • Rater • ESP • Candidate • Interaction •
Curriculum • Textbooks • Measurement • Construct • RASCH • FACETS
1 Introduction
This study investigated measuring the spoken ability among ESP students in an
EFL context, from a Multi-Facet Rasch Measurement perspective. It also addressed
test validation of the intertwined parameters to assess speaking. Many researchers
S. Hidri (*)
Faculty of Human and Social Sciences of Tunis, Tunis University, Tunis, Tunisia
e-mail: sahbihidri@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_2
23
(e.g., Alderson & Wall, 1993; Black & Wiliam, 1998; Jin, 2000) highlighted the fair
nature of language tests where positive washback must take place in teaching,
learning and testing. Central to this point is test validation which has been an
ongoing debate addressed by proponents of language assessment. The core of test
validation rests on deciding whether a test is a facet, among many other facets, of an
appropriate fit for a specific context (Chapelle, 1999). For instance, construct
validity is one facet of test validation (Messick, 1989), among many others. On
the speaking validity, as an essential phase in speaking test design, Fulcher (1996a)
argues that for a test to be valid, it has to reflect the natural environments and
authentic contexts that promote a natural and spontaneous use of language. This
echoes the idea of authenticity, an idea that was highlighted by the communicative
approach even though many researchers have had a controversial issue on this
construct to implement in language test design and to measure fluency. Fulcher
(1996b) argues that “validity considerations must be addressed in the construction
phase of developing rating scales” (p. 208). For instance, Brown (2003) suggests
that oral interviews “have long been a popular method for assessing oral second
language proficiency” (p. 1). To achieve this, there should be a link between a test
and a criterion whose purpose is to preserve inference validity (Lynch & McNa-
mara, 1998) where teachers, for instance, in China gave much importance to “oral
English teaching” even in passive language skills, such as reading (Zhao, 2013).
Spolsky (1990) investigated a historical overview of the different learning and
testing facets that are pertained to the assessment of speaking. There is a greater role
to be played by the test designer in relation to the test-taker. In this regard, Spolsky
(1990) contends that
We start with the assumption then that any public language test will need to satisfy several
different sets of criteria, criteria drawn from different areas and representing the interests of
several different groups. The language tester is responsible for and responsive to theory
derived from two unrelated fields, linguistics and psychometrics, and at the same is directed
and constrained by practical institutional, economic, social and even political demands.
(p. 160)
That is, there should be a comprehensive list of facets that should be present in
testing speaking whose purpose is to consider all stakeholders involved in the
testing operation. For Spolsky (1990), testing should echo the different testing
theories where speaking as a construct can be defined both theoretically and
operationally. This idea was highlighted by Douglas (1994) who reported that
speaking exams should reflect theoretical underpinnings of the speaking construct
and that assigning grades on the students’ speaking performance depends on the
raters’ and candidates’ background. To do so, establishing clear rubrics to measure
the speaking performance is consequential, since they should highlight the theoret-
ical aspects of the speaking construct. In this regard, Shohamy (1994) tackled the
issue of how “direct” and “semi-direct” speaking exams could be and found that
qualitatively both exams had different elicitation tasks and the type of language
used, hence, the necessity to look for different facets to validate speaking exams.
Implementing this, for instance, by native speakers, implies that they should
24 S. Hidri
“perceive the L2 oral construct differently” (Chalhoub-Deville, 1995, p. 16), espe-
cially in measuring oral assessment scales.
Malvern and Richards (2002) addressed the validity of oral interviews by
reflecting on whether there are similarities between natural and spontaneous con-
versation and another formal facet of speaking, which is the interview. Grant (1997)
corroborated that qualitative and quantitative data analyses of test-takers’ and
teachers’ questionnaires and a test section supported the argument for test validity.
Scott (1986) discussed the fact that there are different factors that shape students’
attitudes towards tests, such as format, length, and other practical issues of which
the most important factor remains the affective attitude towards the test. Investi-
gating the affective factors that impact students’ performance in language skills,
such as speaking has been investigated in research (e.g., Shohamy, 1982).
2 Theoretical Background
2.1 Previous Studies on Assessing Speaking
This section basically aims to give an idea about the previous studies undertaken to
assess speaking. Assessing speaking has been given momentum in language testing
and assessment and task methods have been used to test speaking. For instance,
Fulcher (1996a) talked about the one-to-one interview, as a test method, in daily
and normal conversation. The nature of this type of speaking is marked by power
relationship where the interviewer holds the powerful ground and, therefore, gains
the speaking territory over the interviewee. Such a powerful relationship might lead
to an “imbalance” between the two sides of the one-to-one interview, which cannot
result in spontaneous and natural overflow of speaking in any communicative
situations. This might unfavorably build up frequent unwillingness of students to
be part of a one-to-one interview in an exam situation. Such a relationship can
decide on the nature and flow of discourse between the interviewer and interviewee,
which might affect the test-takers’ performance, especially when the relationship is
vertical. Shin (2005) noted that proficiency might harm the validity of the test
because “the appropriateness of the interpretations we make from a single com-
posite score should be called into question because test scores offer different kinds
of information for different subgroups of test-takers” (p. 33).
In investigating speaking proficiency, Clark (1988) concluded that all the test
forms utilized in the study highly correlated together and that examinees had
different conceptions of and attitudes towards the taped test and live interview,
with a favorable attitude to the “face-to-face” interview exam. Conceptions of
assessment have been investigated in the Middle East and North Africa context
even though much needs to be boosted in this regard (for such conceptions, see
Hidri, 2015). Pinget, Bosker, Quené, and de Jong (2014) maintained that what made
the difference between L2 and L1 speech production are oral fluency and foreign
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 25
accent. They stated that “acoustic measures of fluency were good predictors of
fluency ratings” and that “segmental and suprasegmental measures of accent could
predict some variance of accent ratings” (p. 349). Ginther, Dimova, and Yang
(2010) argued that holistic scoring can provide feedback into the inference validity
and scoring rubrics, benchmarks and rubrics of measuring fluency and oral English
proficiency among Chinese, Hindi and English students. They identified a “strong
correlation” of test scores and speech rate. In a study on “validity evidence” of a
“group oral test” using both a Rasch analysis and a generalizability study, Van
Moere (2006) found that there was rater fit, “topic, or prompt, was not a significant
factor” (p. 411) and that students’ performance can be impacted by raters and by the
other test-takers in tasks that call for interaction.
McNamara (1997) emphasized the importance and relevance of co-constructing
an interactive conversation that engages more than one test-taker or peer-to-peer
interaction. Unfortunately, this assessment trend has not been given its due impor-
tance in research. Ducasse and Brown (2009) claimed that the idea of “interactional
competence” rests on interactants mediating “their interactional partner to
co-construct the spoken performance” (p. 424). Such a trend quickly spread during
the 1990s first in teaching and then in testing where speaking as a construct
involved different facets, such as body language. However, other researchers,
such as Ginther et al. (2010) held that students’ performance in speaking depends
on measuring fluency. Other facets, such as impact of fatigue on raters, have been
investigated by Ling, Mollaun, and Xi (2014) who demonstrated that grading open-
ended questions and prompts might lead to construct fuzziness, which could impact
test fairness on the one hand and that fatigue in scoring could “negatively affect
human performance” (p. 479), on the other. Rating consistency is achieved through
“shorter sessions” of rating. Shohamy and Reves (1985) and Wesche (1987)
claimed that test authenticity in speaking requires “direct” tests, since they reflect
real-life context situations. Thus, authenticity refers to real-life situations that
examinees are supposed to encounter in exam contexts.
To investigate the attitudes towards measuring speaking, Chen and Henning
(1985, p. 155) warned against “the cultural, ethnic and gender bias” that might flow
from speaking tests. Zeidner and Bensoussan (1988) contended that students
preferred the written test over the oral one and that males favored oral tests better
than females. However, Blanche (1990) maintained that self-assessment is limited
compared to standard or standardized exam results, since learners with a high and
good language ability are inclined “to underrate their abilities” (p. 202), while
students with a low language ability overrate their performance. Brown (1993)
argues that the examinees’ feedback can play a crucial role in developing the
speaking construct both theoretically and operationally where, as a construct, oral
proficiency can be determined by the interviewer questions, especially in stressing
consistency of the rating process among all raters (Ross, 1992). Swain (1993,
p. 193), in an oral sociolinguistic test, states that “measures of reliability that
depend on the notion of internal consistency are incompatible with our current
understanding of second language communicative proficiency.” As such,
O’Loughlin (1995), in investigating the impact of format as well as task type on
26 S. Hidri
the test-takers’ achievement in direct and semi-direct exams of the Australian
Assessment of Communicative English Skills (access), reported that there are
different factors that impact lexical density in a speaking exam.
Additionally, Lazaraton (1996) argues that the interlocutor support can affect the
test-takers’ use of language in an oral proficiency interaction exam. In the same
vein, McNamara and Lumley (1997) suggested that “interlocutor variability and
audio-tape quality” can impact rating. Wigglesworth (1997, p. 85) showed that
planning time in a speaking exam “may improve accuracy on some measures where
the cognitive load of the task is high, but that this effect does not extend to the
low-proficiency candidates.” Kormos (1999) in stressing “role plays” and “non-
scripted interviews” in exams indicated that “conversation interaction is more
symmetrical in the guided role-play activity with the candidates introducing and
ratifying approximately the same number of topics as the examiners” (p. 163).
Powers, Schedl, and Leung (1999) pointed out that there is correlation between
the TSE scores and the “judgements, reactions and understanding of listeners”
(p. 399). Such findings were meant to target validation of tests in designing fair
and useful tests.
Upshur and Turner (1999) discussed the major differences between Second
Language Acquisition (SLA) and language testing and called for a research blend
of testing and SLA. Salaberry (2000) explained the major issues related to the
measurement of ACTFL Oral Proficiency Interview including different variables
that might negatively impact students’ performance in speaking, such as “(. . .)
apathy, lack of funding for courses with increasing more students per section, TA
job alienation and the increasing work load of language course supervisors”
(p. 289). Swain (2001) and Swain, Brooks, and Tocalli-Beller (2002) focused on
the link between L2 learning and testing in dialogue as an assessment type from a
sociocultural theory of mind and found that the jointly constructed meaning in an
assessment task, such as dialogue, might be opportune to “targets for measurement”
(p. 275). Elder, Iwashita, and McNamara (2001) demonstrated that students did not
have clear and direct attitudes towards task difficulty in an oral proficiency exam.
At another level, in analyzing the effects of gender on rating in an IELTS speaking
exam, O’Loughlin (2002) pointed out that “both the discourse and test score
analyses indicated that gender did not have a significant impact on the IELTS
interview” (p. 169). O’Sullivan, Weir, and Saville (2002) contended that one of the
research instruments to use to validate tests could be observation checklists. In
measuring the TOEFL exam of 222 Japanese students majoring in English, Ockey,
Koyama, Setoguchi, and Sun (2015) argued that the
TOEFL iBT speaking scores are good overall indicators of academic oral ability and that
they are better measures of pronunciation, fluency and vocabulary/grammar than they are of
interactional competence, descriptive skill, and presentation delivery skill. (p. 41)
Using a mixed-methods approach to measure rater performance in an English
proficiency test, Yan (2014) suggested that raters behaved differently in scoring
student’ performance in that they differed in their severity and leniency. In tackling
the impacts of “pre-task planning” on students’ performance in joint oral
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 27
performance exams, Nitta and Nakatsuhara (2014) reported that “planning had
limited effect on performance” (p. 47). However, Leaper and Riazi (2014)
discussed the impact of prompts in group oral tests and observed that prompts
were propitious to “longer, more complex turns” and motivated the examinees to be
more open to talk about their family background.
In assessing Chinese, Jin and Mak (2012) claimed significant correlations
between the speaking feature in measuring the overall communicative speaking
ability. Van Moere (2012) investigated the implementation of psycholinguistics
“constructs” in testing speaking proficiency and noted that “sentence repetition and
sentence building tasks reliably separated test-takers according to proficiency in a
way that accounts for the trade-off between fluency, accuracy and complexity”
(p. 325). Implementing assessment criteria, such as “grammatical accuracy, flu-
ency, vocabulary range, pronunciation and content elaboration/development” to
measure speaking in an EAP context, Sato (2011) suggested that “content elabo-
ration/development (original emphasis) made a substantive contribution to the
intuitive judgments and composite score” (p. 223). In addressing the spoken ability
of 181 learners of Dutch, Hultsijn, Schoonen, de Jong, Steinel, and Florijn (2011)
explained that “linguistic competences, except speed of articulation, discriminated
participants at the two levels of oral production” (p. 203).
2.2 Rationale and Scope of the Study
Measuring the test-takers’ spoken performance in a pair or group discussion has
always been challenging for raters on the ground that assigning a score based on
each individual performance might be the result of some biased judgements, even
though it could be more beneficial for the test-takers over the traditional form of
assessment (Van Moere, 2006). McNamara and Lumley (1997) contended that the
assessment of speaking has become complex. While many studies have been
carried out on investigating the different variables linked to the speaking perfor-
mance in different educational contexts (e.g., Bonk & Ockey, 2003; Fulcher, 1996a,
1996b; Lumley & Brown, 1997; Shohamy, Reves, & Bejarano, 1986) where there
has been a lot on addressing low- and high-stakes exams in different situations, little
is known about other contexts, such as the MENA region.
In this context, mainly students in the Gulf countries, there are major handicaps
to students in developing their speaking ability in English in a natural way. First,
many teachers are constrained in class in engaging the learners in normal and
natural conversations where males and females are not allowed to interact together.
Second, many textbooks designed specifically to match the Gulf countries’ socio-
cultural norms and traditions do not present authentic speaking situations to address
different daily and academic topics in that many topics are completely overlooked
on the grounds that they are culturally inappropriate and that they violate the norms
of the socio-political and religious boundaries. Third, many teachers have
conflicting views about speaking as a construct and most of them think that
28 S. Hidri
speaking is a matter of memorization. Fourth, teachers and raters of speaking have
conflicting and fuzzy construct views about grading students by confusing between
accent and pronunciation. Finally, during the time of data collection, there was no
formal or official training for these teachers in how to score students’ spoken
performance. To investigate such a context, this study then attempted to answer
the following research questions:
1. Are the speaking exams carried out in this context construct valid?
2. Do raters grade speaking rubrics consistently?
3. What implications can be drawn from this study to improve the status of
measuring speaking in such an EFL context?
This current chapter aimed to tackle the shortcomings performed to the assess-
ment of speaking in an ESP program in an EFL context. A second purpose was to
help teachers address the different variables in measuring speaking exams. A third
purpose was to highlight the effectiveness of using the RASCH measurement
(FACETS) to address the raters (in)consistency in objectively measuring speaking
as a construct. A fourth and final purpose was to write a list of test specs to help
teachers and test designers construct fair and useful speaking tests.
3 Method
3.1 Participants
The participants who took part in the study belonged to one of the colleges of
applied sciences in Oman. They ranged in age from 19 to 21 and, during the time of
data collection, they had studied English for 11 years 7 of which were at the
secondary education. All the participants had already studied in the Foundation
program where they were instructed to use the Headway textbook series depending
on their level of performance. During the year of data collection, the students whose
scores in the speaking exam were considered for data analyses belonged to three
disciplines: mathematics, IT and international business administration (IBA). For
practical reasons, according to the policy of the department, they all studied in the
same groups. Their program was called EAP, instead of ESP, even though their ESP
needs clearly stood out in this academic context. Along with all these exams,
students were supposed to write a 600-word project which was composed of an
introduction, body and conclusion. The rationale behind carrying out this project
was to expose students to developing their study skills of data collection and
analysis. They were also introduced to how research instruments, such as question-
naires or interviews, are designed. Like in teaching, all the participants were tested
on the following skills: listening, reading, writing and speaking. For speaking, the
test was carried out in pairs with two interviewers, the class teacher and a
co-assessor. With odd numbers, very few cases of three students sat together for
the speaking exam.
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 29
The raters who were asked to be involved in exam administration and marking
were 12 raters who came from different educational backgrounds, such as Oman,
Iran, Tunisia, Romania, UK, USA, the Philippines, Syria and Russia. Four of the
raters were PhD holders, while the rest had only their Masters of Arts. They were all
involved in the teaching of the foundation and EAP students. Their teaching
experience ranged from 2 to 28 years. However, the 12 raters did not have any
experience in rating, as a professional rater.
3.2 Speaking Exam Questions
The speaking exam (Appendix 1) constituted of four major phases two of which
were only assessed. Phases A and D were meant to introduce and close the exam
respectively. In the introduction part (phase A), one of the interviewers was
supposed to welcome the two candidates and introduce the exam to them, help
them with directions on how to proceed with the test, mainly in giving instructions
to engage them in the exam task in pairs. In the closing part of the exam (phase D),
one of the interviewers was supposed to inform the candidates that the exam was
over and wish them good bye. In phase B (task 1) of the exam, the test-takers were
instructed to perform together to express their thoughts and ideas on the given exam
questions. The interviewers were asked to organize the turn-taking between the two
candidates who were meant to answer questions on one of the themes they had
studied in class. Timing of this task was fixed at 2 min for each student who was
supposed to perform individually. A test bank of speaking questions was made
available for the interviewers to use. This was done for fairness purposes. The third
phase of the exam (phase C/task 2, Appendix 2), called “paired interaction” and
linked to part B/task 1 (Appendix 2), aimed at helping students to be engaged in an
interactive discussion with themselves and with the interviewers. They were also
encouraged to ask questions and respond to them appropriately. Timing of this task
was fixed at 3–4 min. The last phase of the exam, closing, was meant to end the
conversation and wish the candidates good luck. All the speaking exam questions
were related to the themes of the speaking course in class.
3.3 Speaking Exam Rubrics
The speaking exam rubrics (Appendix 3) that were considered in this study
contained the following: task achievement (one and two), fluency, grammar, vocab-
ulary and phonology. Teachers were given these rubrics the day of the exam to start
using in grading the students’ speaking performance. They were also asked to grade
the students’ performance out of 5 marks for each rubric, thus totaling 25 marks.
Table 1 presents the different research instruments, rationale and research questions
that were employed in this study.
30 S. Hidri
4 Data Analysis
Data of the study were analyzed using the FACETS program. The purpose of this
analysis was to have a very clear idea about the students’ ability in speaking and
whether the scores they got reflected their actual ability. The analysis was also
meant to address the bias interaction of the raters by rubrics and whether the raters
graded in a consistent way. It was hoped that such analysis would lead to draw
implications for pedagogy, teaching and assessment on the one hand and for
designing specs, on the other.
5 Results
5.1 Students’ Ability in Speaking
This part attends to the FACETS analyses of test scores in the speaking exam. The
analysis of the results section will cover the analysis of three major parts: stu-
dents’ overall ability, raters (in)consistencies and the bias interaction of raters by
rubrics. Figure 1 portrayed the different facets of the exam that were investigated:
measure, candidate, gender, rater, rubrics and scale. Colum one, measr, showed
the test-takers’ ability that extended over an ability scale of 2 to þ5. Column two,
candidate, indicated the ability of the candidates that ranged from 2 to þ5.
Column 3 was on the gender of the candidates. Column four was on the rater
ID. Column five covered rubrics that were used to measure the students’ perfor-
mance. The last column tackled the scale used, which extended from 1 to 5.
Table 1 Data collection and analyses
Participants Instruments Data analyses Rationale Research questions
– Students
(N ¼ 213)
with an age
range of
19–21 years
– Speaking
exam: four
phases only
two of which
were graded
– FACETS Ana-
lyses of the test
scores: - Ability
estimates
– Rater measure-
ment report
– Scale report and
– Bias interaction
(rater by rubrics)
– To establish
a base line for
students’ overall
ability in
speaking
– To investigate
raters’ behaviour
with grading and
whether it was
(in)consistent
– To check the
bias interaction
of raters by
rubrics
– Are the speaking
exams carried out
in this context
construct valid?
– Do raters grade
speaking rubrics
consistently?
– What implications
can be drawn from
this study to
improve the status
of measuring
speaking in such
an EFL context?
– Raters
(N ¼ 12)
whose teach-
ing experi-
ence ranged
from 2 to
28 years
– Speaking
rubricsa
:
five rubrics
graded out of
25 marks,
5 marks for
each rubric
a
Task achievement, fluency, grammar, vocabulary and phonology
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 31
To comment on this FACETS output, 51 candidates (1 asterisk ¼ 3 candidates;
1 dot (.) ¼ one candidate) had an ability that ranged from 2 to 0, while the rest of
abilities ranged from above 0 to þ5, with only 12 candidates who had an ability that
extended from þ4 to þ5. In column 4, rater, the 12 raters clustered in 6 rows, thus
denoting a large gap of rater inconsistency with raters 9 and 10 being the toughest,
while rater 6 was the most lenient one. In column 5, the toughest scored rubric was
vocabulary, while the most lenient graded one was fluency. The five rubrics
clustered in four rows. Column 6 presented the scale used to grade the spoken
performance with 1 denoting low performance, while 5 indicating high
performance.
5.2 Rater Measurement Report
This section presents the raters’ measurement report in grading the students’ spoken
performance. Recall the 12 raters, who came from different educational back-
grounds, graded this performance. They were paired off to do the task. Column
1 identified rater, column 2 covered rater severity estimate, errors of rater severity
Fig. 1 FACETS variable map of the speaking exam
32 S. Hidri
estimates were manifested in column 3 and column 4 presented the fit statistics that
showed the relative consistency of rating. The measure ranged between 0.59 and
1.53 logits, with rater 6 as most lenient (0.59) and rater 10 as most severe (1.53).
This meant that the difference in severity is high. The mean was 0.00 with a SD of
0.70. The high strata index (5.83) showed that the discrepancy among the 12 raters
was very high, which suggested that these raters could be clustered and classified
into six “statistically distinct levels” in terms of severity (check Fig. 1 on how raters
clustered). That is, raters were divergent in terms of severity, which denoted real
differences in their scoring of the candidates’ speaking ability. To minimize this
severity, it is desirable to have a reliability index of zero (0). The inter-rater
agreement opportunities were 1065 of exact agreements (497) with a percentage
of 46.7%, while the expected one was (462.0) 43.4%. The observed agreement
(46.7%) was higher than the expected one (43.4%). This explained the fact that
raters did not do their rating in an independent way (Linacre, 2011). Table 2 also
presented the fit statistics, which was related to the degree of rater internal self-
consistency in grading the candidates, rubrics and the ability to use the rating scale
that ranged between 1 and 5 to distinctively grade candidates’ performance (Bond
& Fox, 2015). Data of Table 2 showed that some raters were “close to the expected
value of 1.0.” These were raters 6, 4, 12, 11, 2, 7, 3 and 8. Having an expected value
close to zero demonstrated that these raters had a somehow consistent rating and
used the rating scale in a consistent way. This also suggested that they preserved a
severe level of grading across the other facets, such as rubrics and raters. Only rater
4 bordered the level of misfit.
Table 2 Measurement report
(Rater, N ¼ 12)
Severity estimate Model SE Infit MnSq
Rater 6 0.59 0.12 1.03
Rater 4 0.48 0.14 1.00
Rater 12 0.46 0.11 0.97
Rater 11 0.40 0.11 0.93
Rater 2 0.33 0.09 1.17
Rater 7 0.31 0.13 1.15
Rater 1 0.23 0.13 1.35
Rater 8 0.22 0.13 1.02
Rater 5 0.02 0.12 0.75
Rater 3 0.03 0.09 1.13
Rater 9 1.48 0.11 0.74
Rater 10 1.53 0.11 0.78
M (12) 0.00 0.12 1.00
SD 0.70 0.01 0.18
Strata 5.83, Reliability (not inter-rater) 0.97
Fixed (all same) chi-square 460.2, d.f.: 11, significance (proba-
bility): 0.00
Inter-Rater agreement opportunities: 1065, Exact agreements:
497 ¼ 46.7%, Expected: 462.0 ¼ 43.4%
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 33
Table 3 summarizes the candidate facet statistics that ranged between 1.82 and
6.08 “logits with a mean” of 0.50 (SD ¼ 0.12). The “positive mean” showed that the
speaking exam was easy for the candidates. The mean SE was high (1.41) and this
showed that data of the study indicated that more than one score was assigned by
two raters. Recall that the same performance of students in the speaking exam was
graded by two raters. In the separation statistics, the strata and reliability of
separation were 2.58 and 0.87 respectively. The low strata index showed that the
variance among the candidates (2.58) was larger than the SE (1.41) and that the
213 candidates could be divided into two distinct levels on their speaking ability.
Claiming test reliability would be treated with caution, since the strata (2.48) and
reliability (0.87) indices were not high. Similarly, in investigating the validity of the
exam assessment, the value of the mean fit is 0.50 (SD ¼ 0.12) which was far from
the expected value of 1.00. This meant that the speaking exam was not that valid.
5.3 Rubrics Measurement Report
Table 4, rubrics measurement report, presents the five rubrics used to measure the
candidates’ speaking ability. Such rubrics were sorted out from the easiest to most
difficult ones [fluency (0.63) and vocabulary (0.46) respectively]. It was very
difficult for the candidates to obtain high scores in vocabulary with a difficulty of
0.46 logits, but it was easier for them to obtain high scores in fluency with a
difficulty of 0.63 logits. The strata estimate value was low, 0.44, however, the
reliability separation index (not inter-rater) was high, 0.97. The strata (0.44) and
reliability (0.97) indices showed four levels of difficulty of these rubrics (for further
details, check Fig. 1). Such levels in return highlighted two major aspects: the raters’
different perceptions of these rubrics and the candidates’ disparity in performing on
the speaking exam. McNamara (1996) mentions that the fit statistics should be
calculated by doubling the value of one SD to the mean in both directions. The infit
mean of the rubrics is 1.01 and the SD is 0.17. Therefore, the range would be
Table 3 Summary of
candidate facet statistics
Candidate ability estimate (N ¼ 213)
M (Model SE) 1.41 (0.50)
SD (Model SE) 1.42 (0.12)
Min 1.82
Max 6.08
Infit
M 0.50
SD 0.12
Separation statistics
Strata 2.58
Reliability of separation 0.87
Fixed Chi-Square statistic (d.f.:) 1471.5(212), p < 0.00
34 S. Hidri
0.67–1.35 (1.01 þ (0.17  2) ¼ 1.35; 1.01  (0.17  2) ¼ 0.67). This range
(0.67–1.35) indicated that there was neither misfit nor overfit, as all mean values
were within this range.
5.4 Rating Scale Report
Table 5 highlights a description of the five scale categories (1, 2, 3, 4 and 5) used to
measure students’ speaking ability. The purpose of this analysis was to check
whether this scale measured the right construct and whether all raters used all the
scale levels consistently. To comment, Table 5 presented 6 columns: score level
that ranged from 1 to 5 (column 1), observed count, divided into frequency and
percentage), column 2. Column 3, average measure, dealt with the candidate’
speaking ability averaged with each scale level. Colum 4 presented the expected
scores for each scale level. Colum 5 presented the “outfit mean square index for
each scale category” (Myford & Wolf, 2000). McNamara (1996) argued that 1.0 is
the expected value, which denotes that average and expected measure of the
candidate’ ability has the same value, i.e., equal. However, if this value between
the average and expected measure is larger than 2.0, it indicates inappropriateness
of scoring. The last column, step calibration, divided into two (measure and SE),
were about the estimates of the different difficulty levels for choosing one scale at
Table 4 Rubrics
measurement report
Measure Model SE Infit MnSq
Fluency 0.63 0.08 1.23
Task achievement 0.44 0.08 1.13
Phonology 0.28 0.07 0.92
Grammar 0.34 0.07 0.74
Vocabulary 0.46 0.07 1.04
M (5) 0.00 0.07 1.01
SD 0.45 0.00 0.17
Strata 0.44, Reliability (not inter-rater) 0.97
Fixed (all same) chi-square 178.9, d.f.: 4, significance (probabil-
ity): 0.00
Table 5 Scale statistics
Score level
Observed counts Average
measure
Expected
measure
Outfit
MnSq
Step calibration
Freq. % Measure SE
1 90 4 2.28 2.02 0.8
2 311 15 1.16 1.24 1.2 2.95 0.12
3 471 22 0.50 0.56 0.9 0.79 0.08
4 693 33 2.12 2.01 1.0 0.95 0.07
5 555 26 3.01 3.09 1.1 2.80 0.06
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 35
the expense of another one (McNamara, 1996). To comment on Table 5, the least
assigned score was 1 with a frequency of 90 times and a percentage of 4%, while the
most assigned one was scale 4 with a frequency of 693 and a percentage of 33%. In
considering columns 3 and 4, it could be observed that the scales of 1 and 4 were
assigned more (2.28 and 2.12 logits) than expected (2.02 and 2.01 logits),
however, score levels 2, 3 and 5 were assigned less (1.16, 0.56 and 3.01 logits)
than expected (1.24, 0.90 and 3.09 logits). All these instances denoted rater
inconsistency.
5.5 Bias Interaction
The bias interaction of rater by rubrics was highlighted in Table 6 which
explained that there were 4 bias interactions (out of the 60 possible interactions:
12 raters  5 rubrics ¼ 60). In observing the bias interaction in Appendix 5, there
were 15 elements to consider, starting with obsvd score (column 1), exp. score
(column 2) to rubrics (column 14) and measr (column 15). To comment, the two
first columns (obsvd and exp. scores), all the 60 interactions showed that all the
rubrics were not scored as expected. However, the mean of both the obsvd and
exp. scores indicated equal values of 127.9. Column 7 of Appendix 5 presented t
score, which are z-scores, demonstrated that t values which are higher (þ2) or
lower (2) showed “biased interactions”. For instance, rubrics such as vocabu-
lary (column 14) (rater 10, rater 9) phonology (rater 10) and fluency (rater 1) had t
scores of 3.88, 3.82, 2.38 and 2.14 respectively. On the other continuum,
bottom of the table, phonology (rater 11), fluency (rater 9) and fluency (rater) had
t-scores of 2.42, 5.91 and 6.13 respectively. Therefore, all these instances could
be said to have significant biased interactions. A graphic display of this bias was
Table 6 Bias interaction of rater by rubrics
-------------------------------------------------------------
| Iteration Max. Score Residual Max. Logit Change |
| Elements % Categories Elements Steps |
-------------------------------------------------------------
| BIAS 1 -26.2241 112.1 -1.0000 |
| BIAS 2 -7.3130 16.7 -.3702 |
| BIAS 3 .1058 .8 .0104 |
| BIAS 4 .0003 .0 .0000 |
-------------------------------------------------------------
36 S. Hidri
highlighted in Fig. 2 where it showed that the most lenient scored rubric was
fluency, while the most difficult one was vocabulary (for further details on this
bias, see Appendix 5).
6 Discussion
This study investigated the measurement of the speaking ability among learners of
English majoring in an EAP/ESP program. Data collection and analyses used
12 raters and 213 candidates who were paired off for a speaking exam that
contained four parts only of which parts 2 and 3 were graded by two raters. Results
of the study led to different discussion points that were organized according to the
three research questions:
1. Are the speaking exams carried out in this context construct valid?
2. Do raters grade speaking rubrics consistently?
3. What implications can be drawn from this study to improve the status of
measuring speaking in such an EFL context?
Question 1: Are the speaking exams carried out in this context construct valid?
One of the essential points that was taken into consideration was the necessity
to administer speaking exams whose general ideas were already dealt with in
class as part of the 10 themes that were supposed to be covered throughout a
whole term. Adhering to such a testing practice had the goal of activating the
candidates’ background knowledge that could ultimately help them process
the speaking tasks. However, this led so many students to memorize the contents
of these topics already delivered in class. It would, therefore, be inappropriate to
claim that the test was construct valid, since the candidates were not tested on
-1.5
-1
-0.5
0
0.5
1
1.5
1.
rater
1
2.
rater
2
3.
rater
3
4.
rater
4
5.
rater
5
6.
rater
6
7.
rater
7
8.
rater
8
9.
rater
9
10.
rater
10
11.
rater
11
12.
rater
12
Relative
Measure
. Rater
Bias Interaction Rater by Rubrics
Task Achievement
Fluency
Grammar
Vocabulary
Phonology
3
Fig. 2 Bias interaction of rater by rubrics
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 37
their test-taking strategies to handle the speaking topics. Rather, they were tested
on how to memorize some ideas from the different class topics. The candidates’
abilities ranged from 2 to þ5, which displayed a wide range of abilities.
However, it could also be inappropriate to claim that this ability was the result
of a spontaneous overflow of ideas or just a matter of memorization of such
topics. Whether the exam was construct valid necessitated a qualitative analysis
of all the interactions in class. This, however, remains beyond the scope of this
study.
Question 2: Do raters grade speaking rubrics consistently?
The 12 raters came from different educational backgrounds. This difference led
to instances of measurement inconsistencies. Following the section on rater mea-
surement report, it was found that raters (e.g., raters 9 and 10) were more severe
than raters 6 and 4, for instance. That is, these raters were consistently lenient.
However, some measurement inconsistencies (intra-consistency) occurred espe-
cially (inter-inconsistency) where the 12 raters clustered in six different rows,
which indicated a high strata index variance of 5.83, especially in being severe.
In terms of inter-inconsistency, raters were not consistent. However, in terms of
intra-consistency, most of them were consistent in their own degree of being severe
or lenient.
Claiming test reliability would be treated with caution, since the strata (2.48) and
reliability (0.87) indices were not high. Similarly, in investigating the validity of the
exam assessment, the value of the mean fit is 0.50 (SD ¼ 0.12) which was far from
the expected value of 1.00. This meant that the speaking exam was not that valid.
Having problems related to the valid and reliable nature of the test might put the
whole speaking exam at stake. A negative impact of validity and reliability of the
exam might in turn question test usefulness, such as interactiveness, practicality,
impact, washback, validity, reliability and authenticity.
The use of the RASCH measurement (FACETS) in highlighting the measurement
inconsistencies on the one hand and the bias interactions, on the other, has been
investigated in research (Bond & Fox, 2015; McNamara, 1996). Like these studies,
the use of the FACETS helped to diagnose the different issues related to measure-
ment. The most difficult rubric to score was vocabulary with a value of 0.46, while
the easiest one was fluency with a value of 0.63. This might be confusing, mainly
because if such students had tremendous vocabulary problems, then this handicap
would negatively impact fluency. Another possible explanation could be related to
the raters’ misconceptions of the speaking rubrics. It could also be related to the
raters’ lack of experience in grading speaking. The results of Table 5 indicated that
the five scale categories were assigned more (scales 1 and 4) than expected, while
other scales (2, 3 and 5) were used less than expected. This inconsistency in
assigning a specific score denoted that raters probably had difficulties in grading
the spoken performance appropriately.
38 S. Hidri
7 Conclusion
Question 3: Which implications can be deducted from this study to improve the
status of measuring speaking in such an EFL context?
This study had pedagogical implications the first of which was linked to
teachers’ classroom practices which were related to both teaching and testing
speaking. The raters’ perceptions of and behavior with the test indicated that
assessment practices had direct impacts on teaching and vice versa. Such impacts
could be categorized into negative and positive manifestations. Research implica-
tions highlighted the significance of investigating the measurement of speaking as
an important skill to acquire fluency and accuracy and as an important skill whose
teaching should be carried out in relation to the other skills, hence the integration of
the language skills, whether in teaching or testing. The methodological implications
called for the use of the FACETS program as a reliable statistical software to
investigate the different sources of variability that were included in the speaking
exam. However, implementing other qualitative research instruments, such as
questionnaires and interviews, should have been considered in this study.
From the observed speaking tasks administered to the test-takers, it could be
possible to claim that many tasks did not meet the needs of these students, since
they were not as comprehensive as they had to. This was one of the limitations of
the study. For official constraints, all the paired interactions of the speaking exams
were not recorded. This limitation called for a use of a qualitative analysis of all
these interactions that could highlight a better picture of delineating all the major
facets of the speaking language ability of these learners. In addition, most teachers
had a fuzzy idea about the construct, since they all became involved in benchmark
professional development sessions on how to grade speaking. Also, this disparity
in their backgrounds led to different perceptions of grading the speaking
performance.
Similarly, the use of a retrospective interview with the test-takers and teachers
immediately after the speaking exam could have enlightened the researcher in
better investigating the test-takers’ perceptions of the exam and in probing the
teachers’ definition of the speaking construct. In phase B of the exam, students were
tested on the topics they covered in class. This task was meant to test students’
memory of the speaking topics covered in class. Thus, they perceived this part as a
memorization task. This was not as spontaneous as how a speaking task should be
carried on.
None of the teachers used the speaking rubrics (Appendix 3) in class. This might
put their grading at stake. They were just exposed to the rubrics the day of the exam.
In addition, some teachers were not even familiar with the IELTS rubrics and some
of them were asked to be involved as second assessors even though they did not
have any experience in teaching or testing speaking.
As one recommendation tip, a qualitative analysis of the paired interactions
would be more useful for research findings by probing the raters’ behavior and
addressing the different facets that come together to define speaking as a construct.
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 39
Unfortunately, this analysis is beyond the scope of this study. Teachers should be
trained in how to teach speaking in class and there should also be specific guidelines
in how to test speaking in class. These teachers were recruited and embarked on
teaching without involving them in professional development training sessions.
Teachers should be engaged in continuous benchmark sessions in how to standard-
ize grading. Training teachers in how to use the IELTS rubrics, as an international
benchmark, and adapt these rubrics to this EFL context.
At another level, policy makers should be clear about the objectives and out-
comes of this program as well as about what to call it. In fact, the learners who took
part in this study were part of an ESP program that was officially labelled and
nationally recognized as EAP. To consider the basic aspects of an ESP program,
textbook designers should address the cultural constraints deemed sensitive to
students and teachers of this culture. Recommendations for the study could high-
light the following test specs that might have direct implications for classroom as
well as research practices (for specs on other language skills, see Hidri, 2017. The
following test specs should be considered to assess speaking in similar-related
contexts:
Nature and purpose of the exam: final speaking exam delivered when the term ends
for ESP students majoring in IT, MBA and mathematics. This exam is meant to
measure students’ speaking ability after covering a whole term of teaching units.
However, the exam is not about testing their knowledge of the taught topics as
much as it is about measuring their ability to interact with their partners in
communicative situations.
Timing: 10–12 min for all the exam for the individual and paired interactions.
Level: upper intermediate (CEFR level: B2)
Language: English as a Foreign Language. No Arabic use is allowed in the exam.
Number of sections: four. Sections 1, introduction, and 4, closing, are meant to
welcome the test-takers before the start of the test and wish them good luck once
the exam is over. Part 2, which is called task 1, is carried out individually and it
takes 2 min for each student. Part 3, called task 2, lasts 2–3 min where the two
test-takers interact with each other in a paired interaction with two test
interviewers.
Skills to be tested: giving and asking for information, talking about attitudes,
agreeing/disagreeing, socializing, arguing and counter arguing, changing the
topic of a conversation, turn-taking, paraphrasing, summarizing, describing a
picture, talking about a jigsaw story, etc. All such skills and sub-skills should be
related to the students’ field of specialty.
Target language situation: speaking individually and in groups and/or in pairs
about any topics of a general English and/or an ESP genre.
Test method: task one of the exam should be answered individually. Task 2 should
be answered in pairs or in groups. The two interviewers’ role consists in
monitoring the interaction and asking some questions, if necessary.
Task type: authentic interactive speaking tasks that are meant for open debate and
discussion. Topics should respect the cultural boundaries, but they should at the
40 S. Hidri
same time be open, since learning a language requires learning its culture. Tasks
can be about business negotiations, writing business emails, business letters and
correspondences and knowing about work ethics, etc.
Test method: all the speaking topics should be jointly constructed, except for the
tasks where each test-taker has to perform individually. Two test interviewers
have to attend to this interaction and they should interfere only when candidates
dry up. Otherwise, they should help students progress on their performance.
Number of items: for the individual performance, each candidate should be asked
two questions. For the paired interactions, the two candidates should be asked
four questions to answer in an interactive way.
Criteria for correctness: raters should not grade the spoken performance as being
correct or wrong. On the contrary, they should objectively grade the skills and
sub-skills used by the participants to interact on the different tasks and to convey
meaningful messages. Therefore, students should be provided with these rele-
vant strategies and skills to deal with any speaking situation.
The study investigated assessing the spoken performance of 213 learners
majoring in an ESP program. Their performance was graded by 12 raters who
came from different educational backgrounds. The study had interesting, yet
challenging, results that had direct implications for teaching as well as testing.
Perhaps what was relevant during the time of data collection was the necessity to
undergo program evaluation of these ESP learners whose major purpose would be
to reconsider the very specific needs of all the stakeholders. To implement this, a
needs analysis would be established where many stakeholders, such as students,
teachers, textbook designers, parents, administrators, policy-makers and many
other people, should be involved in this enterprise.
Appendix 1 Notes for Oral Assessors
There are two distinct tasks and it is important that each candidate gets a fair chance
at both. Timings and question types should be adhered to strictly.
The mark sheet is designed to permit separate task-by-task marking. This is the
preferred method as the tasks have different focuses. However, this does place a
strain on the assessor who has to make four separate assessments (Candidate A task
1, Candidate B task 1, Candidate A task 2, Candidate B task 2). If assessors find this
unwieldy they may mark holistically, i.e., give marks for the two assessed tasks
overall.
The speaking topics are arranged so that the paired discussion questions are
related to the individual question boxes to the immediate left of them.
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 41
Appendix 2 Paired Format
Interlocutor
Assessor
Phase A Introduction Not assessed
Phase B Task 1 Assessed
Phase C Task 2 Assessed
Phase D Closing Not assessed
Phase A Introduction
Interlocutor
• Good morning/afternoon
• In this interview, we are going to ask each of you some questions relating to the topics you have
studies so far this term. You should direct your answers to your partner. After that, we will
introduce a similar topic for discussion, and we would like you to discuss it together. You are
mainly going to be speaking together and my colleague and I will be listening to you
• Any questions?
The candidates come into the room. The assessors greet and invite them to sit down, smiling and
putting them at their ease
Phase B Task 1 (individual) 2 min each
Interlocutor
• In this part of the speaking test, we would like each of you to answer a question that relates to
one of the topics you have studied so far
• Your question is . . . [from question bank provided—Task 1]
This phase offers the students a chance to talk to their partner to express their opinion on their
own before they discuss together in Phase C
The assessors should make sure marks for this task are recorded on mark sheets
Phase C Task 2 (paired discussion) 3–4 min
Interlocutor
• In this part of the test, we are going to give you a question relating to one of the questions you
have just answered, and we would like you to discuss it together. You are encouraged to interact
with each other by asking questions and responding appropriately. Ok, your question is . . . (from
question bank provided—Task 2)
• Is that clear? Ok you have about 3–4 min for this, so please begin
• Thank you
The assessor should make sure marks for this task are recorded on the mark sheets
Phase D Closing
Interlocutor and Assessor
• That is the end of the test
• Thank you and goodbye
Candidates exit and Interlocutor and Assessor finalize marks using the mark sheets and assess-
ment criteria provided
*Interlocutor has primary responsibility for managing the interaction. Assessor should take no part
in this but concentrate on assessing and completing score sheets. The Interlocutor should be
prepared to offer confirmation/querying of Assessor’s marks
42 S. Hidri
Appendix
3:
Speaking
Test
Assessment
Criteria
Score
Task
Achievement
(Task
1)
Interactive
communication
(Task
2)
Fluency
Grammar
Vocabulary
Phonology
5
Task
accom-
plished
fully
and
effectively
Able
to
initiate
and
respond
with
ease
in
the
interaction
Takes
and
gives
turns
appropriately
Able
to
sustain
flow
of
lan-
guage
necessary
to
accom-
plish
the
tasks
with
occasional
pauses
to
think
Candidate
has
the
range
of
grammar
nec-
essary
to
accomplish
the
tasks
and
is
gener-
ally
accurate
Candidate
has
the
range
of
vocabulary
necessary
to
accomplish
the
tasks
L1
features
do
not
intrude
4
Task
accom-
plished
adequately
Able
to
initiate
and
respond
sufficiently
to
keep
the
interaction
going.
Takes
and
gives
turns
appropriately
Able
to
sustain
flow
of
lan-
guage
to
accomplish
the
tasks
but
with
occasional
pauses
to
think
and
pauses
to
search
for
words
Candidate
lacks
either
the
full
range
neces-
sary
to
accomplish
the
tasks
or
lacks
neces-
sary
accuracy
Candidate
lacks
the
full
range
of
vocabulary
nec-
essary
but
achieves
com-
munication
through
paraphrase
L1
features
present
but
do
not
obstruct
understanding
3
Task
accom-
plished
to
a
limited
degree
Able
to
initiate
and
respond
with
difficulty.
Struggles
to
keep
the
interaction
going.
Takes
and
gives
turns
not
appropriately
Pauses
to
search
for
words
are
frequent
and
the
flow
of
language
necessary
to
accomplish
the
tasks
is
not
always
sustained
Candidate
lacks
both
the
range
and
accuracy
necessary
to
accom-
plish
the
tasks
Candidate
lacks
the
range
of
vocabulary
necessary
to
accomplish
the
tasks
and
has
limited
ability
to
paraphrase
L1
features
present
and
occasionally
obstruct
understanding
2
Task
attempted
but
not
accomplished
Able
to
respond
but
not
initiate.
Relies
on
partner
to
keep
the
interaction
going
Speech
seems
a
little
dis-
connected
and
sometimes
difficult
to
follow
Candidate
has
range
and
accuracy
sufficient
to
attempt
but
not
to
accomplish
the
tasks
Candidate
has
sufficient
range
to
accomplish
the
tasks
and
cannot
paraphrase
L1
features
present
and
often
obstruct
understanding
1
Task
not
attempted
Limited
responses
and
no
initiations
Speech
is
disconnected
and
difficult
to
follow
Range
and
accuracy
inadequate
for
the
test
Range
wholly
inadequate
for
the
test
L1
features
constantly
obstruct
understanding
0
No
adequate
sample
of
language
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 43
Appendix 4
Speaking test score sheet: English 1111/ 1222
Student name
Student ID
Criterion
Task achievement/
Interactive
communication Fluency Grammar Vocabulary Phonology Total
Score
Task 1
Score
Task 2
• Enter a mark 0–5 for each of the criteria
• Enter grand total in box to the right
44 S. Hidri
Appendix 5
Bias interaction rater by rubrics
---------------------------------------------------------------------------------------------------------------------
| Obsvd Exp. Obsvd Obs-Exp| Bias Model |Infit Outfit| |
| Score Score Count Average| Size S.E. t | MnSq MnSq | Sq Nu Rater measr N Rubrics measr |
---------------------------------------------------------------------------------------------------------------------
| 51 66.7 37 -.42| -1.22 .31 -3.88 | 1.7 1.5 | 46 10 rater 10 1.53 4 Vocabulary .46 |
| 52 67.4 37 -.42| -1.17 .31 -3.82 | 1.2 1.1 | 45 9 rater 9 1.48 4 Vocabulary .46 |
| 60 69.5 37 -.26| -.65 .27 -2.38 | 1.1 1.0 | 58 10 rater 10 1.53 5 Phonology .28 |
| 126 132.7 30 -.22| -.62 .29 -2.14 | 1.3 1.2 | 13 1 rater 1 -.23 2 Fluency -.63 |
| 129 135.0 30 -.20| -.61 .30 -2.00 | 1.0 1.0 | 16 4 rater 4 -.48 2 Fluency -.63 |
| 74 82.3 37 -.23| -.49 .25 -1.98 | .0 .0 | 9 9 rater 9 1.48 1 Task Achievement -.44 |
| 180 190.2 48 -.21| -.46 .21 -2.20 | 1.3 1.3 | 14 2 rater 2 -.33 2 Fluency -.63 |
| 177 183.0 41 -.15| -.41 .25 -1.62 | 1.0 1.0 | 11 11 rater 11 -.40 1 Task Achievement -.44 |
| 64 70.2 37 -.17| -.41 .26 -1.57 | 1.4 1.3 | 57 9 rater 9 1.48 5 Phonology .28 |
| 104 108.5 26 -.17| -.41 .29 -1.39 | 1.5 1.3 | 19 7 rater 7 -.31 2 Fluency -.63 |
| 173 182.3 48 -.19| -.40 .21 -1.95 | 1.1 1.1 | 15 3 rater 3 .03 2 Fluency -.63 |
| 181 186.2 41 -.13| -.39 .27 -1.48 | .8 .8 | 24 12 rater 12 -.46 2 Fluency -.63 |
| 75 81.5 37 -.18| -.38 .25 -1.55 | .1 .1 | 10 10 rater 10 1.53 1 Task Achievement -.44 |
| 181 185.4 41 -.11| -.33 .27 -1.23 | .9 .8 | 23 11 rater 11 -.40 2 Fluency -.63 |
| 92 96.2 26 -.16| -.33 .28 -1.18 | .7 .7 | 32 8 rater 8 -.22 3 Grammar .34 |
| 112 116.5 30 -.15| -.31 .26 -1.18 | .9 .9 | 30 6 rater 6 -.59 3 Grammar .34 |
| 94 97.3 26 -.13| -.26 .28 -.92 | .7 .7 | 31 7 rater 7 -.31 3 Grammar .34 |
| 126 128.9 30 -.10| -.25 .29 -.86 | .8 .7 | 18 6 rater 6 -.59 2 Fluency -.63 |
| 156 159.4 48 -.07| -.14 .20 -.69 | .9 .9 | 27 3 rater 3 .03 3 Grammar .34 |
| 120 121.8 30 -.06| -.13 .27 -.50 | 1.2 1.1 | 17 5 rater 5 -.02 2 Fluency -.63 |
| 106 108.0 30 -.07| -.13 .26 -.51 | .4 .4 | 29 5 rater 5 -.02 3 Grammar .34 |
| 107 108.8 30 -.06| -.12 .26 -.47 | .6 .6 | 53 5 rater 5 -.02 5 Phonology .28 |
| 171 173.0 41 -.05| -.12 .24 -.49 | 1.0 .9 | 60 12 rater 12 -.46 5 Phonology .28 |
| 183 183.8 41 -.02| -.06 .27 -.22 | 1.4 1.4 | 12 12 rater 12 -.46 1 Task Achievement -.44 |
| 107 107.6 26 -.02| -.06 .30 -.19 | .9 .8 | 20 8 rater 8 -.22 2 Fluency -.63 |
| 170 171.0 41 -.02| -.05 .24 -.23 | .9 .9 | 35 11 rater 11 -.40 3 Grammar .34 |
| 167 168.1 48 -.02| -.05 .20 -.22 | 1.0 1.0 | 26 2 rater 2 -.33 3 Grammar .34 |
| 125 125.5 30 -.02| -.04 .29 -.15 | .5 .6 | 52 4 rater 4 -.48 5 Phonology .28 |
| 123 123.3 30 -.01| -.03 .28 -.09 | 1.0 1.0 | 40 4 rater 4 -.48 4 Vocabulary .46 |
| 98 98.0 26 .00| .00 .28 .01 | .6 .6 | 55 7 rater 7 -.31 5 Phonology .28 |
| 97 96.9 26 .00| .01 .28 .02 | 1.4 1.3 | 56 8 rater 8 -.22 5 Phonology .28 |
| 122 121.8 30 .01| .02 .28 .06 | 1.2 1.2 | 25 1 rater 1 -.23 3 Grammar .34 |
| 127 126.7 30 .01| .02 .29 .08 | 1.4 1.3 | 6 6 rater 6 -.59 1 Task Achievement -.44 |
| 106 105.6 26 .02| .04 .30 .12 | 1.1 1.0 | 8 8 rater 8 -.22 1 Task Achievement -.44 |
| 123 122.5 30 .02| .04 .28 .14 | 1.3 1.1 | 49 1 rater 1 -.23 5 Phonology .28 |
| 120 119.3 30 .02| .05 .27 .19 | 1.0 .9 | 5 5 rater 5 -.02 1 Task Achievement -.44 |
| 170 168.8 41 .03| .07 .24 .29 | .7 .7 | 47 11 rater 11 -.40 4 Vocabulary .46 |
| 163 160.8 48 .05| .09 .20 .44 | .9 .9 | 51 3 rater 3 .03 5 Phonology .28 |
| 97 95.7 26 .05| .10 .28 .36 | 1.4 1.3 | 43 7 rater 7 -.31 4 Vocabulary .46 |
| 173 169.5 48 .07| .15 .21 .72 | .7 .8 | 50 2 rater 2 -.33 5 Phonology .28 |
| 169 165.1 48 .08| .16 .20 .79 | 1.0 1.0 | 38 2 rater 2 -.33 4 Vocabulary .46 |
| 190 186.1 48 .08| .18 .22 .83 | 1.8 1.7 | 2 2 rater 2 -.33 1 Task Achievement -.44 |
| 135 133.3 30 .06| .19 .34 .56 | 1.5 1.6 | 4 4 rater 4 -.48 1 Task Achievement -.44 |
| 183 178.0 48 .10| .22 .21 1.03 | 1.4 1.4 | 3 3 rater 3 .03 1 Task Achievement -.44 |
| 123 120.2 30 .09| .22 .28 .78 | 1.2 1.2 | 37 1 rater 1 -.23 4 Vocabulary .46 |
| 162 156.4 48 .12| .23 .20 1.13 | 1.1 1.1 | 39 3 rater 3 .03 4 Vocabulary .46 |
| 118 114.7 30 .11| .23 .27 .87 | 1.2 1.2 | 42 6 rater 6 -.59 4 Vocabulary .46 |
| 174 169.9 41 .10| .24 .25 .97 | .8 .8 | 48 12 rater 12 -.46 4 Vocabulary .46 |
| 176 172.1 41 .10| .24 .25 .96 | .8 .8 | 36 12 rater 12 -.46 3 Grammar .34 |
| 121 117.3 30 .12| .27 .27 1.00 | .6 .7 | 54 6 rater 6 -.59 5 Phonology .28 |
| 74 69.3 37 .13| .29 .25 1.18 | .0 .0 | 33 9 rater 9 1.48 3 Grammar .34 |
| 134 130.8 30 .11| .32 .33 .98 | 1.6 1.3 | 1 1 rater 1 -.23 1 Task Achievement -.44 |
| 111 106.1 30 .16| .32 .26 1.25 | .6 .6 | 41 5 rater 5 -.02 4 Vocabulary .46 |
| 74 68.6 37 .15| .34 .25 1.37 | .0 .0 | 34 10 rater 10 1.53 3 Grammar .34 |
| 99 94.6 26 .17| .35 .28 1.22 | .9 .9 | 44 8 rater 8 -.22 4 Vocabulary .46 |
| 130 124.8 30 .17| .45 .31 1.48 | .7 .6 | 28 4 rater 4 -.48 3 Grammar .34 |
| 113 106.5 26 .25| .63 .33 1.94 | 1.4 1.4 | 7 7 rater 7 -.31 1 Task Achievement -.44 |
| 182 171.9 41 .25| .65 .27 2.42 | 1.0 .9 | 59 11 rater 11 -.40 5 Phonology .28 |
| 111 85.7 37 .69| 1.32 .22 5.91 | .0 .0 | 21 9 rater 9 1.48 2 Fluency -.63 |
| 111 84.8 37 .71| 1.37 .22 6.13 | .0 .0 | 22 10 rater 10 1.53 2 Fluency -.63 |
---------------------------------------------------------------------------------------------------------------------
| Obsvd Exp. Obsvd Obs-Exp| Bias Model |Infit Outfit| |
| Score Score Count Average| Size S.E. t | MnSq MnSq | Sq Nu Rater measr N Rubrics measr |
---------------------------------------------------------------------------------------------------------------------
| 127.9 127.9 35.3 .00| -.02 .26 -.03 | .9 .9 | Mean (Count: 60) |
| 37.9 37.2 7.5 .19| .44 .03 1.71 | .4 .4 | S.D. (Populn) |
| 38.2 37.5 7.6 .19| .44 .03 1.73 | .4 .4 | S.D. (Sample) |
---------------------------------------------------------------------------------------------------------------------
Fixed (all = 0) chi-square: 176.4 d.f.: 60 significance (probability): .00
---------------------------------------------------------------------------------------------------------------------
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 45
References
Alderson, J. C., & Wall, D. (1993). Does washback exist? Applied Linguistics, 14(2), 115–129.
Black, P., & Wiliam, D. (1998). Assessment and classroom learning. Assessment in Education, 5
(1), 7–74. https://doi.org/10.1080/0969595980050102.
Blanche, P. (1990). Using standardized achievement and oral proficiency tests for self-assessment
purposes: the DLIFLC study. Language Testing, 7(2), 202–229.
Bond, T. G., & Fox, C. M. (2015). Applying the Rasch Model: Fundamental measurement in the
human sciences (3rd ed.). New York: Routledge.
Bonk, W. J., & Ockey, G. (2003). A many-facet Rasch analysis of the second language group oral
discussion task. Language Testing, 20(1), 89–110. https://doi.org/10.1177/
026553229901600105.
Brown, A. (1993). The role of test-taker feedback in the test development process: Test-takers’
reactions to a tape-mediated test of proficiency in spoken Japanese. Language Testing, 10,
277–301.
Brown, A. (2003). Interviewer variation and the co-construction of speaking proficiency. Lan-
guage Testing, 20(1), 1–25. https://doi.org/10.1191/0265532203lt242oa.
Brown, H. D. (2004). Language assessment: Principle and classroom practices. White Plains, NY:
Longman.
Chalhoub-Deville, M. (1995). A contextualized approach to describing oral language proficiency.
Language Learning: A Journal of Research in Language Studies, 45(2), 251–281. https://doi.
org/10.1111/j.1467-1770.1995.tb00440.x.
Chapelle, C. A. (1999). Validity in language assessment. Annual Review of Applied Linguistics,
19, 254–272. https://doi.org/10.1002/9781405198431.wbeal0126.
Chen, Z., & Henning, G. (1985). Linguistic and cultural bias on language proficiency tests.
Language Testing, 2, 155–163.
Clark, J. L. D. (1988). Validation of a tape-mediated ACTFL/ILR-scale based test of Chinese
speaking proficiency. Language Testing, 5(2), 187–205.
Douglas, D. (1994). Quantity and quality in speaking test performance. Language Testing, 11,
125–144.
Ducasse, A. M., & Brown, A. (2009). Assessing paired orals: Raters’ orientation to interaction.
Language Testing, 26(3), 423–443. https://doi.org/10.1177/0265532209104669.
Elder, C., Iwashita, N., & McNamara, T. (2001). Estimating the difficulty of oral proficiency tasks:
What does the teat-taker have to offer. Language Testing, 19(4), 347–368. https://doi.org/10.
1191/0265532202lt235oa.
Fulcher, G. (1996a). Testing tasks: Issues in task design and the group oral. Language Testing, 13,
23–51.
Fulcher, G. (1996b). Does thick description lead to smart tests? A data-based approach to rating
scale construction. Language Testing, 13, 208–238.
Ginther, A., Dimova, S., & Yang, R. (2010). Conceptual and empirical relationships between
temporal measures of fluency and oral English proficiency with implications for automated
scoring. Language Testing, 27(3), 379–399. https://doi.org/10.1177/0265532210364407.
Grant, L. (1997). Testing the language proficiency of bilingual teachers: Arizona’s Spanish
proficiency test. Language Testing, 14(1), 23–46. https://doi.org/10.1177/
026553229701400103.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hidri, S. (2017). Specs validation of a dynamic reading comprehension test for EAP learners in an
EFL context. In S. Hidri & C. Coombe (Eds.), Evaluation in foreign language education (pp.
315–337). Cham: Springer.
Hultsijn, J. H., Schoonen, R., de Jong, N. H., Steinel, M. P., & Florijn, A. (2011). Linguistic
competences of learners of Dutch as a second language at the B1 and B2 levels of speaking
proficiency of the Common European Framework of Reference for Languages (CEFR).
Language Testing, 29(2), 203–221. https://doi.org/10.1177/0265532211419826.
46 S. Hidri
Jin, Y. (2000). The washback of CET-SET on teaching. Foreign Languages World, 4, 57–62.
Jin, T., & Mak, B. (2012). Distinguishing features in scoring L2 Chinese speaking performance:
How do they work? Language Testing, 30(1), 23–47. https://doi.org/10.1177/
0265532212442637.
Kormos, J. (1999). Simulating conversations in oral- proficiency assessment: A conversation
analysis of role plays and non-scripted interviews in language exams. Language Testing, 16
(2), 163–188. https://doi.org/10.1177/026553229901600203.
Lazaraton, A. (1996). Interlocutor support in oral proficiency interviews: The case of CASE.
Language Testing, 13, 151–172. https://doi.org/10.1177/026553229601300202.
Leaper, D. A., & Riazi, M. (2014). The influence of prompt on group oral tests. Language Testing,
31(2), 177–204. https://doi.org/10.1177/0265532213498237.
Linacre, J. M. (2011). A user’s guide to FACETS Rasch Model computer program. Available
online at www.winsteps.com
Ling, G., Mollaun, P., & Xi, X. (2014). A study on the impact of fatigue on human raters when
scoring speaking responses. Language Testing, 31(4), 479–499. https://doi.org/10.1177/
0265532214530699.
Lumley, T., & Brown, A. (1997). Interviewer variability in specific-purpose language performance
tests. In V. Kohonen, A. Huhta, L. Kurki-Suonio, & S. Luoma (Eds.), Current developments
and alternatives in language assessment: Proceedings of LTRC 96 (pp. 137–150). Jyvaskyla:
University of Jyvaskyla and University of Tampere.
Lynch, B. K., & McNamara, T. F. (1998). Using G-theory and many-facet Rasch measurement in
the development of performance assessments of the ESL speaking skills of immigrants.
Language Testing, 15(2), 158–180. https://doi.org/10.1177/026553229801500202.
Malvern, D., & Richards, B. (2002). Investigating accommodation in language proficiency
interviews using a new measure of lexical diversity. Language Testing, 19(1), 85–104.
https://doi.org/10.1191/0265532202lt221oa.
McNamara, T. (1996). Measuring second language performance. London: Addison Wesley
Longman.
McNamara, T. F. (1997). ‘Interaction’ in second language performance assessment: Whose
performance? Applied Linguistics, 18, 446–465. https://doi.org/10.1111/j.1467-9922.2009.
00496.x.
McNamara, T. (2001). Language assessment as social practice: Challenges for research. Language
Testing, 18(4), 333–349. https://doi.org/10.1177/026553220101800402.
McNamara, T. F., & Lumley, T. (1997). The effect of interlocutor and assessment mode variables
in overseas assessments of speaking skills in occupational settings. Language Testing, 14(2),
140–156. https://doi.org/10.1177/026553229701400202.
Messick, S. (1989). Meaning and values in test validation: The science and ethics of assessment.
Educational Researcher, 18(2), 5–11.
Myford, C. M., & Wolf, E. W. (2000). Strengthening the ties that bind: Improving the linking
network in sparsely connected rating designs (TOEFL Technical Report, N 15). Princeton, NJ:
Educational Testing Service.
Nitta, R., & Nakatsuhara, F. (2014). A multifaceted approach to investigating pre-task planning
effects on paired oral test performance. Language Testing, 31(2), 147–175. https://doi.org/10.
1177/0265532213514401.
O’Loughlin, K. (1995). Lexical density in candidate output on direct and semi-direct versions of an
oral proficiency test. Language Testing, 12, 217–237.
O’Loughlin, K. (2002). The impact of gender in oral proficiency testing. Language Testing, 19(2),
169–192. https://doi.org/10.1191/0265532202lt226oa.
O’Sullivan, B., Weir, C. J., & Saville, N. (2002). Using observation checklists to validate
speaking-test tasks. Language Testing, 19(1), 33–56. https://doi.org/10.1191/
0265532202lt219oa.
Ockey, G. J., Koyama, D., Setoguchi, E., & Sun, A. (2015). The extent to which TOEFL iBT
speaking scores are associated with performance on oral language tasks and oral ability
components for Japanese university students. Language Testing, 32(1), 39–62. https://doi.
org/10.1177/0265532214538014.
Assessing Spoken Language Ability: A Many-Facet Rasch Analysis 47
Pinget, A., Bosker, H. R., Quené, H., & de Jong, N. (2014). Native speakers’ perceptions of fluency
and accent in L2 speech. Language Testing, 31(3), 349–365. https://doi.org/10.1177/
0265532214526177.
Powers, D. E., Schedl, M. A., & Leung, W. (1999). Validating the revised Test of Spoken English
against a criterion of communicative success. Language Testing, 16(4), 399–425. https://doi.
org/10.1191/026553299673108653.
Ross, S. (1992). Accommodative questions in oral proficiency interviews. Language Testing, 9,
173–185.
Salaberry, R. (2000). Revising the revised format of the ACTFL oral proficiency interview.
Language Testing, 17(3), 289–310. https://doi.org/10.1177/026553220001700301.
Sato, T. (2011). The contribution of test-takers’ speech content to scores on an English oral
proficiency test. Language Testing, 29(2), 223–241. https://doi.org/10.1177/
0265532211421162.
Scott, M. L. (1986). Student affective reactions to oral language tests. Language Testing, 3(1),
99–118.
Shin, S.-K. (2005). Did they take the same test? Examinee language proficiency and the structure
of language tests. Language Testing, 22(1), 31–57. https://doi.org/10.1191/
0265532205lt296oa.
Shohamy, E. (1982). Affective consideration in language testing. Modern Language Journal, 66,
13–17.
Shohamy, E. (1994). The validity of direct versus semi-direct oral tests. Language Testing, 11,
99–123.
Shohamy, E., & Reves, T. (1985). Authentic language tests: Where from and where to? Language
Testing, 2, 48–59.
Shohamy, E., Reves, T., & Bejarano, T. (1986). Introducing a new comprehensive test of oral
proficiency. ELT Journal, 40, 212–220.
Spolsky, B. (1990). Oral examinations: An historical note. Language Testing, 7(2), 158–173.
https://doi.org/10.1177/026553229000700203.
Swain, M. (1993). Second language testing and second language acquisition: Is there a conflict
with traditional psychometrics? Language Testing, 10, 193–207.
Swain, M. (2001). Examining dialogue: Another approach to content specification and to validat-
ing inferences drawn from test scores. Language Testing, 18(3), 275–302.
Swain, M., Brooks, L., & Tocalli-Beller, A. (2002). Peer-peer dialogue as a means of second
language learning. Annual Review of Applied Linguistics, 22, 171–185. https://doi.org/10.
1017/S0267190502000090.
Upshur, J. A., & Turner, C. E. (1999). Systematic effects in the rating of second language speaking
ability: Test method and learner discourse. Language Testing, 16, 82–111.
Van Moere, A. (2006). Validity evidence in a university group oral test. Language Testing, 23(4),
411–440. https://doi.org/10.1191/0265532206lt336oa.
Van Moere, A. (2012). A psycholinguistic approach to oral language proficiency. Language
Testing, 29(3), 325–344. https://doi.org/10.1177/0265532211424478.
Wesche, M. B. (1987). Second language performance testing: The Ontario Test of ESL as an
example. Language Testing, 4, 28–47.
Wigglesworth, G. (1997). An investigation of planning time and proficiency level on oral test
discourse. Language Testing, 14(1), 85–106. https://doi.org/10.1177/026553229701400105.
Yan, X. (2014). An examination of rater performance on a local oral English proficiency test: A
mixed-methods approach. Language Testing, 31(4), 501–527. https://doi.org/10.1177/
0265532214536171.
Zeidner, M., & Bensoussan, M. (1988). College students’ attitudes towards written versus oral
tests of English as a Foreign Language. Language Testing, 5, 100–114.
Zhao, Z. (2013). Diagnosing the English-speaking ability of college students in China–Validation
of the Diagnostic College English Speaking Test. RELC Journal, 44(3), 341–359. https://doi.
org/10.1177/0033688213500581.
48 S. Hidri
Investigating the Use of an Empirically
Derived, Binary-Choice and Boundary-
Definition (EBB) Scale for the Assessment
of English Language Spoken Proficiency
Stefan O’Grady
Abstract The drive towards English medium instruction in modern Turkish higher
education has increased the need for valid assessment methodology. However, with
ever increasing numbers of test-takers and a limited period of time to carry out
language assessment, collecting information about prospective students’ language
proficiency can often put a strain on limited resources. The assessment of speaking
ability is notoriously difficult and many universities in Turkey do not have the
capacity to complete large-scale speaking assessment that is both fair and useful. A
common problem with the assessment of spoken ability in this educational context
is disagreement between raters with regards to the assessment of spoken profi-
ciency. The current study maps out the procedure for developing an empirically
derived rating scale that requires binary choices to distinguish the boundaries
between score levels “Empirically derived, Binary-choice, Boundary-definition”
(EBB) (Turner & Upshur, 1996) for a test of second language speaking ability.
Two groups of trained raters used the EBB scale and an analytical scale to assess
speaking samples from 30 English language learners studying in the English
preparatory program of a Turkish university. Analysis of the test results was
conducted using multifaceted Rasch analysis. Results demonstrated that the use
of the EBB scale generated higher levels of inter-rater and intra-rater consistency
than the analytical scale. The findings indicate that the use of EBB rating scales may
be a viable alternative to traditional rating scales for large-scale testing programs in
English medium universities.
Keywords Second language speech assessment • Rating scales • Rasch analysis
S. O’Grady (*)
Izmir University of Economics, Izmir, Turkey
e-mail: stefan.ogrady@ieu.edu.tr
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_3
49
1 Introduction
The purpose of this investigation is to develop a method to increase rater agreement
and rater consistency in the assessment of spoken ability for a university entrance
exam in Turkey. English medium tertiary education is increasingly common in
Turkey (Selvi, 2014) and educational institutions require reliable information about
prospective students’ English proficiency. The ability to communicate is primary in
academic contexts (Read, 2015) and evidence needs to be presented that test-takers
have adequate spoken ability to cope in the “target language use domain” (Purpura,
2016, p. 193) of English medium undergraduate studies. In second language
assessment, a test-taker’s spoken ability is typically assessed by trained raters
using a rating scale. The raters, and their interpretation of the rating scale content,
therefore play an important role in the outcome of the assessment. Variation in rater
severity can produce some very serious outcomes for the test-taking population and
potentially restrict their access to educational opportunities.
2 Theoretical Background
2.1 Holistic and Analytical Rating Scales
The rating scale plays an important role in second language speaking assessment
because scale content is closely related to the test construct; it is an
operationalization of what the test is designed to measure (Fulcher, 2003). In the
language testing literature, rating scales are generally categorised as holistic or
analytical (Weir, 2005). Holistic scales comprise broad summaries of language use
at different levels of ability and require raters to make impressionistic judgements
about language proficiency. A benefit of holistic scales is simplicity of use (Luoma,
2004), however researchers have argued that this simplicity comes at the cost of
validity (Fulcher & Davidson, 2007; Turner & Upshur, 2002). Specifically, group-
ing separate components of spoken ability (control over the grammar, pronuncia-
tion, lexical depth and variety, fluency, syntactical complexity) into one broad level
of proficiency misleadingly induces the rater to anticipate equality between these
elements in the test-taker’s performance. Owing to recent information processing
accounts of second language production (Skehan, 2009) and research into the
limitations of working memory capacity (Baddeley, 2007) the likelihood of this
equality manifesting in test performance is doubtful. Limitations in working mem-
ory capacity mean that a test-taker may be unable to allocate equal levels of
attention to every aspect of their speech production. According to research
conducted within task-based learning, a trade-off between the key areas of language
accuracy, complexity and fluency is commonly observed in language learners’
speech during task performance (Skehan, 2009). The nature of the trade-off is
influenced by task demands and is thus likely to vary between different test tasks
50 S. O’Grady
and test-takers. For example, enhanced concentration on the generation and
retrieval of lexical forms draws limited attentional resources away from speech
fluency. In contrast, efforts to appear fluent may constrain the test-taker into using
well-rehearsed vocabulary and have negative impacts on levels of lexical diversity.
In short, efforts to improve one area of the speech may have detrimental effects on
another. From a cognitive processing perspective of second language speech
production (see Field, 2011), the use of holistic scales is unsupported.
Analytical scales divide the various components of test-taker speech into a series
of subscales that describe each component at different levels of proficiency. Using
an analytical scale, raters assign separate grades for components of the spoken
performance such as pronunciation, grammatical control and lexis. The level of
detail that the analytical approach affords (e.g., for purposes of diagnostic testing)
makes this approach favourable to holistic scales. In addition, any trade-off effects
at work can be accounted for by the rater who may assign separate grades to each
component of the spoken performance identified by the scale. However, adopting a
cognitive approach to the use of analytical scales reveals several weaknesses in the
method. As test-takers are constrained by limitations in working memory capacity
so are raters. Raters can therefore only be expected to attend to a minimal number of
components of the scale at one time (Luoma, 2004). Concentration on one aspect of
the analytical scale may cause the rater to neglect the remaining categories. Weir
(2005, pp. 188–189) posits a “halo effect” that may be present in the use of
analytical scales, in which the rater’s assessment of language proficiency is heavily
influenced by one aspect of the speech: “If a major preoccupation of a marker is
with grammar, and the candidate exhibits a poor performance on this criterion, are
the marks awarded in respect of other criteria contaminated by the grammar mark?”
From the cognitive perspective, limitations in working memory capacity indicate
that contamination across the separate criteria of an analytical scale is a likely risk.
2.2 Rating Scale Development
Both analytical and holistic rating scales comprise a range of descriptions of
language use that are designed to guide raters’ assessment of language proficiency.
The scale development process generally involves a team of experts that are
familiar with the test-taking population and the target language use domain. The
basis of scale content is commonly based on theory, teaching experience, institu-
tional objectives or existing scales (Luoma, 2004). Fulcher (2003) refers to this
method as the intuitive approach. Researchers have identified various flaws with
this method: Criteria are grouped into one band with no evidence that these features
co-occur in the speech, which is common of holistic scales (Turner & Upshur,
2002). In addition, intuitive scales may fail to characterize test-taker language
(Fulcher & Davidson, 2007) and presuppose a linear path of language acquisition,
which is not supported by SLA research (Fulcher, 2003). It has also been argued
that raters may pay little attention to the scale content and grade according to their
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 51
own internal standards (Fulcher, 2003). The literature indicates that intuitive
approaches to scale development do not provide the foundations for valid and
reliable assessment.
Fulcher (2003) recommends that rating scales should be based on some form of
empirical evaluation of what the test-takers actually do during the test. That is,
scales should be empirically derived. One method of empirical scale creation that
has received limited attention in the literature is Turner and Upshur’s (1996) EBB
method. Turner and Upshur (1996, pp. 60–61) write “the scale is empirically
derived, requires binary choices by raters, and defines the boundaries between
score levels (EBB)”. The EBB method requires a group of raters who are familiar
with the test procedures and the test-taking population to evaluate a series of speech
samples that are representative of the population. The raters identify the features of
speech that they regard as salient to their decisions about language proficiency and
design the rating scale around these criteria. In the EBB scale, rating criteria are
presented as a series of binary choices that require the rater to consider whether a
specific feature that was identified during the scale development process is present
in a test sample. The presence or absence of this feature leads the rater to a second
binary choice regarding another feature of the spoken performance. This process is
repeated until the rater arrives at a score for the test sample. The limitations of the
holistic and analytical approaches to scale use do not impact the rater when using
the EBB method as the scale does not assume monotonicity in the various aspects of
performance and the rater is not required to consider various criteria simulta-
neously. Furthermore, as the rating criteria are based on the raters’ own analysis
of the speech samples, the EBB scale should facilitate high levels of agreement
between the raters regarding the proficiency of the test-taking population.
2.3 Research Problem
Rater disagreement and rater inconsistency could potentially play a major role in
the outcome of speech assessment for university entrance purposes in Turkey. This
may be due to variation in the interpretation of the rating scale. Rater disagreement
represents a significant threat to the validity of the test and any decisions based on
the test outcome. The situation is unfair to prospective students who (a) may be
denied access to educational opportunities because they were assessed by a partic-
ularly severe rater or (b) be granted access to an educational environment in which
they do not have the language proficiency to succeed because they were assessed by
a particularly lenient rater. The study attempted to answer the following research
questions:
1. Does an EBB scale increase levels of inter-rater reliability in relation to an
analytical scale?
2. Does an EBB scale increase levels of intra-rater reliability in relation to an
analytical scale?
52 S. O’Grady
3 Method
3.1 Tasks
Test-takers completed two speaking tasks as part of a mock examination in which
they were able to take a version of the university entrance English exam. The tasks
had featured in previous administrations of the entrance examination but were
subsequently retired to be used for mock examination purposes. The tasks were
completed on a one-on-one basis with the researcher conducting the interview. The
tasks were:
A. Tell me about something interesting you have recently heard in the news.
B. Tell me about an experience that has changed your life.
Speech was recorded using the audacity program (2.0.6, 29 September 2014,
http://audacity.sourceforge.net) and saved in MP3 format.
3.2 Participants
3.2.1 Test-Takers
Thirty test-takers were involved in the study. Test-takers were studying in the
English foundation program of a university in Turkey in preparation for the
forthcoming university entrance English examination. They were recruited on a
voluntary basis and signed a consent form informing them of the purpose of the
study. Although they had been studying in an English only environment for
9 months at the time of the study, their capacity to express themselves orally in
the second language was limited. Many test-takers expressed extreme discomfort
when producing speech in English. Large class sizes and the artificiality of speaking
in a second language with classmates that all share the first language were com-
monly mentioned by the test-takers as impediments to the development of their
spoken proficiency.
3.2.2 Raters
Fourteen raters were involved in the study. All raters worked as instructors of
English in the university and regularly took part in institutional language assess-
ment of spoken ability. They had no experience of developing language tests for the
university or professional testing organizations but one rater had experience as an
IELTS examiner. The raters were equally divided between the rating scales so that
seven raters completed the rating with the EBB scale and seven with the analytical
scale.
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 53
3.3 EBB Scale
The scale development process followed guidelines in Turner and Upshur (1996).
Ten samples of speech were first holistically selected by the researcher as repre-
sentative of the range of ability levels in the test-taking population. The group of
raters listened to the ten samples and were asked to take notes on each test-takers’
language ability. The group was instructed to be as specific as possible in their
description of the speech. The raters were then requested to compare notes and
discuss the samples. The samples were rank ordered through a series of paired
comparisons. Six levels of proficiency were identified during this stage (the raters
could not agree upon levels for four of the samples). Raters were then requested to
identify in general terms a characteristic that indicated higher proficiency in the top
three samples. This was then formulated as a yes or no question. This question
constituted the boundary between the top three levels and the bottom three levels on
the scale. Following this, a series of paired comparisons was run on the ranked
samples in order to formulate further yes/no questions which would identify
characteristics of advanced proficiency and serve as boundaries between the levels
of ability. In total, five binary questions were developed to identify six levels of
proficiency on the scale (see Fig. 1. EBB Scale).
3.4 Analytical Scale
The analytical scale was produced by Iwashita, McNamara, and Elder (2001). The
scale describes five levels of ability for speech complexity, accuracy and fluency
Does the student provide a
meaningful answer to the
question?
Does insufficient knowledge
of grammar and vocabulary
impede fluency resulting in
hesitations?
Does the student demonstrate
the ability to be creative with
the language while
maintaining accuracy?
Does the student use simple
structures correctly without
excessive hesitation? Is there a discernible
message?
6
5
4
3
2
1
yes
yes
yes
yes
yes
no
no
no
no
no
Fig. 1 EBB Scale
54 S. O’Grady
(see Appendix). The seven raters took part in a standardization session in which a
series of spoken samples were discussed and assessed according to the analytical
criteria. While the standardization session did provide the raters with the opportu-
nity to discuss the scale content, time constraints and timetable clashes meant that
standardisation was completed within a limited timeframe. A more thorough
standardisation session involving more samples would have been desirable.
3.5 Rating Process
To collect sufficient data to run comparisons between the raters, each test sample
was rated twice. The distribution of the samples involved multiple matches between
raters so that comparisons of rater severity could be made in the analysis. An
evaluation of the most efficient way to match the raters showed that every rater
needed to grade 20 samples. The EBB grades were analysed using the FACETS
program (3.71.4, 18 January 2014, www.winsteps.com) with test-taker ability and
rater severity as the two facets under analysis. The analysis of the analytical scale
was also conducted using the FACETS program but included rating scale category
(complexity, accuracy, fluency) as a third facet.
4 Results and Discussion
The results of the EBB scale and analytical scale are first presented in the form of
Wright maps (see Figs. 2 and 3). In the maps, the first column is the measurement
scale, which renders differences between facets as logits. The logit scale is an
interval scale that ranks the test-takers and raters to demonstrate the degree of
variation within the levels of ability in the test-taking population and severity in the
group of raters (McNamara, 1996). The second column reports the results of the
test-taker analysis. Each asterisk represents an individual test-taker and higher
scoring test-takers are situated toward the top of the map. Low scoring test-takers
are placed toward the bottom end of the scale. The third column reports the analysis
of rater severity. The raters are represented by numbers, which are presented in
order of relative severity with lenient raters situated toward the bottom of the map
and stricter raters at the top. In the analytical map, the fourth column represents the
separate categories of the analytical scale. The categories are ranked in terms of
difficulty. The higher a category appears in the map the more difficult it was to
achieve a high score for this category. Fair average values for each category were
fluency (2.42), accuracy (2.25) and complexity (1.91), indicating that complexity
was the most difficult category on the scale. The final column represents the levels
on the original scales.
Comparison of the maps indicates some key differences between the scales.
There is greater variation in the distribution of the test-takers in the analytical scale
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 55
than the EBB scale. This indicates that the analytical scale separated test-takers into
more levels of ability than the EBB scale. This is confirmed in the strata statistics.
The strata statistics report the number of statistically different levels of test perfor-
mance within a given test-taking population (Linacre, 2013). The strata value can
be understood as a measure of the spread of test-taker ability that accounts for the
degree of imprecision in the model (see Tables 1 and 2: Error). The EBB strata
value is 2.72 whereas the analytical scale strata value is 3.94 strata. Test-takers
were therefore broadly separated into three levels of ability in the EBB scale and
four levels of ability in the analytical scale.
Fig. 2 FACETS variables map EBB scale
56 S. O’Grady
Research question one was designed to evaluate the impact of the EBB scale on
levels of rater agreement, which is defined as inter-rater reliability. This question is
answered by examination of the Wright maps and of the statistical reports provided
in the FACETS output. In terms of rater results, it is clear from the Wright maps that
levels of agreement between raters are higher on the EBB scale. The majority of the
raters tend to cluster around 0 logits on the EBB scale, suggesting that the raters
displayed similar levels of severity when awarding scores to the spoken samples.
However, the spread of rater severity on the analytical scale is greater and ranges
Fig. 3 FACETS variables map analytical scale
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 57
from 2 to 2 logits. The FACETS statistics reported in Table 1 (EBB scale) and
Table 2 (analytical scale) provide a more precise indication of the degree of
variance within each scale analysis. This is observed in the severity estimate,
which reports the relative severity that each rater demonstrated based on their
contextualised, fair average. The range of severity estimates for raters using the
EBB scale is from 1.02 to 0.65. These figures represent a standard deviation of
0.57 from the mean. In contrast, raters using the analytical scale record a range of
2.04 to 2.32 with a standard deviation of 1.50. These figures demonstrate that the
EBB methodology facilitates greater levels of agreement between raters.
The FACETS program further supplies strata statistics that indicate the extent to
which raters vary in severity. In contrast to test-taker strata statistics, which report
the statistically distinct number of ability levels in the test-taking population, rater
strata statistics demonstrate the number of levels of severity in the rater population.
Table 1 EBB report of rater severity
Rater
Severity
estimate Error
Fit
Fixed (all same)
chi-square
(Infit mean
square)
(Standardized
Infit)
3 1.02 0.29 0.88 0.2 χ2
¼ (6) 28.8, p < 0.01
5 0.67 0.29 1.27 0.8
2 0.09 0.26 0.75 0.8
6 0.21 0.27 0.85 0.4
7 0.23 0.27 1.03 0.1
1 0.51 0.27 0.85 0.4
4 0.65 0.27 1.33 1.0
Mean 0.00 0.27 0.99 0.0
SD 0.57 0.01 0.21 0.7
Reliability (reliable difference in severity of raters) 0.77
Table 2 Analytical scale report of rater severity
Rater
Severity
estimate Error
Fit
Fixed (all same)
chi-square
(Infit mean
square)
(Standardized
Infit)
4 2.04 0.23 1.15 0.8 χ2
¼ (6) 270.6,
p < 0.01
6 1.22 0.22 0.58 2.7
1 0.86 0.21 1.44 2.2
3 0.22 0.17 0.58 3.5
7 0.05 0.31 1.09 0.4
5 1.98 0.25 1.13 0.7
2 2.32 0.26 1.19 1.0
Mean 0.00 0.24 1.02 0.1
SD 1.50 0.04 0.30 1.9
Reliability (reliable difference in severity of raters) 0.97
58 S. O’Grady
Strata statistics for the EBB analysis are 2.80, which indicates that there were
roughly three levels of rater severity. The strata value for the analytical analysis is
8.63. This value demonstrates significant disagreement between the raters and
shows that the raters were inconsistent in their assessment (the number of severity
levels exceeds the number of raters). The FACETS program also provides a
reliability statistic for the analysis. The reliability statistic differs from conventional
measures of reliability such as a coefficient alpha (used in classical test theory to
measure agreement) and represents the degree of reliable difference between the
raters. In the FACETS output, reliability measures ranges from 0 to 1. Values close
to 0 indicate high levels of agreement between raters and are therefore desirable.
Reliability values close to 1 represent significant disagreement between the raters.
Comparison of the reliability statistics demonstrates that differences between the
raters were more reliable for the analytical scale (0.97). Differences between the
raters using the EBB scale (0.77) are more likely to have been due to standard error.
To summarise the results of the rater agreement analysis, the EBB scale gener-
ates consistently higher levels of agreement between the raters than the analytical
scale. This is evidenced by the severity estimates, strata statistics and reliability of
rater separation. In the analytical scale, the rater that is assigned to each individual
test-taker has an important impact on the outcome of the test result. Research
question one can therefore be affirmed. The EBB scale records higher levels of
inter-rater reliability than the analytical scale.
The second research question was designed to assess whether levels of within
rater consistency, which is defined as intra-rater reliability, are higher on the EBB
scale in relation to the analytical scale. The question is answered by evaluating the
fit statistics provided in the FACETS analysis. This is a measure of the internal
consistency within individual raters and indicates whether the data are productive
for measurement of language ability. The first fit statistic to be reported is the infit
mean square. The range of acceptable infit mean square statistical values is gener-
ally set at 0.7/0.8 to 1.2/1.3 (Wright & Linacre, 1994). Values below 0.7/0.8
represent model overfit and indicate that raters have not used the full range of the
scale and may therefore be awarding very similar grades to the samples. This may
be due to conservatism or reluctance to award grades on the extreme ends of the
scales. Values above 1.2/1.3 represent model misfit and are indicative of inconsis-
tency in the distribution of scores. When lack of fit is detected, this indicates that the
rater does not use the rating criteria consistently and requires additional training
with the rating scale. Table 1 demonstrates that infit statistics for the EBB raters are
within an acceptable range with the exception of rater 4 who approaches model
misfit. However, rater 4 does not deviate substantially from the model and the EBB
results can be considered productive for measurement. In contrast, the raters using
the analytical scale demonstrate both model overfit (raters 6 and 3) and misfit (rater
1). Rater 1 records an infit mean square value of 1.44, which is an indication of
inconsistency. Raters 3 and 6 record infit mean square values of 0.58. These raters
may be avoiding the lower and upper levels on the scale.
The second fit statistic is the standardized infit statistic. This is a measure of the
precision of the predictions the model can make about the data. Values within a
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 59
range of 1.9 to 1.9 indicate that the model can predict the data and is therefore fit
for measurement. Values below 2 indicate that the data fits the model too well and
the raters are not using the full extent of the scale. A range of 2.0–2.9 indicates that
the data is very unpredictable and is not fit for measurement (Linacre, 2002). The
EBB values are well within an acceptable range of 1.9 to 1.9. However, the
analytical values show substantial levels of misfit and overfit. Rater 1 records a
value of 2.2. This value suggests that rater 1 is notably unpredictable and may
require further training with the rating scale. Raters 6 and 3 record values of 2.7
and 3.5 indicating that these raters are too predictable and award similar scores to
all test samples. In practical terms, raters 1, 6 and 3 represent threats to the fairness
of the test administration and would need to be removed from the test
administration.
To summarise these results, the infit mean square statistics and the standardized
infit statistics demonstrate that the EBB scale leads to higher levels of within rater
consistency than the analytical scale. Research question 2 can therefore be affirmed.
Higher levels of intra-rater reliability are observed in the EBB scale.
The EBB method is designed to identify the features of test-taker language that
raters regard as most salient to their assessment of proficiency and base the rating
criteria on these features. Overall, the findings of this study demonstrate that use of
the EBB method leads to high levels of inter-rater reliability and intra-rater reli-
ability. The strata and reliability values in the EBB analysis, which show substan-
tially less variation in rater severity than the analytical scale analysis, verify the
higher levels of rater agreement. In addition, the fit statistics demonstrate consid-
erably higher levels of internal consistency when raters use the EBB scale.
The results of this study suggest that the EBB method generates considerably
higher levels of inter-rater reliability than the analytical scale. The literature
indicates that the observed increase may have been due to the grounding of the
scale content within an empirically based evaluation of rater criteria (Fulcher,
2003). The raters had substantial experience of assessing the test-taker profile and
through years of immersion in the educational context had assembled a mental
model of the way in which proficiency manifests during speaking assessment. The
EBB method permitted this experience to come to the fore and constitute the basis
for the scoring criteria. In contrast, the analytical scale required more individual
interpretation and resulted in the higher levels of rater disagreement. On this
evidence, the EBB method is preferable to the general purpose, analytical scale.
The EBB method has been shown to increase intra-rater reliability. High levels
of rater consistency are crucial for speech assessment to be successful. Excessive
variation reduces the test-taker’s chances of receiving a fair assessment. On the
other hand, invariability caused by conservatism poses a risk that test-takers are not
sufficiently separated into appropriate levels of ability. Low levels of intra-rater
reliability can have a major impact on the validity of any decisions based on test
results. The analytical scale statistics report substantial inconsistency within the
raters. The degree of inconsistency in scores on the analytical scale reaches the
point in which the test results may not be considered to be adequate representations
of the test-takers’ ability. However, no such inconsistencies were apparent in the
EBB results.
60 S. O’Grady
Despite the advantages derived from the use of such context specific scales,
Turner and Upshur (2002) identify a series of limitations in the use of EBB scales.
Primarily, EBB scales may be limited in terms of general applicability. Assessment
criteria are likely to vary between educational contexts according to institutional
objectives, local culture and the intended purpose of the test. Indeed, it may be the
case that a scale developed for a specific context is not appropriate for general use if
the scale content is heavily dependent upon the scale development team and the
language samples consulted during scale development. In situations requiring
standardization of raters to an EBB scale, the experience of developing the scale
would no longer be common among the raters. The scale contents would, as a
matter of necessity, be imposed upon the new raters regardless of the extent to
which they viewed the criteria as salient. In addition, if applied to test-taker profiles
that the scale was not originally intended to describe, the features of language
production that were identified by the development team may not feature in the test-
takers’ performance. This would reduce the capability for the scale to describe the
test-taking population. Certainly, the pressure for the samples used during the EBB
scale development process to represent an entire test-taking population is immense
and should not be underestimated.
The findings demonstrate the risks involved in using rating scales that are not
designed for specific contexts and with specific test-takers in mind. Analysis of the
analytical scale contents reveals that the scale does not correspond to the range of
ability within the test-taking population. At the higher levels of the scale, test-takers
should speak at the same speed as a native speaker, produce errors that are barely
noticeable and produce a variety of complex language forms. Such criteria proved
unrealistic in the current context where prospective students do not generally come
from an educational background that is amenable to the development of spoken
skills in the second language. It is clear that the analytical scale was not represen-
tative of the test-taking population. Ultimately, this caused a lack of agreement and
consistency in the raters’ assessment of the test-takers’ spoken skills.
5 Conclusion
The implications for the current study are most likely to be of interest to test-
developers and educational institutions that require reliable information about the
language proficiency of prospective students. The EBB scale improves test reli-
ability by increasing rater agreement and consistency. These are central tenets of
second language assessment and crucial for educational institutions that are
required to base potentially life changing decisions on the results of language
tests. However, utilizing the EBB method places a large burden on a minimal
group of language samples to be representative of a population of test-takers. As
the test-taking population increases, the likelihood that the samples will be truly
representative of the range of ability in the population decreases. In a similar way,
the raters who contribute to the scale development must also represent the
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 61
institutional body of raters. In other words, the features of speech that the scale
development group identifies as salient to their decision-making processes must
also be relevant to every rater involved in the assessment process. Furthermore, the
concern that EBB scales are context specific entails that a new scale may be
required for each administration of the examination in order to account for any
differences in the test-taking population. Language assessment is carried out under
stringent time constraints in Turkish higher education and continuous scale creation
may be impractical under such circumstances. Nonetheless, despite these limita-
tions the results of this study indicate that the use of the EBB scale is favorable to
the general purpose, analytical scale.
The EBB method has been shown to constitute a viable alternative to traditional
approaches to rating scale development for second language speech assessment.
The EBB scale leads to enhanced reliability and therefore a stronger validity
argument for this particular language test. Ultimately, EBB methodology has the
potential to improve the validity of institutional decisions relating to university
entrance and help ensure that prospective students are not unfairly rejected or
placed into a situation for which they do not have the means to survive and strive.
One limitation of the study that needs to be acknowledged is the constraints
placed on the time to complete rater standardization to the analytical scale. Had
more time been available to standardize the raters to the analytical scale, the levels
of rater agreement and consistency may have proved more productive for measure-
ment. Furthermore, having the same group of raters award scores using both scales
would have improved the potential for comparisons between the results. As it
stands, the generalizability of the comparison between the analytical results and
the EBB results is impeded by these limitations.
Future research into the use of the EBB method may help overcome some of the
concerns related to the application of EBB rating scales. A large-scale study
involving greater numbers of raters and test-takers would certainly contribute to
the general applicability of the scale to represent a wider range of individual
language learners. There is also a very pressing question about the way raters
orientate themselves to the test-taker’s language production and the assessment
criteria when using the EBB scale. Do raters actually use the EBB criteria in the
designed way? Do they begin listening with the first question in mind and hone in
on specific features as required? These questions might be answered in future
research involving some form of verbal report such as think aloud methodology
in which raters verbalize their thoughts when using an EBB scale. Ducasse (2009)
has highlighted the potential for such methodology to identify shared criteria that
might be included in EBB scale content but our understanding of the thought
processes that raters go through when utilizing this kind of scale remains
incomplete.
62 S. O’Grady
Appendix
1. Analytical Scale
Fluency
5. Speaks without hesitation; speech is generally of a speed similar to a native
speaker.
4. Speaks fairly fluently with only occasional hesitation, false starts and modi-
fication of intended utterance. Speech is only slightly slower than that of a native
speaker.
3. Speaks more slowly than a native speaker due to hesitations and word finding
delays.
2. A marked degree of hesitation due to word finding delays or inability to phrase
utterances easily.
1. Speech is quite disfluent due to frequent and lengthy hesitations or false starts.
Accuracy
5. Errors are barely noticeable.
4. Errors are not unusual, but rarely major.
3. Manages most common forms, with occasional errors; major errors present.
2. Limited linguistic control; major errors frequent.
1. Clear lack of linguistic control even of basic forms.
Complexity
5. Confidently attempts a variety of verb forms (e.g., passives, modals, tense and
aspect), even if the use is not always correct. Regularly takes risks grammatically in
the service of expressing complex meaning. Routinely attempts the use of coordi-
nation and subordination to convey ideas that cannot be expressed in a single
clause, even if the result is occasionally awkward or incorrect.
4. Confidently attempts a variety of verb forms (e.g., passives, modals, tense and
aspect), even if the use is not always correct. Takes risks grammatically in the
service of expressing complex meaning. Regularly attempts the use of coordination
and subordination to convey ideas that cannot be expressed in a single clause, even
if the result is occasionally awkward or incorrect.
3. Mostly relies on simple verb forms, with some attempts to use a greater
variety of forms (e.g., passives, modals, more varied tense and aspect). Some
attempt to use coordination and subordination to convey ideas that cannot be
expressed in a single clause.
2. Produces numerous sentence fragments in a predictable set of simple clause
structures. If coordination and/or subordination are attempted to express more
complex clause relations, this is hesitant and done with difficulty.
1. Produces mostly sentence fragments and simple phrases. Little attempt to use
any grammatical means to connect ideas across clauses.
Investigating the Use of an Empirically Derived, Binary-Choice and Boundary. . . 63
References
Baddeley, A. (2007). Working memory, thought, and action. Oxford: Oxford University Press.
Ducasse, A. (2009). Raters as scale makers for an L2 Spanish speaking test: Using paired discourse
to develop a rating scale for communicative interaction. In A. Brown & K. Hill (Eds.), Tasks
and criteria in performance assessment (pp. 15–39). Frankfurt: Peter Lang.
Field, J. (2011). Cognitive validity. In L. Taylor (Ed.), Studies in language testing 30 examining
speaking (pp. 65–112). Cambridge: Cambridge University Press.
Fulcher, G. (2003). Testing second language speaking. London: Pearson.
Fulcher, G., & Davidson, F. (2007). Language testing and assessment. London: Routledge.
Iwashita, N., McNamara, T., & Elder, C. (2001). Can we predict task difficulty in an oral
proficiency test? Exploring the potential of an information processing approach to task design.
Language Learning, 21(3), 401–436.
Linacre, J. M. (2002). What do infit and outfit, mean-square and standardized mean? Retrieved
from http://www.rasch.org/rmt/rmt162f.htm
Linacre, J. (2013). A user’s guide to FACETS: Rasch-Model Computer Programs (Program
Manual 3.71.0). Retrieved from http://www.winsteps.com/manuals.htm
Luoma, S. (2004). Assessing speaking. Cambridge: Cambridge University Press.
McNamara, F. (1996). Measuring second language performance. London: Longman.
Purpura, J. (2016). Second and foreign language assessment. The Modern Language Journal, 100,
190–208.
Read, J. (2015). Assessing English proficiency for university study. Basingstoke: Palgrave
Macmillan.
Selvi, A. F. (2014). Medium of instruction debate in Turkey: Oscillating between national ideas
and bilingual ideals. Current Issues in Language Planning, 15(2), 133–152.
Skehan, P. (2009). Modelling second language performance: Integrating complexity, accuracy,
fluency and lexis. Applied Linguistics, 30(4), 510–532.
Turner, C., & Upshur, J. (1996). Developing rating scales for the assessment of second language
performance. Australian Review of Applied Linguistics, 13, 55–79.
Turner, C., & Upshur, J. (2002). Rating scales derived from student samples: Effects of the scale
maker and the student sample on scale content and student scores. TESOL Quarterly, 36(1),
49–70.
Weir, C. (2005). Language testing and validation. Basingstoke: Palgrave Macmillan.
Wright, B. D., & Linacre, J. M. (1994). Reasonable mean-square fit values. Retrieved from http://
www.rasch.org/rmt/rmt83b.htm
64 S. O’Grady
The Measurement of Language Ability
and Impairment in Arabic-Speaking Children
Areej Balilah and Lisa Archibald
Abstract The present study adopted an epidemiological approach to assess sensi-
tivity to developmental change and sex differences of several language measures in
school age Arabic-speaking children. Of further interest was the degree to which
individuals exhibit consistently low performance across the language measures.
The study invited all Arabic speakers, from 6 to 9 years of age in several schools in
Saudi Arabia. A total of 421 children completed a battery of Arabic language tests
available in published or unpublished form. Results revealed that the majority of
language measures were sensitive to developmental change in younger children
between the ages of 6 and 7. Significant differences between children ages of 8 and
9 were observed on one test only. In addition, males scored higher than females on
several measures. The findings indicate that Arabic language tests are not sensitive
to age-related differences across the 6–9 year age range. Tests tapping more
complex language skills for older children need to be developed. Sex differences
with higher scores for males in several subtests may reflect cultural differences but
require further investigation. The results suggest that there is a clear need to
establish normative data across the ages studied in the current work. Developing
such extensive norms will inform future Arabic language test design and develop
measures sensitive to language development.
Keywords Measurement of language • Arabic-speaking children • Language
development
1 Introduction
In the course of language development, most children move from speaking their
first words to becoming sophisticated language users in an amazingly short period.
Understanding typical language acquisition is important not only for understanding
A. Balilah (*) • L. Archibald
School of Communication Sciences and Disorders, Elborn College, The University of Western
Ontario, London, ON, Canada, N6G 1H1
e-mail: aballiah@uwo.ca; larchiba@uwo.ca
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_4
65
child development, but also for identifying children who are lagging in language
growth. In order to measure language development, language measures commonly
test a range of abilities beginning with early-acquired forms and progressing to
more complex and later developing skills. Of course, the adequacy of such mea-
sures in capturing developmental progress in language is dependent on the accuracy
with which the tests tap changes in the particular language targeted. To date, the
majority of available language assessments have been developed for English
speaking children. The present study considers the case of Arabic-speaking chil-
dren. The available Arabic measures have largely been based on English measures,
and in some cases, directly translated. The purpose of this study was to examine the
extent to which available Arabic language measures are sensitive to developmental
change in a large unselected sample of Arabic-speaking children from Saudi
Arabia.
2 Theoretical Background
Developing sensitive language measures requires good normative data on language
development covering the range the ages under study (Bishop, 1997). In addition,
the design of language measures should consider the characteristics of children
identified as having a language impairment, and how the manifestation of such a
language impairment may change over the course of development (Bishop, 1994;
Bishop & Edmundson, 1987). It follows from this that the psycholinguistic knowl-
edge required to design linguistic measures sensitive to developmental change and
impairment is highly specific to the language in which the test is being designed.
Nevertheless, normative data on language development and data regarding charac-
teristics of language impairment are unavailable for many languages (Bishop,
1997). In the absence of the necessary language-specific normative data,
researchers have designed language tests based on, guided by, and, in some cases,
directly translated from those for which we have the greatest evidence, English
language tests. This is certainly the case for available Arabic language measures.
The extent to which these Arabic tests adequately capture change across language
development requires careful assessment, which was the focus of the present study.
In general, the majority of language assessment tests used for Arabic are
translated and adapted from English. One vocabulary measure, the Arabic
Receptive-Expressive Vocabulary Test (El-Halees & Wiig, 1999a) was based on
English language tests such as the Test of Word Knowledge (Wiig & Secord, 1992),
the Clinical Evaluation of Language Fundamentals (Semel, Wiig, & Secord, 1995),
and the Test of Language Competence-Expanded (Wiig & Secord, 1989), although
the authors did not indicate how the task had been developed or how the Arabic
words included in the task were chosen. The Arabic Receptive-Expressive Vocab-
ulary Test was designed with fixed start and stop points for different age groups. For
example, all 7-year-olds start with item 20 (receiving credit for previous items) and
end with item 45, whereas 8-year-olds start with item 25 and end with item 50.
66 A. Balilah and L. Archibald
The Arabic Language Screening Test is another test developed by El-Halees and
Wiig (1999b). The aim of the test is to evaluate children’s language development,
and identify children at risk for language disorders, by comprehensively sampling
expressive and receptive Arabic language skills. The Arabic Language Screening
Test (El-Halees & Wiig, 1999b) was based on English language tests such as the
Clinical Evaluation of Language Fundamentals (Semel, Wiig, & Secord, 1989a;
Semel et al., 1995; Wiig, Secord, & Semel, 1992), the Clinical Evaluation of
Language Fundamentals Screening Test (Semel, Wiig, & Secord, 1989b), and the
Test of Word Knowledge (Wiig & Secord, 1992). The authors of the Arabic
Language Screening Test indicated that the Arabic task used stimuli that reflected
Arabic culture and societal values, and that the stimuli were never translated
literally or idiomatically from English. In addition, the items included in the tasks
were designed in light of aspects of Arabic phonology, morphology, syntax, and
semantics, and all the illustrations in the task represented Arabic cultural experi-
ences and values. In addition, the authors indicated that three Arabic-speaking
speech and language pathologists had reviewed all the items that were included
in the task. The authors explained, however, that the design of the Arabic task drew
heavily on the findings of English-speaking children with language impairment
(Wiig & El-Halees, 2000). That is, the authors assumed that the underling aspect of
language behaviors that differentiate English-speaking children with language
impairment from their typically developing peers could be applied to Arabic-
speaking children with language impairment. In the Arabic Language Screening
Test, all children complete all items regardless of age.
Three additional Arabic tasks have been developed by Shaalan (2010) including
a phonological short-term memory task (Nonword Repetition Task), a comprehen-
sive language test (Arabic Language Test), and a receptive vocabulary test (Arabic
Picture Vocabulary Test). Regarding the Nonword Repetition Task, Shaalan indi-
cated that the stimuli of the task were designed to reflect the phonotactic and
morphological rules of Arabic. The 2- and 3-syllable nonwords in the test were
designed to imitate several triconsonantal roots of Arabic (containing a sequence of
three consonants) that do not appear in the Arabic lexicon, and controlled for
phonological complexity by including different types of clusters (Shaalan, 2010).
The Arabic Language Test included three subtests: Sentence Comprehension,
Expressive Language, and Sentence Repetition. Shalaan indicated that the stimuli
of the Sentence Comprehension subtest were designed to reflect Arabic syntactic,
morphological, and morphosyntactic structures; however, several of the items and
illustrations in the subtest were based on English tests such as the Clinical Evalu-
ation of Language Fundamentals (CELF-3, Semel, Wiig, & Secord, 1996). For the
Expressive Language subtest, the subtest used variants of Arabic morphosyntactic
structure that are commonly used by Arabic-speaking children, and included
linguistic structures chosen based on the experiences of the author, clinicians
working with Arabic children, and other available evidence (Abdalla, 2002;
Al-Akeel, 1998; Aljenaie, 2001). Nevertheless, various items and illustrations in
the subtest were based on English tests such as the CELF-3 (Semel et al., 1996), and
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 67
the Preschool Language Scale (PLS-4, Zimmerman, Steiner, & Pond, 1992). For
the Sentence Repetition subtest, the linguistic structures used in the sentences were
closely similar to the linguistic structures used in the Sentence Comprehension
subtest. In regards to the Arabic Picture Vocabulary Test, Shaalan did not indicate
how the Arabic words that were included in the task had been chosen. In addition,
some of the illustrations that were used in the subtest were based on English tests
such as the British Picture Vocabulary Scale (BPVT-2, Dunn, Dunn, Whetton, &
Burley, 1997). In all of the tests that were developed by Shaalan, the child
completes all items regardless of age.
It is clear that the design of Arabic language tests has drawn heavily on the
available evidence regarding typical and atypical language development in English-
speaking children (Wiig & El-Halees, 2000). Given the lack of normative data in
typical and atypical language development in children speaking Arabic
(Al-Tamimi, 2011), current language assessment measures may not utilize the
crucial aspects of Arabic necessary to capture developmental progression across
ages. The present study focused on assessing the sensitivity of the available Arabic
language measures to developmental change in 6 to 9-year-olds, a stage perhaps
reflecting slower language change relative to the preschool years (Pence & Justice,
2008) but still an important period of growth in language complexity (Nippold,
1998). Another important factor to consider in examining Arabic language devel-
opment is potential sex differences. In Arabic countries, such as Saudi Arabia, a
number of cultural practices are related to an individual’s identification as male or
female. In Saudi Arabia, male public schools provide more diverse activities than
female public schools. For example, male public schools provide physical educa-
tion, whereas female public schools do not (Khalaf et al., 2013). Given these
cultural practices, potential sex-based language differences should be evaluated,
as was the case in the current work.
In the present study, a large unselected sample of school age Arabic-speaking
children in Saudi Arabia completed the currently available Arabic language mea-
sures. One purpose of the study was to investigate one psychometric property of the
included language tests, sensitivity to developmental change. Performance differ-
ences across the full 6–9-year age band studied would indicate that the existing
measures capture developmental language growth whereas a lack of such
age-related differences would suggest that the tests fail to capture crucial aspects
of Arabic language development. A second aim of the study was to examine the
consistency with which individuals are identified with low language skills across
tests. Low performance across a number of measures would increase the confidence
with which individual participants could be considered to have a language impair-
ment. The final goal of the study was to explore the possible sex differences on the
measures. Sex differences on language performance would signal the need for
further investigation of this factor in language acquisition and consideration in
test design.
68 A. Balilah and L. Archibald
3 Method
3.1 Participants
The study took an epidemiological approach by inviting all monolingual Arabic
speakers from 6 to 9 years of age in 10 schools (5 male schools, 2 of which were
public; 5 female, all public) in Saudi Arabia (Jeddah) to participate. A total of
421 school-age children (158 males) participated who were, on average, 7.94
years of age (SD ¼ 1.10), with similar age ranges for both the male (M ¼ 7.85,
SD ¼ 1.16, range ¼ 6.16–9.92) and female participants (M ¼ 7.99, SD ¼ 1.06,
range ¼ 6.33–9.83). Schools were located in different demographic regions in
Jeddah, representing a considerable socioeconomic range. In addition, according
to parental reports, all students spoke Arabic as their first and only language. The
dialect spoken in Jeddah is the Urban Hijazi dialect (Sieny, 1978). The high degree
of contact with other languages and dialects and the mixture of ethnic groups
residing in this region influences the Hijazi dialect (Alahmadi, 2015; Al-Essa,
2006). As a result, lexical and phonological variations characterize the Urban Hijazi
dialect (Alahmadi, 2015; Al-Essa, 2006). Dialetical variations commonly observed
in Hijazi were employed in testing to match that spoken by the child, and scored as
accurate in child responses.
3.2 Materials and Procedure
A trained native Arabic speaker tested each child individually in a quiet room in the
child’s school. Data were collected in three sessions of approximately 40 min with
each session occurring about one week apart. Children completed a battery of
assessment measures including tests of language, vocabulary, phonological short-
term memory, and other tasks not reported here. The tests were administered in a
fixed order so that session one involved the administration of the Arabic Receptive-
Expressive Vocabulary Test (El-Halees & Wiig, 1999a), Nonword Repetition Task
(Shaalan, 2010), and the Arabic Language Screening Test (El-Halees & Wiig,
1999b), session two, the Arabic Language Test (Shaalan, 2010), and session
three, the Arabic Picture Vocabulary Test (Shaalan, 2010) and other tasks not
reported here. All research assistants underwent a rigorous training procedure.
After review of test administration, the research assistant administered all tests to
a child not involved in the study in the presence of the first author who indepen-
dently scored the child’s performance. After completion, scored records were
reviewed and any discrepancies discussed. If there were more than three discrep-
ancies, this procedure was repeated until there were fewer than three scoring
discrepancies across the entire test battery.
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 69
The Arabic Receptive-Expressive Vocabulary Test (AREVT) Both the expressive
and receptive subtests of the AREVT were administered to all children. In the
receptive subtest, the children pointed to a picture corresponding to a given spoken
word from a choice of four. In the expressive subtest, children named or described a
picture with a single word or phrase. Raw scores corresponded to the number of
correct responses with maximum possible score for each subtest dependent on the
child’s age (6;0–6;11: n ¼ 40; 7;0–7;11: n ¼ 45; 8;0–8;1: n ¼ 50; 9;0–9;11:
n ¼ 55). Total test scores were the sum of correct raw scores for each subtest.
Subtest and total raw scores were converted to z-scores within each year band in
order to allow comparison across age groups.
The Arabic Picture Vocabulary Test (APVT) In the APVT, the children pointed to
a picture corresponding to a given spoken word from a choice of four. Each
response was scored as correct or incorrect with a maximum possible score of 132.
The Arabic Language Screening Test (ALST) (school-age) The ALST involved
tests of verbal and nonverbal abilities. Tests of verbal abilities included six items
each assessing nouns and verbs, adjectives, morphology, understanding sentences,
forming sentences, remembering instructions, and repeating sentences. For Nouns
and Verbs, children named the object, person, or activity pictured. For Adjectives,
children were first required to point to a picture that illustrated a spoken sentence,
and then give the opposite word. For Morphology, the children were given a
sentence and asked to generate spoken sentences in reference to a picture cue. In
Understanding Sentences, the children pointed to a picture from a choice of three
corresponding to a spoken sentence. In Forming Sentences, the children formulated
a sentence about the visual stimuli presented using target words or phrases. In
Remembering Instructions, the children pointed to pictures in response to oral
directions. In Repeating Sentences, the children repeated sentences presented by
the examiner. The majority of subtests were scored with 1 point for each correct
response with the exception of Formulating Sentences (2 ¼ correct sentence;
1 ¼ few errors; 0 ¼ nonsense or unrelated sentence, or no response) and Repeating
Sentences (2 ¼ correct; 1 ¼ 1–2 errors of omission, addition, or substitution; 0 ¼ 3
or more errors, or no response). For Adjectives, 1 point was given each for correctly
pointing to the picture and correctly naming the opposite word. The highest
possible overall raw score in the Verbal Abilities section was 60.
The Non Verbal Abilities subtest involved five short sections consisting of five
or six items and requiring nonverbal responses. The items, however, tapped lin-
guistic knowledge. In the Missing Part activity, the children pointed to the correct
picture from a choice of four to correspond with the missing object in a target
picture; then the children named the object that illustrated the missing part. In
Matching Words/Sentences, the children pointed to the word/sentence that was
orthographically identical to the target word/sentence. In Classification by Mean-
ing, the children chose the three pictures that were related from a choice of five. In
Classification by Group Membership, the children chose the two pictures that were
related from five and described the relationship. In Arranging a Story, the children
ordered the four given pictures to form a logical story. The majority of the subtests
70 A. Balilah and L. Archibald
were scored with one point for each correct response, with the exception of the
Missing Part activity (2 ¼ correct pointing and naming; 1 ¼ correct pointing or
correct naming; 0 ¼ incorrect pointing and naming, or no response) and Classifi-
cation by Group Membership (2 ¼ correct pointing to the related pictures and
correct describing of the relationship among the group members; 1 ¼ correct
pointing or describing; 0 ¼ incorrect pointing and describing, or no response).
The highest possible overall raw score in the Non Verbal Abilities section was
40 with a maximum possible score of 100 for the overall Arabic Language
Screening Test.
The Arabic Language Test (ALT) The three subtests of the ALT were administered
to all children, and each subtest was divided into a first and second section
(Section A and B). In the Sentence Comprehension subtest, the children pointed
to a picture that corresponded to the spoken sentence from a choice of three or four.
In the Expressive Language subtest, the children were given a sentence and asked to
generate a spoken word or phrase in reference to a picture cue. The Sentence
Repetition subtest required immediate repetition of a presented audio recording of
a sentence spoken by a native, adult male Arabic speaker. The Sentence Compre-
hension and Expressive Language tests were organized into two sections
corresponding to early development or more advanced morphosyntactic structures
(see Shaalan, 2010). Each response was scored as correct or incorrect with a
maximum possible score of 40 for the Sentence Comprehension subtest
(Section A: n ¼ 22; Section B: n ¼ 18), and 68 for the Expressive Language subtest
(Section A: n ¼ 24; Section B: n ¼ 44). The 41 items of the Sentence Repetition
subtest were scored on a 4-point scale (3 ¼ correct; 2 ¼ 1 error; 1 ¼ 2–3 errors;
0 ¼ 4 or more errors or no response) for a maximum possible score of 123 -
(Section A: n ¼ 18; Section B: n ¼ 23).
Nonword Repetition Task (NWR) In the NWR Task, children repeated nonwords of
a presented audio recording of nonwords spoken by a native, adult male Arabic
speaker. The stimuli selected were taken from the task employed by Shaalan
(2010). Children were given one opportunity to imitate 48 nonwords varying in
length (two to three syllables) and cluster type (no cluster, medial cluster, final
cluster, and medial and final clusters). Each response was scored as correct or
incorrect for a maximum possible score of 48.
4 Results and Discussion
4.1 Preliminary Analysis
As a first step, data were examined for distribution and evidence of floor or ceiling
effects. Table 1 presents descriptive statistics for raw scores for all participants on
all language tasks except the Arabic Receptive-Expressive Vocabulary Test (for
which the maximum score differed across age groups and z-scores are shown).
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 71
There were two subtests for which the maximum test score fell within a 1 SD band
around the group mean: Sentence Comprehension-A and Sentence Repetition-A.
This pattern was considered to reflect a ceiling effect. As a result, no further
analysis was conducted on these two subtests. No corresponding floor effects
were observed.
Next, the data were examined based on the four age bands of 6;0–6;11, 7;0–7;11,
8;0–8;11, and 9;0–9;11. These data are presented in Appendix A. The data were
reviewed again for ceiling and floor effects. No additional ceiling or floor effects
were observed within the age groups including for the Arabic Receptive-Expressive
Vocabulary Test. All remaining analyses for the Arabic Receptive-Expressive
Vocabulary Test were completed on the z-scores.
4.2 Sensitivity to Developmental Changes and Sex
Differences
In the next set of analyses, we investigated sensitivity to developmental changes
across our four age bands for language tests without ceiling effects. To do this,
multivariate analysis of variances (MANOVAs) were conducted separately as a
Table 1 Descriptive statistics for the Arabic receptive-expressive vocabulary test z-scores, and
raw scores for all remaining language tasks
Measure M SD Range Skew Kurta
Test’s
max
Arabic receptive-expressive vocabulary test
Receptive 0.00 1.00 2.22–1.86 0.11 0.98 n/a
Expressive 0.00 1.00 2.24–2.53 0.07 0.65
Total score 0.00 1.00 2.09–2.22 0.00 0.12
Arabic picture vocabulary test 101.39 11.68 58–123 0.69 0.43 132
Arabic language screening test
Verbal abilities 38.48 7.88 14–56 0.37 0.38 60
Nonverbal abilities 23.92 5.60 4–36 0.33 0.12 40
Total score 62.40 12.21 22–89 0.42 0.37 100
Arabic language test
Sentence comprehension (A) 19.97b
2.31 7–22 2.63 9.33 22
Sentence comprehension (B) 13.55 2.75 4–18 0.74 0.71 18
Expressive language (A) 18.33 3.20 7–24 0.48 0.04 24
Expressive language (B) 35.24 5.41 19–44 0.67 0.20 44
Sentence repetition (A) 51.30b
4.90 22–54 2.80 9.00 54
Sentence repetition (B) 46.03 10.66 15–69 0.39 0.42 69
Total score 184.43 19.96 115–230 0.59 0.10 231
Nonword repetition task 38.65 7.12 5–48 1.36 2.93 48
a
Kurtosis
b
Subtests with ceiling effects
72 A. Balilah and L. Archibald
function of age group (6–9) and sex (male, female) on the scores of each language
test: (1) the receptive and expressive subtests of the Arabic Receptive-Expressive
Vocabulary Test, (2) the verbal and nonverbal subtests of the Arabic Language
Screening Test, and (3) the four subtests of the Arabic Language Test. As well,
separate corresponding ANOVAs were completed on the raw scores of the Arabic
Picture Vocabulary Test and Nonword Repetition tasks. For all three MANOVAs,
the Hotelling’s T were significant for age, F > 7.9, p < 0.001 (all cases), but not for
the interaction between age and sex, F < 1.55, p > 0.05 (all cases). The main effect
of sex was also significant in each case due to higher scores for the males than
females, F > 3.1, p < 0.05. Results of the univariate comparisons are described
below.
The MANOVA performed on the z-scores of the Arabic Receptive-Expressive
Vocabulary Test revealed significant univariate effects for the receptive subtest on
age, F(3,413) ¼ 752.12, p < 0.05, η2
p ¼ 0.845, and sex, F(1,413) ¼ 4.60, p < 0.05,
η2
p ¼ 0.011, but not the interaction, F(3,413) ¼ 0.038, p > 0.05, and for the
expressive subtest on age, F(3,413) ¼ 320.48, p < 0.05, η2
p ¼ 0.700. The effect
of sex on the expressive subtest and the interaction between age and sex were
marginal (sex: F(1,413) ¼ 3.68, p ¼ 0.056; interaction: F(3,413) ¼ 2.62, p ¼ 0.05).
Pairwise comparisons revealed significant increases with each incremental increase
in age band for both subtests, which is clearly visible in the z-score plot presented in
Fig. 1 ( p < 0.001, all cases; see also Appendix). As well, significantly higher scores
for males than females were observed for the receptive subtest (male: M ¼ 0.053,
SE ¼ 0.031; female: M ¼ 0.030, SE ¼ 0.024).
The MANOVA performed on the two subtests of the Arabic Language Screening
Test revealed significant univariate effects for the verbal abilities subtest on age only,
F(3,413) ¼ 19.32, p < 0.05, η2
p ¼ 0.123. Remaining effects were not significant
(sex: F(1,413) ¼ 0.509, p > 0.05; interaction: F(3,413) ¼ 0.613, p > 0.05).
For the nonverbal abilities subtest, the effects of age, F(3,413) ¼ 17.39,
p < 0.05, η2
p ¼ 0.112, and sex, F(1,413) ¼ 6.58, p < 0.05, η2
p ¼ 0.016, were
significant, but the interaction was not, F(3,413) ¼ 0.566, p > 0.05. As shown in
Fig. 2 displaying the z-scores for each age band, pairwise comparisons for both
subtests revealed significant increases between the 6 and the 7 year olds ( p < 0.001,
both cases) but not the 7 and 8 or 8 and 9 year olds ( p > 0.05, all cases;
-1.5
-1
-0.5
0
0.5
1
1.5
6 yrs. 7 yrs. 8 yrs. 9 yrs.
Mean
z-score
Age in Years
ARVT
AEVT
Fig. 1 Mean z-scores as a
function of age for the
receptive and expressive
subtests of the Arabic
Receptive-Expressive
Vocabulary Test showing
significant performance
growth with each increase
in age band
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 73
see Appendix). On the nonverbal abilities subtest, males scored significantly higher
than females (male: M ¼ 24.78, SE ¼ 0.420; female: M ¼ 23.41, SE ¼ 0.325).
The MANOVA performed on the four subtests of the Arabic Language Test
revealed significant univariate effects for the four subtests on age, F > 9.8,
p < 0.05, η2
p > 0.065 (all cases). For the sentence repetition-B subtest, the effect
of sex, F(1,413) ¼ 9.39, p < 0.05, η2
p ¼ 0.022, and the interaction between age and
sex were significant, F(3,413) ¼ 3.14, p < 0.05, η2
p ¼ 0.022). All remaining effects
of sex and interactions were not significant, F < 1.8, p > 0.05 (all cases). For the
main effects of age as shown in Fig. 3, pairwise comparisons revealed significant
increases between the 6 and the 7 year olds ( p < 0.001, all cases) but not the 7 and
8 or 8 and 9 years old ( p > 0.05, all cases; see also Appendix) for all subtests except
sentence comprehension -B (for which no significant effects were found, p > 0.05,
all cases). As well, the significantly higher scores on sentence repetition -B were
observed for males compared to females (male: M ¼ 47.93, SE ¼ 0.782; female:
M ¼ 44.89, SE ¼ 0.606). The significant effects of sex and age for the sentence
repetition -B subtest were due to higher scores for the males than females at 6 and
7 but not 8 and 9 years of age ( p < 0.05, all significant cases).
The ANOVA performed on the raw scores of the Arabic Picture Vocabulary Test
revealed significant effects of age, F(3,413) ¼ 17.67, p < 0.05, η2
p ¼ 0.114, and
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
6 yrs. 7 yrs. 8 yrs. 9 yrs.
Mean
z-score
Age in Year
Verbal
Non-verbal
Fig. 2 Mean z-scores as a
function of age for the
verbal and nonverbal
subtests of the Arabic
Language Screening Test
showing significant
performance growth
between the 6 and 7 year
olds only
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
6 yrs. 7 yrs. 8 yrs. 9 yrs.
Mean
z-score
Age in Years
EL-A
EL-B
SR-B
SC-B
Fig. 3 Mean z-scores as a
function of age for the four
subtests of the Arabic
Language Test showing
significant performance
growth between 6 and
7-year-old only for the
expressive language but not
sentence repetition subtests
74 A. Balilah and L. Archibald
sex, F(1,413) ¼ 12.96, p < 0.05, η2
p ¼ 0.030, but not the interaction,
F(3,413) ¼ 0.96, p > 0.05. Pairwise comparisons revealed significant increases
between the 6 and the 7 year olds p < 0.05) but not the 7 and 8 or 8 and 9 year olds
( p > 0.05; see Appendix). As well, significantly higher scores were observed for
males than females (male: M ¼ 103.817, SE ¼ 0.863; female: M ¼ 99.88,
SE ¼ 0.669). The ANOVA performed on the raw scores of Nonword Repetition
revealed significant effects of sex, F(1,413) ¼ 7.344, p < 0.05, η2
p ¼ 0.017, but not
age, F(3,413) ¼ 1.939, p > 0.05, or the interaction, F(3,413) ¼ 0.520, p > 0.05.
Pairwise comparison revealed significantly higher scores for females than males
(female: M ¼ 39.373, SE ¼ 0.435; male: M ¼ 37.448, SE ¼ 0.562).
To summarize the results for the sensitivity to developmental change in lan-
guage measures, age effects were observed for all measures except the nonword
repetition task. In all cases of significant age effects, performance differences were
observed between the 6 and 7 year olds with one exception: On the sentence
repetition -B task of the Arabic Language Test, age increases were modified by
sex such that 6 and 7 but not 8 and 9 year old males scored significantly higher than
females. Only the Arabic Receptive-Expressive Vocabulary Test revealed signifi-
cant differences between the remaining incremental age groups. Sex differences in
favor of the male participants were observed for all subtests except the verbal
abilities subtest of the Arabic Language Screening Test, and the nonword repetition
task for which an advantage for females was found.
4.3 Interrelations
To explore the degree to which the language subtests evaluate different or some-
what similar language skills, a correlation matrix was computed on the language
subtests with the greatest sensitivity to developmental change in language: The
receptive and expressive subtests of the Arabic Receptive-Expressive Vocabulary
Test, the verbal and nonverbal subtests of the Arabic Language Screening Test, the
four subtests of the Arabic Language Test, and the Arabic Picture Vocabulary test
using the z-scores of the Arabic Receptive-Expressive Vocabulary Test, and raw
scores for all remaining language subtests. Zero-order correlations are displayed in
the lower triangle in Table 2. The intercorrelations between all language measures
were moderate to small in magnitude, with rs ranging from 0.28 to 0.48 ( p < 0.001,
all cases). The within-test intercorrelations between subtests were large (rs ranging
from 0.84 to 0.61, p < 0.001, all cases) for all measures analyzed.
A partial correlation, calculated while controlling for age in months, provides
more meaningful information about the patterns of association. These coefficients
are shown in the upper triangle in Table 2. The intercorrelations between all
language measures were moderate to small in magnitude, with rs ranging from
0.09 to 0.39 for all subtests ( p < 0.001, all cases). The intercorrelation between the
receptive subtest of the Arabic Receptive-Expressive Vocabulary Test and both
subtests of the Arabic Language Test were small in magnitude, with rs ranging from
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 75
0.09 to 0.14 ( p < 0.05). The intercorrelation between the receptive and expressive
subtests of the Arabic Receptive-Expressive Vocabulary Test was reduced after age
was partialed out, with moderate magnitude, r ¼ 0.45 ( p < 0.001). However, the
coefficients remained large for the two subtests of the Arabic Language Screening
Test, and the two subtests of the Arabic Language Test (r ¼ 0.58, r ¼ 0.56
respectively, p < 0.001).
4.4 Sensitivity to Identified Children with Impairment
Of further interest was the degree to which individuals exhibited consistently low
performance across tests. For this analysis, we chose the subtests with the greatest
sensitivity to developmental change in language including the receptive and expres-
sive subtests of the Arabic Receptive-Expressive Vocabulary Test (AREVT), the
verbal and nonverbal subtests of the Arabic Language Screening Test, the four
subtests of the Arabic Language Test, and the Arabic Picture Vocabulary test.
Standard scores were calculated for each year group after replacing individual
outliers (scores falling 3.5 SD band around the group mean) with the value of
the next highest/lowest non-outlier. Four outliers were identified: One on the
receptive AREVT subtest at age 9, one on the expressive AREVT subtest at age
8, and two on the Arabic Picture Vocabulary Test at age 7. Composite standard
scores were calculated by averaging standard scores across relevant subtests for the
Arabic Receptive-Expressive Vocabulary Test, the Arabic Language Screening
Test, and the Arabic Language Test. Children obtaining a standard score below
86 were considered indicative of a deficit. A total of 148 participants scored below
the cut-off on one or more language measures. Of these, 62% scored below the
cut-off on one measure only, 28% on two measures, 7% on three measures, and 3%
on all four measures. Table 3 presents the number of participants who scored below
the cut-off on one or two language measures.
Table 2 Correlation between the language subtest raw scores; partial correlation (controlling for
age in months) in the upper triangle and zero-order correlation in bottom triangle
Variable 1 2 3 4 5 6 7 8 9
1. Age(months) –
2. AREVT_R 0.89 – 0.45 0.32 0.28 0.17 0.25 0.09 0.14
3. AREVT_E 0.80 0.84 – 0.27 0.34 0.29 0.22 0.32 0.21
4. APVT 0.34 0.44 0.43 – 0.38 0.30 0.32 0.26 0.35
5. ALST_V 0.37 0.45 0.48 0.46 – 0.58 0.35 0.37 0.39
6. ALST_NV 0.31 0.35 0.41 0.38 0.63 – 0.25 0.31 0.32
7. SC_B_ALT 0.32 0.39 0.38 0.40 0.43 0.33 – 0.26 0.19
8. Total
EL_ALT
0.31 0.32 0.43 0.34 0.44 0.38 0.34 – 0.56
9. SR_B_ALT 0.32 0.35 0.38 0.42 0.46 0.39 0.28 0.60 –
All values p < 0.001; values in bold p < 0.05
76 A. Balilah and L. Archibald
The primary aim of this study was to assess the sensitivity to developmental
change and sex differences of available Arabic tests when applied to school-aged
Arabic-speaking children. The study also examined the degree to which individuals
exhibit consistently low performance across those language measures. This is the
first investigation of these Arabic language measures with Arabic-speaking children
from Saudi Arabia. Age effects were observed for all measures except the Nonword
Repetition Task. This study shows that the available Arabic language measures are
sensitive to developmental change in younger children only between the ages of
6 and 7. Only the Arabic Receptive-Expressive Vocabulary Test revealed signifi-
cant performance growth with each increase in age band. Sex differences in favour
of male participants were observed for several of the language subtests.
Among the language tests, the Arabic Receptive-Expressive Vocabulary Test
(El-Halees & Wiig, 1999a) showed the greatest sensitivity to developmental change
among school-aged Arabic-speaking children. The authors, however, did not indi-
cate how the task had been developed or how the Arabic words that were included
in the task had been chosen. For example, it is not clear if the vocabulary used in the
task was based on a systematic investigation of the vocabulary acquisition of
Arabic-speaking children. Another point that should be highlighted is the scoring
method that is used by the Arabic Receptive-Expressive Vocabulary Test. The task
was designed with fixed start and stop points for different age groups, and children
in certain age groups received credit for previous items. This scoring method might
have influenced the test results, as the Arabic Receptive-Expressive Vocabulary
Test was the only test that showed a significant performance growth with each
increase in age band.
The non-significant increases between the 7 and 8 or 8 and 9-year-olds on the
two language tasks—the Arabic Language Test and the Arabic Picture Vocabulary
Test (Shaalan, 2010)—might demonstrate that these language measures became
less challenging with age. It may be the case that the tasks need to include more
complex language skills and a broader range of language abilities to capture the
level of language development in older Arabic children. For example, Abdalla and
Crago (2008) found that Arabic-speaking children with language impairment have
a specific difficulty with tense and subject-verb agreement forms. However, The
Expressive Language (B) subtest that aims to assess more advanced
morphosyntactic structures included only three present verb markers, and three
past verb markers. In addition, all the responses on the Arabic Language Test asked
children to generate a spoken answer of two words in length, or, more often the
case, only one word. Providing more comprehensive assessment by requiring
complex language responses might better capture the level of language develop-
ment in older Arabic children.
Table 3 The number of
participants who scored
below the cut-off on one or
two language measures
AREVT APVT ALST ALT
AREVT 30 4 6 7
APVT 14
ALST 5 22
ALT 5 14 25
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 77
The Arabic Language Screening Test (El-Halees & Wiig, 1999b) also did not
show increases between the 7 and 8 or 8 and 9-year-olds. The school-aged Arabic
Language Screening Test has been designed for children between 6 and 12 years.
All children are required to complete all items in this test regardless of age, and the
task included 71 items completed in 10–15 minutes by typically developing chil-
dren. Nevertheless, the verbal abilities subtest of the Arabic Language Screening
Test included six items only, each assessing nouns and verbs, adjectives, morphol-
ogy, understanding sentences, forming sentences, remembering instructions, and
repeating sentences. The current findings suggest that a broader range of language
abilities is required to capture the level of language development of all ages under
study.
The Nonword Repetition Task (Shaalan, 2010) was the only measure that did not
show sensitivity to developmental change. Even though the task controls for
phonological complexity by including stimuli with different types of clusters, the
length of all stimuli in the task is two to three-syllable. Some Arabic dialects, such
as Gulf Arabic, can have up to seven-syllable words (Bukshaisha, 1985). It may be
that, two to three-syllable nonwords was not sufficiently challenging for our full age
range of Arabic-speaking children. In fact, many studies have shown that stimuli
length affects children’s performance on nonword repetition with older children
reaching ceiling for short nonwords (e.g., Archibald & Gathercole, 2006).
We observed low performance on several language measures by females com-
pared to their male peers. Such differences might reflect cultural differences. That
is, cultural differences in language practices and the social context in which
children learn their language can influence the rate of language development
(Lieven, 1994). Such knowledge raises questions about the impact of sociocultural
differences between males and females on language practice and development in
Saudi Arabia. For example, in Saudi Arabia, public schools provide many physical
and recreational activities for males but not for females (Khalaf et al., 2013). In
addition, males are allowed to engage in more recreational activities outside of
school than females. Sociocultural differences, especially in relation to language
practices in Saudi Arabia, may provide males and females with unequal opportu-
nities to experience language. It may be that the sociocultural differences in relation
to language practices among males and females in Saudi Arabia provide males with
richer language experiences than females. Surprisingly, it should be noted that an
advantage for females was found in the Nonword Repetition Task only. Nonword
repetition tasks are considered to be processing-based measure. Unlike knowledge-
based measures, processing-based measures, such as nonword repetition tasks,
reduce the role of cultural and language-specific experience (Kohnert, 2012). As
a result, it may be that nonword repetition tasks are less sensitive to sex differences
in language development. It is clear that further investigation of the sex differences
observed on language tests for Arabic speaking school age children in the present
study is warranted.
With regards to the relationships among the language measures in the present
study, correlations ranged from small to moderate in magnitude perhaps suggesting
that the measures tap different but related language skills. When individuals
78 A. Balilah and L. Archibald
exhibiting consistently low performance across the language measures were exam-
ined, there was considerable lack of agreement among tasks making interpretation
difficult. For example, the agreement between the Arabic Receptive-Expressive
Vocabulary Test and the Arabic Picture Vocabulary Task (Receptive only) was
low, even though both tasks measured vocabulary skills in this population. It would
appear that considerable work is needed to gain a better understanding of the
characteristics of developmental language impairment in Arabic speakers, and to
develop measures sensitive to both development and impairment.
5 Conclusion
Evaluating children’s language skills in light of their dialectal background, culture
orientation, and ethnicity is very important. However, dealing with dialects that
have significant lexical and phonological variation and change, such as the Urban
Hijazi dialect, can make language assessment challenging. In the present study,
examiners matched the dialectical variations to the child’s spoken output and
accepted as correct commonly observed variations. It may be that future studies
will assess the impact of dialectical variations in more detail.
This study shows that current Arabic language measures have significant limi-
tations. Language measures aim to evaluate whether or not a child follows an
expected level of language development in order to address language concerns. It
is clear from the description of all of the tasks in the abovementioned studies that
the authors have worked to ensure that the design of stimuli was culturally appro-
priate for Arabic-speaking children. In addition, the stimuli were designed in light
of aspects of Arabic phonology, morphology, syntax, and semantics. Nevertheless,
given the lack of normative data for typical and atypical language development in
the majority of Arabic colloquial dialects, the design of the current Arabic tasks
draws heavily on the findings of English-speaking children. The present findings
suggest that currently available measures are not capturing crucial changes in child
language development beyond about 7 years of age. There is a clear need to
establish normative data across the ages studied in the current work in order to
inform future Arabic language test design and develop measures sensitive to
language development across a wider age range.
In this study, a large sample of school-aged Arabic-speaking children from
Saudi Arabia completed individual measures of sentence comprehension, expres-
sive language, sentence repetition, receptive and expressive vocabulary, and
nonword repetition. Results revealed that available Arabic language measures
are sensitive to developmental change in younger children only between the ages
of 6 and 7. Only the Arabic Receptive-Expressive Vocabulary Test (El-Halees &
Wiig, 1999a) revealed significant performance growth with each increase in age
band, although this test was the only measure to employ fixed (and different) start
and stop points for each age group. The low performance on several language
measures by females compared to their male peers may reflect sociocultural
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 79
differences, especially in relation to language practices between males and
females in Saudi Arabia. The results suggest that there is a need to develop
language assessment measures that evaluate a broad range of language abilities
and more complex language skills for Arabic-speaking children, and that are
based on the psycholinguistics of Arabic.
Acknowledgements This work was supported by funding to the first author from King Abdulaziz
University, Saudi Arabia, and a discovery grant to the second author from the Natural Sciences and
Engineering Research Council, Canada.
Appendix
Descriptive statistic far all language tasks scores as a function of age band: 6–6;11
(n ¼ 106), 7–7;11 (n ¼ 104), 8–8;11 (n ¼ 106), and 9–9;11 (n ¼ 105)
Age M SD Range Skew. Kurt.
Arabic receptive-expressive vocabulary testa
Arabic receptive vocabulary test
6–6;11 34.25
(1.27)
2.66
(0.40)
28–40
(2.22/0.47)
0.27 0.50
7–7;11 40.33
(0.36)
2.34
(0.35)
34–45
(1.31/0.35)
0.33 0.51
8–8;11 45.35
(0.40)
2.48
(0.37)
38–50
(0.71/1.10)
0.74 0.26
9–9;11 50.77
(1.23)
2.72
(0.35)
35–55
(0.19/1.86)
2.23 10.09
Arabic expressive vocabulary test
6–6;11 29.90
(1.10)
3.60
(0.52)
23–38
(2.10/0.07)
0.37 0.28
7–7;11 34.70
(0.41)
3.84
(0.55)
22–43
(2.24/0.79)
0.24 0.12
8–8;11 40.23
(0.39)
3.66
(0.52)
27–49
(1.38/1.66)
0.05 0.77
9–9;11 45.25
(1.12)
4.11
(0.59)
35–55
(0.36/2.53)
0.102 0.09
Total-Arabic receptive-expressive vocabulary test
6–6;11 64.15
(1.23)
5.02
(0.39)
53–77
(2.09/0.24)
0.04 0.44
7–7;11 75.03
(39)
5.02
(0.39)
58–88
(1.71/0.60)
0.47 0.36
8–8;11 85.58
(0.41)
5.06
(0.39)
68–99
(0.94/1.45)
0.35 0.54
9–9;11 96.02
(1.22)
5.71
(0.44)
78–109
(0.17/2.22)
0.47 1.26
(continued)
80 A. Balilah and L. Archibald
Age M SD Range Skew. Kurt.
Arabic picture vocabulary test
6–6;11 95.47 10.61 72–116 0.23 0.62
7–7;11 100.04 11.75 58–119 1.11 1.93
8–8;11 103.85 9.61 72–119 0.89 0.89
9–9;11 106.12 11.92 65–123 1.01 1.05
Arabic language screening test
Arabic language screening test (verbal abilities)
6–6;11 34.03 8.33 14–53 0.08 0.57
7–7;11 38.05 7.45 19–52 0.31 0.54
8–8;11 40.23 6.89 22–54 0.51 0.13
9–9;11 41.62 6.64 22–56 0.32 0.01
Arabic language screening test (non-verbal abilities)
6–6;11 20.92 6.41 4–34 0.11 0.48
7–7;11 23.87 5.22 14–34 0.24 0.92
8–8;11 25.55 4.26 16–36 0.14 0.16
9–9;11 25.37 5.11 9–35 0.71 1.26
Total-Arabic language screening test
6–6;11 54.94 13.54 22–81 0.07 0.91
7–7;11 61.91 11.31 39–84 0.18 0.78
8–8;11 65.77 9.77 40–87 0.52 0.06
9–9;11 66.99 10.21 37–89 0.35 0.37
Arabic language test
Sentence comprehension (A)
6–6;11 19.42 2.39 8–22 2.69 10.33
7–7;11 19.83 2.51 9–22 2.29 5.97
8–8;11 20.32 1.75 10–22 2.41 10.58
9–9;11 20.32 2.42 7–22 3.09 11.83
Sentence comprehension (B)
6–6;11 12.34 2.52 4–18 0.69 0.64
7–7;11 13.19 2.79 5–18 0.63 0.39
8–8;11 14.22 2.30 6–18 0.76 1.02
9–9;11 14.47 2.87 4–18 1.26 2.28
Total-sentence comprehension
6–6;11 31.76 4.12 14–39 1.73 5.02
7–7;11 33.02 4.36 16–40 1.61 3.81
8–8;11 34.54 3.46 16–40 1.81 7.02
9–9;11 34.79 4.75 12–40 2.15 6.53
Expressive language (A)
6–6;11 16.91 3.58 7–24 0.195 0.108
7–7;11 18.65 2.98 10–24 0.24 0.21
8–8;11 18.93 3.01 11–24 0.48 0.25
9–9;11 18.82 2.77 11–24 0.69 0.49
(continued)
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 81
Age M SD Range Skew. Kurt.
Expressive language (B)
6–6;11 32.04 6.47 19–44 0.27 0.76
7–7;11 35.43 4.89 23–44 0.40 0.13
8–8;11 36.75 4.49 25–44 0.33 0.69
9–9;11 36.76 4.07 27–44 0.34 0.31
Total-expressive language
6–6;11 48.94 9.41 26–68 1.13 0.61
7–7;11 54.09 7.15 33–68 0.35 0.02
8–8;11 55.68 6.86 40–67 0.31 0.94
9–9;11 55.58 6.23 42–68 0.42 0.33
Sentence repetition (A)
6–6;11 50.30 4.53 34–54 2.03 4.09
7–7;11 51.40 4.69 27–54 2.56 7.65
8–8;11 51.84 3.96 30–54 2.61 8.61
9–9;11 51.68 6.10 22–54 3.30 10.49
Sentence repetition (B)
6–6;11 39.93 11.59 15–64 0.09 0.76
7–7;11 46.58 9.07 26–67 0.09 0.51
8–8;11 47.49 8.92 30–65 0.10 1
9–9;11 50.18 10.19 19–69 0.71 0.11
Total-sentence repetition
6–6;11 90.24 12.30 61–118 0.22 0.27
7–7;11 97.98 8.98 75–115 0.21 0.37
8–8;11 99.33 9.54 71–116 0.29 0.56
9–9;11 101.86 13.86 55–123 1.42 1.73
Total-Arabic language test
6–6;11 170.94 20.90 115–224 0.16 0.01
7–7;11 185.09 15.36 147–217 0.28 0.22
8–8;11 189.55 15.30 149–216 0.30 0.49
9–9;11 192.23 20.63 134–230 1.13 0.73
Nonword repetition task
6–6;11 37.23 7.98 7–48 1.33 2.43
7–7;11 39.23 6.62 16–48 1.27 2.09
8–8;11 38.89 6.66 13–48 1.18 2.02
9–9;11 39.26 7.04 5–48 1.52 4.84
a
Raw scores (z-scores)
82 A. Balilah and L. Archibald
References
Abdalla, F. (2002). Specific language impairment in Arabic-speaking children: Deficits in
morphosyntax. Ph.D. dissertation, McGill University.
Abdalla, F., & Crago, M. (2008). Verb morphology defects in Arabic-speaking children with
specific language impairment. Applied Psycholinguistics, 29, 315–340. https://doi.org/10.
1017/S0142716408080156.
Alahmadi, S. D. (2015). Loanwords in the urban Meccan Hijazi dialect: An analysis of lexical
variation according to speakers’ sex, age and education. International Journal of English
Linguistics, 5(6), 34–58. https://doi.org/10.5539/ijel.v5n6p34.
Al-Akeel, A. (1998). The acquisition of Arabic language comprehension by Saudi children.
Unpublished Ph.D. dissertation, University of Newcastle upon Tyne, UK.
Al-Essa, A. (2006). When Najd meets Hijaz: dialect contact in Jeddah. A paper presented at the
7th International Conference of AIDA, Vienna (Austria), 6–9th September 2006.
Aljenaie, K. (2001). The emergence of tense and agreement in Kuwaiti Arabic children.
Unpublished Ph.D. dissertation, University of Reading.
Al-Tamimi, F. Y. (2011). Assessment of language development in Arabic. Encyclopaedia of
Language and Literacy Development (pp. 1–9). London: Western University.
Archibald, L. M. D., & Gathercole, S. E. (2006). Nonword repetition: A comparison of tests.
Journal of Speech, Language, and Hearing Research, 49, 970–983. https://doi.org/10.1044/
1092-4388(2006/070).
Bishop, D. V. M. (1994). Is specific language impairment a valid diagnostic category? Genetic and
psycholinguistic evidence. Philosophical Transactions of the Royal Society of London Series
B: Biological Sciences, 346, 105–111. https://doi.org/10.1098/rstb.1994.0134.
Bishop, D. V. M. (1997). Uncommon understanding: Development and disorders of language
comprehension in children. East Sussex: Psychology Press.
Bishop, D. V. M., & Edmundson, A. (1987). Language impaired 4-year olds: Distinguishing
transient from persistent impairment. Journal of Speech and Hearing Research, 52, 156–173.
https://doi.org/10.1044/jshd.5202.156.
Bukshaisha, F. (1985). An experimental phonetic study of some aspect of Qatari Arabic.
Unpublished doctoral dissertation, University of Edinburgh, Edinburgh.
Dunn, L. M., Dunn, L. M., Whetton, C., & Burley, J. (1997). The British picture vocabulary scale
(2nd ed.). Windsor: NFER-Nelson Publishing Company Ltd.
El-Halees, Y., & Wiig, E. H. (1999a). Arabic receptive-expressive vocabulary test: Preschool and
school-age. Arlington, TX: Schema Press.
El-Halees, Y., & Wiig, E. H. (1999b). Arabic language screening tests: Preschool and school-age.
Arlington, TX: Schema Press.
Khalaf, A., Ekblom, O., Kowalsk, J., Berggren, V., Westergren, A., & Al-Hazzaa, H. (2013).
Female university students’ physical activity levels and associated factors-a cross- sectional
study in southwestern Saudi Arabia. International Journal of Environmental Research and
Public Health, 10, 3502–3517. https://doi.org/10.3390/ijerph10083502.
Kohnert, K. (2012). Processing skills in early sequential bilinguals. In B. Goldstein (Ed.),
Bilingual language development & disorders in Spanish-English speakers. Brookes: Balti-
more, MD. https://doi.org/10.1111/j.1467-8624.2005.00868.x.
Lieven, E. V. M. (1994). Crosslinguistic and crosscultural aspect of language addressed to
children. In C. Gallaway & B. J. Richards (Eds.), Input and interaction in language acquisition
(pp. 56–73). Cambridge, UK: Cambridge University Press. https://doi.org/10.1017/
CBO9780511620690.005.
Nippold, M. A. (1998). Later language development: The school-age and adolescent years (2nd
ed.). Austin, TX: PRO-ED.
Pence, K. L., & Justice, L. M. (2008). Language development from theory to practice. Upper
Saddle River, NJ: Pearson Education.
The Measurement of Language Ability and Impairment in Arabic-Speaking Children 83
Semel, E. M., Wiig, E. H., & Secord, W. A. (1989a). Clinical evaluation of language fundamentals
(revised). San Antonio, TX: Psychological Corporation.
Semel, E. M., Wiig, E. H., & Secord, W. A. (1989b). CELF-R screening test. San Antonio, TX:
Psychological Corporation.
Semel, E. M., Wiig, E. H., & Secord, W. A. (1995). Clinical evaluation of language fundamentals
(3rd ed.). San Antonio, TX: Psychological Corporation.
Semel, E. M., Wiig, E. H., & Secord, W. A. (1996). Clinical evaluation of language fundamentals
(3rd ed.). San Antonio, TX: Psychological Corporation.
Shaalan, S. (2010). Investigating grammatical complexity in Gulf Arabic speaking children with
specific language impairment. Unpublished doctoral dissertation, University College London.
Sieny, M. E. (1978). The syntax of urban Hijazi Arabic. Beirut, Lebanon: Librairie du Liban.
Wiig, E. H., & El-Halees, Y. (2000). Developing a language screening test for Arabic-speaking
children. Folia Phoniatrica et Logopaedica, 52(6), 260–274. https://doi.org/10.1159/
000021544.
Wiig, E. H., & Secord, W. A. (1989). Test of language competence (TLC) (Expanded edition). San
Antonio, TX: Psychological Corporation.
Wiig, E. H., & Secord, W. A. (1992). Test of word knowledge. San Antonio, TX: Psychological
Corporation.
Wiig, E. H., Secord, W. A., & Semel, E. M. (1992). Clinical evaluation of language fundamentals-
preschool. San Antonio, TX: Psychological Corporation.
Zimmerman, I. L., Steiner, V. G., & Pond, R. E. (1992). Preschool language scale (PLS-3). San
Antonio, TX: Psychological Corp.
84 A. Balilah and L. Archibald
Developing a Formative Assessment
Instrument for an In-service Speaking Course
Rezvan Rashidi Pourfard
Abstract This chapter presents the report of a piece of action research on devel-
oping a classroom-based assessment instrument for a speaking module in an EFL
context. The instrument was designed for formative assessment for a group of
student teachers studying in an in-service ELT course in Iran. The study was
designed to address the problems of the absence of an assessment instrument and
formative assessment in an oral reproduction module. The chapter illustrates the
procedure of developing an instrument and implementing it as a formative assess-
ment tool to promote learning through teacher, self and peer assessment feedback.
The chapter aims at shedding some light on the requirements and components of a
classroom-based assessment instrument. It also demonstrates how the use of the
instrument in the module for the learning purpose benefited the participants both as
learners and teachers. The chapter aims at raising the awareness of novice teachers
and test developers on the basic considerations and challenges of developing an
assessment instrument; it also sheds light on impact of using different types of
feedback on the participants’ learning. The chapter can inspire teacher educators
about how, through loop input, the process of delivering a module can turn into
learning content for student teachers. It provides the reader with the instrument
components included in the appendix, too.
Keywords Speaking • Assessing speaking • Approaches in assessing speaking •
Formative assessment
1 Introduction
Teachers are often asked to assess learners in their classes. Assessment might seem
straightforward when it is a pen and paper task; however, when it comes to testing
speaking, teachers often do not feel confident about how to do the job. Speaking in a
foreign language and its assessment are considered both difficult and essential
R. Rashidi Pourfard (*)
Ministry of Education, 39 Golha, Atash St., Isfahan, Iran
e-mail: rezvan_rashidi@yahoo.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_5
85
skills. As Luoma (2004) states there is a range of factors involved in producing the
oral language that influences the listener’s impression of the speaker’s performance.
Therefore, one of the challenges for teachers is developing criteria that can accu-
rately assess speaking. The challenge becomes more noticeable when another
factor, like presenting content, is added to the language knowledge.
Moreover, over the last decades there has been a shift in the way student learning
and assessment are perceived. Student learning is no longer regarded as a trans-
mission phenomenon but a process in which learners actively construct their
knowledge and skills (Barr & Tagg, 1995). It is believed that to internalize meaning
and make a connection between the new information and what has already been
learnt, students need to interact with the content, transform it and discuss it with
others (Nicol & Macfarlane-Dick, 2006). Parallel to the shift in teaching and
learning conceptions, highlighting the influence of feedback and formative assess-
ment on learning, a new shift in assessment has also emerged. While giving
feedback is mainly considered the teacher’s responsibility, Nicol and Macfarlane-
Dick (2006) argue that students actively assess their own learning and create their
own feedback. As a result, teachers in higher education need to build on this ability
and give students a more proactive role in their learning.
This chapter illustrates how the need for developing a speaking instrument for
formative assessment in an EFL context encouraged the teacher to conduct a piece
of action research. The chapter, then, has mainly adopted a classroom-based
approach in test development rather than a general test construction perspective
to demonstrate how the instrument was developed and used by the teacher for a
specific group of learners. Accordingly, the development and implementation
processes have been elaborated on in the study design part and the instrument
components—the rubric, assessment criteria, rating scale and performance descrip-
tors—have been included in the appendix.
2 Theoretical Background
In this part, first, a concise review of formative assessment is presented. Then,
assessing speaking with a focus on defining speaking is explained. Afterwards,
different approaches in assessing speaking and rating scales are discussed.
2.1 Formative Assessment
Based on the purpose of the assessment and how the results will be used, assessment
within the classroom can be categorized in two main groups of formative and
summative assessment. Formative assessment aims at helping students to achieve
a learning goal and summative focuses on assessing learners’ progress at a partic-
ular point in time (Bloom, 1968; Chappuis & Chappuis, 2008). A review of the
86 R. Rashidi Pourfard
research findings by Black and William (1998) has been a key landmark in the
emergence and recognition of formative assessment as a powerful tool for improv-
ing learners’ learning. Formative assessment specifically intends to generate feed-
back on performance to improve and accelerate learning (Sadler, 1998). Recent
research has also documented the benefits of regular use of formative assessment as
feedback for learning (Black, Harrison, Lee, Marshall, & William, 2004). Assess-
ment for learning involves deepening and furthering learning, (Clarke, 2005),
bringing about a kind of transformation in the learning process (Black, Harrison,
Lee, Marshall, & William, 2003), and involving students in the process of self-
assessments (Stiggins, 2007).
Underpinning the notion of self-assessment was Sadler’s (1989) suggestion of
identifying three essential conditions to make feedback beneficial for students in
academic tasks. He argued that the student must be aware of the good performance
in terms of the desired goals and standards, the relation between the current
performance and the good performance to compare them, and how to act to close
the gap between the current and good performance. To close the gap, he believes
that students need to have some of the evaluative skills their teachers have.
Therefore, it is suggested that teachers share or collaboratively devise the assess-
ment goals and standards with students, particularly in higher education, and
involve them in the self-assessment process (Nicol & Macfarlane-Dick, 2006).
In addition to the teacher and self-assessment feedback, another source of
feedback in educational settings comes from peers. Peer-evaluation helps learners
to become more aware of the learning goals, to learn through evaluation and to learn
from each other. Peer-evaluation is believed to be especially useful in assessing
speaking (Luoma, 2004). Yet, bringing evidence from the research, Nicol and
Macfarlane-Dick (2006) argue that teacher feedback is much more effective in
identifying mistakes or misconceptions in students’ work compared to feedback
from peers or the students themselves.
2.2 Assessing Speaking
As Luoma (2004, p. 29) states, the aim of all assessments is to focus on the right
thing, which provides the basis for construct validity. To develop a speaking
assessment tool, the test developer needs to have a clear view of the speaking
constructs required in the task. In a general definition proposed by Fulcher (2003),
speaking is the verbal use of the language to communicate with others. While
speech is language and governed by the same syntactic and semantic rules, it is
viewed to be different from written language in several ways (Halliday, 1985). The
difference includes sentence structure, informal vocabulary, repetitions and repairs.
Fulcher (2003), however, argues that the difference is noticed more in casual
speaking; in conventional speaking the difference is not as great as it has often
been suggested.
Developing a Formative Assessment Instrument for an In-service Speaking Course 87
Luoma (2004) believes that spoken language is different from written language
and its assessment is a difficult job. One of the challenges is defining pronunciation
and its standards. The sound of speech, or pronunciation, refers to various features
of the speech stream like sounds, speed, pause, stress, intonation, pitch and volume.
Moreover, the distinction between some closely related concepts such as expres-
siveness, naturalness and comprehensibility is not easy to make as the concepts
include some overlapping features. In addition, at the level of the sentence, certain
choices of stress and intonation, though not related to accuracy of word pronunci-
ation, influence the communication and interpretation of the speech. Then, in
developing assessment criteria for pronunciation, the developers should decide on
the features they look for and make sure the task provides them with the information
they need and the criteria include the features. The purpose of the assessment
defines which aspect should be focused on in the task. For example, when the
task focuses on the ability of the test taker to create meaning in discourse, compre-
hensibility is more than accuracy of the individual sounds; the use of stress and
intonation to draw the listener’s attention to the main points is important as well.
Accuracy and fluency are also two commonly discussed concepts in assessing
speaking, which most teachers have an intuitive understanding of them (Fulcher,
2003). Accuracy and fluency are frequently seen as being at opposite ends of a
continuum with the former focusing on communicating smoothly and the latter on
concentrating on building up grammatical rules and accurate production (Brown &
Yule, 1983; Brumfit, 1984). Fulcher (2003) argues that fluency is a much more
difficult concept than accuracy as accuracy can be checked against acceptable rules
of usage. Grammatical mistakes are well-known and available as a performance
standard but fluency is much more fluid and needs some ’speech phenomena’ as
markers. In Fulcher’s view (2003, p. 27), the lack of fluency can be perceived by the
following markers:
• Hesitations: Filled or unfilled pauses
• Repeating syllables or words
• Changing words
• Correcting the use of cohesive devises, especially pronouns
• Beginning in a way that grammar predicts the rest of the sentence but changing
the structure part way through
In general, fluency and accuracy relate to the ‘automaticity of the performance’
and its impact on the listener understanding.
In addition to the mastery of the sound system and speech features, one needs to
have access to appropriate vocabulary and to put words intelligibly in correct order
with minimal hesitation (Luoma, 2004). From an embedded assessment perspec-
tive (Read, 2000) vocabulary knowledge is looked at how one uses vocabulary
within the larger construct of speaking in a holistic context. Sufficient vocabulary
to express oneself clearly counts as an evidence of knowledge of vocabulary
particularly when conveying information is the aim of the task. Assessing gram-
mar among other features seems to be less challenging as the grammatical mis-
takes can be detected easily. Language learning progress is often tracked
88 R. Rashidi Pourfard
according to the grammatical forms that can be produced accurately (Larsen-
Freeman & Long, 1991); they are also well-known and available as a performance
standard.
2.3 Different Approaches in Assessing Speaking
The construct-based and task-based approaches are two general well-known
approaches in the assessment area. The former’s primary focus is on the construct
of general language ability. The latter, however, focuses on using tasks particularly
in professional contexts (Douglas, 2000) and in secondary and tertiary education
when teaching concentrates on a certain type of tasks and the teacher looks for
information about learners’ learning (Norris, Brown, Hudson, & Bonk, 2002).
Speaking tasks, also, are classified in different ways. For example, Brown et al.
(1984, as cited in Luoma, 2004) propose the distinction between talking to chat—
which is interacting with another speaker—and talking to inform—which is trans-
ferring information on a topic and common in teaching and learning situations.
Brown and Yule (1983) make a distinction between different types of information
talk. They state that different information talks follow their own routines of
organizing information for the ease of comprehension. Bygate (1987) divides the
information talk into two categories of faculty-oriented and evaluative talks with
the following subcategories:
• Faculty-oriented talk: Description, narration, instruction and comparison
• Evaluative talk: Explanation, justification, prediction, and decision
Luoma (2004) takes another perspective for defining the speaking construct. She
proposes three main frameworks for construct definition, which are linguistically-
oriented, communication-oriented and situation-based frameworks. The
linguistically-oriented framework is the most familiar concept to teachers. Many
test developers use this view for designing their tasks. Vocabulary, grammar and
pronunciation are typical components of this framework. The target of
communication-oriented concept is the overall communication ability and the
designed tasks within this framework mainly focus on the examinee’s success in
communicating ideas. Situation-based task design fits into the task-based approach,
constructing the task based on the communication needs of the test taker in the
target situation. It is typically used in specific-purpose testing in vocational and
professional education.
Bachman and Palmer’s (1996) model of language analysis—founded on the
earlier theories of communicative competence—is considered an inclusive and
detailed model offering a clear and general description of the communicative
competence components for test development (Bagarić, 2007). With respect to
their model, language use is an interaction between context and other components.
The language knowledge is one of them which consists of two main components of
the organizational and pragmatic knowledge. These two main components
Developing a Formative Assessment Instrument for an In-service Speaking Course 89
complement each other in language use. The sub-components of the language
knowledge are listed below.
1. Organizational knowledge
A. Grammatical knowledge
• Vocabulary
• Syntax
• Phonology/graphology
B. Textual knowledge
• Cohesion
• Rhetorical and conversational organisation
• Imaginative functions
2. Pragmatic knowledge
A. Functional knowledge
• Ideational functions
• Manipulative functions
• Heuristic functions
• Cultural references and figures of speech
B. Sociolinguistic knowledge
• Dialects and language varieties
• Registers
• Natural and idiomatic expressions
The organizational knowledge component of the model, which covers the scope
of this chapter, includes abilities that help the user to recognize and produce
grammatically correct sentences. It also helps them to comprehend their proposi-
tional content. Textual knowledge helps the user to comprehend and produce
spoken and written texts. It includes the knowledge of combining sentences and
utterances into a text, for instance, the knowledge of cohesive ties or the organiza-
tion conventions of starting and closing a talk.
2.4 Rating Scales
Developing a rating scale is considered a complex process as the scale should
indicate what one can do and how well one can do it compared to some verbally
described criteria (Taylor & Galaczi, 2011). A rating scale is an operational
90 R. Rashidi Pourfard
definition of a linguistic construct and a descriptor of the mastery of the linguistic
features and what one can do with the knowledge -like functions and tasks. It also
consists of a series of levels against which a learner’s performance is judged
(Davies et al., 1999).
There are different types of scoring scales to assess speech samples. One
traditional classification is the holistic and analytical rating scales. The holistic
scale expresses an overall expression of the examinee’s ability in one score but the
analytical scale has a few criteria and includes separate scores for each features of
the task. Regarding the focus of the rating scale, it can be real-world or interaction/
ability focused. The focus can be on what test takers can do in the real world or the
ability they need to perform a task successfully (Bachman, 1990). The intuitive and
empirical approaches are also two main approaches taken in rating scale develop-
ment. In recent years, there has been a shift from the intuitive to the empirical
approach—i.e., analyzing samples of real performance to develop assessment
criteria and rating scales rather than relying on the individual or collective experi-
ence and intuition of test developers (Fulcher, 2003). In general, it seems that while
speaking scores are generally believed to express how well a test taker can speak a
language, the development of rating scales is the challenge of describing a complex
phenomenon in a small number of words (North, 1991).
2.5 Research Problem and Rationale of the Study
In order to provide the reader with a picture of the context in which the study was
carried out, a short description of the research context and the module in which the
study was conducted is presented before stating the research problem.
Teacher education in Iran is carried out in different higher education institutions
which include private and state universities. These institutions mainly offer bach-
elor’s and master’s degrees. The minimum required university degree for teaching
at junior secondary school in Iran is an associate’s degree. For the senior secondary
level, he requirement is a bachelor’s. The AA holding teachers who would like to
transfer to the senior secondary school usually take an in-service two-year ELT
course at one of the Teachers University branches to get a BA degree.
In Iran, English is regarded as a foreign language; therefore, both teachers and
students have almost no daily contact with the English language outside the class.
Most of the Teachers University in-service students have learned the English
language in a formal teaching context at teacher training colleges or at university.
Coming mostly from small towns with no experience of living or working in an
English-speaking context, the students usually have a homogenous linguistic and
educational background with low language proficiency and limited presentation
skills.
Developing a Formative Assessment Instrument for an In-service Speaking Course 91
The in-service ELT students take three speaking-oriented modules: A listening-
speaking, an oral reproduction and a story telling module. This study was carried
out in the oral reproduction module. In the oral reproduction module, the students
usually prepare a 15-min oral presentation based on a text which they select from a
book by a local academic publisher. The book covers a range of expository texts
and essays, seemingly, aiming at familiarising students with different text structures
and discourses. The students can also select their texts from other sources; however,
they need to get the teacher’s approval before the presentation regarding the content
and language level of the text. Based on the size of the class, the students may
deliver a whole-class presentation for their final exam marks or they may have two
whole-class presentations which are formally assessed and scored as their mid-term
and final exam marks.
At the time of doing the study, the available course handbook offered no detailed
instructions for delivering and assessing the oral reproduction module. The hand
book said that the students were supposed to deliver a presentation of a selected text
from the book and to be assessed based on their language ability and their success in
text presentation. There were no clearly-stated detailed assessment criteria or any
suggested rating scales to assess the student’s performance. Therefore, the students
were usually assessed by the teacher on a holistic scale.
The second major issue in the oral reproduction module was that the module
received required attention neither from the teacher nor the students; it was con-
sidered a boring unproductive module. The significance of developing content
presentation skills was usually underestimated by the teacher; therefore, the stu-
dents mainly would memorize the texts and present them in a monotonous mechan-
ical way. Consequently, not much learning would take place in the module.
Reflecting on the characteristics of the context, the students’ backgrounds and
needs, and the module objectives, this study aimed at motivating the participants
and involving them in the process of language learning and developing the required
knowledge and skills for content presentation. It also aimed at equipping the teacher
and the students with an instrument to assess the students’ performance and
progress systematically and consistently and providing them with feedback. In
order to achieve the mentioned goals, the following questions were addressed in
the study:
1. How to develop an assessment instrument for the oral reproduction module?
2. How to use the instrument for learning and assessment purposes providing the
participants with consistent feedback?
3. What is the impact of the instrument on the participants’ learning?
92 R. Rashidi Pourfard
3 Method
3.1 Participants
The participants in the study consisted of 15 female student teachers at Teachers
University studying in their second term of the in-service ELT course. They were
all teachers at junior secondary schools with the minimum teaching experience of
1 year. Their age mean was around 30. They all came from homogeneous educa-
tional and language backgrounds.
3.2 Study Design
The study was a piece of action research addressing the two problems of the
absence of an assessment instrument and formative assessment practice in the
oral reproduction module in the ELT course. The study was carried out in two
stages of developing an instrument and using it as a formative assessment tool in the
module. The study design, accordingly, is presented in two parts. In the first part,
the development of the instrument is explained and in the second part the imple-
mentation of it for formative assessment is discussed.
3.2.1 Part One: Development of the Instrument
One phase of the action research involved the development of a speaking assess-
ment instrument for the module. To develop the instrument, it was essential to
select an appropriate approach for assessing the speaking task and to define the task
construct. It was also necessary to develop the assessment criteria, performance
descriptors, rating scale and test rubric. The instrument components were required
not only for the assessment job but for giving the participants a written account of
the good practice and criteria against which their performances and progress were
to be checked (The instrument components are included in the appendix). There-
fore, this part of the study design elaborates on the procedure and the rationale
behind the construct definition, assessment criteria development, rating scale and
performance descriptors production.
Construct Definition and Assessment Criteria Development
Considering the objectives and nature of the oral reproduction module, the task
fitted in the faculty-oriented information talk category based on Brown et al.’s
(1984, as cited in Luoma, 2004) and Bygate’s (1987) model. Although literature
Developing a Formative Assessment Instrument for an In-service Speaking Course 93
offers a few construct frameworks for developing speaking tests, to the knowledge
of the teacher, at that time, no framework thoroughly compatible with the speaking
construct of the module task was available. Therefore, the speaking construct of the
task needed to be defined first. Based on the assumption that the most important
feature of information-related talk is conveying the message (Luoma, 2004), the
language and the content presentation abilities were selected as the two key
components of the task and were included in the assessment criteria. This decision
was also justifiable by the fact that the participants were teachers and they needed to
learn how to present content in their own classrooms in a comprehensible way
highlighting the logical structure of information (Brown & Yule, 1983).
In order to define the sub-components of the construct, a combination model was
adopted for the study. The idea of combining models was inspired by Luoma (2004)
suggesting that for defining a task, teachers as testers can combine different
approaches in their work based on the purpose of the test. Thus, a combination of
Luoma’s communication and linguistically-oriented view and the organizational
knowledge sub-components of Bachman and Palmer’s (1996) model and Fulcher’s
(2003) fluency markers formed the theoretical framework of the task.
Bachman and Palmer’s model with some modifications was used for deciding on
the content presentation subcomponents. The task fitted into the organizational
knowledge and its two sub-categories of grammatical and textual knowledge. In
their model, textual knowledge deals with the learner’s ability to structure their
ideas, to organize utterances and sentences and to make them relevant using
cohesive ties. In Bachman and Palmer’s model, pronunciation is listed under the
category of grammatical knowledge; however, as it seemed that the use of stress,
intonation and volume would help the speaker to highlight the structure of the talk
and draw the attention of the listener to it. The effective use of voice, stress and
intonation was listed in the content presentation subcomponents of the instrument.
For the language component of the task, Bachman and Palmer’s (1996) gram-
matical knowledge was used for the construct definition. The grammatical knowl-
edge in their model includes vocabulary, syntax and phonology, which are widely
recognized features of speaking construct. Their grammatical knowledge compo-
nent is compatible with Luoma’s linguistically-oriented view. Therefore, sufficient
accurate knowledge of grammar to communicate the message completely, the range
of vocabulary related to the text in addition to comprehensibility and fluency were
selected as the subcomponents of the language component. The idea of selecting
comprehensibility and fluency as the pronunciation features of the task was inspired
by Luoma, and Fulcher’s fluency markers. In the task, comprehensibility meant
how the speech at the level of individual sounds, words and sentences sounded
natural and comprehensible in the Iranian context of secondary schools. Fluency
referred to the whole flow of the talk, which included the speed and pauses of the
speech. It should be noted that Fulcher’s fluency markers, in fact, were used to
highlight the features making the speech sound influent.
94 R. Rashidi Pourfard
Rating Scale and Performance Descriptors
After defining the construct and selecting the assessment criteria, the next step was
developing a rating scale. Considering the study aims, using the scale for self, peer
and teacher assessment, the numerical rating scale seemed to serve the purpose
efficiently. Adopting an analytical approach for assessment criteria development
seemed to allow the teacher to assess and give objective feedback on different
features of the participants’ performance. It was assumed that this form of scale
would help the participants to identify and talk about theirs and their peers’
weaknesses, strengths and progress more conveniently. It would also allow the
teacher and participants to give feedback in a more consistent way. The perfor-
mance descriptors of the task were developed at three levels to give the participants
a description of a good, average and poor performance. The performance descrip-
tors were developed to highlight the common language and content organization
features of the task at three different levels of performance. They were also planned
to be used as a written reference document for the participants while assessing the
performances.
3.2.2 Part Two: Implementation of the Instrument for Formative
Assessment
The second part of the study design deals with the implementation of the instrument
as a formative tool in the module. Since the formative assessment aim is providing
feedback to the learner and the source of feedback includes the teacher, peers and
the student, this study included the use of the three types of feedback in its design.
Peer and self-assessment were employed not only as a source of feedback but also
to generate learner involvement, self-confidence and a secure and collaborative
environment for the participants to learn from each other. Moreover, to remove the
evaluative connotation of the term rating scale and to highlight the diagnostic,
learning-based purpose of the instrument, the scale was called a checklist.
As Sadler (1989) suggests, to make self-assessment effective, the learner needs
to be aware of the good performance standards, the relation between the current and
the good performance, and how to close the gap. To close the gap, students need
some of the evaluative skills their teachers have. To equip the participants with
some of the required assessment concepts and skills, a training session was
designed for the participants. In the training session, the teacher drew the partici-
pants’ attention to different text structures and explained about the objectives of the
module, the task and the assessment procedure. She also presented the assessment
criteria and performance descriptors and highlighted the features the participants
needed to focus on while assessing their peers and working on their performances.
After the training session, the participants were divided into groups of three.
Each session they were required to give a presentation in their groups. The group
members were asked to fill out the checklist for their peers’ performances. They
were allowed to compare their checklists and talk to their peers about their rating.
Developing a Formative Assessment Instrument for an In-service Speaking Course 95
The presenter also had to use the checklist for self-assessment. The checklist scores
had no impact on their final marks; nevertheless, all of the checklists were collected
at the end of the session to make the participants take their job seriously and stay
involved in the assessing process. In each session, the teacher joined a group of
participants observing the presentations, filling out the checklist and giving oral
feedback to the presenters in their groups- without marking the presenters’ perfor-
mances. The same checklist was also used for the final summative assessment of the
participants. (The detailed procedure of the instrument implantation has been
illustrated in the rubric in the appendix)
4 Data Analysis
As the purpose of the study was developing an instrument for the formative
assessment and giving feedback to the participants, it made use of two groups of
qualitative data, i.e., the teacher’s observation of the participants’ performances and
the participants’ reflection and feedback.
4.1 Teacher Observation
Comparing the early and the final performances of the participants, the teacher
observed a noticeable improvement in most of the participants’ performances in
terms of the language and presentation skills. In the final presentations, she
observed more accurate and fluent speech, more organized presentation structures
and more confident presenters. Almost no monotonous parrot recitation of the texts
was observed in the final performances. However, the teacher did not observe a
noticeable improvement in the participants’ final performances in terms of their
language ability particularly in the participants with more limited language
proficiency.
Comparing the observed presentations of the participants with the checklists
filled out by the participants, the teacher discovered that in some cases the partic-
ipants’ and the teacher’s feedbacks were not compatible. This phenomenon was
more frequently observed in the early feedbacks but it could still be noticed in the
final checklists of a few participants. While both the teacher’s and participant’s
views mainly agreed on assessing the presentation component, there were some
disagreements on the language sub-component ability. The participants either
overstated or underestimated their own or their peer’s grammar and pronunciation
features. Finally, a distinctly observed phenomenon during the term was a vibrant
class with the participants’ active involvement in the group presentations and
giving feedback process.
96 R. Rashidi Pourfard
4.2 Participant Feedback and Reflection
Two sets of participants’ reflection data were collected in the study, i.e., oral class
reflection during the term and a written reflection sheet at the end of the term.
During the term, an oral reflection time was included at the end of each session and
the participants were encouraged to share with the whole class: What they liked in
the session, what they learned, and what they still needed to learn and work on. The
teacher kept a note of the participants’ reflection in each session.
In terms of what they liked in the session, the early daily reflections included
the module design, class atmosphere, working in groups, presenting in a small
group and teacher’s feedback. In the final sessions’ reflections, the early items
were still present along with practicing self- and peer-assessment. Concerning
what they learned, the early data mainly included the content presentation fea-
tures and the language subcomponents. In the final sessions of the term the
participants started to mention that they learned how to test their students’
speaking and how to use group work, self and peer assessment in their own
classes. The class reflection data revealed that at the beginning of the term the
participants felt they needed to work on almost all of the features of the language
and content presentation components, with the pronunciation features as the most
frequent and grammar as the least frequent sub-components. At the end of the
term, the language subcomponents and effective use of voice, stress and intona-
tion were still among the features that were perceived needing improvement.
Presenting the main idea and clear beginning and ending, however, were no
longer observed in this list.
The final reflection sheet also contained the three parts of what they liked in the
module, what they learned or made an improvement on, and the area they still
needed to work on. Among the features the participants liked, the data included
group work, class procedure, class atmosphere, their progress in the module,
developing self-confidence, self-assessment, peer assessment and teacher feedback.
The data concerning what they learned consisted of speaking, oral presentation of a
text, different text structure and organization of information, organizing ideas in a
logical way, general presentation skills, assessing speaking, group work skills and
reflection. The collected data illustrating what the participants perceived they still
needed to work on included the speaking skill in general, pronunciation features
(stress and intonation), expanding their active vocabulary. They also wrote that they
needed to learn more about different text structures.
5 Discussion and Implications
Reviewing the collected data, the teacher researcher came across some note-worthy
points. Primarily, the major concern of the researcher was defining the construct
and selecting an appropriate framework and rating scale to develop a valid and
Developing a Formative Assessment Instrument for an In-service Speaking Course 97
reliable formative instrument. Nevertheless, the teacher observed some disagree-
ment between the teacher’s and the participants’ self and peer assessment feedback
based on the developed checklist.
The finding can be explained in the light of the teacher and participant percep-
tions of the language ability or assessment criteria. Research indicates that individ-
uals are different in their conceptions of learning (Hidri, 2015), knowledge and
assessment (Entwistle & Entwistle, 1992; Vermunt & van Rijswijk, 1988). Addi-
tionally, research already reports mismatches between the student and the teacher
conceptions of assessment standards (Hidri, 2015), goals and criteria (Nicol &
Macfarlane-Dick, 2006). Doughill (1987) also puts an emphasis on the personal
perceptions as the key elements in evaluation.
It seems that there should have been more interaction and negotiation between
the teacher and the participants on the assessment criteria and performance assess-
ment to bridge the gap between their perceptions and feedbacks. Nicol and
Macfarlane-Dick (2006) state that good feedback practice encourages teacher and
peer dialogue around learning. Based on the research review, they suggest the
student should be provided with written documents of assessment criteria and
performance descriptors. Yet, research indicates that written or verbal explanation
in class is not enough to give students a good grasp of the concepts. The teacher,
then, has to provide students with exemplars of performance. Exemplars of perfor-
mance explicitly illustrate the standards against which the students can compare
performances. Therefore, a possible explanation for the mismatch between the
teacher and the participants’ feedback is that the teacher’s explanations about the
criteria and performance descriptors in the training session were not enough to
bridge the conception gap. The teacher could have shown exemplars of different
levels of performance, or she herself could have presented three performances in
the class to show good, poor and average performance samples. This way, the
participants could have been able to compare the performances against the success
criteria and discuss it with the teacher. Additionally, if they had received a few
training sessions during the term on the performance levels and assessment criteria,
had compared their assessment with the teacher’s and had discussed the discrep-
ancies, feedback consistency would have been increased.
Moreover, the observed mismatch can be interpreted by the fact that the con-
struct definition and success criteria were developed by the teacher and the partic-
ipants were not involved in the process of development. The participants, who were
teacher students, and other teachers teaching the module could have been involved
in the instrument development process. The criteria and the procedure could have
also been reviewed by the participants at the end of the term for possible revisions
and modifications. Nicol and Macfarlane-Dick (2006) also encourage the teacher
and student collaboration to devise or negotiate assessment criteria for a task.
Collaborative test development results in a deeper understanding of the construct
and creates a more valid and reliable instrument. O’Sullivan and Green (2011)
stress that if an instrument is aimed at reflecting test takers’ needs and interests, the
test-takers have to get involved in the development and revision stages. Therefore,
it can be concluded that for developing a test, novice test developers need to be
98 R. Rashidi Pourfard
aware of the significance of the test takers’ objectives and their views of construct
definition and success criteria development.
In addition to developing an instrument, another objective of the study was using
the instrument to promote the participants’ learning. As a result, the three types of
teacher, peer and self-assessment were employed to serve a number of learning-
related purposes such as providing comprehensive feedback, involving the partic-
ipants, and establishing a secure and collaborative environment. The observation
and the reflection data indicated that participant involvement and building a secure
atmosphere were successfully achieved. The teacher observed a significantly
vibrant class during the term where the participants were actively working in
their groups giving and receiving feedback.
Regarding receiving feedback, the participants liked the three types of feedback
and found them beneficial. They mentioned that the checklist helped them with self-
assessment. It also made them more confident with giving feedback to their group
member and made the task more achievable for them. Literature also indicates that
checklists help learners to give and receive a more structured and concrete feedback
(Luoma, 2004).
The teacher’s observation indicated that the participants were more willing to
receive the teacher’s feedback rather than their peers’ as a reliable source of
feedback on their performance, particularly at the beginning of the term. Peer and
self-assessment were more appreciated nearly at the end of the term. The partici-
pants’ daily reflection data also supported this perception. The findings support
Nicol and Macfarlane-Dick’s (2006) argument that the feedback from the teacher is
much more effective in identifying mistakes or misconceptions in students’ work
compared to the feedback from peer or self-assessment. It seems that by giving and
receiving feedback through peer assessment practice in groups, the participants are
more likely to develop self-confidence with assessing skills and learn in an inter-
active environment.
The collected data indicated that the participants also appreciated the use of self-
assessment more significantly in the late sessions of the term. Nicol and
Macfarlane-Dick (2006) state that students already monitor the gap between their
internally set goals and the outcomes they achieve; therefre, teachers can turn this
self-monitoring capacity into a systematic process by using self-assessment and
reflection activities. Taras’s research (2003) indicat that self-assessment integrated
with the teacher and peer feedback in the process of assessment significantly
benefits the learner in the higher education. The findings might imply that different
types of feedback promote learning in different ways and at different levels.
In terms of the participants’ overall assessment of their learning, the reflection
sheet data indicated that the participants had learned and wanted to learn more
about the language component of the speaking skill. The data can indicate that
reflecting on their performances, the participants gained some awareness about the
good performance standards and were also able to notice and monitor their progress
in the module. This finding can be supported by Sadler’s (1989) argument that
there should be three essential conditions to make feedback beneficial for the
learner. The three essential conditions include awareness of the desired
Developing a Formative Assessment Instrument for an In-service Speaking Course 99
performance, the current performance relation to the desired one, and how to act to
close the gap between them.
The teacher observation data also indicated that the participants’ presentation
ability improved more significantly than their language ability. The findings can be
explained by the fact that English is a foreign language in Iran and the participants’
contact with English is typically limited to the classroom setting. Learners, then,
need more input and practice to get a mastery over the language components of
speaking which is a complex skill. Apparently, developing content presentation
skills seems to be more achievable than developing language skills during a
term time.
The class reflection data revealed that pronunciation features and vocabulary
were listed as the most frequent language components which the participants
perceived they needed to practice and improve. Concerning grammar, it seemed
that the participants’ perception of their grammar knowledge changed as they
progressed in the term. In spite of the observed grammatical mistakes in the early
sessions, the participants mainly overestimated their and peers’ accuracy of
grammar; however, in the final sessions more participants could identify the
grammatical mistakes in their own and their peers’ performances. The raised
awareness might be the result of the group discussions, careful comparison of the
performances, or their focus on the grammatical points to fill out the checklist. It
might have also been the result of focusing on the functions and noticing the
forms that fulfil the functions of the text structures.
In terms of the content presentation, the data of the participants’ reflection
revealed that they learned about text structures and how to organize information
in their presentations. They particularly gained confidence in beginning and
finishing a presentation and expressing the main idea. However, they still felt
they needed to practice on the content structure of their presentations and receive
more instruction on various types of expository texts. The findings can be
explaibed by the fact that certain learning tasks, like identifying and presenting
the main idea, are not as cognitively demanding as recognizing the logical
relationship residing in the text and reproducing it in their presentations in their
own way; consequently, the participants felt they needed more input and practice
to get a mastery over them.
In addition to the findings related to the study aims, an interesting point noticed
in this study was the emergence of some data in the last sessions’ oral reflections,
which was also remarkably observable in the final reflection sheets of the majority
of the participants. It was that the participants mentioned they learned how to
reflect, how to assess their student’s speaking and how to use group work effec-
tively in their own classes. The finding can be interpreted in the light of
Woodward’s (1988) loop input that teachers learn about a concept when they
experience the process of learning it and reflecting on it. The participants seemed
to have gained an awareness of how to assess their students’ speaking, how to use
group work and how to reflect systematically by experiencing them as a learner. It
can, then, be implied that teacher educators have to be more aware of their teacher
students’ needs both as a learner and a teacher. They have to make a link between
100 R. Rashidi Pourfard
the content and design of the ELT courses and student teachers’ real needs in their
contexts through loop input. By providing teachers with an opportunity to experi-
ence concepts, techniques and models as a learner and to reflect on the experieince,
they are more likely to develop deeper awareness about learning.
6 Conclusion
This study aimed at addressing the problem of the absence of an assessment
instrument and formative assessment in an oral reproduction module in an
in-service ELT course. The devised instrument was intended to promote learning
through providing different types of feedback, using teacher, peer and self-
assessment. The findings of the study indicated that the instrument had a positive
impact on the participants’ learning both as learners and teachers. The findings
highlight the role of the tertiary education, in general, and teacher education
institutions, in particular, in developing and implementing various assessment
tools and practices to help learners develop required knowledge and competencies.
The findings of this study draw the attention of researchers and test developers to
the practical limitations of the study regarding construct definition, success criteria
development and test scope. The findings can also highlight the grounds for further
research. Further investigation is needed to explore the validity and reliability of the
devised instrument in this study. The construct and success criteria need to be
reviewed and revised by the participants and other teachers. One area of investiga-
tion can be the use of exemplars and exploring their impact on the students’ learning
and on the mismatch between the teacher and student feedback. Moreover, the
developed instrument is a generic tool aiming at assessing the presentation of
different expository texts. Although a generic model seemed to sufficiently serve
the purpose of this study, specific instruments can be developed for assessing
presentation of specific text structures. Furthermore, the design of the study did
not allow the researcher to figure out to what extent the findings of the study were
the result of the developed instrument or the implementation method. More inves-
tigation is required to find out the impact of using the instrument on the partici-
pants’ learning without the use of group work, self and peer assessment.
Furthermore, the study was conducted on a small group of female teacher students,
different results might be attained with different target groups. Additionally, the
participants might not have expressed all of their opinions and feedback in the
reflection sheet and orall reflections. Using other data collection tools and
approaches may result in obtaining data different from what has been collected
through the scope of this research.
Developing a Formative Assessment Instrument for an In-service Speaking Course 101
Appendix 1
The Rubric
Oral Reproduction Task
You have to choose one of the texts in your book ‘Oral Reproduction’ or a similar
text from other sources. However, if you do not select a text from the book, you
need the teacher’s approval in advance before preparing the talk. There might be
more than one person choosing a certain text; however, it is a good idea to put your
names and your selected texts on the paper on the class notice board to let the whole
class see your choices. You are also free to change your choices.
You need to prepare a 15-min talk about your text. The use of visuals is encour-
aged but not obligatory. You are put in groups of three and you need to tell your group
members about the content of the text in a comprehensible and fluent way. The task is
a presentation, not a memorization task. It aims at preparing you to develop presen-
tation skills and the ability to get the message across in an organized, understandable
and fluent way. You will be assessed based on a checklist which you can find at the
bottom of this page. You can also find an outline of the key features this test focuses
on as well as a description of a good, average and poor performance. The checklist
and descriptors cover the points discussed in the class.
The checklist helps you to get prepared and to focus on the points you need to
attend in your presentation. You need to assess your performance after your presen-
tation based on the checklist by putting check marks in it. You also need to assess the
performance of other students in your group. There is also a space for your comments
if you would like to add anything. After each presentation, you have 2 minutes to fill
the checklist for your or group members’ presentations. You can share your views
with your group members to see to what extent you are in agreement with them in
your assessment or you can keep it for your own. After filling up the checklists, you
have about ten minutes to give and receive oral feedback in your groups. All of the
checklists are collected at the end of each session. You need to write your name and
the name of the presenter on the paper. The presentations and checklists have no
impact on your final mark; however, they can help you to develop your presentation,
assessment and giving feedback skills.
The teacher occasionally joins your group to observe your presentation; how-
ever, you are supposed to be focused and stay involved in your groups without the
teacher’s presence. You might have one of the members as the watch-keeper
managing the time in your group. All of you are observed twice by the teacher
during the term and receive oral feedback on your presentations, as well as the
checklists of your presentation completed by the teacher.
There is a 15-min whole-class feedback and reflection time at the end of each
session, in which you can share your ideas. You can ask for any clarification or help
before, after or during the class while the teacher is not observing a student.
Good luck and happy presentation.
102 R. Rashidi Pourfard
Appendix 2
The Instrument
A. Assessment Criteria
The purpose of this talk is to help you have an organized clear and fluent oral
presentation based on the following standards and criteria:
Appropriate use of language and making oneself understood through
• Good knowledge of relevant vocabulary
• Sufficient accurate knowledge of grammar
• Clear comprehensible pronunciation
• Fluent stream of speech with appropriate speed and pauses
Clarity of presentation structure through
• Highlighting the main idea and the supporting ideas
• Organizing information in a logical and easily comprehensible order at the level
of the sentence and text
• Signalling the structure of the presentation using discourse markers and repeti-
tion to make the organizational structure noticeable to the listeners.
• Having a clear introduction/start and conclusion.
• Using voice quality, stress and intonation to highlight the relationship between
the parts of the text and to convey the message effectively
There are other factors such as using body language, gestures, and visual
support which are not directly related to the nature of task but enhance the
effectiveness of the presentation.
B. Performance Descriptors
A good performance would reflect presenters’ ability to express themselves com-
prehensibly in terms of intelligible pronunciation, relevant vocabulary, sufficiently
accurate grammar and effective use of intonation and stress. Presenters’ perfor-
mance would show evidence of their ability of presentation strategies of opening
and ending, Information structuring and organization. The presenters would make
effective use of their voice, cohesive and discourse markers to show the relationship
between the presented pieces of information.
A poor performance, on the other hand, would show little evidence of language
ability which is the use of a limited range of words and frequent grammatical
mistakes. The pronunciation would need a lot of correction. Stress and intonation
are not used to convey meaning and content structure. The presentation would not
follow a logical order and it would be difficult to get the main idea, the supporting
elements of the content and the conclusion of the presentation.
Developing a Formative Assessment Instrument for an In-service Speaking Course 103
An average performance would contain some apparent pronunciation and gram-
matical mistakes which would not lead to complete breakdown of the communica-
tion. There would be some distracting stress and intonation features but the message
structure would get be conveyed. The range of the used vocabulary would be
limited but would convey the meaning. Text structure and organization of the talk
would seem to be not smooth but the whole picture of the content and the
concluding points would generally be understood.
C. Rating Scale
Oral Presentation Checklist Date:
Presenter’s name: Observer’s name:
Look at the checklist and decide to what extent the speaker (or you in your
presentation) has been successful in showing the following factors in her
presentation.
Note: Number 1 means the lowest and number 5 means the highest.
A Language 1 2 3 4 5 Comments
1. Clear comprehensible pronunciation
2. Fluent speed and pause
3. Good knowledge of relevant vocabulary
4. Sufficient accurate knowledge of grammar
B Content Comments
1. Clear main and supporting ideas
2. Logical structure and sequencing
3. Organizational structure signalling
4. Clear beginning and ending
5. Effective use of voice, stress and intonation to highlight
points
Appendix 3
Reflection Sheet
Reflecting on the oral reproduction module in this term, answer the following
questions.
• What did you like in the oral reproduction module?
• What did you learn in the module/what areas did you get confident in?
• What areas do you still need to work on or receive help and support from your
teacher on?
104 R. Rashidi Pourfard
References
Bachman, L. F. (1990). Fundamental considerations in language testing. Oxford: Oxford Univer-
sity Press.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice. Oxford: Oxford University
Press.
Bagarić, V. (2007). Defining communicative competence. Methodik, 8(1), 94–103.
Barr, R. B., & Tagg, J. (1995). A new paradigm for undergraduate education. Change, 27(6),
13–25.
Black, P., Harrison, C., Lee, C., Marshall, B., & William, D. (2003). Assessment for learning:
Putting it into practice. Maidenhead: McGraw-Hill/Open University Press.
Black, P., Harrison C., Lee, C., Marshall, B., & William, D. (2004). Working inside the black box:
Assessment for learning in the classroom. Phi Delta Kappan, 86, 9–21. Retrieved from http://
www.pdkintl.org/kappan/kappanhtm
Black, P., & William, D. (1998). Inside the black box: Raising standards through classroom
assessment. Phi Delta Kappan, 80, 139–148. Retrieved from http://www.pdkintl.org/kappan/
kbla9810.htm
Bloom, B. S. (1968). Learning for mastery. Los Angeles, CA: University of California press.
Brown, G. A., & Yule, G. (1983). Teaching the spoken language. Cambridge: Cambridge
University Press.
Brumfit, C. J. (1984). Communicative methodology in language teaching. Cambridge: Cambridge
University Press.
Bygate, M. (1987). Speaking. Oxford: Oxford University Press.
Chappuis, S., & Chappuis, J. (2008). The best value in formative assessment. Educational
Leadership, 65, 14–19. Retrieved from http://www.ascd.org
Clarke, S. (2005). Formative assessment in action. London: Hodder Murphy.
Davies, A., Brown, A., Elder, C., Hill, K., Lumley, T., & McNamara, T. (1999). Dictionary of
language testing. Cambridge: Cambridge University Press & ULES.
Doughill, J. (1987). Not so obvious. In L. Sheldon (Ed.), ELT textbooks and materials: Problems
in evaluation and development (pp. 29–37). London: Modem English Publications and the
British Council.
Douglas, D. (2000). Assessing languages for specific purposes. Cambridge: Cambridge University
Press.
Entwistle, N. J., & Entwistle, A. C. (1992). Contrasting forms of understanding for degree
examinations: The student experience and its implications. Higher Education, 22, 205–227.
Fulcher, G. (2003). Testing second language speaking. Hong Kong: Pearson Education Limited.
Halliday, M. A. K. (1985). Spoken and written language. Geelong: Deakin University Press.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Larsen-Freeman, D., & Long, M. H. (1991). An introduction to second language acquisition
research. London: Longman.
Luoma, S. (2004). Assessing speaking. Cambridge: Cambridge University Press.
Nicol, D., & Macfarlane-Dick, D. (2006). Formative assessment and self-regulated learning: A
model and seven principles of good feedback practice. Studies in Higher Education, 31(2),
199–218.
Norris, J. M., Brown, J. D., Hudson, T. D., & Bonk, W. (2002). Examinee abilities and task
difficulty in task-based second language performance assessment. Language Testing, 19(4),
395–418.
North, B. (1991). Standardization of continuous assessment grades. In C. Alderson & B. North
(Eds.), Language testing in the 1990’s (pp. 167–177). London: Modern English Publications &
the British Council.
O’Sullivan, B., & Green, A. (2011). Test taker characteristics. In L. Taylor (Ed.), Examining
speaking: Research and practice in assessing second language speaking. Studies in Language
Testing (Vol. 30, pp. 36–65). Cambridge: UCLES & Cambridge University Press.
Developing a Formative Assessment Instrument for an In-service Speaking Course 105
Read, J. (2000). Assessing vocabulary. Cambridge: Cambridge University Press.
Sadler, D. R. (1998). Formative assessment: Revisiting the territory. Assessment in Education, 5
(1), 77–84. Retrieved from http://proquest.umi.com
Sadler, D.R. (1989). Formative assessment and the design of instructional systems. Instructional
Science, 18, 119–144.
Stiggins, R. J. (2007). Classroom assessment for student learning. Upper Saddle River, NJ:
Pearson Education, Inc.
Taras, M. (2003). To feedback or not to feedback in student self-assessment. Assessment and
Evaluation in Higher Education, 28(5), 549–565.
Taylor, L., & Galaczi, E. (2011). Scoring validity. In L. Taylor (Ed.), Examining speaking:
Research and practice in assessing second language speaking. Studies in Language Testing
(Vol. 30, pp. 171–233). Cambridge: UCLES & Cambridge University Press.
Vermunt, J. D. H. M., & van Rijswijk, F. A. W. M. (1988). Analysis and development of students’
skill in self-regulated learning. Higher Education, 17, 647–682.
Woodward, T. (1988). Loop-input: A new strategy for trainers. System, 16(1), 23–28.
106 R. Rashidi Pourfard
Part II
Assessing Writing
Does the Feedback Feed Forward? Student
Response to and Views of Teacher Feedback
in an EFL Academic Writing Class
Eddy White
Teachers need to view feedback from the perspective of the
individuals engaged in the learning.
(Hattie & Temperley, 2007, p. 101).
Abstract Effective, timely feedback is a fundamental aspect of formative assess-
ment. The core of formative assessment has been defined as comprising two
actions: The student must recognize that a gap exists between the current perfor-
mance and the desired one; and the student must engage in effective action to close
the gap. Student knowledge and action in being aware of and attempting to close the
performance gap are seen as pivotal in the feedback process. This chapter, based on
a third-year EFL Academic Writing class at a university in Japan, focuses on how
students respond to feedback provided on the first draft of a research essay, and also
how they feel about the feedback process itself. The report compliments an initial
research study (White, 2017) comprised of a teacher self-assessment of the types of
feedback provided on 21 essay first drafts. This related report looks at how students
responded to the feedback provided in their subsequent final draft. The report com-
pares the first and final drafts of student essays to make some determination of
whether the feedback fed forward into a better final draft. The report also includes a
student survey to ascertain student views on the effectiveness of the teacher feed-
back provided in helping them produce a better final draft for summative assess-
ment. Examples of student writing, the feedback provided, and subsequent changes
made are included in the report. The report concludes that, on the whole, student
final drafts were stronger, better and more clearly written due to student response to
the feedback provided. The feedback survey, completed by 23 students, also
showed an overall high level of satisfaction with the feedback framework used in
this EFL Academic Writing class.
E. White (*)
University of Arizona, 5751 North Kolb Road, Unit 22-201, Tucson, AZ 85750, USA
e-mail: ewhite3@email.arizona.edu
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_6
109
Keywords Formative feedback • Assessing writing • Formative assessment •
Student views of feedback
1 Introduction
In the assessment of student learning, a distinction is often made between the
summative and formative uses of assessment. Whereas summative assessment
involves a focus on forming a retrospective picture of what has been learnt,
formative assessment has as its primary purpose to immediately guide what stu-
dents and teachers will do next (Green, 2014). In the past 20 years or so, this
distinction has also been commonly expressed in the assessment literature and
teaching profession by referring to summative assessment as assessment of learning
(AoL), and formatively purposed assessment as assessment for learning (or AfL).
Relatedly, as noted by Gareis and Grant (2015) the positive role of formative feed-
back in supporting student learning has long been established.
Based in an EFL context in a university writing course, this formative assess-
ment report deals with student response to teacher guidance, in the form of written
feedback, as well as their views of the feedback provided. On an essay-writing task
involving multiple drafts, student actions (i.e., revisions) to close the gap between
current and desired performance in response to teacher feedback are the focus here.
This report is a companion piece to White (2017), a book chapter entitled, Teacher
self-assessment of feedback practices in an EFL academic writing class—a reflec-
tive case study. This formative feedback component occurred in a third year
academic writing class at a university in Japan. While White (2017) essentially
examined the types and amount of teacher feedback provided to students on the first
draft of their essays, this sister report focuses on how students responded to the
feedback provided, what they did with it in revising their essays, and how they felt
about the formative feedback process implemented by the teacher (gleaned from an
end-of- course survey).
A summary of the primary report, as well as contextualizing the setting for the
investigation at a Japanese university, is necessary here. Situated in a third-year EFL
university academic writing class in Tokyo, the White (2017) report involved a
teacher self-assessment of written feedback provided on 21 student essay first drafts.
A close analysis of the more than 800 feedback interventions documented on those
essays (e.g., use of correction code, teacher commentary) showed a number of
patterns of practice in giving feedback, including: extensive use of questions in
teacher commentary, very limited use of praise comments as well as varying amounts
of feedback provided on student essay drafts. Results of that report also showed the
feedback practices discovered through that investigation aligned well with recom-
mended best practices in providing feedback, as summarized in the seven feedback
conditions that influence learning by Gibbs and Simpson (2004–2005).
The two related feedback investigations, in White (2017) and here, both hap-
pened in an academic writing class for third-year Japanese university students
110 E. White
called Junior Composition (hereafter, JC). This is a two-semester course, JC I and
JC II, with 16 classes per semester. In my particular JC class, students were required
to write three essays in the first semester and two in the second, all with linguistics-
related subject matter. For all essays, students were required to write both a first
draft and final draft. The first draft of the essays was submitted and formative feed-
back provided on them. These drafts were returned, and students had 1 week to edit
and revise before submitting the final version. This version was then summatively
assessed and given a final grade. In the weekly 90-min classes students were given
instruction and carried out tasks related to both the particular linguistics topic being
focused on in the essay task, and to academic essay writing itself (e.g., writing
effective thesis statements, paraphrasing sources). Figure 1 shows the seven steps in
a typical cycle of essay writing and feedback in the JC course.
All JC essays followed the same seven-step pattern, before proceeding to the
next essay topic. Brown and Glover (2006, p. 43) refer to such a writing-feedback
cycle as ‘the performance-feedback-reflection-performance-feedback loop’.
While that initial White (2017) report primarily dealt with the first draft of the
student research essays, and the teacher feedback provided on them (Step 4 in the
feedback cycle above), this companion report looks at the second (i.e., final) version
of the essays and compares the two drafts to examine the changes students made
based on teacher feedback (Steps 5 and 6 in the feed cycle above). The essay task
described here is the fourth essay this group of third year students had written in the
JC course (over two semesters). This particular research essay was required to be
7-800 words in length, and it focused on the linguistic topic of slang. Students were
provided with three related essay prompts of which they chose one for their
MLA-formatted research paper.
1. Instruction,
students
become
familiar with
source material
(3/4 classes)
2. Essay
topics
provided,
considered,
decided
(week 1)
3. Students
submit first
draft of
essay
(week 2)
7. Teacher
final draft
feedback,
essay graded,
(summative
assessment)
6. Students
submit final
draft of
essay
(week 4)
5. Students
degin
editing and
revising
(week 3)
4. Teacher
first draft
feedback
(formative
assessment)
Fig. 1 JC essay writing and teacher feedback cycle
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 111
2 Theoretical Background
2.1 Feedback and Formative Assessment
Effective, timely feedback is a fundamental element of formative assessment; “the
two concepts of formative assessment and feedback overlap strongly” (Black &
Wiliam, 1998a, p. 47). Considered one of the central components of AfL theory and
practice, feedback is “defined in terms of “closing the gap” between actual and
desired performance” (Stobart, 2006, p. 141). Influenced by the seminal writings of
Sadler (1989), the core of formative assessment was defined by Black and Wiliam
(1998b) as comprising two actions: The student must recognize that a gap exists
between the current performance and the desired one; and the student must engage
in effective action to close the gap. Student knowledge and action in being aware of
and attempting to close the performance gap are seen as pivotal in the feedback
process. While the teacher acts as stimulator and guide to action in closing the gaps
in performance, the work (and the learning) must be done by the student. Mean-
while, students’ perceptions of assessment are impacted by teachers’ conceptions
and practices of assessment (Hidri, 2015).
2.2 Student Response to Feedback: Brief Literature Review
Most feedback research focuses on ‘the input side of the equation’, with much less
focus on how students interpret and deal with the feedback (Polos & Mahoney,
2008). Ellery (2008) summarizes the sometimes conflicting evidence in the
research literature in regard to how students respond to feedback: Feedback is
often misunderstood; it is often not read; it may be read but not acted upon; and
it sometimes has no effect on student learning. According to Lee and Schallert
(2008) studies of second language writing show that ESL students were willing to
follow closely the feedback provided by teachers, but such commentary “had the
potential of miscommunicating and of being misunderstood” (p. 165). Weaver
(2006) contends that students’ intellectual maturity and previous experience both
play a large part in their learning and as a result the extent to which they are able to
engage with teacher feedback.
With ESL and L2 writing, feedback is more positively viewed. It is recognized
that L2 writers want and benefit from effective teacher feedback and produce better
final products in a multiple-draft writing class. Wojtas (1998) reported that many
students improved their work after understanding the purpose of feedback and the
assessment criteria being used. In looking at writing research involving multiple-
draft classrooms, Lee (2008) concluded that the research evidence shows teachers
feedback comments are attended to, and students believe such feedback helps their
writing improve. In an extensive review of the formative feedback literature, Shute
(2008) reported “formative feedback has been shown in numerous studies to
112 E. White
improve students’ learning and enhance teachers’ teaching to the extent that the
learners are receptive and the feedback is on target (valid), objective, focused, and
clear” (p. 182). Shute (2008) concluded that the research evidence indicates that if
feedback is delivered correctly, learning processes and outcomes can be improved
significantly.
2.3 Feedback as Feedforward
Following in the work of earlier writers on the feedback issue (for example, Black
& Wiliam, 1998a; Ramaprasad, 1983; Sadler, 1989) Hattie and Timperly (2007)
provide an effective framework for considering feedback, explaining that the
purpose of feedback is the reduction of discrepancies between current understand-
ings/performance and the desired goal. Feedback becomes feedforward when it is
“forward-looking so that it can improve students’ learning and enhance their future
performance on assessed tasks” (Carless, Joughin, Ngar-Fun, & Associates, 2006,
p. 12). Thus, feedforward means providing useful information to the student, while
it still matters, that will help them recognize where gaps in performance (and
learning) are, and to use that information to close the gaps and move forward.
Feedback that feeds forward can enhance students understanding and achievement
level. Students learn faster and much more effectively when they have a clear sense
of how well they are doing and what they might need to do in order to improve
(Carless, 2006). In terms of ‘closing the gap’ between actual and desired perfor-
mance students are provided a clear sense of the width of the gap, and how to
minimize it (and ideally to eliminate it altogether).
2.4 Rationale of the Study
Lee (2007) described four essential conditions required in order to use feedback to
promote assessment for learning (i.e., formative feedback):
1. Students are told about their strengths and what needs to be done in their
writing—e.g., areas for improvement, in terms of content, organization, lan-
guage, etc.; the assessment is prospective.
2. Information is communicated clearly and made intelligible to students in terms
of what they have learnt, hence a close link between teaching, learning and
assessment.
3. Students act on the teacher feedback and are provided with opportunities to
improve their learning based on the teacher feedback.
4. Students play an active role in managing their own learning. (p. 182)
This investigation primarily looks at condition 3, and how students act on the
teacher feedback provided. What do they do with the feedback to improve their
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 113
essay drafts and close the gaps between current and desired performance on this
research essay task?
This report will address the following two questions related to both the impact of
the feedback provided and student perceptions of it:
1. Are students producing better writing based on the teacher feedback provided?
(i.e. Are gaps being closed?)
2. How do they feel about the feedback provided in the JC course?
Answers may be provided to these questions by examining changes students
made to their initial drafts, and also to a student survey. We begin by examining
student responses to first draft feedback and the changes that resulted in the final
draft. Some forty examples of student response to the teacher ‘feedback interven-
tions’ (or FI’s) are presented and provide, a ‘before (feedback) and after (feedback)’
picture.
3 Method
This report is based on the collection of two data sets: Hard copies of one student
essay (both first and final drafts) that had been assessed (both formatively and
summatively) by the instructor/researcher, and (2) the feedback survey. This
section and the following to two sections related to methodology, data analysis,
and discussion will be divided into two parts to focus on these data sets individually
(i.e., Part 1—Essay Feedback and Student Response, Part 2—Feedback Survey)
3.1 Part 1: Essay Feedback and Student Response
Midway through the second semester of the JC course, I told students that I was
interested in examining the feedback that I was providing to them on their essays. I
asked students if they would return to me the essay set (both first and final draft) of
the previous assignment. Of 23 students in class, 21 returned to me their essay sets.
As noted in White (2017), my teacher feedback on student essays came in three
forms:
1) Use of indirect correction code symbols;
2) Direct marginal teacher comments; and,
3) An overall feedback report attached to the back of the essay.
In terms of correction code symbols, the formative feedback follows the now
commonly accepted practice of using a correction code indicating the location and
type of errors or other problems with the writing. Research suggests that the use of
such a correction code is effective in stimulating student response and developing
114 E. White
self-editing strategies (Hyland, 2003). Student first drafts were marked using the
following codes shown here in Table 1.
It should be remembered that by the time of this essay (focused on the linguistic
topic of slang), students had already written three essays in which this coding
scheme was used on first drafts. In the first semester, students became quite familiar
with the correction symbols (and were provided with a hard copy of the coding
scheme), and needed simply to be quickly reminded about them for deciphering
feedback on this slang essay. Students were also reminded why it would be more
effective if they could fix their own errors rather than having the teacher do
so. These particular symbols arose from the most commonly used in the course
resource book, shortened and simplified so as not to overwhelm students with
excess correction coding on their essay first drafts. Based on the three forms of
teacher feedback provided (i.e., correction code symbols, marginal comments on
essay, formative feedback report attached to first draft), students were given 1 week
to revise their essays and submit them for summative grading.
The initial White (2017) report closely examined the teacher feedback provided,
identifying and classifying 839 example of feedback interventions on 21 first drafts,
by looking at only the first draft of student essays. In order to complete this picture
and examine the issue of feedforward, this report involves looking at both the first
and final drafts of student essays, and examining the changes made to the final draft
based on the draft-one feedback.
3.2 Part 2: Feedback Survey
In the final class of the Junior Composition course, I administered a student survey
about the first draft feedback process in place. During this class, students also
submitted the final draft of their last essay for summative assessment. This was to
be graded and picked up by them at the departmental office 1 week later. Following
the typical writing/feedback cycle for the course, the first draft of this final essay
would have been returned to students in the penultimate class. So, in the week
before completing the feedback survey, students had engaged in their last cycle of
responding to teacher feedback in the JC course.
In completing the survey, which only deals with first draft feedback, students are
reflecting back throughout the academic year and are provided with the opportunity
for expressing their views not on individual first draft feedback, but on the feedback
Table 1 Correction symbols used on essay first draft feedback
? ¼ meaning not understood 0 ¼ missing word
R ¼ rewrite (awkward, non-standard English) pl ¼ plural
g ¼ grammar problem cap ¼ capitalization
ww ¼ wrong word sp ¼ spelling
wf ¼ wrong word form ¼—join together (sentences)
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 115
process as a whole and their views of it. Note that 23 students responded to the
survey, while only 21 essay sets were returned to the teacher for research purposes
(with two students not returning their essays). With these contextual factors in play,
the student survey was administered at the end of the last class, and took students
about 10 min to complete.
4 Results and Discussion
While there are a number of levels at which research can be categorized and
analyzed, it is the responsibility of the researcher to demonstrate that methods
and findings are credible and important (Duff, 2001). One useful model for cate-
gorizing research methodology is the model proposed by Grotjahn (1987). In his
model, three different aspects of the research process can be distinguished:
1. The method of data collection (whether through experiment, or
non-experimentally).
2. The type of data (qualitative or quantitative).
3. The type of analysis (statistical or interpretative).
Following this model, this report may be described as non-experimental, pri-
marily qualitative, and utilizing interpretative data analysis. Grotjahn (1987) labels
such research as falling within an ‘exploratory-interpretative’ paradigm. While
having some quantitative elements, the feedback report may be broadly categorized
as a qualitatively oriented inquiry (Cumming, 2004).
4.1 Part 1: Essay Feedback and Student Response
Examining student response to feedback by comparing first and final essay drafts
How did students respond to the feedback interventions (FI’s) that I provided?
Were they able to close the gaps between current and desired final performance?
Did the teacher feedback feed forward to better student writing in the final essay
draft? Examples of student writing provided in the tables below help provide
answers to these questions.
In this report, a successful response to a feedback intervention is considered as
one which clarified the writers’ intended meaning, or otherwise improved the
content and structure of the text. An unsuccessful response would be considered
final draft writing that did not achieve these results of clarity or message improve-
ment in the final version of the essay. This section, and Appendices, will present
examples of both successful and unsuccessful revisions by students.
In White (2017), an average of 39 feedback interventions (FI’s) per script were
provided on the 21 first drafts of student essays. An overall total of 828 such
interventions were documented, including correction code symbols, commentary
116 E. White
and other FI’s. In this section, 40 responses to teacher feedback are provided
(including 15 examples in the Appendix) as a representative sample of students’
attempts to close the writing gaps between initial (first draft) and desired perfor-
mance (final draft). Table 2 shows one such example of a revised thesis statement.
The numerical correction code symbol (1) indicates that I thought this to be a
good quality thesis statement. Two other correction code symbols are provided here
(ww ¼ wrong word, R ¼ rewrite) on the first draft. (Note-I wonder why I did not
interject any revision request for the phrase ‘from adult’). The better, more clearly
written final draft version is considered a successful response to the FI’s provided.
The above example shows one instance of a student ‘closing the gaps’ to produce an
improved thesis statement in the final draft. We will examine some other
gap-closing attempts by students, but first a brief introductory note is required
about the distinction between ‘global’ and ‘local’ errors or concerns in student
writing.
In her book entitled Treatment of Error in Second Language Student Writing,
Ferris (2002) makes the distinction between “global errors—those that interfere
with the overall message of the text—and local errors, which do not inhibit a
readers’ comprehension” (p. 57). In broad terms, global-level concerns deal with
content and overall argument and paragraph structures, while local-level concerns
deal with sentence structure and grammatical problems. In looking at student
response to teacher feedback, we make note of this global/local distinction.
Table 3 shows some examples of local errors being successfully revised in the
final draft in response to the teacher feedback, the FI, on the first draft. The
examples come from eight different students/essay sets.
Examples 1 and 2 in Table 3 are related to the mechanics of proper MLA
documentation formatting, and considered a local rather than global issue. The
local writing concerns presented in Table 2 are relatively minor items, and of a
more grammatical- and lexical-level type “that do not impede understanding” of the
text (Ferris, 2002, p. 22). However, one of the four criteria which students are
evaluated on in the assessment rubrics (both formative and summative versions) is
Table 2 Closed gaps thesis statement revision from one student (cursive script ¼ teacher feedback)
First draft
Final draft
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 117
‘language usage’. Through oral feedback to the class, students were reminded that a
preponderance of such local errors in their final drafts would have a negative impact
on the author/researcher, indicating a less than thorough editing/revising of the
final draft and would negatively impact their essay grade.
Upon being given the topics for writing the first drafts of their essays, students
were told that in the first draft they should focus on content and organization of
ideas, while grammatical and other language use problems could be focused on for
the final draft. The written feedback provided to students covered both global and
local errors/concerns, but particular emphasis was provided to the more serious
writing issues that led to communication problems with the reader. In her exami-
nation of error treatment in L2 student writing, Ferris (2002) stressed the need for
teachers to prioritize students’ most frequent errors, and global errors: “Errors that
should receive the greatest attention should include serious (‘global’) errors that
interfere with the comprehensibility of the text” (p. 22). Below we pay particular
attention to student attempts to close gaps dealing with such more serious global
concerns related to content, organization and which lead to communication break-
down with the reader. In this JC essay, due to the emphasis placed on proper MLA
documentation of sources used, and the care needed to avoid suspicions of plagia-
rism, use of such documentation (or neglecting to use it) was considered a more
serious global concern.
In Table 4 we can see some examples of students ‘closing the gaps’ and
successfully revising global problems in their first drafts, to produce clearer and
more comprehensible writing (i.e., expression of meaning) in the final draft. It
should be noted that the ten examples of global errors shown in Table 3 are taken
Table 3 Closed gaps local errors successfully revised
First draft
Feedback
intervention (FI)
Final draft revision (response to
feedback)
1. “It is generally . . .” (Alan 100) R, use family
name
“It is generally . . .” (Gardiner
100)
2. Slang is sometimes considered to
be rebellious and subversive
(Gardiner)
page # needed Slang is . . . subversive (Gardiner
102)
3. Frompkin et al. (1998) definite that sp, ? Frompkin et al. (1998) say that
one reason . . .
4. . . . feelings like regrettable or wf [word form] . . . feelings like regret or . . .
5. Slang has strong power to make a
great expression on readers and . . .
ww [wrong
word]
Slang has strong power to make a
great impression on readers and
. . .
6. If people do not use slang, they
sometimes isolated
g [grammar
problem]
If people . . . they are sometimes
isolated
7. Slang is useful and familiar to us R, avoid per-
sonal pronouns
like this
Slang is useful and familiar to
people
8. . . . slang itself, who uses it and
where is used
0 [missing word/
phrase]
. . . slang itself, who uses it and
where it is used
118 E. White
from ten different essays. Also the feedback primarily consists of marginal com-
mentary/questions (rather than just the correction code type seen in Table 3).
An additional set of 15 examples of successful revision of global issues can be
found in the Appendix. It should be noted that this total collection of 25 examples of
successful revisions of global concerns uses all 21 essay sets, and is representative
Table 4 Closed gaps: Global errors successfully revised
First draft Feedback intervention (FI)
Final draft revision
(feedback response)
1. Reliability of slang
[essay title]
? Which essay question are
you answering?
Negative image caused by
slang
2. What are the main reasons
why the use of slang is so
popular and commonplace?
[thesis statement]
A thesis statement should
not be a question
There are three features of
slang which affect its
popularity
3. On the other hand, there are
many slang disappear. For
example, Japanese word
‘naui’ means modern
R + ‘Use an example from
English, not Japanese
On the other hand, many
slang words disappear. For
example in England, ‘bob’
was a slang expression for
shilling
4. From these reasons, it is
concluded. . .
[concluding paragraph]
Repeat the reasons again
here for reader
By examining its origin,
character, words and
expressions, it is concluded
. . .
5. Since it is absolutely free to
make and use slang, . . .
What do you mean? It is true that everyone has a
chance to produce and use
slang, . . .
6. A possible reason why slang
. . . expressions disappear
rapidly
Source? Citation needed A possible reason why
slang . . . expressions dis-
appear rapidly (Ross 105)
7. . . . so people who do not
know the background can-
not understand
What background? . . . so people who do not
know the background of the
group or subculture or gen-
eration cannot understand
8. Even though there is such
risk, people use it
What risk? Even though there are such
risks as some people not
understanding or having a
negative impression, peo-
ple use it
9. The reason some people
have a negative reaction to
slang is because slang does
not gain its social position
? The reason some people
have a negative reaction to
slang is because slang is not
suitable to serious settings
10. In addition, young people
. . .
[This section continues for
seven lines of text]
This is a somewhat different
issue from your main point.
You should revise this part.
Or delete it
In brief, slang is informal
and can be mean as com-
pared to standard language
[Edited to become one sen-
tence, and off-topic infor-
mation is removed]
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 119
of the JC class as a whole. However, not all final draft revisions were successful nor
did the feedback intervention feed forward to students producing writing of better
clarity or other improvements. Gaps remained unclosed and therefore problematic.
Table 5 provides some examples of unsuccessful revisions which did not clarify the
writers intended meaning or otherwise improve the content or structure of the
message. These are taken from seven different student essay sets (i.e., first/final
draft).
4.2 Part 2: Feedback Survey
The 12-item feedback survey asks students about such things as how they viewed
and responded to the teacher feedback provided. The feedback survey was com-
prised of two main parts, followed by a third open-ended part for any additional
comments about the feedback process.
The survey items were devised after doing some preliminary background read-
ing about the feedback issue, and prominent related ideas in the literature (for
example, understanding feedback, use of correction codes, emotional reactions to
feedback). In the survey results presented here, I simply use descriptive measures of
response numbers and percentages to identify general features of student attitudes
toward the feedback process.
Table 5 Open gaps: Unsuccessful revisions
First draft
Feedback
intervention
Final draft revision (response to
feedback)
1. Slang is Considered Corrupt Words
[essay title]
R Slang is Considered Corrupt
Words except for specific situation
2. In the following sentences, some
main characteristics are shown
[thesis statement]
R Therefore, . . . it has some main
features and the following
sentences show some of them
3. However, if they use slang that
involves rebellion against it, they
will not endanger themselves,
because the slang is more rebellion
of language
Confusing However, if she uses slang, she can
express the same meaning as the
word by it because it replaces the
word
[Still confusing]
4. . . . he may feel that he will be
extreme and use slang
? . . . he may feel that he will take the
fashion in advance and use slang . .
. [Revision also marked with a ‘?’]
5. For these reasons, the use of slang
. . .
[concluding paragraph]
Repeat the
reasons here
for the reader
For these reasons, the use of slang
. . .
[No change]
6. (Introductory paragraph is four lines
long)
Intro is too
short
[No change, virtually the same
contents and length]
7. However, it is notable. . .
(Wikipedia)
Direct quote? [No change in final version]
120 E. White
In both parts of the survey, two response formats are used. One format presents a
statement and asks for students’ level of agreement, with a Likert scale of four
options available along a response continuum (agree, tend to agree, tend to dis-
agree, disagree). This format enables students to take a position, while allowing
them to express some degree of reservation (tend to agree/disagree) if they wish to
do so. The other, more common, format used a five-point Likert scale in the survey
simply asks students to indicate a frequency response on an always—never contin-
uum. The one survey item which differs from these two formats is the final item (#
12). This asks the student to rate the first draft feedback system on a four point
continuum from poor to excellent. Survey items and descriptive statistics of their
responses are presented below.
4.3 Survey: Part 1
Part 1 of the survey sought to ascertain student views of the correction code used
and the teacher commentary provided. Table 6 presents the survey items and
student responses. The first three items in part one deal with the correction code
symbols used, while the remaining items and responses documented in this table
focus on the issue of teacher written commentary. Responses in this table (as well as
in the following Table 7) are simply presented in numerically, followed by a
percentile breakdown.
4.4 Survey: Part 2
In this section students were asked to express their views on the first draft feedback
system as a whole. The survey sheet reminded students that the ‘feedback system’
includes all three elements; correction code, teacher comments, and feedback end
sheet. Four items (numbers 9–12) are included in this section and student responses
are presented in Table 7.
4.5 Part 1: Essay Feedback and Student Response
The examples of student response to teacher feedback provided in this report may
be viewed as ‘snippets of dialogue’ in which the teacher is responding to students’
original writing, and then the student responds to the feedback intervention.
Through the FI’s provided here, students were told that there were some points of
communication breakdown or lack of meaning clarity as a result of what they had
written, and therefore signaling the need for a revision of the text. These examples
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 121
from Tables 3 and 4 show students successfully responding to my FI’s with
improved final draft writing.
But what about the unsuccessful revisions that the data analysis also identified?
How can such unclosed gaps be accounted for? As noted by Lee and Schallert
(2008), teacher commentary has the potential of being miscommunicated or mis-
understood. Example 7 from Table 4 is perhaps a good example of me
‘miscommunicating’ the intended message. A clearer, more explicit FI from me
would be something like this: ‘Direct quote? If so, be sure to include quotation
marks’. Lea and Stierer (2000) noted that a possible reason for unsuccessful
Table 6 Survey Part 1 correction code, teacher comments (N ¼ 23)
1. I understand why the teacher used a
correction code rather than just fixing
my essay problems/mistakes.
agree
always
2. From the correction code used, I
could understand the type of
errors/problems in the essay.
3. I had trouble knowing how to fix
problems/mistakes marked with a
correction code.
4. Teacher comments on the first
draft were clearly written and easy
to read.
5. The meaning of comments was
clear and easy to understand.
6. Any negative comments about my
writing made me feel a bit upset or
hurt.
7. The feedback end sheet helped me
understand the strong and weak parts
of my first draft.
8. Generally, the number of teacher comments written on essay first drafts were:
too many = 1 (4%) appropriate = 21 (92%) too few = 1 (4%)
13 (57%)
agree
0
6 14 0 0
0
0
0 0
0
0
7 11
10 (43%)
1 (4%)
(26%)
9
(39%)
4
(17%)
9
(39%)
13
(57%)
4
(17%)
(61%)
11
(48%)
4 (17%)
3 (13%)
3 (13%)
1 (4%) 14 (61%)
16 (70%) 6 (26%) 1 (4%)
1 (4%)
(30%) (48%)
0 0
usually sometimes seldom never
tend to
agree
tend to
agree
tend to
disagree
tend to
disagree
disagree
disagree
122 E. White
revisions may be the potential discrepancy between the intended meaning of
feedback and the student interpretation of it. Student difficulty in ‘making sense’
of teacher comments has also been noted by Norton and Norton (2001). It should
also be remembered that these JC students are L2 writers with various levels of
proficiency, and may not have the writing competence to successfully revise and
clarify intended meaning. This factor may help explain the ‘unclosed gaps’
remaining in examples 1, 3 and 4 in Table 5. For other reasons, for example, time
pressure or consciously choosing to ignore the FI, teacher commentary is not
addressed and the message remains unchanged. Examples 5 and 6 in Table 5, in
which the final and first drafts are the same despite the FI provided, may be a
reflection of this fact.
Despite the challenges and difficulties students sometimes had in improving the
intended meaning of their messages, more often than not they were able to suc-
cessfully revise their writing and produce better work for the final draft of the slang
essay. Most revisions linked to the feedback provided did lead to text improvement
in the final draft, supporting empirical research that feedback in multiple-draft
Table 7 Survey Part 2 feedback system overall (N ¼ 23)
9. The feedback system made it
clear what I needed to do to
improve my first draft.
10. The feedback system gave me
enough advice in how to write a
better final draft.
11. I ignored (did not use( first draft
feedback when editing/revising for
the final draft.
12. In terms of giving clear, useful feedback
for helping to write a better final draft, the
first draft feedback system for this Junior
Composition course was:
(30%) (61%)
15 (65%)
8 (35%)
8 (35%)
2 (8%)
2 (8%)
13 (57%)
0 0
0
0 0
0 0 9 (39%) 14 (61%)
7 14
agree
always
poor average good excellent
usually sometimes seldom never
agree
tend to tend to
disagree
disagree
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 123
classrooms does lead to improved student writing (see for example, Ferris, 1997,
2006; Hyland & Hyland, 2006; Lee, 2008). JC students were serious about improv-
ing academic writing skills and becoming better writers. They worked hard to
produce the best possible final drafts for the graded summative assessment. While
there are examples of ‘unclosed gaps’ in the student essay data set, without a doubt
the 21 essays improved through the drafting process, sometimes significantly so,
through students effective editing and revising practices in responding to the
feedback provided.
The feedback interventions I provided were signals and signposts for first
draft editing and revising, but it was the high quality of student engagement
with the feedback, ‘the crucial variable’ as Gibbs (2006) wrote, that produced
better final drafts. Active student engagement was essential in closing the
gaps indicated by the FI’s provided. Taras (2002) captures well the key AfL idea
that the active involvement of students is required for true learning from feedback
to occur:
[Feedback] does not count as formative feedback unless the student has understood what
the purpose of the assessment was, how it was assessed or judged, and how they can use
their shortfalls in the future. Even this is not sufficient. Formative feedback is not complete
until the students have produced an equivalent piece of work where the issues have been
addressed and remedied (p. 506).
In terms of this JC class, students: (1) understood the assessment purpose;
(2) knew how their work would be judged; (3) were provided with feedback
indicating shortfalls and how to strengthen their first drafts; and (4) they produced
a related piece of work (the final draft) where issues were usually addressed and
remedied to produce a better final product. The first draft feedback provided to
students was usually successfully acted upon, feeding forward to future work- an
improved final essay draft.
Despite some weaknesses in both my feedback responses and students attempts
at revision, the data set collected and examined shows the feedback did feed
forward and helped students successfully improve their first drafts. As such, this
leads to an affirmative answer to the research question posed in the introduction as
to whether gaps were closed in the quality of student writing in response to the
feedback provided.
While the formative assessment of student first drafts were not graded (following
recommended AfL practice), and therefore cannot provide a direct point of com-
parison, it may be useful here to briefly record the students’ summative grades for
the slang essay. These grades are provided in Table 8. Final drafts were sum-
matively graded using a criterion-referenced rubric with four core criteria of
content, organization, language use, and use of source material.
Table 8 Summative grading breakdown for slang essay final draft (N ¼ 21)
A+ A A B+ B B C+ C C D F
0 6 8 2 3 1 1 0 0 0 0
124 E. White
As a class, the slang essays received quite good grades. The formative feedback
provided to students, and (just as importantly) their active engagement with it to
close the gaps between the quality of first and final drafts, were key factors in the
generally high final grade results achieved.
4.6 Part 2 Feedback Survey
In the feedback literature, there is a body of research related to student views,
primarily from one-off surveys concerned with student preferences and expecta-
tions. But as Lee (2008) notes much of this research is decontextualized and “there
have rarely been any attempts to link student reactions to actual teacher feedback in
specific contexts” (145). This report of student perceptions does deal with actual
feedback in a specific context, and these views come at the end of a year-long JC
course in which students received formative feedback (and summative grades) on
five different research essays.
This discussion of the feedback responses are related to the data presented in
Tables 6 and 7 for the 12-item survey. In survey item 1, the majority of students
(16 of 23) showed a clear understanding of why a correction code was used, rather
than me directly correcting their errors or other problematic aspects in the first-draft
texts. As noted previously, Gibbs and Simpson (2004) highlight the importance of
feedback being understandable to the student. Survey item 2 reported that 22 of
23 students could either always (39%) or usually (57%) understand the type of error
or problem indicated from the correction code used.
However, recognizing that a problem exits and fixing it (i.e. closing the gap) are
not the same. In terms of closing the gaps between the actual performance (first
draft) and the desired performance (final draft), survey item 3 is an important one. A
combined 78% of students (18 of 23) indicated that they either usually (17%) or
sometimes (61%) had trouble knowing how to fix problems with their writing. One
student reported always having trouble knowing how to fix problems indicated by
correction code symbols and close the gaps.
Again, the context of an L2 writing class must be kept in mind here, and one of
the reasons why teaching writing is so challenging is that most classes contain a
mixture of students (Kroll, 2001). This mixture of writing skills and English
language proficiency was evident in this JC class with some students showing
weaknesses in both areas as they “struggle to make meaning in a foreign language”
(Hyland & Hyland, 2006, p. 207). In a finely crafted phrase, Pintrich and DeGroot
(1990) tell us that “Pupils need to have both the ‘will’ and the ‘skill’ to be successful
in classrooms” (p. 53). While all JC students had the motivation (will) to become
better essay writers (as they consider the looming senior year graduation thesis),
their writing ability and English language proficiency (skill) did vary. Some
students were more skillful than others in revising and reshaping their texts,
negotiating and producing clearer meaning messages for the reader. Yet, as
shown in the summative grades in Table 5, on the whole students were able to
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 125
make successful revisions on the final draft of their essays, a fact reflected in the
generally good grade breakdown depicted at the end of the previous section. As
determined in White (2017), an average of 20 correction code symbols was
provided per essay, and some degree of difficulty in successfully responding to
these could be expected in even the more proficient writers in the JC class.
Items 4–8 on the survey deal with the teacher written commentary provided in
the body of student first drafts. For item 4, a total of twenty students reported that
teacher comments were always (9 students) or usually (11 students) clearly written
and easy to read. Related survey item 5 deals not with the clarity of presentation of
my feedback commentary, but the meaning the comments intended to convey about
closing the gaps and fixing the indicated problem. A similar total percentage in the
always/usually categories, 87%, was reported as item 4, with 20 students saying that
they could always (6 students) or usually (14 students) easily understand the
meaning of the comments that I wrote on their first drafts. Three students reported
sometimes having problems understanding feedback comment meaning. As men-
tioned, feedback is ‘an act of communication’ and item 5 responses signal to me
that there is room for improvement in providing comments which are “consistent,
clear, helpful, and constructive” (Hyland & Hyland, 2006, p. 223).
Survey item 6 attempted to tap into some of the affective elements at play in the
feedback process by inquiring whether students felt ‘a bit upset or hurt’ by any
negative comments I provided on their first drafts. Less than half (48%) responded
never here, indicating that 12 students had experienced some degree of such upset
or hurt feelings. A combined 21% of students reported usually (one student) or
sometimes (four students) having such reactions. Responses to this item are a
reflection of the fact that assessment is a process rife with emotion and because
students put their time and themselves into the assessment tasks we set, the feed-
back we provide engages them on an emotional level (Lee, 2007). Hyland and
Hyland (2006) write, “Our comments can transform students’ attitudes to writing
and lead to improvements, but our words can also confuse and dishearten them”
(p. 223). While it is necessary to remain aware of the potential damaging effects of
feedback commentary, on the whole, the number of students reporting never or
seldom feeling upset or hurt was 78%.
A key component of effective feedback is letting students know the main
strengths and weakness of the draft they have written. Among the feedback process
described here, the feedback end sheet in particular was intended to show these
characteristics of the first draft. In survey item 7, 13 students (57%) reported that the
end sheet helped them understand the strengths and weaknesses of their first draft.
Ten students checked the tend to agree response here indicating some degree of
reservation and perhaps showing that the end sheet, and my commentary on it,
could have done a better job of doing this. However, no students disagreed with
item 7 and this fact reveals a degree of effectiveness for the end sheet in the
feedback framework for the course.
In calculating the amount of feedback commentary provided to students, I noted
with surprise that some essays contained from 15 to 20, or more, of my written
comments (the mean average was 13). I began to wonder whether I was providing
126 E. White
excessive feedback, something that can be as much of a problem as too little
response. However in item 8 on the survey sheet, only one student reported the
number of comments as being too many, with the same singular response to the too
few comments option. Responses showed that 92% of students indicated that the
number of teacher feedback comments provided was appropriate. The responses to
this item show that for almost all JC students the quantity of feedback written
commentary was neither excessive nor deficient.
With regard to closing the gap between the first draft performance and the
desired final draft, item 9 is important in asking students if the feedback system
made it clear what they needed to do to improve the first draft (i.e., close the gap).
While 65% agreed, eight students seemed to express some reservations here, opting
for the tend to agree response. This seems to indicate that there is room for
improvement here in terms of my feedback clarity and specificity with regard to
making improvements. However, none of the 23 students expressed disagreement
with item 9, indicating that all students, at least to some degree, knew what they
needed to do to ‘close the gaps’.
Item 10 on the survey is a continuation of this theme, asking students whether the
feedback system provided enough advice for closing the gaps and writing a better
final draft. Only 35% of students (8 of 23) agreed that they received enough advice
about how to write a better final draft. More than half (13 students) chose the tend to
agree response, while two students disagreed with the statement. Perhaps the
message here is that, while I am providing plenty of advice about what the gaps
are, students seem to be reporting that the teacher feedback could do more to tell
them how to close the gaps. One student commented as follows on the survey: “I
think first draft feedback is very useful for me to improve my essay and to make it
better, but I want you to give me more detailed advice for improving”. Perhaps there
is room for more clarity to individual students about the specifics of improving their
first draft, but there is an obvious conflict here between student numbers, the
amount of feedback provided (three different types in this system) and the speci-
ficity of diagnostic advice. Prioritizing specific advice for making improvements to
the first draft may be a possible solution here.
Item 11 asks student if they ever ignored my feedback when editing and revising
the first draft. Almost all students (91%) responded never or seldom here, providing
an indication of student earnestness in closing gaps and producing better final
drafts. Two students responded that they sometimes ignored feedback, presumably
because it involved miscommunication, they did not know how to make a revision,
or even being pressed for time in completing the final draft. Hyland (2003) provides
some useful commentary about students’ desires and uptake of teacher feedback:
It is also important to note that what individual students want from feedback-and
the use they make of it-varies considerably. Some students want praise, others see it
as condescending; some want a response to their ideas, others demand to have all
their errors marked; some use teacher commentary effectively, others ignore it
altogether. It can be difficult for teachers to cater to all these different perceptions
and expectations . . . (p. 180). The difficulties in catering to all students’ needs and
wishes to everybody’s satisfaction should be noted when considering student
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 127
responses to feedback; and indeed when considering their responses to this feed-
back survey.
The final survey item asks students to offer an overall assessment of the JC first
draft feedback system in terms of giving clear, useful feedback to write a better final
draft. Of the 23 students survey, 14 (61%) considered it to be excellent, while the
remaining 9 students (39%) evaluated it as a good system. The key argument in
Sadlers’ (1989) view of feedback, and fundamental to AfL, is that the power of
feedback comes from closing the gap between where the students are and where
they are aiming to be. Responses to survey item 12 shows that, while there may be
room for improvement, students feel that the feedback system is a very good one; a
powerful feedback response framework in enabling them to close the gaps in their
writing and produce their best work for the final drafts of the JC essays. Part 3 of the
survey asked if students had any additional comments. Only 5 of the 23 students
made a brief comment, including the one recorded above in the discussion of item
10. There was nothing particular noteworthy in the commentary, all included some-
thing to the effect that the feedback system was ‘good’, ‘useful’ and ‘effective’.
According to Lantolf and Pavlenko (2001) students are active agents in the
feedback process and “construct the terms and conditions of their own learning”
(p. 145). The 23 students in this Junior Composition class worked hard in
constructing their own learning and endeavoring to become better academic writers
in English. As Hyland (2002) reminds us, “fundamentally, writing is learned, rather
than taught”, and while the feedback system in place provided signals of gap
identification and closing guidance, it was primarily the student’s efforts that
resulted in gaps being closed and better-written final essay drafts being produced.
5 Conclusion
As noted, there is a need for more published accounts like this one of how student
actually respond to teacher feedback on their writing. An obvious way of doing this,
and the methodology followed here, is by having access to and comparing students
initial and final drafts of their submitted work. Actual examples can be drawn from
comparing the drafts of when gaps in learning are being closed or remain open.
Such investigations into whether the feedback is feeding forward helps promote
knowledge in the feedback and classroom assessment areas that are so important to
our language teaching profession. More importantly perhaps, it helps feed into the
teacher-as-learner paradigm in the classroom, and such research will help indi-
vidual teachers become more effective practitioners of their craft. Relatedly, more
contextualized survey research is needed to see how students feel about the feed-
back that is provided to them, and whether it helps them improve upon the texts
they are engaged in revising and editing. Such survey results can also inform
teaching practice and effectiveness.
128 E. White
In Rethinking Foreign Language Writing, Scott (1996) observes that “Writing is
clearly not a simple act, but rather an intricate set of steps and choices” (p. 31). To
this perceptive statement we can add that neither teacher provision of feedback or
student response to it are simple acts. As this report and the research it describes
reiterates, a formative feedback process also involves ‘an intricate set of steps and
choices’ by both the provider and receiver of these informational responses to the
work produced.
This complimentary report to White (2017) examined the feedback issue
from the student side of the coin. This part focused on two issues: whether the
feedback provided feeds forward to the closing of gaps in student writing and the
production of an improved final draft; and student perceptions of the first draft
feedback framework implemented in one academic writing course. In examining
whether student achievement levels were being enhanced by the feedback, the first
and final drafts of student essays were compared to examine whether gaps were
being closed. While examining student responses to the more than 800 feedback
interventions documented showed examples of remaining problems in the final
drafts (‘unclosed gaps’), on the whole student final drafts were stronger, better and
more clearly written due to student response to the feedback provided. The first
draft feedback survey, completed by 23 students, also showed an overall high level
of satisfaction with the JC feedback framework, despite some indications that the
feedback provided has room for improvement (for example, in more detailed advice
in how to make improvements).
The feedback framework and processes described in this report reveal a
learning-centered course in which all participants, including this teacher/researcher,
collaborate and learn together by “using feedback as a pedagogical tool for improv-
ing the teaching and learning of writing” (Lee, 2007, p. 180). Thus, the simple
response to the question posed in this chapter title is yes, the feedback did feed
forward.
Appendix: Closed Gaps: Additional Global Errors
Successfully Revised (N ¼ 15)
First draft
Feedback
intervention Final draft (response to feedback)
1. To show the authority of the word
is to show how the word is
prosperous
? Deleted from final draft
[Oral class feedback included my
teaching point that deleting
something can sometimes be a
better writer decision that
attempting revision]
2. Therefore slang is a good means to
express their thinking, feeling, and
insistence freely
? Therefore . . . thinking, feeling and
emotion freely
(continued)
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 129
First draft
Feedback
intervention Final draft (response to feedback)
3. In other words, the attraction of
slang and the inseparable connec-
tion between people . . .
What is this
connection?
As stated in this essay, the attrac-
tion of slang and its capacity to
strengthen the solidarity among
group members . . .
4. Because of slang’s fertility, people
. . .
What does this
mean?
Slang has a large amount of
words, so it is possible to express
one subject in several ways.
People . . .
5. Slang has mainly four characteris-
tics which cannot see in the Stan-
dard one
[thesis statement]
R There are mainly four character-
istics for slang that shows its
essence
6. Finally, slang is made up of many
purposes of usages
R Finally, slang has some important
characteristics
7. Slang is over flowed in the world R Generally, slang is widely used by
many people
8. The second one is that it is infor-
mal, alive, and rebellious. [Para-
graph topic sentence]
Second what?
þ Meaning?
(alive)
The second characteristic is that it
is informal and rebellious
[‘Characteristic’ has been
inserted. ‘Alive’ deleted]
9. Therefore, the use of slang may
give interest to our communication
R That is to say, it makes commu-
nication more interesting
10. In this way, slang is a kind of
trending . . .
? Is this a
word?
In this way, slang is a kind of
popularity that people use . . .
11. In conclusion, slang is flourished
by people. People use it for many
reasons
? In conclusion, people use slang for
many reasons
12. As has been mentioned, it was
marijuana smoker who use the
slang word ‘weed’
Why mention
this again
here?
[Deleted]
13. The most important reason is that
the use of slang can give people
the opportunity to maintain their
belongings
Meaning what? The most . . . slang can give peo-
ple the opportunity to enter a
group and make a bond between
their group members stronger
14. There are some reasons why the
use of slang . . . [thesis statement]
How many will
you discuss?
Tell the reader
There are three main reasons why
the . . .
15. By understanding the meanings of
slang members can make their
community easy and deepen
familiarity
? By understanding the meanings of
slang, members can make their
communication easy and. . .
130 E. White
References
Black, P., & Wiliam, D. (1998a). Assessment and classroom learning. Assessment in Education:
Principles, Policy and Practice, 5(1), 7–74.
Black, P., & Wiliam, D. (1998b). Inside the black box: Raising standards through classroom assess-
ment. Phi Delta Kappan, 80(2), 139–148.
Brown, E., & Glover, C. (2006). Evaluating written feedback. In C. Bryan & K. Clegg (Eds.),
Innovative assessment in higher education (pp. 81–91). New York: Routledge.
Carless, D. (2006) Differing perceptions in the feedback process. Studies in Higher Education,
31(2):219–233
Carless, D., Joughin, G., Ngar-Fun, L., & Associates. (2006). How assessment supports learning.
Hong Kong: Hong Kong University Press.
Cumming, A. (2004). Broadening, deepening, and consolidating. Language Assessment Quarterly,
1(1), 5–18.
Duff, P. (2001). Research approaches in applied linguistics. In R. Kaplan (Ed.), The Oxford hand-
book of applied linguistics (pp. 14–23). London: Oxford University Press.
Ellery, K. (2008). Assessment for learning: A case study using feedback effectively in an essay-
style test. Assessment and Evaluation in Higher Education, 33(4), 421–429.
Ferris, D. R. (1997). The influence of teacher commentary on student revision. TESOL Quarterly,
31(2), 315–339.
Ferris, D. (2002). Treatment of error in second language student writing. Ann Arbor, MI:
The University of Michigan Press.
Ferris, D. (2006). Does error feedback help student writers? New evidence on the short-and long-term
effects of written error correction. In K. Hyland & F. Hyland (Eds.), Feedback in second language
writing: Contexts and issues (pp. 81–104). Cambridge: Cambridge University Press.
Gareis, C. R., & Grant, L. W. (2015). Teacher-made assessments: How to connect curriculum,
instruction, and student learning. New York: Routledge.
Gibbs, G. (2006). How assessment frames learning. In C. Bryan & K. Clegg (Eds.), Innovative assess-
ment in higher education (pp. 23–36). London: Routledge.
Gibbs, G., & Simpson, C. (2004–2005). Conditions under which assessment supports learning.
Learning and Teaching in Higher Education, 1, 3–29.
Green, A. (2014). Exploring language assessment and testing: Language in action. New York:
Routledge.
Grotjahn, R. (1987). On the methodological basis of introspective methods. In C. Faerch &
G. Kapser (Eds.), Introspection in second language research (pp. 230–250). Clevedon: Multi-
lingual Matters.
Hattie, J., & Timperly, H. (2007). The power of feedback. Review of Educational Research, 77(1),
81–112.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary and
university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hyland, K. (2002). Teaching and researching writing. London: Pearson Education Limited.
Hyland, K. (2003). Second language writing. Cambridge: Cambridge University Press.
Hyland, K., & Hyland, F. (Eds.). (2006). Feedback in second language writing: Contexts and issues.
Cambridge: Cambridge University Press.
Kroll, B. (2001). Considerations for teaching and ESL/EFL writing course. In M. Celce-Murcia
(Ed.), Teaching English as a second or foreign language (pp. 219–223). Boston: Heinle &
Heinle.
Lantolf, J., & Pavlenko, A. (2001). (S)econd (L)anguage (A)ctivity theory: Understanding second
language learners as people. In M. Breen (Ed.), Learner contributions to language learning
(pp. 172–182). London: Longman.
Lea, M., & Stierer, B. (2000). (Eds.) Student writing in higher education. Buckingham, U.K.:
Open University Press
Does the Feedback Feed Forward? Student Response to and Views of Teacher. . . 131
Lee, I. (2007). Feedback in Hong Kong secondary writing classrooms: Assessment for learning or
assessment of learning. Assessing Writing, 12, 180–198.
Lee, I. (2008). Student reactions to teacher feedback in two Hong Kong secondary classrooms.
Journal of Second Language Writing, 17(3), 144–164.
Lee, G., & Schallert, D. (2008). Meeting in the margins: Effects of the teacher student relationship on
revision processes of EFL college students taking a composition course. Journal of Second Lan-
guage Writing, 17, 165–182.
Norton, L., & Norton, J. (2001). Essay feedback: How can it help students improve their aca-
demic writing? Paper presented at First International Conference of the European Association
for the Teaching of Academic Writing across Europe (EATAW). Gronigen, 18–20 June.
Pintrich, P. R., & DeGroot, E. V. (1990). Motivational and self-regulated learning components of
classroom academic performance. Journal of Educational Psychology, 82, 33–40.
Polos, A., & Mahoney, M. (2008). Effectiveness of feedback: The students’ perspective. Assess-
ment & Evaluation in Higher Education, 33(2), 143–154.
Ramaprasad, A. (1983). On the definition of feedback. Systems Research and Behavioral Science,
28(1), 4–13.
Sadler, R. (1989). Formative assessment and the design of instructional systems. Instructional Sci-
ence, 18, 119–144.
Scott, V. (1996). Rethinking foreign language writing. Boston: Newbury House.
Shute, V. (2008). Focus on formative feedback. Review of Educational Research, 78(1), 153–189.
Stobart, G. (2006). The validity of formative assessment. In J. Gardner (Ed.), Assessment and
learning (pp. 133–147). London: Sage Publications.
Taras, M. (2002). Using assessment for learning and learning from assessment. Assessment and
Evaluation in Higher Education, 26, 501–510.
Weaver, R. (2006). Do students value feedback? Students perceptions of tutors’ written responses.
Assessment & Evaluation in Higher Education, 31(3), 379–394.
White, E. (2017). Teacher self-assessment of feedback practices in an EFL academic writing class – A
reflective case study. In E. Cano & G. Ion (Eds.), Innovative practices for higher education
assessment and measurement (pp. 162–187). Hershey, PA: IGI Global.
Wojtas, O. (1998, September 25) Feedback? No, just give us the answers. Times Higher Education
Supplement.
132 E. White
Designing and Rating Academic Writing Tests:
Implications for Test Specifications
Amani Mejri
Abstract Based on local teaching and assessment considerations, this study
aimed at investigating academic writing teachers’ design practices as specifica-
tion writers and writing test raters. On the other hand, test takers’ conceptions of
writing assessment and score interpretation were addressed. The aim was to
capture a comprehensive view of writing assessment in an EFL context. To this
end, a rating scale questionnaire was administered to 10 academic writing
teachers in different Tunisian universities. Another rating scale questionnaire
was administered to 25 third year English students in an EFL context. This
study essentially dealt with theoretical and operational writing construct defini-
tion throughout test development processes for test designers, and through sitting
for the test for test takers. Students’ test scores were obtained to investigate social
aspects of writing assessment in the Tunisian setting. The quantitative data for
this paper were analysed using SPSS and indicated there is a gap between teachers
and students’ views on the writing construct and what they endorsed and
represented concretely. Both teachers and students’ questionnaires along with
test scores indicated that there is a remarkable focus on writing as a linguistic
competence, while other (social, pragmatic, and communicative) competences are
overlooked in the assessment process. A subsequent discussion of the study
findings and its theoretical, pedagogical and methodological implications for
the local writing assessment context was maintained.
Keywords Writing assessment • Test specifications • Construct, theoretical and
operational definition • Validity • Quantitative analysis
A. Mejri (*)
Faculty of Human and Social Sciences of Tunis, Tunis, Tunisia
e-mail: amany.mejri@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_7
133
1 Introduction
Investigating the writing assessment cycle rests upon addressing major a priori and
a posteriori design procedures condensed in the multi-fold writing construct defi-
nition, operationalization and evaluation. In fact, eliciting the definition of writing
ability as a test construct is responsive to the context where the assessment episode
takes place (Cumming, 2002), and is based on a clear assessment purpose to be
articulated in test specifications (Alderson, Clapham, & Wall, 1995). In addressing
the importance of classroom context of assessment, it is important to note that in
this framework the way the writing skill is conceptualized, whether from a product-
based approach, process-based approach, or a socio-cultural approach (Barkaoui,
2007), has clear implications on the definition of writing as a test construct and on
the components of test specifications. Discussing writing skill theories is a needed
task to understand how stakeholders perceive it in their assessments.
2 Theoretical Background
To formulate the theoretical definition of writing, prior to its empirical definition
emanating from instructional purposes and practices (Bachman & Palmer, 1996;
Brown, 1996), test designers, being teachers in this regard, need to attend to the
socio-cultural aspect of the skill when engaging in the very initial process of test
development (Weir, 2005). A communicative approach to this definition constitutes
a comprehensive frame of reference since the act of writing is situated in its societal
setting, being the classroom, and the communicative goals, the linguistic constitu-
ents, as well as the cognitive processes immersed are subsequently identified and
inform test item writing (Bachman, 1990; Weigle, 2002).
Yet the referentiality of classroom context enshrines other operative variables
that punctuate writing assessment process and frame test specifications. These
variables include the learning environment and teachers’ instructional goals
(Moss, 2003). Other factors are classroom characteristics featuring test takers’
level, background, needs and degree of interaction with the program of instruction
(Reid & Kroll, 1995). Seemingly, these variables are controlled by test designers
who are knowledgeable about students’ characteristics and assume that these test
takers will have a uniform understanding of the writing assignments (Ruth &
Murphy, 1984). On a mundane level, as a complete control of classroom charac-
teristics is mandatory to design effective writing tests, missing them yields
unspecific writing tests that lead to questioning the validity of these assignments
and the scoring procedures observed when rating students’ scripts.
To gauge test makers’ control of their classroom context and test usefulness, it is
a proviso to refer to the information supplied in their test specifications. It is equally
important to address their awareness of the construct to be measured in order to
detect any possible match or mismatch between test specifications content and the
134 A. Mejri
operationalization of this definition into test content (Alderson et al., 1995). In fact,
the centrality of construct definition arises from the considerations to which test
developers attend when balancing and refining their writing construct components
with their classroom characteristics (Reid & Kroll, 1995).
As this context is about achievement assessment, the criterion-referenced test
design approach forwarded by Lynch and Davidson (1994) is relevant. In essence,
this approach rests upon a detailed criterion definition that informs the entire test
development process. In formulating this principled approach to organize
specification-writing process, Lynch and Davidson (1994) posit a caveat of a
possible discrepancy based on the focus on test takers’ linguistic ability in the
assessment process rather than investing this knowledge for communicative pur-
poses. This caution is reminiscent of Messick’s view of validity (1989) that is
threatened by construct under-representation or construct fuzziness, which is a
mismatch between the criterion to be measured and what is concretely measured
through test items. This difference has clear implications on the rating process
observed to evaluate the elicited samples of writing (Ruth & Murphy, 1984).
For criterion-referenced test development to be comprehensive, the concept of
mandate is elaborated (Lynch & Davidson, 1994). As it takes various appellations
with other approaches to test development (Alderson et al., 1995; Bachman &
Palmer, 1996; Popham, 1978) such as test purpose or general description, this
concept is essential for classroom-grounded writing assessment where curricular
goals are embedded and articulated in test design. Being clearly associated with the
criterion to be measured, the mandate is susceptible to refine test item writing that is
elaborated in an iterative test development process. Along these lines, a detailed
description of the prompt attributes and response attributes are derived from the
concept of the assessment mandate, which is responsive not only to the limited
classroom context, but equally to the overall assessment policy.
Another major characteristic of writing classroom context is that teachers who
are test designers are also test raters. They constitute a unique audience to student-
writers since they entertain a knowledge base of the test takers’ features and
potential understanding of the writing test (Reid & Kroll, 1995). This knowledge
is integral to developing the scoring criteria to be observed in the rating process,
which is another central part of test specifications. Not only do teacher-raters
interact with their knowledge about their student-writers, but they similarly interact
with the writing prompt that constitutes an influential variable in the scoring process
(Weigle, 1999). As peculiar readers, this interaction is one aspect of teacher-raters’
background, conducive, as some research studies point out (Brown, 1991; Swedler-
Brown, 1993), to applying various scoring criteria. The score is, therefore, resultant
from an interaction among student ability, prompt features, the scoring criteria
descending from the construct under investigation, the rating process, and rater
background.
Specifying the rating scale is to be based on clear statement of the features to
which the scorers will attend in the scripts examined (Lumely, 2002). Some scales
tend to prioritize some aspects over others and they range from comprehensive
content, language use, and rhetorical organization. These criteria represent initial
Designing and Rating Academic Writing Tests: Implications for Test. . . 135
rating decisions raters make at specification-writing level since they operationalize
the construct of writing (Bachman & Palmer, 1996). Teachers, in this paradigm,
need to develop scales that are devoid from any wording ambiguity and loose level
descriptors toward accommodating test construct and grounding test takers’ scores
(Lumely, 2002). Due to statements clarity, some of the problematic rating processes
can be explicated as scales account for scorers’ decisions. As Cumming, Kantor,
and Powers (2001) proclaim, some procedures in the evaluation of composition are
unknown. For this reason, aligning them with a clear definition of rating criteria in
test specifications and minute application of the scale might produce evidence of
the evaluation practices sensitive to the overall educational system where test takers
need to be able to interpret their scores and contextualize them in the broader
system accommodating them.
Akin to any other educational system, the Tunisian educational system has
various flaws that can instantly be captured in the assessment practices being
endorsed. Such practices emanate from their assessment concepts. As a
marginalised area, assessment in the local context is marked by fragile grounds of
implementations which are conducive to invalid interpretations and uses of the
assessment tools. Such a fact is due to the remarkable lack of training on the part of
teachers to be knowledgeable test designers, proficient specification-writers, and
raters (Hidri, 2014, 2015). Teachers’ exposure to assessment is not preceded by any
formal training that might impart to them assessment background and theories. This
gap in assessment training, which calls for urgent remediation, yields flawed tests
marked by items with fuzzy constructs.
More than this, the fact that only teachers have access to assessment procedures
in the Tunisian setting has led to total exclusion of test takers as active participants
in the assessment cycle. Hence, they overlook self-assessment as a major enterprise
in building up fair and useful tests. Alternatively, Tunisian test takers constitute
passive audience to teachers’ assessment instruments. Their role is reduced to
responding to test items and to receiving test scores that seem to be indecipherable
in terms of task fulfilment and language ability. This stands in sharp contrast with
international benchmarks, such CEFR. Scores are mere tools for success or failure.
Test takers have no discussion concerning and access to rating processes and
evaluation criteria that their teachers observe in designing and rating their tests.
At a more practical level, that which concerns the overall aim of this study,
designing and rating writing assessments, stakeholders’ awareness of test construct
is an exceedingly marginalised topic of discussion for teachers principally and for
policy makers to supply appropriate training to writing test designers. In fact,
teachers’ perceptions of writing assessment in the local context seem to be still
confined to the traditional essay tests. Rare are any assessments carried out by
focusing on portfolio writing. Test takers are generally supposed to respond to a
prompt or question introduced to them as a topic. The major focus of these topics
seems to be placed on the linguistic aspect of language. Other communicative
(pragmatic and sociolinguistic dimensions) are occluded as students are graded
according to their linguistic proficiency rather than communicative competence.
This linguistic focus is the legitimate result of having no training on how to design
and rate writing tests, hence construct underrepresentation in writing assessment.
136 A. Mejri
Another major issue pertaining to assessment in the Tunisian context is the lack
of continuum between assessing and teaching writing. Conceptions of teaching
writing whether from a cognitive process, product-based approach, or socio-
cultural account are not overtly translated in the writing assessment. This means
that assessment is just viewed as a tool of success or failure, though invalid.
Assessment is thus reduced to its very simplistic forms. It is not conceptualised
as a learning process for learners to learn more from tests and for test designers to
inform future specification writing, and entire test design. Traditionally, learning in
the local context is limited to teaching with the teacher playing the integral role of
information transmission (Hidri, 2014).
2.1 Rationale of the Study
In light of the aforementioned considerations of specifications writing, this study,
framed in the Tunisian assessment context, aims at investigating teachers’ aware-
ness of construct definition in test development process and its implications on test
specifications components. Academic writing teachers’ questionnaire and students’
test scores are used to have insights on teachers’ perceptions of test design process
and their rating behaviours. As part of validity evidence emanates from the initial
procedures in test development, teachers are mainly asked about decisions for
which they opt when developing the test and contextualizing it in their teaching
context. Formulating an account about their behaviours is equally relevant insofar
as to gauge their degree of commitment to their initially-taken decisions at speci-
fication level. On the other hand, students’ questionnaires are used as another
research method toward analysing their awareness of the test purpose and the
evaluation criteria. Test takers’ awareness of the evaluation criteria is a central
variable in the test-taking process. This is equally relevant to the interpretations test
takers elicit from their scores based on their awareness of the evaluation criteria and
test construct.
3 Method
3.1 Participants
The study participants are test designers and test takers. Test designers are teachers
of academic writing at the Tunisian tertiary level. Their teaching experience is not
limited to teaching academic writing only. Therefore, their assessment exposure
varies according to the subjects they have taught in higher education. Equally, their
professional development practices as well as their assessment training differ in
accord with their involvement in research contexts and field of expertise. Teachers
Designing and Rating Academic Writing Tests: Implications for Test. . . 137
design the materials they will employ in teaching academic writing. They set their
course objectives and the outcomes they will elicit through their instruction. They
are the only scorers of their tests, which means they are the sole audience of their
tests and students’ writing.
The other study participants are third year university students which is the final
year of the Licence Fondamentale (the first university degree Tunisian students
obtain). In this year, academic writing is introduced as a major subject in the
curriculum. This is due to the fact that pursuant to the graduation, some students
will be oriented to the postgraduate level, where academic writing constitutes a
central component of the curriculum and the research to be conducted, hence
success. Upon the completion of their university degree, students have been taught
English language as a foreign language for 10 years starting from their high school.
Their classroom context represents the main area of practice, input and output.
Third year students have been taught different courses such as grammar, oral
communication skills, and reading/writing, along with other subjects of British
and American civilization and literature. In their third year, the focus is rather
oriented on academic writing as a comprehensive subject inclusive of communica-
tive competence, linguistic competence, pragmatic competence, rhetorical compe-
tence, and sociolinguistic competence. For students, and necessarily teachers,
academic writing is a form of assessment since it hinges upon the various areas
of the writing skill (Weigle, 2002).
3.2 Teachers’ Questionnaire
Teachers’ questionnaire is a rating scale questionnaire using Likert 5-point scale:
strongly disagree, disagree, neutral, agree, strongly agree (Cohen, Manion, &
Morrison, 2007). It is divided to four parts starting with the initial process of test
development, the definition of the construct of academic writing and teachers’
profile as raters, and score interpretation. The final part is devoted to participants’
background information concerning their professional age, assessment training,
course materials design, etc. The questionnaire layout is inspired from the process
of test development, although it is an iterative process (Alderson et al., 1995). Yet
the choice is informed by the aim to investigate what constitutes the basis to
develop achievement writing tests and how the construct is elicited. In this para-
digm, the components of test specs are identified whether implicitly or explicitly.
3.3 Students’ Questionnaire
Similarly, students’ questionnaire is a rating scale questionnaire based on a five-
point scale: strongly disagree, disagree, neutral, agree, strongly agree. Students’
questionnaire is divided into two parts: the test of academic writing that includes
138 A. Mejri
different sections and the background information part. In the first part, participants
are supposed to reply to items concerning test purpose and content that is their
understanding of the test purpose and the degree the test represents the instructional
practices of teachers, hence test authenticity (Bachman & Palmer, 1996). In the
subsequent section, students are asked about their score interpretations that are in
part related to the evaluation criteria teachers endorse. In the following Sects. 3 and
4, test takers are encountered with items about interpretations test designers make
about their test scores, constituting the basis of decisions and test use, and about the
relevance of test tasks to their background, needs, and course content, respectively.
Therefore, students’ questionnaire provides a basis for introducing classroom
characteristics to which teachers are expected to attend in order to develop their
tests. On the other hand, addressing students’ score interpretations has clear impli-
cations on the consequential basis of the test that is related to validation consider-
ations. In general terms, their involvement in the assessment cycle is sought.
3.4 Academic Writing Test Scores
Another source of data is students test scores of an academic writing exam
administered by the end of the term. This set of scores constitutes of the raw scores
teacher-raters assigned to students. Using these scores aims at investigating raters’
behaviour while rating test takers’ compositions. The scores are invested as a source
of data to characterise their distribution and to identify what aspects in particular
teacher-raters’ focus on. Therefore, the aim is to measure the continuum between
initially endorsed decisions at the design stage and the implementation of these
same decisions at the rating stage.
3.5 Data Collection
Data for this study are obtained from a pilot study conducted in the Tunisian
context, which accounts for the limited number of the study participants. For the
first set of the participants, academic writing teachers, the questionnaire was
administered to them as a hard copy pursuant to explaining to them the main
objectives, motives and context of the main study. As the number is limited, the
questionnaire was administered to teachers in three Tunisian universities: Manouba
Arts University, Faculty of Human and Social Sciences of Tunis (FHSST), and
Higher Institute of Languages of Tunis. The Number of the questionnaires returned
is 10 (N ¼ 10).
The other study participants are third year English students. They pursue their
studies in FHSST. They constitute the main study target population. Therefore, as
part of the initial pilot study, the students’ questionnaire was administered to
25 persons (N ¼ 25) in class. Upon distributing the instrument to them as hard
Designing and Rating Academic Writing Tests: Implications for Test. . . 139
copy to ensure a high return rate, it was essential to explain to them the study
objectives and the importance of taking part in it. Equally explaining some terms
figuring in the questionnaire deemed necessary toward ensuring that they under-
stood what was required. Third year students filled in the questionnaire in the
presence of the researcher. Any relevant comments or discussions between the
researcher and the participants were documented as data for the pilot study.
To have access to the students’ academic writing test scores, it was necessary to
contact the administration of FHSST, where the main study participants pursue
their university studies. Accordingly, full access to the scores was granted. They
were provided as hard copy. Obtaining full access to the scores could not be
possible before publishing them for the students on the university’s official website.
4 Results and Discussion
In the process of analysing the piece of datum under investigation, it is quintes-
sential to document test designers’ behaviours while embarking on the very initial
process of test design. This may entail, as included in the questionnaire and
corroborated by assessment specialists (Alderson et al., 1995; Bachman, 1990;
Lynch & Davidson, 1994), a thorough examination of what the test purports to
gauge, hence the test purpose or the test construct, along with charting test takers’
needs, level and exposure to English language should be entertained. In this study,
academic writing teachers exhibit an acute awareness of the importance of
defining test level (50%), of taking into account the needs of their test takers
(40%) which represent some of the students’ characteristics to be observed in the
test development process, and of contextualising their test construct in their
teaching framework, which hinges upon test authenticity as an achievement
test. As indicated in Table 1, participants’ clustering is detected in their awareness
of the centrality of defining test construct and observing test takers’ needs, being
translated in high SD value. At this level, this awareness seems to be at a
Table 1 Initial test design process of Tunisian academic writing teachers
M Mode SD
1. To design an academic writing test, I define test level 4.2 5 1.03
2. To design an academic writing test, I define what academic writing is 3 3 0.9
3. To design an academic writing test, I base this definition on what I have
taught in the classroom
2.9 3 0.99
4. To design an academic writing test, I communicate this definition to my
students
3 3 0.94
5. To design an academic writing test, I examine my students’ needs to
design appropriate writing tasks
3.5 4 1.26
6. To design an academic writing test, I prepare a checklist to analyze the
aspects of academic writing ability
3.9 4 0.99
140 A. Mejri
theoretical level. Investigating it at a mundane level is yet to be conducted
according to the practical and contextualised form of test specifications advocated
by Lynch and Davidson (1994).
At a practical level when test designers are transferred in their process to
translating their construct definition into theoretical and operational definition at
specs level, variation is recoded among the study participants.
Interestingly, while 30% of the participants concur that linguistic ability is
integral to academic writing, the other 30%, as shown in Table 2 of the study
participants find language knowledge an additional element of academic writing.
This may entail that students might perform well on any academic writing assign-
ment without manifesting a linguistic competence (a detailed display of these
findings is displayed in Appendix 1). This might be problematic as Bachman’s
(1990) definition of language performance stipulates that the students should have a
good linguistic command of the target language, along with communicative and
pragmatic competence. More than this, the fact that the study data could not could
not provide a clearer picture on test designers’ view on the importance of linguistic
knowledge is due to the confined piece of datum supplied for this analysis. Yet the
account provided triggers more prospective research to comprehensively compre-
hend teacher designers’ conceptions of academic writing in general and in their
testing practices and specifications writing in particular.
When examining their profiles as unique audience and raters of their student-
writers’ scripts, the study participants, teachers, seem to have a confined expertise
when asked about their training as proficient writing raters. Only 20% of the
participants claimed to be trained raters. The other participants, opting for the
neutral position in this item constitute 60% (6 participants) (SD ¼ 0.87, for a
detailed display of the finding refer to Appendix 2). Controversially, participants
proclaim that their scores provide a detailed profile of their student-writers’ per-
formance (60% ¼ 6 participants). It is tempting to note that teachers as raters have
acquired their expertise in scoring written scripts through experience (50% ¼ 5
participants) rather than direct training in rating and assessment.
To measure students’ understanding of the test purpose and test construct, an
internal consistency of the items using Cronbach’s Alpha reliability coefficient was
used. The overall reliability of the items is indicated by α ¼ 0.63, suggesting that
the reliability level is acceptable. It equally suggests that test takers are aware of the
test purpose (μ ¼ 3.44). This is accompanied by having ample access to relevant
information prior to sitting for the writing test. Surprisingly, however, a consider-
able variation is documented when test takers are asked about a theoretical defini-
tion of writing as a construct, in other words as a test purpose to be measured. This
is inspected through the inter-item correlation matrix. When addressing the
Table 2 Test designers’
conceptions of language
knowledge as part of
academic writing ability
Valid
Disagree Neutral Agree Strongly agree
Frequency 3 3 1 3
Percent 30 30 10 30
Designing and Rating Academic Writing Tests: Implications for Test. . . 141
linguistic competence as part of the construct, it is detected that it has an internal
consistency alpha value ranging from α ¼ 0.02 to α ¼ 0.43. This means that
notwithstanding the reached consent on apprehending the test purpose, test takers
diverge in their views on the linguistic proficiency as integral to writing construct.
As regards other parts of the construct theoretical conceptualisation, such as text
organisation and writing functions, the internal consistency values of their
corresponding items range from α ¼ 0.50 to α ¼ 0.31 and α ¼ 0.38 to
α ¼ 0.22, respectively. This means that there is a relatively higher awareness
and agreement that structural organisation and writing functions constitute a part of
writing as a test construct (Table 3).
Interestingly and as the study findings suggest, the seeming ambiguity among
test takers of what is and is not measured by the test is traced back to the absence of
teacher-student discussion on the evaluation criteria of their scripts and the scoring
procedures that teacher-raters observe in this regard. For these very reasons,
students in this study are asked about their own interpretations of their scores and
the relevance as well as the referentiality of these scores toward evaluating their
academic writing competence. Although most study participants could elicit infor-
mation on their linguistic ability, pragmatic competence and rhetorical organisation
from their test scores, 44% of them (16% ¼ Strongly Disagree, 28% ¼ Disagree) do
believe that the scoring procedures are unfair. Such a finding is contradictory with
the amount of information they could obtain from their scores regarding their
writing competence in English. This is further articulated in test takers belief that
teacher-raters tend to score their compositions in their entirety rather than scoring
parts separately. This is translated in only 20% of the population agreeing that
teacher-raters tend to score scripts’ parts separately. Such a finding has explicit
implications on the level descriptors and the scoring guide teacher test-designers
tend to develop at the very initial process of test making. It even questions what
parts of writing as test construct are included in the scoring process and to what
extent is information relevant to this guide communicated to test-takers (See
Appendix 3 for a detailed display of data concerning students’ views on test tasks).
In light of the aforementioned teachers’ and students’ questionnaires data, it could
be argued that student performance on the test along with teacher performance at
specification-writing level might yield high test scores to indicate a remarkable
student command of the writing ability. This could be deduced mainly when
scrutinising student awareness of the writing construct on the one hand, and teacher
awareness of the theoretical and operationalised definition of their test construct at the
initial level of test design on the other. Interestingly, however, students’ academic
writing test scores bear other findings disparate to what could be anticipated.
Table 3 Student-teacher discussion of the evaluation criteria
Valid
Strongly disagree Disagree Neutral Agree Strongly agree
Frequency 6 3 10 3 3
Percent 24 12 40 12 12
142 A. Mejri
Third year students’ test scores (N ¼ 289) display a notable variation under a
negatively skewed distribution of a value of (0.18). This variation should nor-
mally be grounded in different levels of performance that range from complete
command of the test construct translated in high scores to a limited command
translated in a low score. Yet the mean value of the scores under study is low
(μ ¼ 8.69). More than 50% the test takers obtained scores below the mean value. In
other words, and as displayed in Table 4, by means of calculating the z-scores of the
academic writing test scores a considerable range of the scores occur between
(0.23) and (1.63), meaning between 4 and 8 scores. This cluster of the marks
indicates a very poor test performance on the academic writing test, giving rise to
disparate proposals that might be antithetical to what has been shown by the
questionnaires.
It is tempting to note that test takers having this particular performance on the
test have almost similar background, since English constitutes to them a foreign
language. This means that their target domain is their classroom practice. Further-
more, the designer of their test is their classroom teacher. Thus, the test developer
entertains an important knowledge base of the potential test takers’ characteristics
and assessment needs. Such considerations are an integral part to the process of test
design as teachers in their questionnaires claimed to account for their students’
needs when developing the test and to appropriately define test level and construct.
With these considerations in mind a particular variation of test takers’ performance
is anticipated to represent what has been assumed as test construct and purpose at
the specification level (Bachman, 2004).
The variability resultant from the scores, however, is telling insofar as it feeds
back to the test takers their raw awareness of academic writing as a test construct
and to test designers their degree of integrating relevant details in their specifica-
tions. To start with teachers as raters and student-writers’ audience and in probing
adequate interpretations from test scores in connection to test design, it is clear that
Table 4 Distribution of third
year students’ academic
writing z-scores
Frequency Percent
Valid 2.33023 3 1
1.98166 7 2.4
1.63309 17 5.9
1.28452 13 4.5
0.93595 33 11.4
0.58738 36 12.5
0.23881 26 9
0.10976 14 4.8
0.45833 61 21.1
0.80690 24 8.3
1.15547 33 11.4
1.50404 14 4.8
1.85261 6 2.1
2.20118 2 0.7
Designing and Rating Academic Writing Tests: Implications for Test. . . 143
they allude to various proposals such as test difficulty degree, test practicality
considerations and test relevance to what it purports to measure. The fact that the
mean value (μ ¼ 8.69) and standard deviation (SD ¼ 2.86) are low indicates that the
poor performance is due to test difficulty, which means the test might be inappro-
priate to the setting in which it has been administered. The degree of difficulty, if
ever it exists in any given test, needs to be consonant with the learning setting and
the teaching objectives of teachers (Moss, 2003). Yet surprisingly the gap that is
detected between what teachers tend to observe in their specifications and what the
scores indicate might refer to a limited consideration accorded to these character-
istics while developing the test.
Another important proposition to be elicited from these scores is connected to
teachers as composition raters. At a theoretical plane, any rating behaviour is
consistent with and echoes the rating scale and the construct definition at specifi-
cation level, respectively (Alderson et al., 1995). In this situation, consistency
between academic writing test specifications and the scores obtained could not be
easily detected and evaluated. The fact that teachers are not trained to rate writing
samples lends the scores of the academic writing test to various interpretations. For
rater background is a paramount variable that interacts with other variables such as
rater knowledge of test takers and the context of the test to result in a given test
score (Weigle, 1999). Part of the scores assigned to test takers in this study alludes
to the background and focus of teacher-raters. The inherent severity, which cannot
be teased out by a second rating to the same scripts, could be traced back to a
limited rater training, as some studies indicate that more trained raters tend to be
lenient in their scoring conduct (Lumely, 2002), and to their focus in the scoring
guide that mismatches test content and construct.
The rating of this test should rest upon a criterion that has been the focus of a
teaching input. Therefore, the assessment context is achievement assessment. Yet
via the data obtained, score distribution is akin to a notable extent to norm-
referenced distribution being normal distribution with the peak in almost the middle
point, as shown in Fig. 1. Such an inspection implies that not only is the rating of the
60
40
20
0
0 5 15 20
10
Frequency
Mean = 8,69
Std. Dev. = 2,869
N = 289
Fig. 1 Distribution of third
year academic writing test
scores
144 A. Mejri
scores to some extent norm-referenced based on individual test takers’ performance
(Bachman, 2004) rather than against a set of criteria in the teaching purpose and
context and articulated in the development process, but also it draws on linguistic
competence rather than classroom-based construct. It entails a solid questioning of
the rational upon which the rating scale is built and the method of scoring endorsed
whether impressionistic, holistic or analytic, hence score validity in its entirety
(Weigle, 2002).
Test scores equally feedback to test takers regarding their test performance, their
understanding of course and test content, and principally their understanding of the
test purpose and construct. In gauging to what extent student awareness of aca-
demic writing as a test construct can be represented through their test performance,
it is important to note that test takers are a unique audience for test specifications
(Alderson et al., 1995). As a peculiar document prepared to them, test specifications
can supply information on the test content, the writing prompt, response attributes
and mainly the evaluation criteria. Considering students as an audience to test
specifications helps to avoid misinterpretations of the test construct as a linguistic
focus devoid of communicative (pragmatic and sociolinguistic) aspects (Lynch &
Davidson, 1994). Controversially, student understanding of the test construct to
which they sat is blurred and antithetical with what they posited in their question-
naire responses. With comprehensive awareness of and exposure to academic
writing as a test criterion, test takers’ performance is anticipated as a complete
command of the criterion. However, with a SD ¼ 2.86 and a cluster of scores
between 4 and 8, it is noteworthy that student understanding of the construct does
not parallel the empirical definition of the construct in the test. This discrepancy can
be inspected although the scores are not very informative since they are single
scores of the writing scripts. Score interpretations do not confirm the limited
questionnaire sample findings.
At a further level, scores cannot be accurate measures of test takers’ writing
ability. There is no ample support to ground the proposal that the test itself is based
on a well-defined target language use, therefore providing similar tasks to be
encountered in the real-world context. In a similar vein, it could not be argued
that the test is inauthentic since more than 50% of the students fail to exceed the
mean (μ ¼ 8.69). With these practicality use considerations (Bachman & Palmer,
1996), the scores under disposition are not very informative to allow test users,
teacher-raters and student-writers in this regard, to generalise score-based deci-
sions. The validity of the inferences to be elicited could not comprehensively be
supported. Construct validity that is the basis of all validation endeavours (Messick,
1989) could not be satisfied through scores providing quantitative symbols almost
devoid from relevant interpretations conducive to construct awareness, language
ability, test content appropriateness to test setting, and a convincing rating process,
although this process may be intricate with inaccurate information (Cumming et al.,
2001).
Designing and Rating Academic Writing Tests: Implications for Test. . . 145
5 Conclusion
The main focus of the study was to investigate practices pertaining to designing and
rating academic writing tests. Akin to any study it has various implications to be
observed at different levels of practice and in potential research studies.
As far as the theoretical implications are concerned, it was indicated that a
comprehensive view of writing theories needs to be developed toward ensuring
that these theories inform teaching the skill, classroom foci and general task
orientation (Hyland, 2003). Being aware of these theories, on the part of teachers
and students, not only helps determine and understand the teaching foci, but
similarly indicates assessment foci, which alludes to another practical implication
that will be discussed further. This means that an understanding of writing as a
process [process models proposed by Flower and Hayes (1981) and Bereiter and
Scardamalia (1987)] where the focus is placed on cognitive processes of planning,
translating, editing and goal-setting, and an understanding of writing as a
contextualised practice where the focus is on communicating a given message to
the reader in a particular setting (being specifically a socio-cultural model where the
writing genre and the concept of discourse community are taken into account
(Swales, 1990) introduce the writing skill in its entirety in the classroom. Such a
view facilitates which writing aspects to address abreast of other aspects that are not
overlooked.
Other theoretical implications concern teachers’ awareness of assessment as a
practice integral to their students’ evaluation, course evaluation and their own
teaching. In more practical terms, through the current study findings it is indicated
that teachers need to entertain a knowledge base on assessment in general, and be
trained in writing assessment in particular. The former is about the major assess-
ment concepts that have to be observed, such as specification writing, validity and
reliability, which represent assessment literacy for teachers to design appropriate
assessment instruments (Hidri, 2015; Malone, 2013). At a more specific level, an
understanding of writing assessment is focal to design fair, appropriate and
relevant assessment tasks. This was shown through the study findings that con-
veyed a remarkable gap in the Tunisian context regarding the quality of assess-
ments administered due to a notable lack of trained writing assessors. An
awareness of and a training in writing assessment fundamentals supply teachers
with the needed input and expertise to design appropriate writing prompts respon-
sive to the context, where the content, genre, language and expected response are
framed (Kroll & Reid, 1994) and to score the scripts their test takers produce
(Weigle, 2002).
In light of these aforementioned theoretical implications, teachers can select the
scoring schemes that are compatible with their tasks and take into consideration the
writing ability to be measured (Weigle, 2002). Once equipped with these funda-
mentals, teachers, as writing test designers, could be able to evaluate their writing
prompts by specifying a set of standards inclusive of contextual dimensions,
146 A. Mejri
content, genre, linguistic aspects and attributes of expected response (Reid & Kroll,
1995). These evaluations feed back into their writing assignments particularly as
the standards set are a list of specifications on the one hand, on the other the writing
assessment tasks will be specific regarding the construct to be measured and what
evaluation criteria to follow. Score revisions will then be drawn on both the scoring
scheme used and the task with which it is associated. Therefore, ultimate scores can
be accessed, interpreted and essentially validated.
The study implications also address pedagogical considerations in the teaching
and assessment of the writing skill. Through the study findings, it was indicated
that there is a gap between writing teaching and assessment in the Tunisian context.
To remediate this gap, tracing a continuum between both learning ends, i.e.,
teaching and assessment, is central. It could be achieved via various methods.
Contextualising assessment at the heart of teaching is an effective method. Instantly
scoring schemes or rubrics that will be used in the assessment instruments can be
introduced in the writing course (Hyland, 2003). In such a way, teachers implicitly
address a set of gaps, being students’ lack of awareness of the writing construct and
evaluation criteria articulating in their turn the construct to be assessed, the foci of
the writing course whether context, content, purpose, processes, or all of them
combined and synthesised together, the focus of the writing assignment, and
ultimately specification writing.
Not only do teachers decide which scoring methods to endorse, whether
analytic or holistic methods (Weigle, 2002), but they equally render the assess-
ment process a contextualised task since the teaching input imparts some assess-
ment information to prospective test designers and test takers. At another level,
being a contextualised task not distanced from classroom practices, assessment
data feeds back into post-assessment teaching. In other words, the scoring method
utilised feeds back to student-writers on their performance on the task on the one
hand, on the other teachers in their instructions to address the gaps they detected
in their students’ performance and to develop appropriate teaching methods with
gap-filling orientations. In general terms, therefore, pedagogical implications
coalesce around teaching particularly the writing construct and providing empir-
ical input to assess it. This leads to creating a theoretical and empirical awareness
of the construct.
The study methodological implications are based on the type of evidence
provided for writing test validity. Deploying test takers’ views on the measured
construct and on their awareness of relevant assessment evidence does enlarge the
scope of validation studies. Yet more focus on this evidence, in both its qualitative
and quantitative forms, is conducive to producing accurate validity judgments
regarding the quality of tests administered. It allows for more in-depth accounts
on test taking strategies relevant to the construct being measured. Test takers, as
integral to validation process, could provide a social account on how scores are
accessed and interpreted. This means that addressing the social aspect of validity,
consequential validity evidence (Messick, 1989), is accomplished through test
takers’ accounts.
Designing and Rating Academic Writing Tests: Implications for Test. . . 147
Besides, validation studies have extensively been focusing on quantitative data
(Bachman, 1990). A needed orientation to qualitative evidence, such as post-test
interviews with test designers and test takers, is crucial to address the limitations of
quantitative construct validation studies. Another example can be a priori qualita-
tive data could provide comprehensive accounts on the processes of test develop-
ment, the decisions adopted and what methods used to produce writing test
specifications. The idiosyncrasy of specifications as an assessment document,
particularly as regards the amount of details it enshrines (Alderson et al., 1995),
how they are translated into a test and what principles guide them, trigger research
studies to examine their fundamental procedures.
Similar to other studies, this study is subject to various limitations. The data set
for the questionnaires, though informative to some extent, is not comprehensive
enough to be generalised to other populations. The sample of the participants
represents the target population, yet a more complex sample is required to elicit
more generalizable findings and to construct a comprehensive account of tertiary
writing assessment in the Tunisian setting. Another major limitation is lack of test
items to which test takers responded. With the test under disposition, more
construct-referenced interpretations could be elicited and more elaborate statis-
tical analyses to be conducted. For limited data set cannot answer all the ques-
tions pertaining to specification writing, to the scoring process, and to validity
claims.
While observing the study limitations, it is recommended that along with
enlarging the questionnaires sample, qualitative data should be incorporated.
Interviewing raters in the process of their test design and pursuant to rating their
students’ writings will yield relevant insights on what is included in their specifi-
cation document and whether it is observed in test design cycle and the rating
procedures. Such data will equally supply a qualitative view on validity implica-
tions since teachers as test designers and raters will justify their scores by relating
them to their construct definition. The limited data set in this study, particularly on
the part of teachers and students, triggers more research to comprehend the dynam-
ics of achievement writing assessment in the Tunisian setting that is under-
researched.
At a more practical level, a priori and posterior design interviews need to be
conducted. These research instruments not only supply data on the assessment
procedures in their entirety. They similarly provide a global, yet very practical,
view on the assessment context with all its variables, meaning the Tunisian assess-
ment context. In this way, a priori interviews provide the pedagogical background
of the assessment process pertaining to teaching and conceptualizing writing, while
the posterior interviews allow the researcher to comprehend the methodological
procedures punctuating the assessment act, belonging to design and
operationalization processes. Ultimately, validity requirements can be satisfied
with qualitative and quantitative means.
148 A. Mejri
Furthermore, conducting test item analysis is another source of data which can
provide insights on test content toward gauging test authenticity and practicality
and eliciting more validation claims. By working on the different evidence sets in
the assessment context, the teaching framework might be related to the assessment
process, which leads to grounding score inferences in their relevant setting and to
feeding back fair decisions to test users, teachers and test takers mainly.
By attempting to capture and chart processes teachers as test designers and raters
might indulge in, an initial, yet incomplete, picture is provided on the reality of
writing assessment in the Tunisian context. Notwithstanding the limitations of
generalisability of the study interpretations, it could be argued that writing assess-
ment as a complex task is yet to be elaborated and yet to be mature in the local
context. By investigating teachers’ processes on the one hand, evaluating student
understanding on their test construct and analysing their test scores on the other
hand, a gap in the design is inspected as no adequate interpretations could be
extracted from the scores, which implies very limited a priori validity evidence.
Decisions based on the scores’ lack of justifications undermine validity implica-
tions in this paradigm.
In light of the aforementioned discussion, more training on specification writing,
rating, and assessment course are required toward producing authentic, practical
and valid writing assessments. Academic writing constitutes a success subject for
many test takers. Yet not being able to be accurately evaluated leads to misinter-
pretations that are inadequate with the assessment instrument, context and score.
More than this, the initial trial to capture student’s view on score awareness paves
the way for considering social aspects of validity, particularly when addressing its
consequential aspects through test takers’ view on the entire assessment cycle.
Validity can be documented through comparing test designers’ interpretations and
justifications of test scores and test users’ evaluations and illustrations of these
interpretations and justifications.
Assessment in the Tunisian context lacks various grounds to be held accountable
as an effective tool in the entire Tunisian educational system. To address this gap, it
is prerequisite to trace back that the main point to address at the very initial level is
the way writing is conceptualized, endorsed, taught and assessed. Once investigated
and comprehended, the way teaching is performed and assessment is formed could
have a clear liaison. This liaison, though very advanced and intricate, refers to
“reconciliating” writing and teaching assessment, a link that this study addressed at
the level of theoretical and pedagogical implications. The remarkable gap that is
noted at both ends can be addressed only if assessment and teaching are introduced
in the Tunisian educational context on a continuum, where both paradigms serve an
exclusively learning end.
Designing and Rating Academic Writing Tests: Implications for Test. . . 149
Appendix 1 Tunisian Academic Writing Teachers’ Views
on Defining Writing as a Test Construct
M Mode SD
1. To define academic writing for evaluation purposes, I state that lan-
guage knowledge is a central part of academic writing ability
3.40 2a
1.26
2. To define academic writing for evaluation purposes, I state that lan-
guage knowledge includes vocabulary knowledge syntactic structure,
written code, and appropriate language use
3.70 3a
1.25
3. To define academic writing for evaluation purposes, I state that prag-
matic knowledge is a central part of academic writing ability
3.70 4 0.67
4. To define academic writing for evaluation purposes, I state that com-
municative competence is a central part of academic writing ability
4.20 5 1.03
5. To define academic writing for evaluation purposes, I state that rhe-
torical organization is a central part of academic writing
4.20 4a
0.78
6. To define academic writing for evaluation purposes, I state that aca-
demic writing is linguistic and cognitive process of planning, writing and
editing
3.80 4 0.78
7. To define academic writing for evaluation purposes, I state that the
socio-cultural context is central to academic writing
3.90 4 0.99
a
Multiple modes exist. The smallest value is shown
Appendix 2 Descriptive Statistics: Tunisian Academic
Writing Teachers’ Background as Raters and Scores
Interpreters
N Sum M SD
1. As an academic writing teacher, test maker and rater, I am trained
in grading writing tests
10 29 2.90 0.87
2. As an academic writing teacher, test maker and rater, I am profi-
cient in writing test specifications for academic writing tests
10 35 3.50 0.52
3. As an academic writing teacher, test maker and rater, I am an
experienced rater
10 33 3.30 1.25
4. As an academic writing teacher, test maker and rater, I used double
rating
10 26 2.60 1.07
5. The scores I get from my test provide a detailed evaluation of my
students’ writing ability
10 35 3.50 0.70
6. The scores I get from my test provide a single score for a writing
test
10 30 3 1.33
7. The scores I get from my test provide an estimation of their
performance on similar tasks in real world context
10 36 3.60 1.07
8. The scores I get from my test provide an accurate measurement of
my definition of academic writing
10 35 3.5 0.52
Valid N (listwise) 10
150 A. Mejri
Appendix 3 Descriptive Statistics: Students’ Views
Regarding their Academic Writing Test Tasks
N Min Max M SD
1. Test tasks are good measures of my writing ability 25 1 5 3.20 1.22
2. Test tasks are familiar to me 25 1 5 3.68 0.98
3. Test tasks reflect my needs of academic writing 25 1 5 3.24 1.23
4.Test tasks reflect the real-world context of academic writing 25 1 5 2.76 1.2
5.Test tasks involve arguing for or against various ideas 25 1 5 3.12 1.3
6.Test tasks meet the purpose of the test 25 1 5 3.40 1.15
7. Test tasks involve interaction with my background
knowledge
25 1 5 3.68 1.03
Valid N (listwise) 25
References
Alderson, J. C., Clapham, C., & Wall, D. (1995). Language test construction and evaluation.
Cambridge: Cambridge University Press.
Bachman, L. F. (1990). Fundamental considerations of language testing. Oxford: Oxford Univer-
sity Press.
Bachman, L. F. (2004). Statistical analyses for language assessment. Cambridge: Cambridge
University Press.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice: Designing and developing
useful language tests. Oxford: Oxford University Press.
Barkaoui, K. (2007). Teaching writing to second language learners: Insights from theory and
research. TESL Reporter, 40(1), 35–48.
Bereiter, C., & Scardamalia, M. (1987). The psychology of written composition. Hillsdale, NJ:
Erlbaum.
Brown, J. D. (1991). Do English and ESL faculties rate writing sample differently? TESOL
Quarterly, 25(4), 587–603. https://doi.org/10.2307/3587078.
Brown, J. D. (1996). Testing in language programs. Upper Saddle River, NJ: Prentice Hall.
Cohen, L., Manion, L., & Morrison, K. (2007). Research methods in education. London:
Routledge.
Cumming, A. (2002). Assessing L2 writing: Alternative constructs and ethical dilemmas.
Assessing Writing, 8(2), 73–83. https://doi.org/10.1016/S1075-2935(02)00047-8.
Cumming, A., Kantor, R., & Powers, E. D. (2001). Scoring TOEFL essays and TOEFL prototype
writing tasks: An investigation into raters’ decision making and developing of preliminary
analytic framework [Monograph]. TOEFL Monograph Series. Princeton, NJ: Educational
Testing Service.
Flower, L., & Hayes, J. (1981). A cognitive process theory of writing. College Composition and
Communication, 31, 21–32. https://doi.org/10.2307/356600.
Hidri, S. (2014). Developing and evaluating a dynamic assessment of listening comprehension in
an EFL context. Language Testing in Asia, 4(4), 1–19. https://doi.org/10.1186/2229-0443-4-4.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hyland, K. (2003). Second language writing. Cambridge: Cambridge University Press.
Designing and Rating Academic Writing Tests: Implications for Test. . . 151
Kroll, B., & Reid, J. (1994). Guidelines for designing writing prompts: Clarifications, caveats and
cautions. Journal of Second Language Writing, 3(3), 231–255. https://doi.org/10.1016/1060-
3743(94)90018-3.
Lumely, T. (2002). Assessment criteria in a large-scale writing test: What do they really mean to
raters? Language Testing, 19, 246–276. https://doi.org/10.1191/0265532202lt230.
Lynch, B. K., & Davidson, F. (1994). Criterion-referenced language test development: Linking
curricula, teachers and tests. TESOL Quarterly, 28(4), 727–743. https://doi.org/10.2307/
3587557.
Malone, M. E. (2013). The essentials of assessment literacy: Contrasts between testers and users.
Language Testing, 30(3), 329–344. https://doi.org/10.1177/0265532213480129.
Messick, S. (1989). Validity. New York: American Council on Education Macmillan.
Moss, P. A. (2003). Reconceptualizing validity for classroom assessment. Educational Measure-
ment: Issues and Practice, 22(4), 13–25. https://doi.org/10.1111/j.1745-3992.2003.tb00140.x.
Popham, W. J. (1978). Criterion-referenced measurement. Englewood Cliffs, NJ: Prentice Hall.
Reid, J., & Kroll, B. (1995). Designing and assessing effective classroom writing assignments for
NES and ESL students. Journal of Second Language Writing, 4(1), 17–41. https://doi.org/10.
1016/1060-3743(95)90021-7.
Ruth, L., & Murphy, S. (1984). Designing topics for writing assessment: Problems of meaning.
College Composition and Communication, 35(4), 410–422.
Swales, J. (1990). Genre analysis: English in academic and research settings. New York:
Cambridge University Press.
Swedler-Brown, C. (1993). ESL essay evaluation: The influence of sentence-level and rhetorical
features. Journal of Second Language Writing, 2(1), 3–17. https://doi.org/10.1016/1060-3743
(93)90003.
Weigle, S. C. (1999). Investigating rater/prompt interactions in writing assessment: Quantitative
and qualitative approaches. Assessing Writing, 6(2), 145–178. https://doi.org/10.1016/S1075-
2935(00)00010-6.
Weigle, S. C. (2002). Assessing writing. Cambridge: Cambridge University Press.
Weir, C. J. (2005). Language testing and validation. Hampshire: Palgrave Macmillan.
152 A. Mejri
Part III
Assessing English for Specific Purposes
Academic Writing Needs Assessment:
A Case Study of MPH Students, University
of Khartoum
Abuelgasim Sabah Elsaid Mohammed
Abstract This study aims at assessing Master’s of Public Health (MPH) students’
needs for academic writing. The study attempts to identify students’ target needs for
academic writing and their present situation in writing academic English. The study
adopted the qualitative approach through collecting data employing an interview
with two professors at the Faculty of Public and Environmental Health (PEH) and
analysing the Guidelines for the submission of theses and dissertations for higher
degrees in medical and health sciences, a manual prepared by the Medical and
Health Studies Board (MHSB), Graduate College, University of Khartoum and five
sample theses written by MPH students. The results showed that the students
needed to write a research report (thesis) containing four chapters with an abstract.
In addition, the students encountered problems in academic writing regarding
grammar, vocabulary, punctuation, and functions of language in academic writing.
Keywords Needs assessment • Academic writing • Needs analysis • ESP • EAP
1 Introduction
After the end of the Second World War, tremendous development happened in
science, technology, and economics. This development was international, and
consequently an international language was required. Due to the economic position
of the United States, English language became that language (Hutchinson &
Waters, 1987). Accordingly, learning English has become purposeful, not only
for pleasure, but because it is the language of technology and commerce. Learners
of English have become more aware of the reason for which they were learning
English language. These learners can be classified into two groups. The first are
those who study English for their jobs such as doctors or businessmen. The second
A.S.E. Mohammed (*)
University of Khartoum, Khartoum, Sudan
College of Science and Humanities, Prince Sattam bin Abdulaziz University, Hotat bani
Tamim, Saudi Arabia
e-mail: abuelgasims@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_8
155
are students at tertiary level who need to read the literature in their specialization,
the majority of which is found in English language (Hutchinson & Waters, 1987;
Lynch & Hudson, 1991).
As a result, learning English is needed to fulfil two types of needs: Immediate
and future needs. The former, is the needs of students who require English to
succeed in their academic study. The latter, is the needs related to job requirements,
environment, or workplace such as an engineer who needs to consult manuals in
English, and a practicing doctor who needs to cope with the latest developments in
his field (Hutchinson & Waters, 1987). In Sudan, Arabic is the language of
instruction in universities, but English is taught to undergraduate students as a
university requirement in the first two academic years (Ibrahim, 2010). Ibrahim
maintains that in the first year, students learn General English (GE), and in the
second year, they study ESP. The ESP material used in teaching students are not
based on any kind of needs analysis. Consequently, the amount of English and the
material taught may not be enough to improve students’ level of proficiency.
Nevertheless, most Master’s programme, at University of Khartoum (U of K) are
taught in English language, and students are required to write their theses in
English. In the taught part of the Master’s programmes, students do not study a
course in academic writing to prepare them for their theses writing. Students;
therefore, start writing their theses unaware of the requirements of writing academic
English. This might lead to sever problems regarding students’ writing product.
Accordingly, the purpose of this study is to investigate the Faculty of Public and
Environmental Health (PEH) Master’s students’ (MPH) needs for academic writing
in English language. Particularly, the study attempts to analyse students’ target
situation needs (TSA) and present situation (PSA). Therefore, the study aims to find
answers to the following questions:
1. What are Masters’ of Public Health (MPH) students target needs for academic
writing in English?
2. What is the MPH students’ present situation regarding writing academic
English?
2 Theoretical Background
2.1 ESP
Hutchinson and Waters (1987) define ESP as an ‘approach’ rather than a ‘product’.
They intend to say that ESP is not a specific type of language, methodology, or
teaching materials. Similar to Munby, they suggest that ESP is an approach to
language learning that depends on learner needs. ESP is based on a simple question:
Why does this foreign learner need to learn a foreign language? The answer to this
question connects the learner, the language, and the learning context. Thus, Hutch-
inson and Waters establish the importance of needs analysis in ESP. They conclude
156 A.S.E. Mohammed
by suggesting that ESP is an approach to language teaching in which all the contents
and methods of teaching rely on the learner’s reason for learning English. In their
definition of ESP, Dudley-Evans and St. John (1998) employ absolute and variable
characteristics. The former consists of three components. First, ESP is developed to
cater for certain needs of the learner. Second, it uses the methodology and the tasks
of the field it serves. Third, it focuses on the language (grammar, lexis, and
registers), skills, discourse, and genres suitable for these tasks.
The latter (variable characteristics) are four. Firstly, ESP may be related to or
planned for a specific specialization. Secondly, it may apply, in particular teaching
situations, a various methodology from that of General English. Thirdly, it is
probably designed for adult learners either at university level or in a job situation,
yet it could be taught to secondary school level students. Fourthly, it is designed for
students with higher language levels (intermediate or advanced) because it antici-
pates basic knowledge of the language. However, it can be taught to beginners
(Dudley-Evans & St. John, 1998). Johns (1991) explains that ESP is a trend, which
depends on the idea that all language teaching should be designed to meet the
specific learning and language use needs of a certain group of learners. In addition,
it should cater for sociocultural setting where the language will be used.
A number of scholars such as Mackay and Mountford (1978), Hutchinson and
Waters (1987), Johns (1991), and Basturkmen (2010) provide a classification of
ESP in terms of diagrams and acronyms. Belcher (2009) states that ESP will have as
many branches as there are specific learners’ needs and target situations in which
learners hope to survive. English for Specific Purposes is divided into two main
classes: English for Academic Purposes (EAP) and English for Occupational
Purposes (EOP). EAP is divided into four branches. English for Science and
Technology (EST) has been the main branch; however, English for Medical
Purposes and English for Legal Purposes (ELP) have always had their place.
English for Management, Finance, and Economics (EMFE) is a recent branch,
which has increasingly become important on Master’s of Business Administration
(MBA) courses, but no specific acronym has been established for it (Dudley-Evans
& St. John, 1998).
2.2 Needs Analysis (NA)
2.2.1 Definition and importance of NA
Hutchinson and Waters (1987) differentiate between two types of needs. The first is
target needs, which refer to what students are required to do in the target situation.
Target needs can be further divided into three classes. The first class is necessities
that means what students have to experience to perform in the target situation. The
second class is lacks. This means the gap between what students already know and
what is needed in the target situation. The third one is wants, which is used to refer
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 157
to what students feel they need. The second type of needs proposed by Hutchinson
and Waters (1987) is learning needs. This type involves taking into consideration
how learners learn. Furthermore, it includes information about learners, reasons for
learning the language, and the ESP course time and location (Hutchinson & Waters,
1987).
Nunan (1988) defines NA as the techniques used for gathering data to be
employed in curriculum design. He classifies NA into two kinds. The first is the
learner’s analysis. It generates information about the learner such as his purpose of
learning the language. In this case, objective and subjective information can be
obtained. The former refers to information about student’s age, nationality, and
native language. The latter is information about student’s beliefs, aims, and prior-
ities. Task analysis is the second type suggested by Nunan. This is used to
determine and identify the language skills needed to perform real-life communica-
tive activities. It often comes after learner analysis in which these activities are
identified (Nunan, 1988).
A number of scholars such as Hutchinson and Waters (1987), Nunan (1988),
Brindley (1989), Robinson (1991), Brown (1995, 2009), Seedhouse (1995), West
(1997), Graves (2000), Richards (2001), and Long (2005) agree that NA plays an
important role in ESP or GE course design. Hutchinson and Waters (1987) state that
in fact it is the consciousness of learners’ needs that differentiates between ESP and
GE. NA; therefore, is a main feature of ESP course design to the extent that Graves
(2000) suggests that NA should be considered as something which teachers can
practice as a part of their teaching.
Ali (2011) states that NA is the corner stone of ESP, and can produce a focused
course. Richards (2001) says that ESP begins with the analysis of students’ needs.
Various learners have different needs. This imposes some restrictions to both ESP
teachers and the ESP course. ESP teachers should be restricted to the needs of their
students. The ESP course restriction levels are identified by Strevens (as cited in
Richards, 2001). The NA restricts the basic skills; vocabulary, grammar forms, and
language functions; themes or topics, and communicative needs. Astika (1999)
states that NA is not only the starting point for material development, but it also
guides selection of contents, assessment, and classroom activities. Richards (2001)
explains that NA produces data, which can be used in variety of ways such as
evaluating a course, setting objectives, designing tests and assessment tools, and
providing information about a programme to an outside body or organization.
2.2.2 Approaches to NA
Various approaches can be identified to study NA (Ali, 2011; Kaewpet, 2009;
Songhori, 2008). Ali (2011) proposes that TSA and PSA are basic constituents
for analysing students’ language learning needs. The following are the approaches
used to study NA.
158 A.S.E. Mohammed
Target Situation Analysis
According to Songhori (2008), TSA was used by Chambers, who calls it commu-
nication in the target situation, in 1980 when he attempted to clarify the terminol-
ogy confusion. Kaewpet (2009) and Songhori (2008) agree that Munby employed
this model when he introduced his Communicative Needs Processor (CNP) in 1978.
For Hutchinson and Waters (1987) the CNP marked the maturity of ESP. The
machine for investigating any group of learners’ needs had been provided. Course
developers only had to run it.
West (1997) states that TSA was the most former approach to NA. It was used in
the work of Council of Europe in the 1970s. At that time the language needs of the
target situation was discovered by observing and investigating those situations,
which already exist in that context. These needs were called necessities or objective
needs since they act as the target of Language for Specific Purposes (LSP). TSA
works at different points to identify priorities regarding the language to be taught
(English, German, French etc.), the skills in the chosen language (reading, speak-
ing, writing etc.), the situation, and the functions or activities (speaking on the
phone, listening to lectures, etc.).
TSA can be best understood as a term that includes necessities, lacks, and wants
(Hutchinson & Waters, 1987). Target situation needs means what the learner is
required to perform in the target situation. TSA depends on asking questions about
the target situation and the attitudes of the participants toward that situation. TSA
includes six main questions, which are further divided into a number of other
questions. The main questions ask about the purposes for which the language is
needed, how the language is used, the content areas, who is involved in the
communication process, the context in which the language will be used, and the
time when the language will be used (Hutchinson & Waters, 1987). Robinson
(1991) considers TSA as a concentration on what students need at the end of the
course. Hyland (2006) propose that TSA involves “objective, perceived, and
product-oriented needs” (p. 73).
Present Situation Analysis (PSA)
PSA may be seen as a complement to TSA (Robinson, 1991; West, 1997). TSA
aims at identifying what learners should be able to do after the course whereas PSA
seeks to establish what learners are like at the beginning of the course. It states that
the “weakness and strength in language, skills, and learning experience” (Dudley-
Evans & St. John, 1998, p. 125). PSA; therefore, provides the starting point of the
course.
According to Songhori (2008), PSA was first introduced by Richterich and
Chancerel in 1980. It derives data from students, the educational organization,
and the professional establishment. It can be identified by placement tests. Never-
theless, information about learners’ years of learning English and educational level
can provide sufficient data about their aptitudes.
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 159
Robinson (1991) confirms that NA should be seen as a combination of TSA and
PSA. Hyland (2006) suggests that PSA provides both objective data (age, profi-
ciency, previous learning experience) and subjective information (self-perceived
needs, weaknesses, and strengths).
2.2.3 NA Data Collection Instruments
There is a common agreement on the variety of the methods used to collect data in
NA. For example, Basturkmen (2010), Brown (1995, 2009), Long (2005), Hyland
(2006), Richards (2001), Graves (2000), Dudley-Evans and St. John (1998), and
West (1997) agree that NA can be conducted using questionnaires, observations,
interviews, and analysis of authentic spoken and written texts. Interviews are used to
ask open-ended questions. They permit collecting private information from individ-
uals. This can result in real opinions (Brown, 1995). They are time-consuming;
therefore, they are used as follow up to another method such as questionnaires. There
should be a time limit for the interview, and it should be recorded to enable the
interviewer obtain suitable information (Dudley-Evans & St. John, 1998).
Analysing authentic texts is a means of conducting NA. It involves analysing
written texts, or audio and video recordings of lectures, meetings, or classroom
activities. It offers information about the target situation by identifying the linguis-
tic features of the situation (Dudley-Evans & St. John, 1998). Brown (1995)
suggests that authentic texts may exist inside or outside the programme location.
Examining students’ past evaluation is an example for information found inside
programme location while conducting a literature review is an example for infor-
mation that can be located outside. Texts analysis assists in identifying what
learners have to read and write if they will use the language mainly for reading
and writing (Dudley-Evans & St. John, 1998).
2.3 Related Studies
Rahaman, Ming, Abd Aziz, and Abdul Razak (2009) conducted a study to propose
an ESP writing course for foreign graduate students in three faculties at the National
University of Malaysia (NUM). To this end, they analysed the students’ PSA and
TSA. The study adopted an interview and document analysis to collect data. The
study sample was 10 foreign postgraduate students in the three faculties, NUM. four
master’s theses were analysed to identify the participants’ problem in writing
academic English. The study concluded that the subjects encountered difficulties
such as writing theses and journal articles in English language. Additionally, it was
found that the most frequent genre was writing a research reports (theses) the
majority of which were organized in five chapters: Introduction, literature review,
material and methods, results and discussion, and conclusion. In terms of sentence
structure, the study revealed that a big number of the sentences were incomplete
160 A.S.E. Mohammed
(fragments) and the sentences were not accurately punctuated. Moreover, few
cohesive devices were found in the theses. Most of the sentences were simple
sentences. The theses frequently used the present simple, but the past simple active/
passive voice was employed as well. Finally, semi-technical and technical vocab-
ulary were present in the four theses. Rahman and his co-authors proposed an ESP
writing course that covers areas identified to be problematic to their participants.
The course incorporates types of genres, grammar covering tenses; voice; modals;
articles; and connectors, paragraph writing, vocabulary and reference and research
skills.
Keong and Mussa (2015) investigate the academic writing problems encounter-
ing Iraqi postgraduate students at Malaysian universities. The study employed a
questionnaire and an interview for data collection. The study found that Iraqi
postgraduate students suffer from vocabulary problems, excessive grammatical
mistakes, and inability to paraphrase. Furthermore, the study revealed that students
have organizational difficulties such as poor referencing skills, weak organization
in their writing, and weak ability to express ideas.
Chien (2015) studied the Taiwanese graduate students’ problems in writing
research articles in English language. To collect data, Chien utilized a semi-
structured interview with 30 students. The study revealed that the students
confronted two types of problems in writing English. The first type was related to
students’ content knowledge such as choosing an acceptable research topic and
reviewing literature. The second was connected to the use of English which include
insufficient command of written academic English, lack of lexis; syntax; and
discourse to improve their writing skills. Moreover, students’ first language
affected their writing. Nevertheless, the students were motivated to solve these
problems due to their awareness to the importance of English language to academic
research writing. The study recommended that more practice in academic writing is
needed, writing courses must be developed and provided for training, and individ-
ualized guidance should be offered to the students.
Abdulkareem (2013) attempted to investigate the academic writing problems
encountered by Arab postgraduate students at University technology Malaysia
(UTM). Data were collected via a questionnaire and a writing task. The sample
was comprised of 80 Arab students who participated in the questionnaire, other five
students did the writing task. Abdulkareem concluded that 50% of the respondents
were good at English language. Mostly, students had problems in structuring
sentences, expressing ideas, using vocabulary, and writing words correctly (spell-
ing). The study recommended that students’ motivation toward academic writing is
needed to be promoted, constructivist approach should be adopted in teaching
writing, and causes behind these problems should be investigated.
Al-Khasawneh (2010) investigated academic writing problems of the Arab
postgraduate students at the College of Business, University of Utra Malaysia
(UUM), to suggest some solutions to these problems. The study employed inter-
views to collect data from the subjects. Al-Khasawneh concluded that students
needed to write a number of text types such as research reports, proposal, and case
studies. Students were faced by problems in vocabulary, grammar, organization,
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 161
expressing ideas, spelling, and referencing. The study proposed some solutions to
the problems. These included enhancing teachers’ roles in following up students
writing, increasing practice in writing, and emphasizing learners’ role in solving
these problems.
3 Method
The study adopted the quantitative approach. It is an analytic study that analysed
MPH students’ needs and academic English writing problems. To this end, the
study employed two data collection methods.
3.1 Instruments
The first instrument was an interview. Interviews are one data collection methods in
NA, and they can provide in-depth information regarding a certain topic. The
interview questions were open-ended focusing on gathering information about
MPH students’ present situation in academic writing in English, target situation,
and the syllabus focus. Thus, it contained three questions.
1. What do MPH students need to write? And how?
2. How can MPH students’ writing in English be described?
3. What should an academic English writing course focus on?
The second data collection tool was documents analysis. It is another means of
NA data collection which is regarded essential (Dudley-Evans & St. John, 1998.
The purposes of document analysis were to two folds. First, it attempted to discover
the contents of MPH students these. Second, it sought to investigate MPH students’
problems in writing academic English. First, the study analysed the Guidelines for
the submission of theses and dissertation for higher degrees in medical and health
sciences, a manual prepared by the Medical and Health Studies Board (MHSB),
Graduate College, U of K. The purpose of this analysis was to investigate the
requirements of writing a thesis as decided by the MHSB. This analysis informs
about the target needs and how students must structure their theses. The second
document analysis focused on assessing theses written by MPH students. The
analysis sought to identify the students’ present situation in writing academic
English. Analysing students’ writing can be informative in terms of quality. It
shows students’ writing in practice. The analysis was conducted based on the
framework proposed by Ellis and Johnson (1994). Table 1 shows the framework.
Five theses were selected for analysis. These theses were written during the
academic years 2013–2016. They are written in English as partial fulfilment for the
requirements of the degree of MPH. They were of varying sizes ranging between
90 and 108 pages (Table 2).
162 A.S.E. Mohammed
3.2 Participants
The semi-structured interview was conducted with two professors at the Faculty of
Public and environmental Health. The first one is the MPH Programme coordinator
and the second was one of the teaching staff. Both of them are assistant professors
with more than 10 years teaching experience. They have not only been teaching in
the Programme, but they also supervised students’ theses for many years.
3.3 Procedure
The informal semi-structured interview was conducted with two professors at the
faculty of Public and Environmental Health. It was in Arabic language and it
Table 1 Ellis and Johnson framework for language analysis
Focus Description
Genre What type of text is being analysed? (e.g., report, letter, memo, etc.) Who is
the target audience?
Organization How is the text organized? What is the layout? How many paragraphs/
sections are there? Is there an introduction/conclusion? Is there list of points/
cohesive paragraphs?
Sentence
structure
Are sentences complete or in note form? Are they correctly punctuated? Are
they linked with cohesive devices? Are sentences simple/complex? Are
there relative or other clauses? Are there ellipses in sentences?
Function What functions are being expressed? e.g., condition, intention, description,
request, order
Grammatical
structure
What are the most frequent grammatical structures? e.g., active/passive,
verb forms, complex noun phrases, prepositional phrases, verb tenses
Lexis What type of vocabulary is used?
Table 2 Titles of theses under analysis
Sample Title of thesis Year Notes
1. Knowledge, attitudes, and practices among mothers
towards diabetes in Elsahafa Administration Unit,
Khartoum State
2015
2. An Assessment of measles surveillance system in White
Nile state, in the period from 2009 to 2012
2013 The title is writ-
ten in this way
3. Food taboos among pregnant women in hospital and
health centres, Khartoum State
2016
4. Influence of ergonomic factors among dentists in
Khartoum Dental Teaching Hospital
2016
5. The efficiency of lighting in libraries and lecture rooms in
the Medical Campus, U of K
2016
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 163
discussed the professors’ opinion about their students’ writing, the genre students
asked to write and how it should be written, and their ideas about the syllabus to be
proposes by this study. The interview lasted for 10 min. The five these were
numbered 1–5 to make it easy for analysis. The analysis covered areas including
genre, organization, function, sentence structure, grammatical structure, and lexis.
The analysis then looked at the various chapters in the thesis to identify how
students write these chapter and what structures they used.
4 Results and Discussion
This section focuses on the results and their discussion. The results are shown under
the respective research question.
What are Masters’ of Public Health (MPH) students target needs for academic
writing in English?
This question aimed to identify MPH students target needs and it was
represented in two questions in the interview. The interviewees stated that MPH
students needed to write a research project (thesis) to meet the requirements of the
Degree. The thesis should contain four chapters: Introduction and literature review,
material and methods, results, discussion, and a conclusion. Additionally, there
should be an abstract in both English and Arabic, table of contents, list of refer-
ences, and appendices if applicable. The thesis should use either Harvard or
Vancouver referencing system.
Similarly, the first research question is answered by the analysis of the Guide-
lines for the submission of theses and dissertation for higher degrees in medical and
health sciences. The analysis revealed that it contains 11 pages covering various
points required in theses and dissertations. There are a number of sections in the
Guidelines. The first one focuses on the type of paper, space, margins, font type and
size, number of copies to be submitted. The second section deals with the lay out of
the thesis. It starts with the abstract which should be written to cover the following
areas: Background, design, setting, objectives, materials and methods, results, and
conclusion. Students were shown what to include in each of these subheadings in
the abstract. The following section concentrates on the materials and methods of the
thesis. It shows students how to write this chapter including describing the study
design, setting or location or study area, data collection tools, statistical package
used, and procedures. The next section discusses the result chapter which should
include tables/figures with suitable expressive titles. The discussion section states
that it should be consistent with the results chapter, compared with that of the
previous studies section in the literature review, explained and interpreted. The
discussion must be followed by a conclusion and recommendations that must be
written in points and should be relevant to the findings. The last section in the
manual deals with the referencing system that must be either Harvard or Vancouver.
It provides some examples.
164 A.S.E. Mohammed
The analysis of the five theses also catered for genre, organization, and function
which are relevant to the first research question, students’ target needs. In terms of
genre, it was found that the students needed to write a research report (thesis). The
thesis is a partial fulfilment for the requirements of the degree of MPH. regarding
the thesis organization, the analysis revealed that four theses (samples 2, 3, 4, and 5)
contained four chapters—introduction and literature review, materials and
methods, results, and discussion—as required by the Guidelines while one of
them (sample 1) consisted of five chapters. This sample thesis separated the
introduction and literature review as chapter one and two in addition to the other
three chapters. This may be due to the supervisor’s preference. All the theses
enclosed appendices. It was also noted that all the theses did not include any
introductions and conclusions in all the chapters. Two theses employed lists and
bullets specifically in chapter two. As for the function of the theses, it was mainly
description. These findings suggest that the MPH students were provided with clear
guides that assist them write their theses appropriately. The guides make writing the
theses easy and accessible. These results concur with what was found by Rahaman
et al. (2009). They found that their students needed to write a thesis containing four/
five chapters with similar titles revealed by this study.
What is the MPH students’ present situation regarding writing academic
English?
This study question aimed at identifying students’ present situation in writing
academic English and it was answered by the second question in the interview and
the theses analysis. The interviewees provided a negative report about their MPH
students’ abilities in writing academic English language. They stated that the
students were poor in writing English. Their writing comprised mistakes related
to grammar, vocabulary, punctuation, and expressing ideas. They commented that
this lead to problems such as delaying students’ graduation. The theses analysis
focused on sentence structure, grammatical structure, lexis, and the language used
in writing the theses chapters.
4.1 Sentence Structure
In terms of sentence structure, all the theses contained sentence fragments and
run-on sentences. For example, the following fragments are found in the five
samples.
1. Mothers at home without taking account of age or home ownership or rent, it can
be the economic breadwinner for her family.
2. Table 3 Shows that the Supervision for Measles surveillance activities as flow:
3. Khartoum state’s population about eight million people
4. The results of this study adopting inadequate postures to gain better vision of the
oral cavity, performing exaggerated flexions or cervical torsions could produce
muscular pain.
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 165
5. Table 10 shows if student feel comfortable in library pointed out 89 students feel
comfortable and 277 not feel comfortable.
The fragments lacked a subject and a verb in sample (1), a subject in samples
(2 and 5), a verb in sample (3), and verbs in sample (4).
The following run-ons are from the five samples.
1. A descriptive cross sectional community based study was conducted in Elsahafa
Administrative Unit with objective to study knowledge, attitude, and practice
among mothers towards diabetes in Khartoum Locality and it revealed the
following findings as show in the tables below:
2. In Sudan measles is an endemic disease it is the third common cause of
childhood deaths, preceded by gastroenteritis and non-specific fever.
3. There are educational services including; basic, higher secondary schools and
universities, in Khartoum State there are 1651 governmental basic school;
915 private basic school, 398 governmental secondary school and 552 private
secondary schools, 7 governmental universities and 33 private universities.
4. A descriptive cross—sectional study, the study was conducted in Khartoum
Teaching Dental Hospital in Khartoum State with an objective to assess the
Ergonomics factors among Dentists in the Hospital.
5. This descriptive cross section study was conducted in assessment of lighting of
Lecture rooms and libraries at the medical campus in Khartoum University with
an objective of the studying assessment lighting efficiency of libraries and
lecture rooms at medical campus in Khartoum University.
Run-on sentences result in long fused sentences. This implies that the
researchers suffer from the problem of writing grammatically correct sentences.
The results also suggest that this problem is might be caused by poor proficiency in
writing due to students’ previous experience in learning English. These findings are
consistent with those of Rahaman et al. (2009), Keong and Mussa (2015), Chien
(2015) and Abdulkareem (2013). They revealed that the studies they investigated
contained problems in structuring sentences. Moreover, there were several punctu-
ation problems in all the theses. For example, in samples (2 and 5) the following is
located:
2. An interview was carried out with Epidemiology department manager revealed
that; Although locality EPI managers were not involved in measles surveillance
system but regular feedback was received from central level and distributed to
them.
5. This descriptive cross section study was conducted in assessment of lighting of
Lecture rooms and libraries at the medical campus in Khartoum University
The researchers either used a punctuation mark where it was not needed or they
did not use it where it was required. Punctuation is one of the most essential
components in writing good and accurate English. It seems that the five students
did not have adequate training in punctuation. These results are in accord with what
Rahaman et al. (2009) found. They concluded that the postgraduate students at
NUM had problems in punctuating sentences correctly.
166 A.S.E. Mohammed
Regarding cohesion and the use of cohesive devices, the majority of the
theses did not show them. This resulted in tasteless texts. It might be predicted
that the researchers were not aware of the importance of cohesion and cohesive
devices. The findings also indicate that the five writers did not know about
cohesion and cohesive devices. These findings are in line with what was
revealed by Rahaman et al. (2009), Chien (2015), and Al-Khasawneh (2010)
who concluded that their participants did not use the cohesive devices and their
writing lacked cohesion.
The sentences in the five theses ranged between simple and compound sentences
with few complex sentences. The use of various sentence types lends variety to the
style and formality in academic writing. Nevertheless, lack of complex sentences
tends to make students’ writing less complex. Thus, this leads to lack of grammat-
ical complexity required by academic writing. This finding contrasts to what was
found by Rahaman et al. (2009). They found that the sentences in theses they
examined were mainly simple. It was noticed that in two theses (samples 1 and 4) a
number of sentences begin with numbers, particularly in reporting results as shown
in the following examples.
1. 58.2% of mothers were knew the complications of diabetes and 41.8% did not
know as shown in Tables 4-13.
98.9% of diabetic mothers intake one bread in meal as shown in Table 4-40.
4. 55.2% of the dentists consider that the work stool is uncomfortable.
51.6% of the dentists consider that the work environment is always noisy.
This is an indication to the fact that the two researchers were not aware to the
correct academic style. This might be caused by lack of training on formal writing.
4.2 Grammatical Structure
This section investigates four areas including the most frequent structures, subject-
verb agreement, wrong use of tenses, and the use of articles. First, the most frequent
grammatical structure in the five theses was the present simple active and passive.
Furthermore, they also contained past active and passive simple tense. Second, in
terms of subject verb agreement, all the sample theses showed verbs that do not
agree with their subjects.
1. Similar findings was obtained by William, et al., 2010 found that only (29%) of
respondents had good knowledge of signs and symptoms of diabetes while (71%)
of respondents had poor knowledge on what diabetes is.
2. Study revealed that; overall, reported suspected measles cases has been
increased from (21) cases in 200 to (308) in 2012 this showed in Table 1.
3. pregnant women preferred to eat certain types of food during pregnancy such as
milk and fruits because it strengthens
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 167
4. Occupational hazards is defined as a risk to a person in his/her working
environment.
5. all lecture rooms is complain with WHO standards (200___500 lux for library
and lecture room),
Third, concerning the wrong use of tenses, there were numerous examples in
four theses shown below.
1. From Tables (4-1a) and (4-1b) a number of (77.6%) of mothers were work, and
(22.4%) were not work.
2. Table 2 Showed that the standard measles case definition was presence by
(100%) in state and localities levels both in EPI and Epidemiology Department,
3. Table 1 showed the socio-demographic characteristic among pregnant women.
4. The majority 74.0% of sample were in age group (Less than 30 years).
5. The study showed that (73.8%) of students said litigating is suffusion in library.
Finally, article usage also constituted a problem to the five researchers as there
were extensive examples for this. It was noted that the majority of these errors were
in using the definite article. The following are examples from the five theses.
1. The majority of mothers were work a number of (77.6%) of mothers were work.
The Discoveries of twentieth century:
2. During the past years White Nile State faced by many outbreaks such as. . .
3. The main water sources in Khartoum State are River Nile; ground water through
4. Majority of dentists had musculoskeletal disorders problems in more than one
site, which may be due to the complex nature of work.
5. The study raveled that the average of lighting in library at faculty of public and
environmental health at day is (556 lux) and at night is (480 lux)
The existence of these grammatical errors suggests weaknesses in the English
language courses taught to these five students in universities or secondary schools.
They also indicate a gap in MPH programme in preparing its students to write
correct English. This gap might be caused by lack of academic writing course or
training workshops.
The previous studies reported in this study, generally, investigated grammatical
problems students encountered without providing any details. However, the find-
ings if this study agree with what was found by Rahaman et al. (2009) who reported
that the most frequent structures in graduate students’ theses at NUM were the
active/passive forms of the present simple and the past simple.
4.3 Lexis
In the five sample theses, a considerable number of academic vocabulary was
found. This provides a sense of formality to the theses. In addition, scientific
vocabulary was also noted. This can be attributed to the nature of public and
168 A.S.E. Mohammed
environmental health field which is closely related to medicine. These results are
consistent with those of Rahaman et al. (2009). They found that semi-technical and
technical vocabulary were located in the theses they investigated. The following are
examples from three theses studied by this study.
1. Academic vocabulary: Hypothetical, knowledge, attitudes, practices, present,
majority,
Scientific vocabulary: Mellitus, metabolic, Prevalence
2. Academic vocabulary: Classified, transmission, Communicability, contracting,
specimen
Scientific vocabulary: Infection, respiratory, epithelium, nasopharynx,
asymptomatic, incubation period
3. Academic vocabulary: Environmental, adequate, ignorance, fundamental,
incidence
Scientific vocabulary: Proscriptions, transmitted
Concerning spelling mistakes, a number of mistakes occurred in the theses,
specifically in sample 5. This implies that the students are not aware of the correct
spelling in English and its importance. This may be an effect of the researchers’
mother tongue, Arabic, whose spelling is different from English. These findings
match those revealed by Rahaman et al. (2009), Abdulkareem (2013), and
Al-Khasawneh (2010) who concluded that spelling mistakes were common in the
studies they examined. Chien (2015) also revealed that his students’ mother tongue
affected their writing.
4.4 The Language in the Theses Chapters
The TSA showed that MPH students needed to write a research report containing an
abstract, four chapters; introduction and literature review; materials and methods;
results; and discussion including conclusion and recommendations. When examin-
ing the sample theses chapters, it was observed that the language used to show the
various functions of the chapters was not appropriate. This section discusses some
features noted.
First, in the introduction was divided into three sections: Introduction, literature
review, and previous studies. Reporting verbs commonly utilized in reporting
literature were rarely used in these sections. In addition, quotation marks were
completely absent. These results suggest that the researchers are not
acknowledgeable about the importance of using reporting verbs and quotation
marks. It was; therefore, difficult to classify what is written in these sections as
summary, paraphrase or quotation. The language used was straightforward and
correct what implies that this might be direct copying from sources. The results also
imply that the students did not know about how to avoid plagiarism.
In writing the previous studies section, all the theses do not show a systematic
pattern to report previous research. All the studies showed the authors of the
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 169
reported studies at the end of the paragraph. They did not show the methodology
followed by the studies, and they did not show any comments on the papers they
reviewed. For instance, the following extract is from sample 2.
Study was conducted to assess communicable diseases surveillance system in Khartoum
state, Sudan in period 2005–2007 revealed that poorly documented at lower levels, the
system was not updated, and it lacked proper feedback system for both data reporting and
supervision. The system was also facing a problem of staff shortage at lower levels. In
addition, the epidemic preparedness was centrally organized and it was poorly functioning
at lower levels. The system was not representative, it was incomplete, it lacked timeliness,
it was partially flexible, and it had poor incentives among the staff, which led the system to
be not very useful.
Second, in the results chapter, all the theses incorporated tables and two of them
used tables and figures to display the results. It is worth mentioning that the titles of
these tables/figures were so long and complicated. The Table 3 is from sample 3.
Another point is that three theses described the results immediately after the
tables/figures, but the other two showed only the tables/figures and the results were
reported in a section entitled results review. In reporting the results from the tables/
figures, four of the sample theses used the past simple instead of the present simple.
All the samples employed the verb showed repeatedly to report the results. It seems
that this problem resulted from inadequate training on writing research results. This
might be attributed to absence of academic writing course in the MPH programme.
The examples below are from samples 2, 3, and 4.
2. Table 2: Showed that the standard measles case definition was presence by
(100%) in state and localities levels both in EPI and Epidemiology Department,
while it was found in health facilities by (100%) in EPI Department and (90%) in
Epidemiology Department.
3. Table 17 showed that 64.1% of pregnant women refuse to eat certain foods
during pregnancy for personal reasons, and 35.9% for community reasons.
4. Table 6 showed there was statistical association between gender and presence of
muscular pain among dentists (p value ¼ 0.027).
Third, the discussion chapter in all the sample theses starts in the sentence: “This
descriptive cross-sectional (. . .) study, was conducted in (. . .) with an objective to
study (. . .), and it revealed the following findings as explained in the discussion
below”: The studies then showed their results. Four of the samples compared their
findings to other previous studies on the same topic. Nevertheless, this inappropri-
ately stated as shown in the following example from sample 2.
Table 3 Types of reasons
behind refusing to eat certain
types of food during
pregnancy among pregnant
women—Khartoum State
2016 (n ¼ 145)
Reasons No. %
Personal reasons 93 64.1
Community reasons 52 35.9
Total 145 100.0
170 A.S.E. Mohammed
The study revealed that updated guideline surveillance (developed edition 2011) was
presence and available in all levels both two departments of the system, although lower
rate in health facility (11.7%) in EPI department presence results were better than the
Khartoum CDSS 2005–2007 where most of the studied health facilities had not even the
outdated manual available.
Moreover, the discussion chapters did not incorporate any interpretations for the
results, something required in research writing. Once more, these findings indicate
that absence of academic writing course could be the major cause of these prob-
lems. Fourth, recommendations, the third section in the fourth chapter in four
samples and in chapter five in one, were poorly written. They did not employ any
of the modal verbs required in recommendations section. For instance, samples
1 and 3 wrote the recommendations in the following way:
1. Thus, there is need for arranging large scale awareness programs for the general
public and also to identify and use media to spread the message which could
change the attitude of our public in the future.
2. Establishing a nutritional counseling during the monthly visits among pregnant
women to change pregnant women’ behavior toward food taboos during
pregnancy.
The results confirm the researchers’ improper preparation for writing academic
English. The results also indicate that the researchers did not pay attention to
academic writing conventions when reading other research. This is because most
of these errors could easily be avoided when reading previous research.
5 Conclusions
In conclusion, this study aimed at assessing MPH students’ needs for writing
academic English through analysing their TSA and PSA. The study revealed that,
in future, the students needed to write a research report (thesis) that contains four
chapters (introduction, literature review, materials and methods, results, and dis-
cussion) with an abstract and a list of references following Harvard or Vancouver
referencing systems. The study also found that the students encountered a number
of problems in grammar, vocabulary, punctuation, and writing technicalities in all
chapters. The students were not aware of many academic writing conventions and
style. The study attempted to fill in the gap in the area of academic writing needs of
Sudanese postgraduate students. The needs of MPH students are made ready by this
study, so an academic English writing syllabus is desperately needed to meet these
needs.
The results pose some implications. Methodologically, using quantitative instru-
ments would be advantageous. For instance, a questionnaire could be used to
investigate students’ perceptions of the problems they confront in writing academic
English. Moreover, interviewing students could add more information which would
be useful. Pedagogically, the results suggest that University of Khartoum policy of
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 171
teaching English language needs revision. They also imply that teaching writing
generally and academic writing in particular face a problem in Sudan. English
courses and teachers neglect focusing on writing mechanics, grammar in writing,
and writing practice. The results also imply that postgraduate programmes at U of K
suffer from lack of writing courses to assist students write good English. In terms of
research, it appears that most research tends to ignore academic writing problems
facing Sudanese postgraduate students. More research on other fields focusing on
students’ academic English writing problems would provide insight and enables the
generalization of the results. Additionally, research on postgraduate studies policies
is urgently needed.
Based on these findings, the study suggests the following recommendations.
• University of Khartoum must review its policy toward teaching English lan-
guage at the undergraduate level regarding number of credit hours.
• English language courses taught to undergraduate students must be reviewed.
• Academic writing courses must be taught to the fourth or fifth year students.
• Medical and Health Studies Board should introduce academic writing courses to
Master’s students to prepare them for future research writing.
• An academic writing syllabus must be designed to solve MPH students’
problems.
• Research on other disciplines students’ academic writing problems is needed.
References
Abdulkareem, M. (2013). An investigation study of academic writing problems faced by Arab
postgraduate students at Universiti Teknologi Malaysia (UTM). Theory and Practice in
Language Studies, 3(9), 1552–1557. https://doi.org/10.4304/tpls.3.9.1552-1557.
Ali, A. A. (2011). A needs analysis for designing an ESP syllabus for the students of Sudan Naval
Academy. Unpublished doctoral dissertation, Al Neelain University.
Al-Khasawneh, F. M. S. (2010). Writing for academic purposes: Problems faced by Arab post-
graduate students of the College of Business, UUM. ESP World, 2(28), 1–23. Retrieved form
http://www.esp-world.info
Astika, G. (1999). The role of needs analysis in English for specific purposes. TEFLIN Journal, 10
(1), 17. Retrieved from http://www.teflin.org/index
Basturkmen, H. (2010). Developing courses in English for specific purposes. London: Palgrave
Macmillan.
Belcher, D. (2009). What ESP is and can be? An introduction. In D. Belcher (Ed.), English for
specific purposes in theory and practice (pp. 1–20). Michigan ELT: Michigan.
Brindley, G. (1989). The role of needs analysis in adult ESL programme design. In R. K. Johnson
(Ed.), The second language curriculum (pp. 63–78). Cambridge: Cambridge University Press.
Brown, J. D. (1995). The elements of language curriculum: A systematic approach to programme
development. Boston: Heinle & Heinle Publishers.
Brown, J. D. (2009). Foreign and second language needs analysis. In M. H. Long & C. J. Doughty
(Eds.), The handbook of language teaching (pp. 269–293). London: Wiley-Blackwell.
Chien, S. (2015). Graduate students’ perceptions of the problems in writing research articles in
English in higher education: A Taiwan-based study. The Asian Conference on Education &
International Development 2015. Retrieved from http://papers.iafor.org/papers/aceid
172 A.S.E. Mohammed
Dudley-Evans, T., & St. John, M. J. (1998). English for specific purposes: A multi-disciplinary
approach. Cambridge: Cambridge University Press.
Ellis, M., & Johnson, C. (1994). Teaching business English. Oxford: Oxford University Press.
Graves, C. (2000). Designing language courses: A guide for teachers. Boston: Heinle & Heinle
Publishers.
Hutchinson, T., & Waters, A. (1987). English for specific purposes: A learning-centred approach.
Cambridge: Cambridge University Press.
Hyland, K. (2006). English for academic purposes: An advanced resource book. London:
Routledge.
Ibrahim, A. M. (2010). ESP at the tertiary level: Current situation, application, and expectation.
English Language Teaching, 3(1), 200–204. Retrieved from http://www.ccsenet.org/elt
Johns, A. M. (1991). English for specific purposes (ESP): Its history and contributions to ELT. In
M. Celce-Murcia (Ed.), Teaching English as a second or foreign language (2nd ed.,
pp. 67–78). Boston: Heinle & Heinle Publishers.
Kaewpet, C. (2009). A framework for investigating learner needs: Needs analysis extended to
curriculum development. Electronic Journal of Foreign Language Teaching, 6(2), 209–220.
Retrieved from http://www.e-flt.nus.edu.org
Keong, Y. C., & Mussa, I. H. (2015). Academic writing difficulties of Iraqi postgraduate students
in Malaysia. International Journal of Education and Research, 3(6), 25–34. Retrieved from
http://www.ijern.com
Long, M. H. (2005). Overview: A rationale for needs analysis and needs analysis research. In
M. H. Long (Ed.), Second language needs analysis (pp. 1–15). Cambridge: Cambridge
University Press.
Lynch, B., & Hudson, T. (1991). EST reading. In M. C. Murcia (Ed.), Teaching English as a
second or foreign language (2nd ed., pp. 216–230). Boston: Heinle & Heinle.
Mackay, R., & Mountford, A. (1978). English for specific purposes. London: Longman.
Medical & Health Studies Board. (n.d.). Guidelines for the submission of theses and dissertations
for higher degrees of the University of Khartoum in medical and health sciences. Khartoum:
University of Khartoum Press.
Nunan, D. (1988). Syllabus design. Oxford: Oxford University Press.
Rahaman, M., Ming, T. S., Abd Aziz, M. S., & Abdul Razak, N. (2009). Needs analysis for
developing an ESP writing course for foreign postgraduates in science and technology at
National University of Malaysia. Asian ESP Journal, 5(2), 34–59. Retrieved from http://
www.asian-esp-journal.com
Richards, J. C. (2001). Curriculum development in language teaching. Cambridge: Cambridge
University Press.
Robinson, P. (1991). ESP today: A practitioner’s guide. New York: Prentice Hall International.
Seedhouse, P. (1995). Needs analysis and the general English classroom. ELT Journal, 49(1),
59–65.
Songhori, M. H. (2008). Introduction to needs analysis. English for Specific Purposes World, 4
(20), 1–25. Retrieved from http://www.esp-world.heliohost.org
West, R. (1997). Needs analysis: State of the art. In R. Howard & G. Brown (Eds.), Teacher
education for languages for specific purposes (pp. 68–97). London: Multilingual Matters.
Academic Writing Needs Assessment: A Case Study of MPH Students, University. . . 173
Teachers’ Conceptions of Assessment
in an ESP Context
Yosra Naimi
Abstract This study is an attempt to investigate the field of ESP testing, determine
its teachers’ conceptions of assessment and find out the extent to which those
conceptions correspond to classroom practices. It was carried out on a sample of
150 ESP teachers from different universities and institutes in Tunisia. Data were
collected using a questionnaire, interview and test scores. Data analysis demon-
strated that teachers have positive attitudes towards assessment. They maintained
that testing plays an important role in both language learning and teaching and
provides information about the efficacy of materials and teaching methodologies.
Results of this study further show that testing has a major influence on the different
aspects associated with the curriculum: Lessons’ content, classroom materials since
teachers make use of parallel forms and test-related materials. Teachers’ under-
standing of the notion of washback effect was further confirmed by tests ‘scores as
well as their classroom practices; yet it is rather recommended that other studies
would be carried out in the same environment because of the lack of local sources
and studies on the status of ESP.
Keywords Teachers’ perceptions • Washback • Teaching materials • Teaching
methodologies • Curriculum • Teaching outcomes • Teaching practices
1 Introduction
The present study, examining teachers’ perceptions of assessment in the Tunisian
ESP context, is grounded within the field of Applied Linguistics (AL) which is an
umbrella term that covers a wide range of studies that focus on language users and
the way in which they use language in a variety of contexts. The study investigated
teachers’ conceptions of English language for specific purposes (ESP) assessment
Y. Naimi (*)
Department of English, The University of Letters, Arts and Humanities of Manouba, Tunis,
Tunisia
e-mail: naimiyosra@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_9
175
in Tunisia. Then it highlighted the importance of tests and assessment in ESP
teaching and investigated the gap between teachers’ conceptions and practices.
Tests are known to serve a variety of functions within a language program; they
may help administrators and teachers to make classroom decisions (Bachman &
Palmer, 1996). In addition to determining students’ weaknesses and strengths, tests
help evaluate the effectiveness of different approaches to language decisions: In
other words, tests can make sound choices about teaching methodologies and
materials. Aware of this, policy makers use them to manipulate the educational
systems, the curricula, and the teaching methods and materials (Cheng &
Watanabe, 2004). In other words, tests seek providing continuous feedback on
performance, materials, processes, and learning outcomes. The obtained feedback
helps improve language programs and ensure the coherence of different compo-
nents of the language curriculum. Testing in ESP has been considered by Dudley-
Evans and St. John (1998) to be a distinctive part of a general movement of English
language testing. They go further to explain that it aims basically to measure
determined uses of English language among different groups of people such as
secretaries, lawyers, businessmen and others. They claim also that it occupies an
important place in the ESP process as it interacts with needs analysis, and is
dependent on course design. That means that tests do play an important role in
ESP as they enhance learning, motivate learners and intensify their sense of
accomplishment.
2 Theoretical Background
An important factor that interferes in ESP testing is the washback effect that was
defined by Green (2013) as that of impact that investigates the relationship between
test use and the situation in which it is employed. In other words, it is the effect tests
have both on language teaching and classroom instructions or simply on the content
offered to the language learners and the way they are being taught. Tests and
especially important ones may generate two types of washback: A positive or
negative one. If the ESP test is based on the same language needs that learners
would have to accomplish in their real-life, it will be more likely to generate a
beneficial effect both on the way they learn the language and their interest on the
course, which will influence also the teacher and motivate him to improve his
materials and teaching methodologies. Teachers play an essential role in testing due
to the fact that they have the immense ability, to inspire their students, to help them
master language skills, and to model how to deal with tests and their outcomes
(Bailey, 2005). Therefore, teachers’ conceptions of assessment are very important.
This issue was the center interest of several studies that investigated the connection
that exists between the conceptions teachers have and the way they act in their
classrooms. However, each concentrated on a different context and tackled it from a
different perspective.
176 Y. Naimi
2.1 Research Problem and Rationale
ESP teachers represent an important element in Language teaching. In Tunisia, they
do not play only the role of teachers. In most cases, they are responsible for
designing their courses, tests and providing the necessary materials (the Ministry
of Higher Education provides no program, teaching materials for ESP teachers)
although most of them are not trained to do this. ESP teachers at the tertiary level
have no formal training in teaching ESP (Daoud, 1996), nor in testing it. No
program has been established to train them to face the demands of their roles as
ESP teachers. This lack of specialized training, materials and program may influ-
ence teachers’ performance and the way they deal with tests either in the process of
their development or in their content. As a result, we may say that teaching quality
and efficiency has a great impact on tests and their results.
Different studies dealt with the subject in hand and were interested in determin-
ing the nature of the relation between tests and language teaching, such as the one
conducted by Wu (2012) about GEPT and English language teaching and testing
where GEPT is an abbreviation of General English proficiency test and the study
conducted by Amengual (2010) about the effect of a “high-stakes English test” on
the teaching of English in Spanish Upper Secondary schools. However, few sources
and studies (Daoud, 1996; Hidri, 2015) carried out concerning this issue in the local
environment are available. There is hardly any study conducted on the investigation
of ESP teachers’ conceptions and practices of assessment. Therefore, the present
study will be an attempt to determine LSP teachers’ conceptions of assessment in
the Tunisian ESP context: How do they deal with the curriculum, materials and
what teaching methodologies they opt for? It focuses also on determining the
impact tests have on teaching (washback effect): How does it affect the teaching
procedure and whether this effect can be controlled or predicted form teachers’
perspective? The last point will be to check whether a gap exists between ESP
teachers’ conception of assessment and their classroom practice. The study, there-
fore, was an attempt to answer the following questions:
1. How do ESP teachers conceive of assessment?
2. Can the washback effect be controlled or predicted from ESP teachers’
perspective?
3. To what extent do teachers’ perceptions of assessment correspond with their
instructional practices?
3 Method
This study was conducted at twenty different Tunisian universities where English is
taught for specific purposes such as the Institute of Business studies Carthage, the
Higher School of commerce Tunis and the Higher Institute of Biological Sciences.
Hence specific aspects of the language were considered. Most ESP students in the
Teachers’ Conceptions of Assessment in an ESP Context 177
concerned universities sit for this course during the two semesters of the academic
year, about 2 h per week. At the Higher institute of Biological Sciences for example
students study English for 2 h a week in groups that combine two classes. The
subjects of this study were Tunisian ESP teachers giving courses at different
universities in Tunisia. They have been chosen randomly: The only criteria to
choose them, is their job as ESP teachers and of course their willingness to answer
both the questionnaire and the interview. Teachers’ profiles were better explained in
the following table that classifies the study participants according to their gender,
teaching experience and the previous teaching and testing ESP trainings they had.
The ESP teachers’ sample used in this study presented 23.33% of males and 75%
of females with 23.33% of teachers having a teaching experience less than 5 years,
63.33% with a teaching experience that ranges between 6 and 10 years and 13.33%
who taught ESP for more than 10 years. The sample consisted also of only 38% of
teachers who had a previous ESP teaching training and 40% of teachers who were
trained in ESP testing.
Data for the study were collected by different tools of data collection. The first
was a questionnaire directed to ESP teachers. It is based on a preliminary one (see
Amengual, 2010) which addressed the same issue of teachers’ perceptions of tests’
effects on: The curriculum, materials, teaching methodologies as well as their
attitudes and feelings. Qualitative data, however, were gathered using a semi-
structured interview that was adopted from a previous one used by Tsagari
(2007). The last data collection phase was collecting tests’ scores in order to
analyze and compare them to both questionnaire and interview results. The Statis-
tical Package for the Social Sciences (SPSS) 17.0 software was employed to probe
the questionnaire results. The data gathered out of the questionnaire were exposed
to descriptive statistics using mean, Standard deviation, Pearson Correlation and
One-way ANOVA and accordingly different tables and figures were drawn. Qual-
itative data collected by means of the interview were inserted when analyzing the
quantitative data. Finally, tests’ scores were included in order to determine the
efficiency of tests and the gap between the conception and the practice. The
following table was intended to present the different phases of the study (Tables 1
and 2).
Table 1 Demographic
features of the participants
(n ¼ 150)
Features Total %
Gender
Male 37 24.66
Female 113 75.33
Teaching experience
Five or less 35 23.33
6–10 years 15 13.33
11–20 years 80 63.33
Previous ESP Teaching Training 57 38
Previous ESP Testing Training 60 40
178 Y. Naimi
The questionnaire’s first section focused basically on ESP teachers’ perceptions
of assessment. It was divided into four parts according to the aspects of the
classroom they tackled. The first part examined the influence that ESP tests are
supposed to have on the curriculum. The analysis of the findings of Table 3
illustrated that most of the teachers’ responses ranged in the area between
“undecided” and “agree”. Most participants in this survey thought that assessment
is paramount and should be integrated within the teaching practices because of their
usefulness and the role they play (SD of 0.685 that reflected the agreement between
the study participants). This result was further confirmed by the interview
responses. One of the participants affirmed that testing is an important phase of
language learning and teaching, therefore ESP teachers should not neglect it. They
have to focus on this phase and prepare it correctly in terms of students’ prepara-
tion, the time devoted, the materials used and the skills included.
Table 2 Data collection and analyses
Phases Tests Results Rationale
Phase
one
– Mean
– SD
• M ¼ 32.45, SD ¼ 6.999 (for
the curriculum)
• M ¼ 31.05, SD ¼ 7.235 (for
the teaching materials)
• M ¼ 32.04, SD ¼ 6.904 (for
the teaching methodologies)
– To check the participants’
perceptions
– To compare the answers
with the mean values
Phase
two
– One-way
ANOVA
– Bonferroni post-
hoc test
– Test of homoge-
neity of variances
Std. Error ¼ 0.056
df1 ¼ 2
df2 ¼ 147
p ¼ 0.252
– To compare the variances
among the Different groups
Phase
three
– Reliability analy-
sis: Cronbach
Alpha
– Pearson Correla-
tion analysis
• α ¼ 0.80 (for the curriculum)
• α ¼ 0.75 (for teaching
methodologies)
• α ¼ 0.71 (for teaching
materials)
– To check if the fit value is
acceptable
– The check the correlation
between the different items
Phase
four
– Scores’ analysis • 29% < 10
• 10 < 63% < 15
• 15 < 8%
– To compare teachers’ per-
ceptions and classroom
practices
Table 3 Teachers’ conceptions of assessment
Mean SD
1. Assessment identifies the strengths and weaknesses of a curriculum and
provides feedback of its effectiveness
4.09 0.721
2. You as an ESP teacher have to devote as much time as it should be to the
preparation of students for the test
4.08 0.685
3. You as an ESP teacher have to teach your students the skills and contents
that will be included in the test during the academic year
4.21 0.791
Teachers’ Conceptions of Assessment in an ESP Context 179
The analysis of item two which supposed that ESP teachers have to devote as
much time as it should be to the preparation of students for the test, illustrated that
the mean of the teachers’ answers to this statement was 4.08. That means that about
90% of the participants agreed that much time should be devoted to the students’
preparation for the. Tests represent a chief part of the language learning and
teaching process therefore preparing students to sit for is an important issue that
ESP teachers should not neglect. Descriptive statistics were reported, and analysis
of variance ANOVA with the amount of time they devote to test preparation in the
dependent variables list and teaching experience i.e., “Group” as an independent
variable were conducted. The independent variable group had three levels: Less
than 5 years of experience, between 5 and 10 years and more than 10 years of
experience. Table 4 displays the results of analysis of variance ANOVA. The
between-subject test revealed that there was no significant difference between the
groups in terms of the time devoted to students’ preparation for the test F
(2, 24) ¼ 0.950, p ¼ 0.40. The obtained results demonstrated statistically significant
similarity in the groups’ perceptions depending on their previous experience in
teaching ESP. In other words, the results indicated that ESP teaching experience
does not have a significant effect in the teachers’ perceptions concerning the
importance they give to students’ preparation, and that there were not significant
differences between those who had previous experience and those who did not. All
groups agreed on the tests’ importance and on the necessity of providing much time
to prepare students to sit for.
4 Results and Discussion
4.1 Teachers’ Conceptions
To statistically examine the differences among ESP Teachers’ groups, the
Bonferroni post hoc test was conducted. The results demonstrated that there is no
significant difference in terms of time ESP teachers devote to their students’
preparation for the test between the groups ( p < 0.05). Hence, the Bonferroni
post hoc test provided further evidence to the findings of the first statement of
Table 3 about the role of ESP tests. It also indicated that ESP teachers’ positive
attitudes toward ESP tests do not depend on their teaching experience in the field.
Dealing with the skills and content to be tackled in the ESP lesson, ESP teachers’
responses revealed a clear understanding of the status of the content and skills to be
Table 4 Analysis of the variances of the devoted time
Sum of squares df Mean square F Sig.
Between groups 0.734 2 0.367 0.780 0.460
Within groups 69.140 147 0.470
Total 69.873 149
180 Y. Naimi
introduced during the lesson. The mean obtained from statement 3 (4.21 which
ranged between “strongly agree” and “agree”) and the SD of 0.685 revealed that
the participants shared the same point of view concerning the skills and content
included in the tests. Most of them agreed that they should not be limited to the
tasks that they intend to tackle in their tests but they have also to include other tasks
and activities (Table 5).
Teaching materials represent an important tool in the ESP context. They are
supposed to facilitate the task of both teachers and students by providing them with
the basic elements and content of the lesson. The following table takes into
consideration and analyzes some materials from ESP teachers’ perceptions (Fig. 1).
The first conclusion extracted out of Table 6 was the low SD values all the
statements had (They ranged between 0.530 and 0.853). This result can be regarded
as an agreement among the participants on the answers they gave to all the
statements. The first statement was used to gauge the participants’ perceptions
with regard to test related materials and their use. It gained a high mean value of
Table 5 The results of the Bonferroni post-hoc test
Mean Std. error
95% Confidence interval for mean
Lower bound Upper bound
Five or less 4.03 0.079 3.87 4.19
6–10 years 4.21 0.101 4.00 4.41
11–20 years 4.09 0.116 3.86 4.33
Total 4.09 0.056 3.98 4.20
Fig. 1 Means plots for the devoted time
Teachers’ Conceptions of Assessment in an ESP Context 181
4.20 which means that 85% of the answers to this question ranged between “agree”
and “strongly agree”. This high score implied that ESP teachers agreed on the
importance of exam related materials and used them to prepare their students for the
test. Teachers’ responses also revealed that they relied on past papers, parallel
forms and self-made materials. The corresponding statements gained relatively
high mean values which means that most participants agreed on their usefulness
and relied on them in their lessons and preparation of their students to the test.
Exam related materials were the subject of an important emphasis. Questionnaire
participants agreed that they have to be available in all universities and institutes
(mean value ¼ 4.20 i.e., the answers ranged between agree and strongly agree).
This leads once again to the washback effect, as it reflected the influence tests had
on teaching materials and the great importance ESP teachers gave them. Self-made
materials were subject to one way ANOVA analysis, in order to determine the
variances of ESP teachers’ beliefs concerning their use as demonstrated in Tables 7
and 8.
The obtained results revealed an agreement among the ESP teachers’ groups on
the use of self-made materials and their importance. The mean values were very
close as they ranged between 3.58 and 3.65, however it reached its highest value
with the second group (between 6 and 10 years experience) as can be deduced from
Fig. 2. The test of homogeneity of variances indicated a relatively high significance
p ¼ 0.584 which once again confirmed the obtained results. One last aspect of the
classroom that should not be neglected at this level of analysis is teaching meth-
odologies and the way they are affected by tests from ESP teachers’ perception. The
Table 6 Testing effects on teaching materials
Mean SD
1. You as an ESP teacher have to make use of exam related materials to
prepare students well for the test
4.20 0.853
2. You as an ESP teacher have to make use of parallel forms that imitate the
format of the test in your English lessons
3.69 0.791
3. You as an ESP teacher have to make use of self-made materials in your
English lessons
4.01 0.530
4. Exam related materials have to be available in the institutions and
universities
4.20 0.752
Table 7 Analysis of the variance of self-made materials
Sum of squares df Mean square F Sig.
Between groups 0.127 2 0.063 0.041 0.960
Within groups 225.447 147 1.534
Total 225.573 149
Table 8 Test of
homogeneity of variances
Levene statistic df1 df2 Sig.
0.540 2 147 0.584
182 Y. Naimi
following table presents the mean values obtained from the answers to some
questions of the questionnaire that tackled this issue.
As far as learner testing effects on the teaching methodology are concerned, the
total mean values for the concerned statements was 3.45, which belong to the
medium range of the scale. This suggested that teachers were uncertain about the
effects a test may have on their teaching methodology. Most answers ranged
between “undecided” and “agree”. The SD values presented in Table 9 can be
considered as low values. In other words, although ESP teachers agree on the fact
that testing has remarkable effects on their classes, they are still undecided about its
degree concerning the methods they use to teach. Most participants of the interview
affirmed that they do not teach their exam preparation classes in the same way they
teach other classes. One participant stated that he had to focus on the activities and
skills included in the test and to use many exercises with his students similar to
Fig. 2 The use of self-made materials
Table 9 Teaching methodologies
Mean SD
1. The preparation of students for the test affects the way they are taught in the
class
4.05 1.021
2. The preparation of students for the test affects the subject content 3.05 0.638
3. Assessment information modifies ongoing teaching methodology 3.49 0.686
4. A new test format may affect the ESP teacher current teaching
methodology
3.12 1.105
Teachers’ Conceptions of Assessment in an ESP Context 183
those that they will be assessed on. A second point that he emphasized was that the
preparation of students for the test affects the way he taught them in class; he could
not teach ordinary classes in the same way he did with exam preparation classes.
Analyzing his answers revealed that this teacher focused basically on the content
of the test; what made him focus on similar tasks and activities. This result
confirmed the findings of the questionnaire questions in terms of the existence of
testing effects on the teaching methodology teachers use in the exam preparation
classes. A second participant, however, gave a different opinion in response to the
same question when he agreed on the idea that being bound by a test makes the
teacher concentrate on its skills and activities, but he still thinks that he had the
possibility to teach exam preparation classes in the same way he did with other
classes. In other words, being bound by test did not oblige this teacher to focus only
on the exam content and skills, he was also able to include further activities and
skills.
A one-way ANOVA analysis revealed that although ESP were uncertain about
the effects tests have on their teaching methodologies, all the groups’ answers were
very close (mean values ranged between 3.40 and 3.65) i.e., the three groups
showed a significant agreement on the use of a new teaching methodology if they
were not bound by a test (F (2, 24) ¼ 3.87 p ¼ 0.03) (Table 10).
In response to statement 4 the mean values ranged between “undecided” and
“agree” which uncovered the participants’ uncertainty about changing their teach-
ing methods if a new test format was introduced or if there were no tests. Linked to
the previously presented answers of the interview, this result implied that although,
they shared the same view concerning the existence of tests’ influence on this aspect
of the classroom, ESP teachers did not show a clear and precise way of dealing with
them. In general, the responses obtained from the whole conceptual perspective
indicated that despite their appreciation and agreement on testing effects, partici-
pants of this study lack a clear understanding of the exact way they had to deal with.
The uncertainty of their responses made it difficult to draw a clear image of their
perceptions of testing effects on the teaching methodology. It was therefore neces-
sary to insert Table 11 to further investigate teachers’ views and attitudes toward
ESP tests.
The mean values for favorable and unfavorable items were 4.35 and 1.80
respectively. The highest mean score was obtained in item 1, in which the partic-
ipants expressed the usefulness and importance of assessment in the ESP context.
This was well explained by one the interview participants who stated that assess-
ment provided him with information about students’ levels and necessary feedback
about the different aspects of the classroom. Answers to the second statement
Table 10 Analysis of the variances using new teaching methodologies
Sum of squares df Mean square F Sig.
Between groups 1.379 2 0.689 0.974 0.380
Within groups 104.061 147 0.708
Total 105.440 149
184 Y. Naimi
reported that new specific courses for ESP teachers are needed to help them prepare
students well for the tests. Most participants agreed on this statement (a mean value
of 4.00). This implied that the majority (about 80%) of the participants appreciate
the introduction of new specific courses to facilitate the task of preparing their
students for the test. Answers to statement 3 however, ranged between “disagree”
and “undecided” (mean score of 2.85) ESP teachers believe that the first priority of
language learning in the ESP context is not passing the test. This result was further
emphasized by statement 4 answers which ranged between “agree” and “strongly
agree”. Although, it runs counter the principals of the washback effect (unfavorable
item) it had a mean value of 1.80. This value implied an agreement among the
participants of the study (SD of 0.787) that learning practical language use is one of
the most important issues in the ESP context.
A Pearson correlation coefficient analysis was conducted to investigate the
correlation patterns between the different items of the questionnaire dealing with
teachers’ conceptions. The correlation between the role of tests and the time
devoted to them with coefficients of 0.88, was significant at (p  0.001). For
exam-related materials and the exam date the correlation coefficients were 0.71
(p  0.001), however they were 0.45 (p  0.001) between a new test format and the
teaching methodology. For classroom interaction and students’ preparation for the
test the Pearson correlation coefficients were 0.40 (p  0.005) and 0.19 (p  0.005)
between the usefulness of assessment and new specific courses, while the coeffi-
cients were 0.14 (p  0.001) between passing a test and practical language use
(Figures 3 and 4). Analyzing the difference among the ESP teachers’ attitudes
towards the first priority of language learning seemed very important at this level of
analysis. It is worth mentioning that there was a slight difference between the
groups concerning the two purposes of language learning. The results were
F (2, 24) ¼ 0.687, p ¼ 0.05 for passing the test and F (2, 24) ¼ 0.452, p ¼ 0.06
given that the 11–20 years of experience group had the highest mean (mean ¼ 2.70),
followed by the 6–10 years of experience group (mean ¼ 2.56) and the less than
5 years of experience group (mean ¼ 2.42) (Tables 12 and 13).
The above analysis suggested that even if there is a modest difference between
the ESP teachers’ attitudes and views toward the tests, they still agree on their
importance and major role they play. To sum up, the above analysis suggested that
ESP teachers shared positive attitudes toward testing, what was further stressed by
the low SD values obtained. They agreed on tests’ importance not only in providing
Table 11 Teachers’ views and attitudes
Mean SD
1. Assessment is useful and necessary as a whole in the ESP context 4.35 0.540
2. New specific courses for ESP teachers are needed to help them prepare
students well to the tests
4.00 1.012
3. The first priority of language learning in the ESP context is to pass the test 2.85 0.662
4. The first priority of language learning in the ESP context is to learn
practical language use
1.80 0.787
Teachers’ Conceptions of Assessment in an ESP Context 185
Fig. 3 Passing the test
Fig. 4 Learning practical language
186 Y. Naimi
information about students’ levels but also in giving feedback about all the different
aspects of the classroom: The teaching materials, teaching methodologies, curric-
ulum. The second point to be emphasized is teachers’ agreement that passing the
test is not more important than learning practical language in the ESP context. That
means that the test outcomes are not limited to passing or failing it, they rather
include the type of language and information students obtained. The following
section will therefore consider ESP tests’ outcomes.
The total mean values for the concerned statements was 4.01 which can be
considered to be high mean value what presupposed a relatively clear understand-
ing of tests’ outcomes. Statement 1 received positive responses that ranged between
“agree” and “strongly agree”. It showed that participants of this study agreed
(SD of 0.886) that assessment do not only have intended outcomes but also
unintended ones. Those outcomes can be controlled and predicted before their
occurrence as statement 2 and 3 implied. What was clearly supported by teachers’
answers to question 9 of the interview. One of the participants affirmed that
assessment outcomes can be more or less predicted before their occurrence. He
went further to affirm that knowing his students’ levels and abilities, helped him
predict those who are going to pass and those who are not. A second participant
added that predicting tests’ outcomes is an easy task if the teacher knows what his
students were capable of. What enabled him to control the outcomes and avoid
negative ones. That means that from the participants’ point of view if the teacher
knows his students levels he will be able to manipulate tests’ outcomes. He can
predict the negative as well as the positive results and as a result he can control
them. Answers to the last statement ranged between “agree” and “strongly agree”
(a mean value of 4.05). This can be justified by a high agreement of the participants
Table 12 Analysis of the variances of the first priority of language learning
Sum of
squares df
Mean
square F Sig.
The importance of passing the
test in the ESP context
Between groups 0.605 2 0.303 0.687 0.505
Within groups 64.728 147 0.440
Total 65.333 149
The importance of practical
language use in the ESP context
Between groups 2.292 2 1.146 0.452 0.638
Within groups 373.041 147 2.538
Total 375.333 149
Table 13 Tests’ outcomes
Mean SD
1. Assessment has both intended and unintended outcomes 4.20 0.886
2. Test outcomes can be controlled 4.00 0.781
3. Test outcomes can be predicted before their occurrence 3.80 0.656
4. The negative outcomes of tests are turned to exert a positive influence in
support of curriculum and materials innovation
4.05 1.108
Teachers’ Conceptions of Assessment in an ESP Context 187
in this survey with this question. One participant to the interview went further to
explain that an ESP teacher is capable of making use of the negative outcomes of
his test as well as the positive ones. If students’ marks were not good enough that
means there is something wrong that he has to check: He has to check his teaching
methodology, materials, skills and activities in order to discover where the problem
is. Doing so, may exert a positive influence in favour of curriculum and material
innovation.
A one-way ANOVA analysis was conducted to further confirm the aforemen-
tioned results. It did not indicate a significant difference between the groups with
regard to tests’ outcomes. The results were F (2, 24) ¼ 0.258, p ¼ 0.02 for
controlling tests’ outcomes and F (2, 24) ¼ 0.211, p ¼ 0.09 for predicting them.
Answers to the last statement of Table 14 ranged between “agree” and “strongly
agree” (a mean value of 4.05). This can be justified by a high agreement of the
participants in this survey with this question. One participant to the interview went
further to explain that an ESP teacher is capable of making use of the negative
outcomes of his test as well as the positive ones. If students’ marks were not good
enough that means there is something wrong that he has to check: He has to check
his teaching methodology, materials, skills and activities in order to discover where
the problem is. Doing so, may exert a positive influence in favor of curriculum and
material innovation.
A Pearson correlation coefficient analysis was conducted to investigate the
correlation patterns between the different items of the questionnaire dealing with
tests’ outcomes. The correlation between predicting the tests’ outcomes and con-
trolling them were 0.71 (p  0.001). However they were 0.86 at (p  0.001) for
controlling the outcomes and turning the negative outcomes and into positive ones.
To sum up participants’ answers concerning testing outcomes revealed a clear
understanding of this notion. They agreed that testing has both intended and
unintended outcomes and that these outcomes can be controlled and predicted
before their occurrence. Although predicting tests’ outcomes is an easy task, ESP
teachers can do it. What enabled them to control the outcomes and avoid the
negative ones. A comparison between teachers’ conceptions of assessment and
their real practices seems therefore to be of great importance at this level of
analysis.
Table 14 The analysis of the variances of tests’ outcomes
Sum of
squares df
Mean
square F Sig.
Controlling ESP tests’
outcomes
Between groups 2.413 2 1.207 0.258 0.287
Within groups 140.980 147 0.959
Total 143.393 149
Predicting ESP tests’
outcomes
Between groups 0.030 2 0.015 0.211 0.989
Within groups 198.963 147 1.353
Total 198.993 149
188 Y. Naimi
4.2 Teachers’ Practices
Table 15 presented the results of teachers’ responses to aspects of frequency to
some practices related to their ESP lessons and tests. Six items were used to
determine the importance they attribute to each practice. Answers to statement
one of Table 15 ranged between “often” and “nearly always”. This revealed a
frequent occurrence of this teaching practice. In other words, ESP teachers focused
on language skills or abilities that they wanted to focus on or stress through tests.
Statements 3 and 6 also received the same result with mean values of 4.00 and 4.30
respectively. What suggested that the study participants often adjust items format to
achieve the expected results and add materials to those provided by the university to
meet their students’ needs. This point was further confirmed by the interview
responses. One participant stated that he opted for adding further materials to stress
the skills and activities of the test and control its outcomes. Statements 2 and 5 also got
a considerably high mean values (3.90 and 3.80) what implied their frequent occur-
rence. Teachers claimed that they often achieve the expected results from tests and feel
satisfied with their ESP lessons. This result can be justified by the trials they do to turn
the negative outcomes of tests and exert a positive influence in support of curriculum
and materials innovation (statement 4 which got a mean value of 4.05).
The analysis above revealed a clear understanding of the notion of washback and
its effects on the different classroom aspects; although the last point faced some
uncertainty. This understanding enabled ESP teachers to manipulate their tests’
outcomes and control their results what made them feel satisfied with their lessons
and achieve the expected results from the tests. This satisfaction can be considered
as the result of hard work and well-studied teaching practices such as adjusting
items format to achieve the expected results, adding further materials, designing
necessary materials in case of need and focusing on the negative outcomes in order
to turn them into a source of positive influence in support of curriculum and
materials innovation. This point was further stressed by the interview responses
as most participants emphasized the importance tests have in the ESP context. They
also asserted that tests’ effects are clearly realized in its different aspects. So they
had to adjust the materials they use and their teaching methodologies to testing
Table 15 ESP teachers’ practices
Mean SD
1. You focus on certain language skills or abilities that you want to focus on
or stress through tests
4.10 0.960
2. You achieve the expected results from tests 3.90 0.401
3. You adjust items format to achieve the expected results 4.00 1.070
4. The negative outcomes of tests are turned to exert a positive influence in
support of curriculum and materials innovation
4.05 0.419
5. You feel satisfied with your ESP lessons 3.80 0.641
6. You add materials to those provided by the university to meet your
students’ needs
4.30 0.994
Teachers’ Conceptions of Assessment in an ESP Context 189
necessities. A Pearson correlation coefficient analysis was conducted to investigate
the correlation patterns between the different items of the questionnaire dealing
with tests’ outcomes (Table 16).
Making use of tests’ scores seems of quiet importance at this level of analysis. It
will help comparing teachers’ perceptions and their real practices. The following
table contains a sample of ESP tests’ results classified into a three levels scale: Less
than 10, between 10 and 15, and finally more than 15.
Results of the table revealed that 29% of the students had less than 10 in their
tests, 63% had between 10 and 15 however, only 8% had more than 15. Analyzing
those results implied that most of the students got average marks which questioned
the feeling of satisfaction the teachers expressed. Most of the participants of the
questionnaire claimed that they were satisfied with their tests and that they achieved
the results they seek. However, and through the marks analysis we remarked that
29% of the students had less than 10 which is relatively a high number. This point
can be explained in two ways either that most of the participants did not show their
reel feelings and attitudes toward their tests or that average marks are considered to
be enough and satisfying in the ESP context.
This study was conducted with the main endeavor of investigating the field of
ESP testing, determine its teachers’ perceptions of assessment and find out the
degree to which those ideas correspond to classroom practices. The analysis of the
results was based on six research hypothesis.
The first hypothesis of the study was that ESP teachers give much importance to
assessment. The study results confirmed this hypothesis as most of the participants
in the study emphasized the role of tests and that it represents an important phase of
language teaching and learning. In other words, assessing learners’ abilities and
acquisition represents an important issue from ESP teachers’ perspective. Dudley-
Evans and St. John (1998) also shared the same view when they stated that an ESP
test is “an aid to learning” (p. 212). It has the ability to provide students with a
meaning of achievement and a sense that it is in line with the competences and
content that have been overcast as it encompasses benefits such as reinforcement,
confidence building, involvement and building on strength.
The second hypothesis was that assessment influences the way ESP teachers deal
with the curriculum. The study results proved the correctness of this hypothesis. ESP
teachers’ shared theoretical perceptions with regard to the influence tests have on the
curriculum. They have the same views regarding the role of assessment, the useful role
tests play in revealing students’ level and providing necessary feedback about the
curriculum they adapted. They affirmed also that assessment has to be integrated
within the teaching practices because of the major role it plays. This result confirmed
Table 16 Students’ scores Score %
Less than 10 25
10–15 65
More than 15 10
Mean ¼ 12.64
190 Y. Naimi
what Mehrens (1991) states: “The test preparation practice should not increase
students’ test score without simultaneously increasing students’ mastery of the content
domain being tested”. In other words, teachers can come up with better methods and
tools to support students’ learning and control their teaching plans through assessment.
However, it should not be viewed as an end in itself, but rather as a fundamental
element of the learning process. Most of the participants in the study stated, however,
that their lessons should include different activities and skills.
The third hypothesis was that assessment influence ESP teachers’ teaching
methodologies. The results revealed that preparing students for the test affects the
teaching methodology either in terms of the way they are taught and prepared or in
terms of the information it provides about the methods used and their efficiency. In
other words, although ESP teachers agree on the fact that testing has remarkable
effects on their classes, they are still undecided about its degree concerning the
methods they use to teach. In addition, most of the participants affirmed that they do
not teach their exam preparation sessions in a similar manner to this they employ
with other classes. The uncertainty of their responses made it difficult to draw a
clear image of their perceptions of testing effects on the teaching methodologies.
Some researchers believe that test preparation courses are to ameliorate test results
by appropriate teaching of what is included on the test. Mehrens (1991) claimed
that: “Preparation courses are mostly those courses which are set up for training in
test taking strategies, familiarizing students with the test, and giving them practice
under exam conditions.”. Other studies however, found that teaching content was
more likely than teaching methodology to be influenced by tests (Cheng, 1997).
The fourth hypothesis suggested that preparing the students for the test influence
the materials ESP teachers use. This hypothesis too was confirmed by the study as
its results demonstrated that ESP teachers agree on the importance of exam related
materials and use them to train their students to sit for the test. This kind of
materials was the subject of an important emphasis. The study participants agreed
that they have to be available in all universities and institutes. They also stressed the
necessity of making greater use of them as the exam approaches. The study results
are in line with those of Sadeghi’s (2014) study where he found that: “All the
teachers made use of the exam-oriented materials, but only teacher D used materials
which was a combination of exam-related, practice-oriented, and some authentic
materials he had personally designed for the course. This could be interpreted as an
indication of positive washback.” (p. 557).
The fifth hypothesis was that the washback effect can be predicted and con-
trolled from ESP teachers’ perspective. The results suggested that ESP teachers
shared positive views and attitudes toward testing. They agreed on their importance
not only in providing information about students’ levels but also in giving feedback
about all other aspects of the classroom. They also agreed that assessment does not
only have intended outcomes but also unintended ones. Those outcomes can be
controlled and predicted before their occurrence from ESP teachers’ perspective: If
the teacher is aware of his students ‘abilities he will be able to predict the outcomes
of his tests and turn the negative outcomes into good ones. Adding new materials to
those provided by the university might enable ESP teachers to concentrate on the
Teachers’ Conceptions of Assessment in an ESP Context 191
competences included in the test and control the tests’ outcomes and results. This
may be regarded as one explanation of the frequent occurrence of this teaching
practice in the ESP lessons. This result is in line with Harlen and Crick’s (2003)
claim that “Even though there are limits to the action teachers can take to use
assessment effectively to help their students learning; they are the only ones whose
actions directly affect students” (p. 203).
The last hypothesis was that the ways ESP teachers perceive assessment corre-
spond to their instructional practices. The study results proved the correctness of
this hypothesis too. ESP teachers demonstrated a clear understanding of the notion
of washback and its effects on the different classroom aspects. This understanding
was the major reason behind teacher’s ability to manipulate their tests outcomes and
control their results what made them feel satisfied with their lessons and achieve the
expected results from their tests. This satisfying result is due to the well-studied
teaching practices ESP teachers are adopting such as adjusting items format to
achieve the expected results, adding further materials, designing necessary mate-
rials in case of need and focusing on the negative outcomes in order to turn them
into a source of positive influence in support of curriculum and materials innova-
tion. This result was however questioned by the tests’ scores analysis which
revealed an average level. This study showed also a strong correlation between
teachers’ perceptions and classroom practices what was considered to be a further
argument of the hypothesis in hand.
5 Conclusion
The study can be of value for both SLA researchers, theorists, and ESP teachers.
Firstly, it may be helpful to researchers and theorists in that it contributes to the
understanding of the notion of teachers’ conceptions of assessment. It added to the
number of studies conducted on the subject on hand, and it tackled a very contro-
versial issue. Therefore, its results further highlight this issue and constitute another
evidence in favor of the previous studies that valued ESP teachers’ conceptions of
assessment and the way they affect the different classroom aspects. Secondly, this
study is of benefit for researchers in the field of ESP in that it provides a hint about
the practical procedures used to investigate teachers’ perceptions about assessment,
which had a direct relation to classroom practices. The study might probably be
relevant to different contexts in investigating the major demands of testing as well
as both teachers and learners’ conceptions. The procedure and methods employed
in this present study maybe applicable to the secondary education context too.
Finally, another important issue was raised while interviewing the teachers. It is
basically about the necessity of being aware of the exam requirements: Aims,
specifications and administrative procedures what will enable them to familiarise
their students with the test and draw a comprehensible image of its requirements.
One second point that they should not neglect is the teaching materials and
methodologies they use.
192 Y. Naimi
A number of limitations should be considered when clarifying and generalizing
the findings of the present study. One notable limitation is that data of the study
were gathered using a questionnaire and interview. The sample of 150 ESP teachers
was important enough to give reliable results. However, adding another data
collection tool: Direct observation could have given more reliable and general
results especially when comparing teachers’ perceptions with their classroom
practices. A further limitation is related to the fact that perceptions are shaped by
many complex factors that may not be captured in surveys. In relation to the study
in hand, ESP teachers’ perceptions of assessment could be affected by their
personal situation and background, their belief system, untested expectations, etc.
Moreover, teachers’ practices may have been guided by the questionnaire. Thus,
they may be different from those they reveled in response to the questionnaire. The
option of audio or video recording was well respected but not used with regard that
the teachers may find it demanding and thus may result in artificial behaviors.
Factor analysis is a very relevant method of analysis in carrying out studies about
perceptions (e.g., Hidri, 2015). However, it was not opted for in this study because
it requires at least 400 participants to yield valid and reliable results. This option
was not possible because of the difficulty of reaching this amount of participants in
the ESP context and particularly with such time constraints. One last limitation to
be mentioned is related to the correlational nature of the study. The different
variables (testing, teaching materials, teaching methodologies, curriculum,
teachers’ perception) were associated and affected by each other under certain
circumstances and in specific context (ESP context). Thus it may be uncertain
whether the correlational findings will be the same under different circumstances
and with a different population.
The importance of this project is twofold: First, despite its methodological
limitations, it is nonetheless based on an elaborate methodological pattern. It
assumed a positivist paradigm in an attempt to increase our understanding of the
studied phenomenon. It followed a combination of quantitative and qualitative
epistemological orientations. Instruments used to collect data and the procedures
employed to analyze them from both types were used during the study, i.e., the
mixed-method approach is chosen in this research. This study presented a detailed
follow-up of the reasons behind the disparity between prescribed theory and
classroom practices with regard to ESP assessment. However, it must be stressed
that it paves the way for other related studies. A further step to follow the path taken
could be simply to carry on a follow up study investigating the real problems
teachers face when trying to implement ESP assessments in the Tunisian context
where this kind of studies is hardly found. The success of this project will then
depend on decisions to be taken by the university authorities and the teachers
themselves.
The study dealt with an issue that bridges the concern of both teachers and ESP
researchers. Teachers are concerned with the effects testing has on their ESP
classes. In the same vein, researchers are concerned with the washback effect and
teachers’ perception toward the way tests affect the different aspects of ESP tests.
The present study’s findings corroborate with the view that assessment shapes the
Teachers’ Conceptions of Assessment in an ESP Context 193
ESP course as it interferes with the teaching methodology, the materials used as
well as the curriculum and content. Moreover, it is obvious that teachers’ percep-
tions are at the center of the teaching-learning process as they affect the way ESP
teachers deal with assessment and their classroom practices. This pivotal concept
should be attributed a paramount importance since it influences to a large extent the
different aspects of the classroom. Test specifications in the ESP context represent
also an important issue that has to be considered by both ESP teachers and
researchers. Unlike the other contexts of language learning, the ESP context has
its own specificities and characteristics. Therefore, further researches should be
conducted in this concern. They have to take into consideration the degree of
specificity needed and provide the appropriate types of tests. Writing a list of test
specifications in the ESP context should therefore, be the subject of new studies.
References
Amengual, M. (2010). Exploring the washback effect of a high-stakes English test on the teaching
of English in Spanish Upper secondary schools. Revista Alicantina de Estudios Ingleses, 23(2),
149–170. Retrieved January 4, 2015, from https://rua.ua.es/dspace/bitstream/10045/17429/1/
RAEI_23_09.pdf
Bachman, L., & Palmer, A. (1996). Language testing in practice: Designing and developing useful
language tests. Oxford: Oxford University Press.
Bailey, K. M. (2005). Teachers and tests: Promoting positive washback. Paper presented at the
TESOL Arabia Conference, Dubai, United Arab Emirates.
Cheng, L. (1997). How does washback influence teaching? Implications for Hong Kong. Lan-
guage and Education, 11(3), 38–54.
Cheng, L., & Watanabe, Y. (2004). Washback in language testing: Research contexts and
methods. Mahwah, NJ: Lawrence Erlbaum.
Daoud, M. (1996). English language development in Tunisia. TESOL Quarterly, 30(3), 598–605.
Dudley-Evans, T., & St. John, M. (1998). Developments in ESP: A multi-disciplinary approach.
Cambridge: Cambridge University Press.
Green, A. (2013). Washback in language assessment. International Journal of English Studies, 13
(2), 39–51. Retrieved September 28, 2015, from http://revistas.um.es/ije
Harlen, W., & Crick, R. D. (2003). Testing and motivation for learning. Assessment in Education:
Principles, Policy & Practice, 10(2), 169–208. https://doi.org/10.1080/0969594032000121270.
Hidri, S. (2015). Conceptions of Assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Mehrens, W. A. (1991). Facts about samples, fantasies about domains. Educational Measurement:
Issues and Practice., 10(2), 23–25. https://doi.org/10.1111/j.1745-3992.1991.tb00190.x.
Sadeghi, S. (2014). High-stake Test Preparation Courses: Washback in Accountability Contexts.
Journal of Education & Human Development, 3(1), 17–26. Retrieved January 5, 2014, from
http://jehdnet.com/journals/jehd/Vol_3_No_1_March_2014/29.pdf
Tsagari, D. (2007). Review of washback in language testing: How has been done? What more
needs doing? Lancaster University, UK. Retrieved June 4, 2015, from http://www.academia.
edu/517669/Review_of_Washback_in_Language_Testing_What_Has_Been_Done_What_
More_Needs_Doing
Wu, J. (2012). GEPT and English language teaching and testing in Taiwan. Language Assessment
Quarterly, 9(3), 11–25. Retrieved May 4, 2015, from tandfonline.com/doi/abs/10.1080/
15434303.2011..553251
194 Y. Naimi
Part IV
Assessing Translation and Literature
Assessing Textual Competence in Translation
into a Second Language
Tamara Kavytska and Olga Kvasova
Abstract Assessing translation competence or its components is one of highly
debatable issues of translation pedagogy. In the Ukrainian context, this issue has
been particularly neglected despite considerable progress achieved in teaching
vocational translation. As a result, conventional quantitative and error-based assess-
ment schemes prevail in most national translation training environments. Thus, the
objective of this article is to reconsider generally accepted approaches towards
translation quality assessment and suggest the instruments for assessing the textual
competence in translation from Ukrainian into English. This study relies on the
experimental training of the first-year Translation Majors of the Master’s program
(n ¼ 33). Tools included a general language proficiency test, a pre- and post-
training test in translation skills, assessment scales and the survey of translation
teachers. The assessment procedure of the experimental training had a strong
correlation (89%) with the expected learning outcomes and training objectives of
the course. The research results indicate that the suggested instruments are effective
in measuring the textual competence of Translation Majors in the classroom
focused on translation as a process rather than a product. The experimental training
has also revealed the impact of the trainer’s profile on the assessment procedure,
which can become the object of further research.
Keywords Translator competence • Textual competence • Translation into a
foreign language • Assessment instrument • Assessment scale • Translation majors
1 Introduction
The current study is a new stage of the previously conducted research (Kavytska,
2015) which focused on developing textual competence (TС) in translation from
Ukrainian into English. The emphasis, in particular, was placed on designing an
effective system of tasks and exercises in order to develop this competence in the
T. Kavytska (*) • O. Kvasova
Institute of Philology, Taras Shevchenko National University of Kyiv, Kyiv, Ukraine
e-mail: kawicka_t@ukr.net
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_10
197
first-year Translation Majors of the Faculty of Oriental Languages (Master’s pro-
gram). The research had a twofold objective: Providing a theoretical background
for designing translation tasks and finding some evidence of their effectiveness for
training translation majors. The system of tasks and exercises was piloted in the
course of experimental training (34 h) and proved its effectiveness for enhancing
TC in translation from Ukrainian into English. The conducted study, however, has
revealed two major problems: The lack of reliable techniques to assess students’ TC
and an urgent necessity to build testing and assessment skills in translation trainers.
The latter, in particular, is rooted in the national system of training translation
teachers. With degrees in Translation studies or Linguistics, they often lack knowl-
edge in teaching methodology, testing and assessment. The 2012 survey of univer-
sity translation trainers shows, for instance, that 31 out of 35 respondents (88%)
assess the quality of the target text (TT) applying an error-based assessment;
4 respondents only (12%) tend to assess the competence or its components as
well as the progress in its acquisition (Kavytska, 2015, p. 221). Unexpectedly, the
survey, revealed not only the trainers’ considerably undeveloped skills in testing
and assessment but also their reluctance to improve their assessment skills.
Thus, the goal of this chapter is to reconsider the prevailing national approach
towards assessing competence in vocational translation and validate the develop-
ment of instruments to measure TC in translation into a second language (L2). With
this purpose we will first specify the componential structure of the TC in L2
translation, then discuss possible approaches towards TC assessment and, finally,
describe the development of measuring instruments to assess TC in L2 translation.
2 Theoretical Background
Until recently, translation into L2 has been a controversial issue and was denounced
both by the theory of translation and translation pedagogy. However, the attitudes
towards this activity have changed within last 15 years, since it became evident that
translation into L2 was a reality in many settings of the world, especially in the
cultures that use languages of limited diffusion, Ukrainian among them. A signif-
icantly conceptual contribution here belongs to Campbell (1998, p. 4) who
suggested that L2 translation was a normal and no less widespread activity than
L1 translation. Thus, the theoretical framework underlying this study relies on his
fundamental work as well as the research findings provided by other experts in L2
translation pedagogy (Kelly, 2000; Kiraly, 2000; Mackenzie, 2000; Pavlović, 2007;
Stewart, 2008).
Noteworthy is the fact that the demand for translation into L2 is on the increase
in many countries these days. A questionnaire survey, conducted by Pavlović
(2007, p. 13) in Croatia, for instance, reveals that for 73% of full-time translators
and interpreters “more than half of their workload consists of L2 translation/
interpreting”. Even a higher proportion (76%) is demonstrated by a similar survey,
conducted by the authors of this article in Ukraine among translators/interpreters
198 T. Kavytska and O. Kvasova
from eight leading translation agencies of the city of Kyiv. The respondents
claimed, in particular, that the demand for L2 translation mostly extended to the
varieties of institutional discourse, including administrative, legal, academic and
technical texts (Kavytska, 2015, pp. 218–221).
Thus, according to Kelly (2002), the indicated genre variety of texts on demand
requires that a translator—apart from a language, instrumental, extra-linguistic,
strategic competence etc.—should possess
the capacity: To understand and analyze a range of different types of (both oral and written)
texts from different fields produced in languages A, B and, later, C; to develop the capacity
to produce different types of texts from different fields in languages A and B; and to ensure
that the characteristics and conventions of the major text genres and sub-genres used in
professional translation and interpretation are made known in the distinct cultures in which
languages A, B and C are spoken (as cited in Montalt Ressurrecció et al. 2008).
In other words, a translator should have a developed textual competence (commu-
nicative and textual sub-competence, according to Kelly).
Paradoxically, despite the fact that TC is viewed as a component of most
translator competence (TrC) models, there are a few investigations that purpose-
fully focus on this competence as well as on the ways of its acquisition and
assessment. Moreover, there does not seem to be a clear and generally accepted
componential structure of a TC in translation. Some TrC models include TC as an
independent element, whereas in others, it is a constituent of a certain
sub-competence (e.g., as part of a bilingual competence in the model of PACTE
group or part of a communicative and textual sub-competence in Kelly’s model).
Bearing in mind that we have to define the object of our assessment as clearly as
possible, we have deliberately resorted to the analysis of those TrC models that
have a TC as an independent component, with PACTE model being the only
exception. We select this model for the analysis for two reasons: First, TC is a
constituent of a bilingual sub-competence; and, second, the PACTE model is now
overwhelmingly accepted in translation pedagogy. The idea behind this analysis
was to specify knowledge and skills which were explicitly or implicitly referred to
as TC elements. The models analyzed are presented in Table 1.
Table 1 Translation competence models analyzed
No. Model/author Sub-competences
1. Neubert (2000) Language, textual, cultural, subject, transfer
2. Kelly (2002) Communicative and textual, cultural, thematic, professional instru-
mental, psycho-physiological, interpersonal and strategic
3. PACTE (2003) Bilingual, extra-linguistic, knowledge about translation, instrumental,
strategic + psycho-physiological components
4. Komissarov
(1997)
Language, communicative, textual, instrumental + personal qualities
5. Schäffner (2000) Linguistic, cultural, textual, domain/subject specific, (re)search
competence, (re)search
6. Campbell (1991) Textual, disposition, proficiency
Assessing Textual Competence in Translation into a Second Language 199
It is worth mentioning that only two of six models (Campbell; Kelly) present the
construct of L2 translation. Moreover, they emphasize on great significance of TC
for inverse translation. The importance of genre competence, as part of a TC, for
enhancing translator competence was proved by Montalt Ressurrecció et al. (2008).
PACTE group, in its turn, stresses on the leading role of instrumental competence in
L2 translation (PACTE, 2003). Another particular L2 skill was specified by us as a
result of personal experience in L2 translation and interviews with the translators
who are engaged in Ukrainian-English translation on a regular and frequent basis.
This skill is editing the source text (ST) in case of necessity, a taboo in translation
practice until recently. The data provided by the interviewees coincided with our
personal observations as L2 freelance translators: 40% of Ukrainian source texts
(scientific, in particular) undergo editing by translators. This is mostly done with the
aim of making the texts fit into L2 genre/style conventions or publisher’s
requirements.
So, all the views mentioned above were carefully studied and taken into con-
sideration in the process of modelling a TC in L2 translation. Additionally, we
studied the syllabi for Master trainees in translation (five Ukrainian universities) as
well as European Master’s in Translation (EMT) Strategy (2009). Again, the
purpose of this analysis was to clearly define the TC components to address them
in the process of training as well as align them with the assessment criteria when
measuring the level of TC acquisition. Thus, a thorough examination of the TrC
models and educational standards has enabled us to define the TC as a system of
underlying knowledge about text and text creation norms in two languages as well
as skills and abilities to create various types of target text (TT) that culturally fit into
the target language (TL) norms (Kavytska, 2015). Therefore, the componential
structure of TC in L2 translation has been specified and presented in Table 2.
Having specified the structure of the TC in L2 translation, we intend to make use
of its components in the process of developing the assessment instruments. We
assume that three major components should make up three measuring instruments
with the criteria aligning with particular knowledge, skills and abilities that consti-
tute those major components.
Despite numerous researches on testing and assessing translation (House, 1997;
Kashirina, 2005; Kussmaul, 1995; Orozco and Hurtado, 2002; Shevnin, 2009;
Waddington, 2001, 2004; Williams, 2004, 2009, 2013 etc.), the issue is still the
subject of disagreement among experts in translation pedagogy. The assessment
paradox lies here in the fact that a single, reliable and objective evaluation standard
should be applied to an a priori subjective activity.
Given the fact that translation is deemed to be a cognitively complex task that
requires not only logic but also intuition, imagination, creativity and inspiration,
even in the academic environment there can hardly be a universal approach towards
assessing students’ achievements in translation (Kashirina, 2005, pp. 48–56). More-
over, cognitively complex tasks are characterized by unpredictability of results as
well as impossibility of applying universal solutions: Every new task—translation
including—is unique. Another attribute of such tasks is a specific emotional state
200 T. Kavytska and O. Kvasova
accompanying the task completion: Lack of confidence, doubts, indecisiveness etc.
Such emotions practically rule out a single and universally correct solution. In light
of the above mentioned, some researchers claim that in academic setting the only
acceptable term is optimal translation, whereas translation quality is the notion of
professional world (Mansell, 2008).
However, whatever subjective the activity might be, if it is taught and assessed in
the academic setting, the measuring techniques should be valid and reliable
(Hughes, 2003). In the context of translation pedagogy, validity is defined as “the
extent to which an evaluation measures what it is designed to measure, such as
translation skills” (Williams, 2009, p. 5). In this case then we deal with construct
validity. Williams further differentiates between content validity (the extent to
which an evaluation covers the skills necessary for performance) and predictive
validity (the extent to which the result of the evaluation accurately predicts
future performance). Reliability is viewed as “the extent to which an evaluation
produces the same results when administered repeatedly to the same population
under the same conditions”; thus, for the assessment procedure to be reliable the
evaluators have to be consistent in their judgments and apply stable criteria
(Williams, 2009, p. 5).
In the context of this research, we focus on the assessment in translation
teaching. According to Hurtado and Martinez (2001, p. 273), it is one of the three
evaluation areas of translation: Published translation; professional translation and
translation teaching. The researcher suggests that assessment in academic setting is
Table 2 The structure of TC in L2 translation
Textual competence in L2 translation
Components
1. Contrastive textual
knowledge
Declarative—knowledge of discourse and text types in L1 and L2;
knowledge of genres and their conventions in L1 and L2
Procedural—knowledge of strategies and techniques appropriate
for translation of a certain text type
2. Skills and abilities To paraphrase (lexically, grammatically, stylistically, pragmati-
cally) in L1 and L2
To summarize in L1 and L2
To define the ST particularities which are relevant for L2 translation
To indicate genre-related functional markers in SL and TL
To predict possible translation problems
To understand and analyze a macrostructure of the ST
To transfer the ST composition and coherence in L2 translation
To analyze a microstructure of the ST
To transfer a microstructure of the ST in L2 translation
To analyze comparable texts and build up a glossary
To use comparable texts as the samples of authentic language
(extensive reading in L2 for linguistic authenticity)
To assess the quality and edit the TT
To assess the quality and edit the ST (in cast of necessity)
3. Capacity To choose and make use of proper external resources in the process
of translation
Assessing Textual Competence in Translation into a Second Language 201
dependent on several aspects: Object (student translator competence, study plans
and programs); type (product, process or qualitative assessment); function (diag-
nostic, formative, summative); aim (academic, pedagogical or speculative) and
means (translations, evaluation criteria, correcting criteria, grading scales, tests,
exercises etc.).
This statement, in fact, is a major principle of carrying out a successful assess-
ment in translation teaching. In the line with this principle, Hurtado and Martinez
(2001) formulates hers:
(1) The evaluator should adhere to some specific criteria and the evaluee should be aware
of them (particularly in the case of translation teaching);
(2) The assessment criteria depend on the assessment context (published translations,
professional translation, translation teaching) and its function (summative, diagnostic,
formative). Also, therefore, the need to consider why, for what purpose and for whom
the assessment is being carried out;
(3) The object of assessment (what is being assessed?) must be clearly defined, as well as
the level at which it is being carried out. The evaluator should also consider what he
should and what he can evaluate;
(4) The evaluator should consider which indicators enable him/her to observe whether or
not and to what extent the evaluee possesses the competencies being evaluated
(Hurtado and Martinez, 2001, p. 283).
We intend to follow the above mentioned principles in order to apply them to
building a construct for assessing textual competence in L2 translation. Strategi-
cally, we will rely on three basic concepts:
1. A TC assessment should be positive and motivating. The approach is borrowed
from Hurtado and Martinez (2001) and Waddington (2001). It is based on a
penalizing-and-awarding principle (penalizing for unsuccessful solutions and
awarding points for successful solutions);
2. The assessment should be competence-oriented;
3. Given the fact that the object of assessment is competence in translation into a
foreign language, the assessed TT is supposed to be of “relative quality”
(Williams, 1989).
As far as we are knowledgeable, neither in terms of its acquisition nor in terms of
its assessment has the competence in L2 translation been the subject of extensive
research. Thus, it seems reasonable to adapt and apply findings in TrC assessments
to the needs and objectives of our research.
3 Method
Given that the objective of our research is to develop a construct for assessing TC in
L2 translation, the framework for the study is laid down by the following steps:
(1) Specifying the model of TC competence in L2 translation; (2) identifying the
object, type, function and means of assessment; (3) building and validating the
measuring instruments. Conceptually, the investigation relies on Hurtado’s &
202 T. Kavytska and O. Kvasova
Martinez (2001) approach towards assessing a translator competence acquisition.
The idea of componential-holistic scales, however, is borrowed from Williams
(2009). Thus, with the focus on the translation process rather than its result, the
object of assessment in our case is TC in L2 translation. The assessment is carried
out with a pedagogical purpose in the context of formative assessment. The tools
involve: Tests, grading scales, evaluation criteria and observation or fixation reports
which are part of measuring instruments.
The test, as the main testing tool within instruments, consists of three function-
ally different sections. Sections One and Two are targeted at testing students’
knowledge, skills and abilities. In particular, Section One contains 20 multiple-
choice questions to be answered in 10 min. The questions test the trainees’
knowledge in two domains: Discourse/text theory (10 items) and translation theory
(10 items). Section Two includes a task brief and three short texts (220 words) to be
translated from Ukrainian into English within 60 min. The texts belong to three
genres of academic discourse: Academic certificate, course description and an
abstract of a research article. Section Three involves the testees’ reports in which
they indicate what external resource was used in the process of translation.
Apart from the students’ fixation reports, direct observations reports on external
resource usage were also submitted by the teachers. Thus, the three sections of the
test make up three testing tools whereas grading scales, evaluation criteria and
indicators are measuring tools within the developed instruments.
3.1 Participants
The participants in this study involve: (1) Thirty-three Master students
(Ukrainian-Russian natural bilinguals) majoring in Translation Studies. The lan-
guage combinations include English as the first FL and German, French, Spanish or
Italian as the second FL; (2) Five University translation trainers who are also
Ukrainian-Russian bilinguals with an extensive teaching experience ranging from
10 to 25 years. Three of them combine teaching translation with teaching English as
a foreign language and hold PhD degrees in Linguistics or Translation Studies. One
of these three also works as a freelance translator. Two teachers obtained Master’s
degrees in Translation Studies and are currently engaged in teaching translation;
(3) Five professional translators into L2 who were requested to cross-assess the
students’ translations holistically.
3.2 Data Analysis
The research objective and the framework of the study have determined the
application of the following research methods:
Assessing Textual Competence in Translation into a Second Language 203
1. Analysis and interpretation of the testees’ performance in the instrument-related
tests. The analysis was made on the basis of the post-test reports of the teachers.
The trainees were tested twice for their competence in translation from
Ukrainian into English: In their second and fifteenth weeks of studies. The
tests were developed by the authors of the article;
2. Interviewing. Four interviews with teachers-participants of this research were
held. The objective of the interviews was to find out what tests the trainers were
using in their practice and what assessment approach they apply. This analysis of
the interview data was also reinforced by expert judgment;
3. Expert judgment. Along with the teachers’ assessments, interpretations of the
testees’ performance were provided by professional translators into L2. They
were to assess the level of task completion rather than the quality of the TT. The
goal of applying the method was to look at the correlation between the assess-
ments done by the teachers and professional translators;
4. Translation trainers’ questionnaire which aimed at finding out how far the
assessors are knowledgeable of translation evaluation and assessment;
5. Direct observation. The method was used to collect data on using external
resources by the students in the process of translation during the test.
4 Results and Discussion
The level of textual competence in L2 translation that was specified by us will thus
make an object of assessment. The assessment construct will include three ele-
ments: Contrastive textual knowledge; skills and abilities to transfer the ST content
and genre characteristics in L2 translation; the capacity to choose and make use of
proper external resources in the process of translation (additional component).
Instrument One, Knowledge Measuring Instrument (KMI) is targeted at
assessing students’ knowledge of text and discourse in L1 and L2 as well as the
knowledge of translation theory. As we have mentioned, a testing tool of the
instrument is a multiple-choice test (see Appendix) that evaluates knowledge of
theory of translation and theory of text/discourse. Thus, the grading scale of the
instrument applies two criteria: (1) Knowledge of translation theory; (2) knowledge
of the theory of discourse/text. The calculation technique for both criteria relies on
calculating the number of correct answers (every correct answer weighs 1 point,
whereas the total weight of the instrument is 20 points). The obtained points are
then transferred into grades, with the level of knowledge defined.
Instrument Two, Abilities Measuring Instrument (AMI) is applied in order to
assess skills and abilities to transfer textual characteristics of the ST in L2 transla-
tion and relies on three criterion groups: (1) Transfer of ST macro-structure;
(2) transfer of ST micro-structure; (3) transfer of ST genre-related language partic-
ularities. We further describe the groups in detail.
Criterion group 1 includes two criteria: (1) Composition and genre conventions;
(2) content. The first criterion measures the students’ ability to create a TT text that
204 T. Kavytska and O. Kvasova
fits into the target language (TL) genre conventions. The second criterion reflects
the ability to transfer the ST content fully and clearly in L2 translation. It is an
accepted fact that the receptive stage of translation into L2 (understanding the ST
content) does not, as a rule, entail much difficulty, but the productive stage (transfer
of the ST content) is dependent on L2 competence, and thus poses the biggest
challenge.
It is worth noting that AMI relies on the holistic-componential scale which was
described by Williams (2013). Williams agrees with Biggs and Tang (2007,
pp. 184–85), that a valid assessment, though aiming at total performance, must
relate the whole to its parts. In our case, such approach is justified, since a holistic
assessment of the CT can be obtained from an assessment of overall performance
against specific criteria that align with particular knowledge and skills (componen-
tial assessment). Moreover, the holistic-componential scales are easy to apply,
which is an indicator of their practicality. Containing the quantitative dimension
of assessment, they also facilitate reporting and justification of grades (Williams,
2013, p. 441). The scale based on criteria of Group 1 is presented in Table 3.
Criterion group 2 aims at assessing skills to transfer a ST micro-structure and
includes two indicators: (1) TT coherence; (2) TT cohesion. Coherence that
involves logical consistency and clarity of the lay-out is an important characteristic
of any text, especially in academic discourse. However, experience shows that a ST
does not always possess flawless logic, so a translator has to find the ways to
eliminate such textual flaws. Thus, the first criterion evaluates the ability of students
to analyze logical consistency of the ST and reproduce it in TT. The second
criterion assesses the ability of the trainees to find the elements of textual cohesion
and transfer them in TT. The cohesive elements relate to genre conventions and are
Table 3 Scale for assessing a ST macro-structure transfer (AMI)
Points/
CT level
Transfer of macro-structure
Composition and genre conventions Content
Indicators Indicators
9–10/
Very
high
TT composition and genre conventions
fit into their ST format. Genre charac-
teristics of the TT are perceived as
authentic
The content is transferred clearly and
fully. The unit of transfer is a concept
with its full adaptation to TL culture
7–8/High TT composition and genre conventions
nearly fit into their ST format. Genre
characteristics of the TT are perceived as
nearly authentic
The content is transferred clearly and
fully. The unit of transfer is a concept
with its partial adaptation to TL culture
5–6/
Moderate
TT composition and genre conventions
nearly fit into their ST format. Genre
characteristics of the TT are not always
perceived as nearly authentic
The content is transferred fully. The unit
of transfer is a concept with no adapta-
tion to TL culture
0–4/Low TT composition and genre conventions
do not always fit into their ST format.
Genre characteristics of the TT are not
perceived as authentic
The content is not transferred clearly.
The unit of transfer is a word. TT dem-
onstrates a few cases of literal
translation
Assessing Textual Competence in Translation into a Second Language 205
different in L1 and L2 cultures. Inappropriate cohesive devices can be very mis-
leading and infringing the line of logic or argumentation in the text. Further is
presented the scale based on criterion group 2 (Table 4).
Criterion group 3 aims at assessing the skills to transfer genre-related language
characteristics of ST in L2 translation. The criteria, in particular, include:
(1) vocabulary; (2) grammar; (3) style. These aspects, being the subject of special
difficulties, tend to generate the biggest number of errors and inadequacies in L2
translation. According to Shevnin (2009), the initiator of Translation Error Studies
in Russia, the most numerous in translation are three types of inadequacies:
(1) cognitive inadequacies which are caused by interlanguage interference and
result in infringement of lexical or grammatical combinability in the target lan-
guage; (2) cultural inadequacies that involve violations of the speech register or
genre conventions; (3) language inadequacies that are caused by erroneous orthog-
raphy and grammar. The scale below (Table 5) embraces all types of translation
inadequacies mentioned above.
Instrument Three, Capacity Measuring Instrument (CMI) is designed with the
aim of assessing the trainees’ capacity to choose and make use of appropriate
external resources in the process of text creation in L2 translation. The indicators
of the scale are associated with certain external resources (a bilingual dictionary; a
monolingual dictionary; comparable texts; parallel texts; online-sources; field
experts). In fact, this instrument evaluates cognitive, strategic and behavioral
aspects of the students’ L2 text creating process. Given the fact that the data
collection within the instrument is carried out through direct observations of the
trainees’ performance as well as testees’ indications of the resources used, the
Table 4 Scale for assessing a ST micro-structure transfer (AMI)
Points/
skill
level
Transfer of micro-structure
TT coherence TT cohesion
Indicators Indicators
9–10/
Very
high
TT demonstrates a clear and full trans-
fer of ST lay-out, logic and coherence
TT reflects a creative and functionally
adequate transfer of the ST cohesion.
A wide range of synonymic cohesive
devices is employed
7–8/High TT demonstrates a full transfer of ST
lay-out, logic and coherence. Very few
cases of clarity infringement
TT reflects a functionally adequate trans-
fer of the ST cohesion. A range of syno-
nymic cohesive devices is employed
4–6/
Moderate
TT demonstrates rare cases of insuffi-
cient transfer of ST lay-out, logic and
coherence. Very few cases of clarity
infringement
TT reflects an adequate transfer of the ST
cohesion with a few cases of functionally
inappropriate cohesive means. A limited
range of synonymic cohesive devices is
employed
0–3/Low The TT does not transfer the ST
lay-out, logic and coherence. Very few
cases of clarity infringement
TT reflects a literal or inadequate transfer
of the ST cohesion, which violates the ST
lay-out and logic. Inappropriate cohesive
means are frequently employed
206 T. Kavytska and O. Kvasova
subjectivity rate here might be relatively high. Thus, we consider this instrument
minor compared to other two instruments. The scale designed for this instrument is
given in the table below (Table 6).
We have described three instruments to measure a TC in L2 translation. The
instruments include grading scales with 10 criteria that weigh 10 points each. The
scoring procedure involves adding up the scores assigned to all criteria and calcu-
lating the percentage. The total overall score (percentage) is 100 (100%). The level
of TC is considered satisfactory if a student obtains at least 70 points (70%).
Table 5 Scale for assessing the skills to transfer genre-related language characteristics of ST
(AMI)
Points/CT
level Transfer of: Vocabulary (1); Grammar (2); Style (3)
27–30/Very
high
TT shows an adequate transfer of the ST vocabulary, including terminology. No
cases of inappropriate lexical usage or violated combinability are observed. The
vocabulary is diverse and TT reads like authentic writing
TT demonstrates a functionally adequate transfer of the ST grammar, with a
wide range of syntactical diversity. No cases of violated grammar or syntactical
interference are observed. TT grammar reads like authentic writing. TT evi-
dences a functionally adequate and creative transfer of the ST style. TT style
reads like authentic writing
21–26/High TT shows an adequate transfer of the ST vocabulary, including terminology.
1–2 cases of inappropriate lexical usage or violated combinability are observed.
The vocabulary is diverse enough and TT reads like nearly authentic writing
TT demonstrates a functionally adequate transfer of the ST grammar, with a
range of syntactical diversity employed. 1–2 cases of violated grammar or
syntactical interference are observed. TT grammar reads nearly like authentic
writing
TT evidences a functionally adequate transfer of the ST style. TT style reads
nearly like authentic writing
14–20/
Moderate
TT shows a rather adequate transfer of the ST vocabulary, including terminol-
ogy. 2–3 cases of inappropriate lexical usage or violated combinability are
observed. The vocabulary lacks diversity and TT reads like nearly authentic
writing
TT demonstrates a relatively adequate transfer of the ST grammar, with a
limited range of syntactical diversity employed. 3–4 cases of violated grammar
or syntactical interference are observed. TT grammar does not read like
authentic writing
TT evidences a relatively adequate transfer of the ST style. TT style does not
read like authentic writing
0–13/Low TT shows an inadequate transfer of the ST vocabulary, including terminology.
3–4 cases of inappropriate lexical usage or violated combinability are observed.
The vocabulary lacks diversity; TT does not read like authentic writing
TT demonstrates a inadequate transfer of the ST grammar, with a limited range
of syntactical diversity employed. 4–5 cases of violated grammar or syntactical
interference are observed. TT grammar does not read like authentic writing
TT evidences a relatively adequate transfer of the ST style. TT style does not
read like authentic writing
Assessing Textual Competence in Translation into a Second Language 207
The scales, developed for all three TC measuring instruments, were piloted in
the educational setting (Taras Shevchenko National University of Kyiv). Thirty-
three translation tests, done by Translation majors of the Master’s program, were
assessed twice. The students (N ¼ 33) did two translations within the same semester
as part of their formative assessment (a course in L2 Academic Translation).
The assessors made up two groups: five University translation trainers who
applied the instruments suggested by the authors of this article and five experts
(professional translators) who cross-assessed the quality of translation holistically.
Noteworthy is the fact that neither the teachers, nor professional translators have
been trained in language testing and assessment.
The data obtained from the assessors have demonstrated a sound inter-rater
reliability score of 89%. Thus, the agreement rate between the two groups of
assessors is rather sound, although not particularly high. The assessment reports
show that the teachers who combine teaching translation with teaching English as a
foreign language tend to be tougher assessors, which agrees with Kussmaul’s
(1995) criticism of “a teacher of FL view on assessment” in translation training.
The main problem here was that these teachers focused too much on the errors as
items of the sentence level and thus assessed skills of pedagogical translation or
practically a students’ language competence. The highest inter-rate reliability score
(94%) was demonstrated by the teacher who combined teaching translation, teach-
ing English and translation freelancing.
Thus, professional profiles of translation trainers as assessors have added
some limitations to this research. On the one hand, the inter-rate reliability
score indicates that the TC measuring instruments, described in this article, are
reliable and valid. On the other hand, considerable score gaps in the group of
teachers is an indicator of lack of literacy in translation testing and assessment.
This factor might prompt a further research into developing assessment literacy
in translation trainers. Although the developed measuring instruments were used
for formative assessment, we assume they can be also effective for summative
assessment if its object is the level of textual competence acquisition in L2
translation.
Table 6 Scale of assessing the capacity to choose external resources (CMI)
Points
Choice of external resources Skill
level
Indicators
9–10 A student preferably uses comparable texts and monolingual dictionaries
and turns to a domain expert in scientific or technical translation
Very
high
7–8 A student preferably uses comparable texts and bilingual dictionaries and
turning to a domain expert in scientific or technical translation
High
5–6 A student preferably uses on-line sources and bilingual dictionaries and
rarely makes use of comparable texts
Moderate
0–4 A student uses on-line sources and bilingual dictionaries Low
208 T. Kavytska and O. Kvasova
5 Conclusion
The research results have produced a positive washback effect. They encouraged
the teachers-participants to reconsider a traditionally accepted approach to
assessing translation skills. As a result, three out of five trainers have switched
from error-based to competence-oriented assessment. The research has also given
us some new insights into how translation tests should be designed and managed
and how the assessment procedure is influenced by the translation trainer’s profile.
These research outputs, however, require a new comprehensive study in order to
produce more reliable evidences.
The study, presented in this chapter, resolved the research questions initially set:
1. Relying on the model of TC in L2 translation, specified by us, we developed an
assessment model that includes three instruments: Knowledge measuring instru-
ment; Abilities measuring instrument and Capacity measuring instrument. The
assessment model applies a componential-holistic approach which proved rather
effective when a targeted object of measuring is a competence.
2. The assessment model evidenced its validity which is based on several factors:
(1) It has a sound inter-rater reliability score; (2) it produces meaningful infer-
ences about the level of TC in L2 translation; (3) it ensures the alignment of the
criteria with particular components of the TC and thus assesses the intended
knowledge, abilities and capacities. Moreover, the model is easy to apply, so it is
practical.
3. The assessment procedure, however, demonstrated lack of literacy in translation
trainers. This fact determined that urgent changes should be introduced into the
system of training both translator trainees and trainers.
Appendix
A Sample of the Test
Student’s Name and Surname_________ Group___________ Date_______
1. Section One. Choose the correct answer.
1. Для усного перекладу характерною є послідовність когнітивних
операцій:
(а) човникового характеру;
(б) лінійного характеру;
(в) човноково-лінійного характеру.
Assessing Textual Competence in Translation into a Second Language 209
2. Об’єктом буквального письмового перекладу виступає:
(а) слово;
(б) речення;
(в) текст.
3. Критерієм адекватності, згідно з когнітивною теорією перекладу, є:
(а) відтворення стилістичних характеристик ТО у ТП;
(б) відтворення концептуальної структури ТО у ТП;
(в) відтворення функціонально-прагматичних ТО у ТП.
4. Найбільш поширеними прийомами перекладу українськомовних
текстів наукового дискурсу на англійську мову є:
(а) вилучення мовних одиниць та генералізація;
(б) додавання мовних одиниць та конкретизація;
(в) синонімічний та антонімічний переклад.
5. Теоретичне обгрунтування письмового перекладу на іноземну мову
було вперше запропоновано:
(а) A. Нойбертом;
(б) A. Уртадо;
(в) С. Кемпбеллом.
6. Форенізація та доместикація – це:
(а) прийоми перекладу;
(б) види перекладу;
(в) стратегії перекладу.
7. Вибір перекладацьких стратегій зумовлений:
(а) видом перекладу;
(б) жанровими особливостями текстів, що перекладаються;
(в) системними мовними розходженнями.
8. Aбсолютна якість ТП є необхідною властивістю:
(а) професійно-орієнтованого перекладу;
(б) навчального перекладу;
(в) професійного перекладу.
знання теорії перекладу та двох контактуючих мов.
9. Компресія тексту НЕ є ознакою:
(а) усного послідовного перекладу;
(б) письмового перекладу;
(в) усного синхронного перекладу.
210 T. Kavytska and O. Kvasova
10. Одиниці двох мов ніколи не перебувають у відношеннях
еквівалентності, згідно з:
(а) лінгвістичною теорією перекладу;
(б) комунікативною теорією перекладу;
(в) когнітивною теорією перекладу.
11. До різновиду текстів мінімальної жорсткості/конвенційності
належать:
(а) педагогічна анотація; рецензія;
(б) доручення; свідоцтво про одруження;
(в) стаття, доповідь.
12. Логічна цілісність тексту пов’язується з:
(а) когерентністю;
(б) когезією;
(в) типом тексту.
13. Засоби когезії НЕ є релевантними для текстів:
(а) наукового дискурсу;
(б) адміністративного дискурсу;
(в) художнього дискурсу.
14. Тексти академічного дискурсу характеризуються відсутністю:
(а) складного синтаксису;
(б) фактуальної інформації;
(в) емоційно-образної лексики.
15. Aнглійськомовний науковий дискурс характеризується відсутністю
жанру:
(а) монографії;
(б) автореферату;
(в) анотації.
16. Найвищим ступенем формальності відзначаються тексти:
(а) адміністративного дискурсу;
(б) наукового дискурсу;
(в) педагогічного дискурсу.
17. Виклад думок у першій особі однини властивий для:
(а) українськомовного наукового дискурсу;
(б) російськомовного наукового дискурсу;
(в) англійськомовного наукового дискурсу.
Assessing Textual Competence in Translation into a Second Language 211
18. Комунікативною метою текстів наукового дискурсу є:
(а) інформування про результати наукових досліджень;
(б) доведення істинності точки зору автора;
(в) популяризація автора наукового дослідження.
19. Чіткість, ясність думки та логічність найменш властиві для:
(а) наукового дискурсу;
(б) художнього дискурсу;
(в) педагогічного дискурсу.
20. Когезія виступає елементом:
(а) макроструктури тексту;
(б) змісту тексту;
(в) мікроструктури тексту.
2. Section Two. Translate Texts A, B and C from Ukrainian
into English. You can use any external resource indicated
in Section Three.
A.
Київський національний університет
імені Тараса Шевченка
Інститут філології
11.04.2015 ДОВІДКA
Київ
Цим засвідчуємо, що студент 2-го курсу факультету східних мов Діброва
Євген Васильович посів друге місце в студентській олімпіаді з японської
мови, що відбулася 18-20 березня 2009 року й була організована
Київським національним лінгвістичним університетом. У зв’язку з
цим Діброва Є.В. рекомендований для участі у програмі обміну
студентами. Довідка видана для пред’явлення у посольстві Японії в
Україні.
Директор Інституту (підпис)
Б. AНОТAЦІЯ НAВЧAЛЬНОЇ ДИСЦИПЛІНИ
Назва: Aктуальні проблеми історії української літератури.
Спеціальність: Слов’янські мови та літератури.
Викладач: xxxxx, д.філол. н., проф.
212 T. Kavytska and O. Kvasova
У пропонованому курсі аналізується російсько-українські літературні
взаємини в перші половині ХІХ ст., в межах формування романтизму і
реалізму, особливості індивідуального стилю класиків нової української
літератури, поетика жанрів, співвіднесеність ідейно-естетичної
традиції та новаторства.
Дисципліна викладається 1 семестр. Студенти складають іспит.
Шифр спеціальності: Філологія – 0305.
Кредити ECTS – 3.
Тип підготовки– гуманітарна, професійна.
Мова викладання – українська.
В.
ТЕКСТОТВІРНA КОМПЕТЕНТНІСТЬ ПЕРЕКЛAДAЧA: ПРОБЛЕМИ
ВИЗНAЧЕННЯ
У статті пропонується уточнена дефініція текстотвірної
компетентності перекладача та розглядається її місце серед інших
складових фахової компетентності перекладача. Внаслідок аналізу
одинадцяти найбільш поширених моделей фахової компетентності
перекладача визначено структуру текстотвірної компетентності, що
охоплює контрастивні текстові знання, навичка та вміння, а також
додатковий компонент – уміння вибрати й залучити зовнішні ресурси
до процесу створення тексту перекладу. Дослідження виконано з позицій
текстоцентристського підходу до навчання перекладу.
Ключові слова: текстотвірна компетентність, фахова,
компетентність перекладача, жанри текстів.
3. Section Three. Mark every usage of the external resource
with a tick in the table below.
No. Resource Usage data
1. Bilingual dictionaries
2. Monolingual dictionaries
3. Comparable texts
4. Online sources and translators
References
Biggs, J., & Tang, C. (2007). Teaching for quality learning at university. Maidenhead: Open
University Press.
Campbell, S. (1991). Towards a model of translation competence. Meta: Translators’ Journal, 36
(2–3), 329–343.
Assessing Textual Competence in Translation into a Second Language 213
Campbell, S. (1998). Translation into the second language. London: Longman.
European Commission. European Master’s in Translation (EMT) Strategy. (2009). Retrieved from
http://ec.europa.eu/dgs/translation/programmesemt/key_documents/emt_strategy/.
House, J. (1997). Translation quality assessment: A model revisited. Tübingen: Gunter Narr Verlag.
Hughes, A. (2003). Testing for language teachers. Cambridge: Cambridge University Press.
Hurtado, A., & Martinez, N. (2001). Assessment in translation studies: Research needs. Meta:
Translator’s Journal, 46(2), 272–287.
Kashirina, N. (2005). Kriterii otsenki kachestva pismennyh perevodov kak metodicheskaya rabota.
Lingvodidakticheskie osnovy prepodavaniya yazykov I kultur: sbornik statei pod redaktsiei
I.A. Tsaturovoi. Taganrog: Izdatelstvo TRTU (pp. 48–56).
Kavytska, T. (2015). Formuvannia tekstotvirnoi kompetentnosti maibutnih filologiv u pysmovomu
perekladi z ukrainskoi movy na angliysku. Dysertatsia na zdobuttia naukovogo stupenia
kandydata pedagogichnyh nauk. Kyiv: KNU.
Kelly, D. (2000). Diversity in unity: Translation into non-mother tongues in translator training in
Spain. In M. Grosman et al. (Eds.), Translation into non-mother tongues in professional
practice and training (pp. 185–191). Tübingen: Stauffenburg Verlag.
Kelly, D. (2002). Un modelo de competencia traductora: bases para el dise~
no curricular. Puentes.
Hacia nuevas investigaciones en la mediaci
on intercultural, 1, 9–20. Retrieved from
http://www.ugr.es/~greti/puentes/puentes1/02%20Kelly.pdf
Kiraly, D. (2000). Translation into a non-mother tongue: From collaboration to competence. In
M. Grosman et al. (Eds.), Translation into non-mother tongues in professional practice and
training (pp. 117–123). Tübingen: Stauffenburg Verlag.
Komissarov, V. (1997). Teoreticheskie osnovy metodiki obucheniya perevodu. Moskva:
Izdatelstvo REMA.
Kussmaul, P. (1995). Training the translator. Amsterdam: John Benjamins Publishing.
Mackenzie, R. (2000). Resource research strategies: A key factor in teaching translation into the
non-mother tongue. In M. Grosman et al. (Eds.), Translation into non-mother tongues in
professional practice and training (pp. 125–131). Stauffenburg Verlag: Tübingen.
Mansell, R. (2008). Optimality in translation. In A. Pym & A. Perekrestenko (Eds.), Translation
research projects (pp. 3–12). Tarragona: Intercultural Studies Group.
Montalt Ressurrecció, V., Ezpeleta Piorno, P., & Garcı́a Izquierdo, I. (2008). The acquisition of
translation competence through textual genre. Translation Journal, 12(4), 2–12.
Neubert, A. (2000). Competence in language, in languages, and in translation. In A. Neubert,
C. Schäffner, & B. Adab (Eds.), Developing translation competence (pp. 3–18). Amsterdam:
John Benjamins.
Orozco, M., & Hurtado, A. (2002). Measuring translation competence acquisition. Meta: Trans-
lators’ Journal, 47(3), 375–402.
PACTE. (2003). Building a translation competence model. In F. Alves (Ed.), Triangulating
translation: Perspectives in process oriented research (pp. 43–66). Amsterdam: John
Benjamins.
Pavlović, N. (2007). Directionality in collaborative translation process. Unpublished doctoral
dissertation, Universitat Rovira I Virgili.
Schäffner, C. (2000). Running before walking? Designing a translation programme at undergrad-
uate level. In C. Schäffner & A. Beverly (Eds.), Developing translation competence
(pp. 143–156). John Benjamin: Amsterdam.
Shevnin, A. (2009). Paranormativy v perevode: nesootvetstviya ekspressivnogo tipa. Izvestiya
Rossiyskogo gosudarstvennogo pedagogicheskogo universiteta im. A.I. Gertsena, 117,
157–164.
Stewart, D. (2008). Vocational translation training into a foreign language. Intralinea Online
Translation Journal, 10, 1–17.
Waddington, C. (2001). Different methods of evaluating student translations: The question of
validity. Meta: Translators’ Journal, 46(2), 331–325.
214 T. Kavytska and O. Kvasova
Waddington, C. (2004). Should student translations be assessed holistically or through error
analysis? Lebende Sprachen, 49(1), 28–35.
Williams, M. (1989). The Assessment of Professional Translation Quality: Creating Credibility
out of Chaos. TTR, 2(2), 13–33.
Williams, M. (2004). Translation quality assessment: An argumentation-centred approach.
Ottawa: University of Ottawa Press.
Williams, M. (2009). Translation quality assessment. Mutatis Mutandis, 2(1), 3–23.
Williams, M. (2013). A holistic-componential model for assessing translation student performance
and competency. Mutatis Mutandis, 6(2), 419–443.
Assessing Textual Competence in Translation into a Second Language 215
Assessing Literature for the Classroom Among
Female Learners of English in an EFL Context
in Saudi Arabia
Manal Qutub
Abstract Assessing the construct of literature in an EFL context, such as Saudi
Arabia, has been a neglected area basically for different educational, social and
practical reasons. This study investigated the assessment of a literature course
among 60 female students that ranged in age between 17 and 20 at a tertiary level
in KSA. An achievement exam on testing a literature task was administered to these
students. The results of the study conducted at the Girls’ College of Education in
Makkah, Saudi Arabia, indicated that some of EFL students were incapable of
understanding and dealing with literary texts. Their deficiencies were attributed to
the traditional teaching methods applied to the teaching of literature on the one
hand and to the language weaknesses and the conventional ways of presenting
literature in EFL classes, on the other. The different statistical tests carried out
showed that there were no statistically significant differences between the means of
freshmen EFL students’ achievement scores in the control and experimental groups
on the fiction achievement post-test at p < 0.05 significance level. Recommenda-
tions were highlighted to consider defining the construct of literature both theoret-
ically and operationally and to design a list of test specifications that literature
teachers should implement to measure this construct for learners of English in a
similar context. The study had implications for research and pedagogy.
Keywords Literature • Assessment • Literary tasks • Construct • Achievement •
Test specifications
1 Introduction
This article investigated the teaching and assessment of literature among students in
an EFL context in Saudi Arabia. It is assumed that all aspects of foreign language
teaching are interrelated, and literature teaching which is one of the aspects of
foreign language teaching programs has a focal interest because of a myriad of
M. Qutub (*)
University of Jeddah, English Language Institute, P. O. Box 7320, 21462 Jeddah, Saudi Arabia
e-mail: Mqutub@uj.edu.sa
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_11
217
written materials offered via literature teaching (Ogeyik, 2007). Many researchers
(e.g., Widdowson, 1990) have highlighted the importance of literature in teaching
and assessing language and there has been an increasing awareness of integrating
literature in EFL curricula, especially in academic programs mainly because it
helps the students increase their intellectual as well as linguistic abilities and their
awareness of the world. Baird (1994, p. 5) argues that “the rethinking of assessment
in literary skills has forced educators to ask searching questions about the place of
literature in school curricula and its justification as a part of the curriculum.”
Savvidou (2004) points out that there are many benefits to using literature in the
EFL classroom, such as creating opportunities for personal expression as well as
reinforcing learners’ knowledge of lexical and grammatical structures. Therefore,
the use of literary text in the language classroom is a significant pedagogic tool. In
this study, the researcher concentrated on fiction as a distinctive form of literature.
For Brown-Chidsey, Johnson, and Fernstrom (2005), there should be a match
between instructional and assessment materials.
Literature is a kind of art that broadens the students’ imagination as well as their
intelligence and gives them a sense of joy and interest while learning. It enables
them to see how histories, societies, thoughts and aspects of language differ from
one context to another. Kodama (2012) highlights the role of literature in the
EFL/ESL environments where it is used as a communicative language teaching
tool to teach this subject to students. Additionally, it can be considered as a means
for language development, since it enables students to see how words and sentences
are organized and how language is used for actual communication (Zyngier, 1994).
Literary texts are intellectually stimulating in that they allow readers to create
worlds with which they may not be familiar. In building meaning, the reader
reconstructs what s/he thinks the writer is trying to convey. In this sense, the reader
becomes a performer, an actor or even a constructor in a communicative event
(Brumfit, 1990). For Adam and Harper (2016), assessing literary texts in class
brings in cultural awareness of aspects that are novel to the EFL learners, even
though it could be regarded as problematic.
2 Theoretical Background
2.1 Defining Fiction
Fiction is an imaginary writing that most often takes the form of the short story
and novel, both of which are forms of narrative. The purpose of fiction is to
entertain as well as enlighten the reader by providing a deeper understanding of
the human conditions (Pickering, 1998). In EFL situations, using fiction is invalu-
able as a method of illustrating the foreign culture in an authentic context. It
creates perfect venues for presenting ideas for reading, discussion and writing
218 M. Qutub
tasks. Besides, it provides in-depth ideas about life that can be meaningful to
students. As noted by Kauffman (1996)
The narrative texts of fiction can present real-life situations which reflect the human
condition in its universal timeless themes. Those authentic situations are mostly useful
for foreign language learning. Moreover, students are bound to form relations and connec-
tion, they can begin to think more deeply about the themes; questioning, connecting, and
developing more complex ideas to represent their own analysis. (p. 399)
This in turn helps foster critical thinking which is considered so important for
students with an advanced level in such a subject.
The best way into exploring literature is through a view of language as discourse
(Hall, 2005). Discourse must be understood in its widest sense: every utterance
assumes a speaker and a hearer, and in the speaker, the intention of influencing the
other in some way is the very variety of oral discourse of every nature from trivial
conversation to the most elaborate oration. Carter and Long (1991) contrast a view
of literature considered as text with a view of literature seen as discourse. The first
view promotes in the student knowledge about literature, a more traditional aim of
literary studies, where a view of literature as discourse promotes knowledge of
literature. A traditional literary education urged attention to decontextualized
“words on the page”. Literature traditionally claimed access to universal values
and qualities, such as truth and beauty. A discourse view, on the other hand,
significantly more in line with wider later twentieth century thinking, sees a text
as constructed, contingent and requiring active interpretations in contexts. Litera-
ture as discourse is open to interrogation, confrontation and interventions (Pope,
1995).
Literature in education concerns itself with the study of the interactive
approaches among literary texts, teachers and students in specific educational
contexts in order to improve existing practices of literature teaching (Bredella &
Delanoy, 1996). The possible linguistic advantage of literary texts for learners of a
language would seem if anything to be its sheer range, which is unparalleled in
another text types, and the creative strategies required of the successful literary
reader.
A crucial development for the role of literature in foreign language teaching
programs was the burgeoning of the Communicative Language Teaching (CLT)
approach, notably from the 1980s, particularly in more privileged educational
situations, often replacing a “grammar-translation” method of language teaching
whose final humanistic aim was to enable the students to read successfully the
classic literature of the language. For CLT, by contrast, literature would come to be
seen as irrelevant or at best a useful means or “resource”, rather than the end of
study (Hall, 2005). Kodama (2012) addressed the importance and relevance of the
role of literature in the EFL and ESL classrooms through the use of the CLT
approach and that such trend has been just revisited to be widely used in different
educational contexts.
Hall (2005) argues that foreign language curricula which incorporate literary
texts have tended in general to be vague, conservative, over-specified in terms of
Assessing Literature for the Classroom Among Female Learners of English in. . . 219
educational aims, as if the value of literature was obvious, or simply to make
assertions which lack clear empirical evidence to back them. The strict “Beowulf
to Virginia Woolf” approach of old-style Oxbridge English study was uncritically
exported to the colonies and beyond. Teachers from the Middle East and North
Africa (MENA) context often display detailed knowledge of literary history of this
kind, literature as a subject of study for its own sake. Their scholarly erudition is
admirable, but it is not what their students need. Literature in such situations easily
turns into the study of dates, facts and plot summaries which will not in any obvious
way support improved use of language, which is the aim of most students on a
language course.
2.2 Literature Assessment in Foreign Language Teaching
Alderson (2000, p. 66) maintains that “if it is the case that readers’ response to
literary texts in personally meaningful, often idiosyncratic ways, it is hard to see
what sort of meaning one could test to say that a reader had actually understood a
literary text.” That is, examinations are not designed merely to check whether the
students have read and remembered certain books but to test his or her progress as a
reader of literature. Lee and Goldman (2015) assessed reasoning in literary reading
comprehension texts and the different skills and strategies students use to get the
meaning of such texts. Assessment of literature reading in a second or foreign
language must be even more centrally concerned with evaluating understanding of
the language of the text where syllabus documents claim or imply that reading of
literature will pay second language acquisition dividends, and since one knows the
importance of language proficiency to successful second language reading. Too
often, the key criticism made of assessment exercises in second language literature
education is that “such questions can be answered if the candidate has read only a
translation or even a simplified version of the text” (Carter & Long, 1991, p. 217).
Thus, typical questions ask for plot summaries, character sketches, themes or the
like, in short “content” which is propositional rather than evidence for processing of
literary texts written in another language. Al Faki (2014) argues that
the teaching of literature has recently witnessed a very hard time of absence due to a faulty
political decision. That period witnessed a remarkable deterioration in the students’ lin-
guistic ability. This is because literature is vitally important in providing the students with
the required help and assistance to go about learning the target language i.e., English
language. Now literature is reintroduced to accompany the present taught English language
course-books in the secondary schools (Spine 4, 5, and 6). The idea is to help improve
students’ ability in English language. (p. 9)
One response to a difficult situation in the University of Cambridge examining,
for instance, has been to retreat from testing of literature as such to, for example in
First Certificate of English (FCE) a supposed reading of William Golding’s Lord of
the Flies as input to a conversation with an examiner designed to probe oral
220 M. Qutub
proficiency, but in no way assessing literary proficiency. Language should be an
important factor in the assessment of second language literature reading.
A second general aspect is the factor of what a reader does with, or takes from, a
text. Reading is an interactive process in the sense that different individuals with
differing backgrounds and interests take differently from texts, perhaps especially
where literary texts are concerned (Halasz, 1991). Bernhardt (1995), Alderson
(2000) and others have shown how second language reading study and assessment
often focused on the text at the expense of the reader and reading behaviors. Second
language reading competencies often assumed, however implicitly, to be a matter
of moving from linguistically “easier” to longer and “more difficult” texts. Even
leaving aside questions of how exactly constructs like “easy/difficult” are
operationalized, such an approach ignores the question of reader variables in the
sense of what readers can and might want to do with a text. Alderson (2000) sees
this situation as one ground for the need for more qualitative approaches to second
language reading research. The problems of such “literalist” weaker readers of
literature in integrating the information taken from the text into larger propositional
units can be usefully studied through verbal protocols. More recently Cambridge
ESOL continues to suggest optional set texts for reading for advanced Certificate of
Proficiency in English (CPE) examinations, to encourage extended reading as a
basis for the enrichment of language study (Hall, 2005).
2.3 Literature in a Learner-Centered Classroom
Kramsch (1985) asserts that literary texts continue to be taught as finished products,
to be unilaterally decoded, analyzed, and explained, or they are used to illustrate
grammatical rules and enrich the reader’s vocabulary. Such a traditional approach
only confirms the students’ belief that their major block to understanding is a lack of
vocabulary and grammar and solidifies their dependence on the teacher. It is as
important to sensitize the students to the process of literary creation, as it is to
initiate them in the construction of interactive spoken discourse. Thus, literature
often represents challenging material for learners and teachers alike, but potentially
leads beyond marrow instrumental views of language and language learning to
wide-ranging and fundamental features of all our lives which should be of value and
of interest to investigate, discuss, and understand better (Hall, 2005).
2.4 Literature and Communicative Language Teaching
There was a time when advocates of the use of literature in ELT felt compelled to
lay out a carefully reasoned case for that use before discussing actual applications
of literature in the language classroom. This was particularly true in the 1980s,
when literature began to make a comeback in ELT after a long period of exile.
Assessing Literature for the Classroom Among Female Learners of English in. . . 221
Although such defenses continue to appear, there is not that same sense of urgency
about defending the use of literature, as it seems to have regained a modest place in
the ELT mainstream. At present, then, the debate increasingly centers not so much
on whether to use literature as on how to use it (Hirvela, 1996).
As the communicative movement gained momentum in the 1980s, teachers and
materials designers faced the challenge of creating conditions for learning in which
learners could use the target language in circumstances which approximated gen-
uine communicative contexts. As Collie and Slater (1987, p. 5) explain, this was
partly because “engaging imaginatively with literature enables learners to shift the
focus of their attention beyond the more mechanical aspects of the foreign language
system.” This imaginative engagement could then be used as the basis for designing
instructional activities which require learner production of discourse in the process
of describing that engagement. Such discourse production involves the expression
of learner responses to texts through tasks assigned by teachers. This approach is
seen in such popular literature-based teaching texts as Bassnett and Grundy (1993),
Collie and Salter (1987), and Maley and Duff (1989). The essence of their com-
municative, personal-response approach is seen in this citation from Maley and
Duff (1989) as
the primary aim of our approach is quite simply to use literary texts as a resource for
stimulating language activities. What we are interested in is engaging students increasingly
with the text, with fellow students, and with the teacher in the performance of tasks
involving literary texts, In so doing, students are obliged to pay careful attention to the
text itself and to generate language in the process of completing the task (p. 5).
What makes this both a communicative- and a personal-response approach is the
intention of the tasks assigned to learners. These are not gap-filling activities
examining learner comprehension of the text. Instead, learners are asked to generate
personal responses to something in the text, responses which necessitate the
production of original discourse. These tasks can be stylistic in nature, in the
sense of asking learners to discuss the author’s choice of language in certain
portions of the text.
Research findings emphasize that the advantage of teaching literature in EFL
programs is mainly connected with two factors the first of which is selecting the
appropriate materials for students’ level. The second factors rest on applying
innovative methods to present those materials (Abdul-Jabbar, 1990). This corre-
sponds quite closely with suggestions made in recent years by those who subscribe
to constructing a new pedagogy for language and literature teaching. Faber (1998,
p. 83) suggests the need to depart from traditional classroom routines and considers
alternatives to teaching literature. Different studies report that when the learners’
individual learning style is considered, the quality of learning is enhanced (Ford &
Chen, 2001; Rasmussen, 1998; Riding & Grimily, 1999). Thus, a shift from a
traditional to a progressive model of education has led to an increased interest in
learners’ individual differences. The new paradigm is student-centered, based on
inclusiveness, cooperative learning, and encourages diversity (Zywno & Waalen,
2002).
222 M. Qutub
3 Method
Freshmen EFL students at the Girls’ College of Education in Makkah face some
problems in dealing with the literary texts (Abdul-Jabbar, 2003). In particular, they
face several difficulties while reading short stories and novels that some students
cannot understand the underlying meanings of the texts. In some cases, they find
difficulties in analyzing and interpreting fiction to get the implicit ideas and implied
meanings. In some classes, freshmen EFL students are unable to express their ideas
and convey their thoughts about the text of fiction in correct oral or written English.
In consequence to these deficiencies the majority of freshmen EFL students gained
low achievement in studying fiction, which indicated some problems. These prob-
lems were attributed to the conventional way of teaching fiction in EFL classes.
English has been of central importance in KSA as it serves many functions in the
educational, economic, socio-cultural and political life of Saudis. Undoubtedly, a
Saudi citizen with a strong command of English would be very much advantaged
and favored in the job market of both public and private sectors’ organizations.
Policymakers in KSA have consequently given English its due emphasis in recent
years (Al-Hazmi, 2007). English has become a major component of the Saudi
educational system which consists of 12 grades; 6 elementary grades, 3 intermediate
grades, and 3 secondary grades. English was the first and only foreign language
students learn in public schools. English starts from the sixth grade in government
schools (i.e., at the age of 12) and there were plans to extend it to the Fourth Grade.
In most higher education institutions in KSA, English has been a major disci-
pline in faculties of arts, education, languages, and translation in universities such
as: King Saud University (1957), King Abdul-Aziz University (1961), Imam
Mohammad Ibn Saud Islamic University (1974), King Faisal University (1976),
Umm Al-Qura University (1980), King Khalid University (1998), and Taibah
University (2003). English is also the language of instruction in scientific depart-
ments (e.g., medicine, engineering, computing) in all universities. As for King
Fahad University, English has been the language of instruction since its establish-
ment in 1963. The first English department in KSA was established in King Saud
University in 1957 (Baird, 1994). During the 1970s the significance of English
increased considerably. Investigating literature assessment has been given momen-
tum in research at the international level. However, addressing its status in the
Saudi context has been a neglected area for different reasons, such as cultural and
religious. Therefore, the study attempted to address the following questions:
(a) How are teaching and assessing literature perceived by freshmen EFL students
at Girls’ College of Education in Makkah have?
(b) What are the effects of assessing literature among first grade freshmen EFL
students’ achievement in the fiction course entitled “Introduction to Fiction 1”
(1111)?
Assessing Literature for the Classroom Among Female Learners of English in. . . 223
3.1 Participants and Sampling
The sample of this study consisted of 60 first year students enrolled at the English
Department of Makkah’s College of Education. All of them had similar educational
background: they studied English as a mandatory course in intermediate and
secondary schools. The participants received at least 6 years of formal English
instruction. Moreover, the participants took the Aptitude Test, and they met the
required standards before entering Girls’ College of Education. The average age of
the participants ranged from 17 to 20. The teacher selected to take part in this study
was a proficient teacher, specialized in the teaching of English literature. She
enjoyed trying new teaching pedagogies. Her open and friendly personality won
the students’ respect. Most importantly, she liked to socialize with her students,
which was a very important qualification for teaching with the multiple intelli-
gences theory. During research period, this teacher instructed both the experimental
group and the control group. To answer the research questions, an achievement
post-test was administered to both groups to investigate the effect of using the
suggested strategy based on the multiple intelligences theory on freshmen EFL
students’ achievement in the fiction course.
3.2 Instruments
To investigate the effect of the suggested strategy, the researcher designed an
achievement test for the course “Introduction to Fiction 1”, which was both a pre-
and post-test. This test was intended to measure freshmen EFL students’ ability to
understand and interpret the literary texts of fiction. The test comprised four parts
(see Appendix 1); the first part contained ten items designed in a multiple-choice
format. These items were intended to measure students’ general comprehension for
the literary texts. Part 2 contained six items, that required students to define the
elements of fiction. Part 3 included four items, in which students should fill a blank
story-elements map. Part 4 comprised three open-ended questions to examine
students’ competence in interpreting and analyzing the literary texts of fiction.
A scoring rubric (see Appendix 2) was developed by the researcher for scoring
part 4 of the exam. Five main domains were specified to be measured via this rubric.
Each domain represented a main literary competence feature; the first feature is
about appropriateness of interpretations for the literary text, the second is on
pertinence of development for fictional narrative, the third one deals with efficiency
of organization, the fourth one is about effectual language use, voice, and style, and
the fifth is about control of mechanics and conventions. The score scale (the rubric)
is four-point. Each one of the five specified domains of literary competence was
evaluated separately and assigned a score of “1” (lowest), “2”, “3”, or “4” (highest).
The scale was a continuum representing a range of quality. Each score point on the
continuum was defined by domain-specific scoring guidelines.
224 M. Qutub
The test was submitted to a jury of specialists in teaching literature in EFL
classes. They were requested to read the test items and the rubric to make sure the
test and the rubrics were appropriately established. It was recommended by two of
the members to add a part concerned with defining the elements of fiction. Some of
the members suggested simplifying the language used in the items of the fourth part
of the test. These suggestions were considered, and the final version of the test was
confirmed. Reliability of the test was determined by using test–retest method,
which was based on getting Pearson Correlation Coefficient. In the first week, the
test was administered to 15 freshmen EFL majors for the first time. During the
second week, the test was administered again for the same sample. A test–retest
reliability of r ¼ 0.88 was found for the test. For the statistical analysis of the
collected data, the researcher used a paired-sample t-test to analyze the results. The
Statistical Package for the Social Sciences software (SPSS) was employed to run
the data. A paired-sample t-test was carried out to determine if there were statisti-
cally significant effects on the mean scores of the control group and the experi-
mental group in the fiction achievement pre/post test.
4 Results and Discussion
The results of the study conducted at Girls’ College of Education in Makkah
showed that some of EFL students were incapable of understanding and dealing
with literary texts. Their deficiencies were attributed to the traditional teaching
methods applied to teaching literature (Abdul-Jabbar, 2003; Al-Saedi, 1998). To
investigate the problem, the researcher reviewed the results of the literary subjects
(drama, prose, and poetry) for freshmen EFL students at the Girls’ College of
Education in Makkah during years 1 and 2. Freshmen EFL students’ achievement
reflected obvious decline in Prose course, as shown in Table 1.
Reviewing freshmen EFL students’ results in prose course throughout five
academic years represented low achievement which indicated serious problems in
studying prose as shown in Table 2.
Table 2 Freshmen EFL
students’ results in prose
course throughout five
academic years
Academic years Prose results (%)
2001 45
2002 35
2003 45
2004 45
2005 43
Table 1 Freshmen EFL
students’ achievement in
literary subjects
Literary subjects Year 1 (%) Year 2 (%)
Drama 66 62
Prose 45 43
Poetry 53 56
Assessing Literature for the Classroom Among Female Learners of English in. . . 225
An investigation for the opinions of the staff members of the English Depart-
ment at the Girls’ College of Education in Makkah proved that freshmen EFL
students faced various difficulties while dealing with the literary texts of fiction.
These difficulties were attributed to language weaknesses, and the conventional
way of presenting literature in EFL classes.
There were no statistically significant differences between the means of
freshmen EFL students’ achievement scores in the control group and the experi-
mental group on the fiction achievement pre-test at p < .05 level of significance.
The t-test was used to compare the scores of the two groups in the fiction achieve-
ment pre-test.
Data in Table 3 indicated that there was no significant difference between the
two groups in the fiction achievement pre-test. The result of the t-test demonstrated
that t-value ¼ (8.62) and p < 0.05 level showed this. The means of control and
experimental groups were 46.33 (SD ¼ 4.51) and 46.82 (SD ¼ 5.03) respectively.
The means showed no significant difference in the fiction achievement pre-test. The
two groups were almost identical in fiction achievement. There were no statistically
significant differences between the means of freshmen EFL students’ achievement
scores in the control group and the experimental group on the fiction achievement
post-test at the p < 0.05 level of significance. The t-test was used to compare the
scores of the two groups in the fiction achievement post-test.
Table 4 shows that there was a statistically significant difference between means
of scores obtained by the control and experimental group in favor of the experi-
mental group in the fiction achievement post-test. The experimental group got a
higher mean (86.07) than that obtained by the control group (61.33). The result of
the t-test demonstrates that t-value ¼ (11.75) and p < 0.05 level. The mean of the
fiction achievement pre-test for the experimental group was 46.82 (SD ¼ 5.03)
prior to the new experience. However, as a result of the new teaching strategy, this
mean of the fiction achievement had increased to 86.07 (SD ¼ 9.52) indicating a
significant difference between the two groups. Freshmen EFL students’ achieve-
ment in fiction had reasonably developed.
The use of the pre-test at the beginning of the course indicates no differences
between the experimental and control group in prior knowledge. Moreover, prior to
the beginning of the course, the two groups of students involved in this study
Table 3 T-value for differences between the two groups in the fiction achievement pre-test
Fiction achievement pre-test M SD t-value DF Sig
Control 46.33 4.51 8.62 31 0.05
Experimental 46.82 5.03
Table 4 T-value for differences between the two groups in the fiction achievement post-test
Fiction achievement posttest M SD t-value DF Sig
Control 61.33 6.50 11.75 58 0.00
Experimental 86.07 9.52
226 M. Qutub
displayed none of the preexisting differences in demographic characteristics. Like-
wise, the course format and grading philosophies of the course instructor did not
vary between the two groups. Therefore, it seemed reasonable to conclude that the
advanced post-test performance of the experimental group was the result of the new
teaching strategy. The findings of this study, therefore, augmented the findings of
Abdullah (2005), Anderson (1998), Gohlinghorst and Becky (2001) and Haley
(2004). Overall, the results of this study confirmed that freshmen EFL students’
achievement in fiction reasonably developed. In the spirit of relating theory and
research to ESL/EFL practice and pedagogy, the present study demonstrated under
controlled conditions that a particular pedagogical practice, beneficial in an L1
context, could yield a positive outcome when applied to an EFL context as well.
The findings of the current study also supported the findings of the studies that
suggested the need for applying innovative approaches to teaching literature in EFL
context, such as the integrative approach (Stern, 1985), the eclectic approach
(Al-Saedi, 1998), the learner-centered strategies (Chang, 2003), and the internet
technology-namely the internet group (Arikan, 2008). All findings of these studies
pointed out the importance of applying more inventive methods to teaching liter-
ature in foreign language classes. The result of the present study, however,
contradicted the findings of some of the studies reviewed earlier. While Nguyen
(2002) found no differences between students’ achievement in the traditional
school program, the current study found positive results for developing students’
achievement in the fiction course. While the reasons for such challenges and
failures were complex and many, one evident cause must be English language
education programs. EFL learners in KSA critically had need of a comprehensive
approach to integrate English education across pre-university and university levels
in order to make language learning relevant to further academic and professional
life situations.
The teacher-dominated EFL classes in KSA continued to marginalize the role of
students, who still believed that teachers were responsible for learners’ learning.
The EFL teacher’s role needs to be shifted from material presenter and classroom
manager to learning facilitator, as the former, tends to neglect the student as an
individual by imposing traditional teaching methods and conventional lecturing
techniques. EFL teachers are supposed to implement innovative teaching methods
and strategies that meet the diverse changing needs of Saudi students in the
globalization age. Meaningful learning needs to become the objective of all edu-
cation and training to equip learners to think critically and creatively, to exploit the
largest part of their talents and capabilities, to pose and solve problems, to work
with one another, and to become independent and life-long learners.
Despite all the views maintaining the appropriateness and beliefs of teaching
literature in foreign language context, teaching literature is sometimes referred to as
a discouraging and difficult task. The reasons why teachers often considered
literature inappropriate to language classroom may arise from the common beliefs
about the difficulties of reading literary texts and literary language. In this context,
the research conducted by Akyel and Yalcin (1990) showed that the desire to
broaden learners’ horizons through exposure to classic literature usually had
Assessing Literature for the Classroom Among Female Learners of English in. . . 227
disappointing results. In addition, exposure to literary analysis created difficulties
when students were not directed in both analytical methods and self-confidence to
put forward their own views; accordingly, they often avoided critical judgments and
did not have confidence to attempt a personal interpretation of the literary work they
deal with (Carter & McRae, 1997; Widdowson, 1975). The reasons behind these
complications may stem from the inappropriate methods of teaching literature, the
purpose of the literature courses, and the ways selecting suitable literary texts
regarding students’ needs and expectations (Ogeyik, 2007).
5 Conclusion
Teaching English as a foreign language should not be limited to the linguistic skills,
and thus the scope of language learning should be broadened into one that incor-
porates inter-cultural awareness and understanding as well (Kim, 2002). There has
been an increasing awareness of integrating literature in EFL curriculum, especially
in higher education programs in that it helps the students increase their intellectual
as well as linguistic abilities. In the field of teaching literature in EFL classes, few
studies have conducted to investigate the effect of using certain teaching strategies
or innovative approaches to develop EFL majors’ achievement in literature. This
study will hopefully add to the existing data constructive findings in this
interesting area.
The primary purpose of this study was to investigate the effect of a suggested
strategy to teaching fiction based on freshmen EFL students’ achievement in the
fiction course entitled “Introduction to Fiction 1” at the Girls’ College of Education
in Makkah. The literature classes in the Girls’ College of Education should not
focus on the literary analysis exclusively. The conventional teaching methods
which concentrated on the literariness of the texts; plot, analysis of characters,
background, and themes, are inefficient in developing literary understanding among
EFL learners. As Abdul-Jabbar (2003) suggests the necessity of revising the
methods of teaching English literature to Saudi EFL students in ways that help to
advance their language competence and enhance their literary understanding as
well. She asserted that this can be done so effectively through engaging students in
many activities which involve them in interaction with their teachers and with each
other, and encouraging them to have more interaction with the text. The method-
ological implications of the study called for the use of pre- and post-tests, as
research instruments, to target the students’ progress in learning literature at the
tertiary level. At another level, the pedagogical implications had a call to consider
the different ways to teach literature to learners of English in a similar-related
context. The research implications, however, could invite MA and PhD students in
such a context to further investigate this area.
For the recommendation tips, the massive exposure to literature can enhance the
students’ competence in English (Quda, 1994). However, for a literature course to
be effective, it should be accomplished in such a way that it becomes neither a
228 M. Qutub
language course concentrating on mere language skills nor a pure literature course
with a view to specialization in literature. It should aid to develop both literary
awareness and academic competence. In accordance with educational reform
movement, staff members in English departments can positively impact student
performance by meeting the individual needs of students. The methods of teaching
literature in EFL classes seriously required revision and reconstruction. Yet, it is
one step in a long road toward improvement. Reading about the various methods
that have dealt with strengthening the reader/text relationship through using differ-
ent activities and realizing the efforts that have been exerted in this field have
proved that the excellent teacher is not and should not be “a stereotype”
(Quattrocki, 1976). This might be an incentive to us to suggest the necessity of
holding conferences with the staff members in all English departments in Saudi
Arabia for exchanging different suggestions about English language teaching.
It has been agreed that literature is actually language in use, and the massive
exposure to literature can enhance language proficiency among EFL majors.
Accordingly, one believes that increasing the hours of teaching literature will
help teachers engage students in as many activities as possible. One would suggest
3 h instead of two for each of the literary courses. Workshops should be initiated to
provide EFL students with help outside the classroom with their areas of weak-
nesses. Very rarely does one specific methodology serve all the needs, priorities,
and emphasis desired by instructors and students in this specific field of endeavor;
however, it is recommended that faculty members become acquainted with the
available materials and methodologies that help them to meet the individual needs
of students through their multiple intelligences. Classrooms should be provided
with means of instructional technology such as computer systems and data show,
cinema screens, projectors, tape recorders . . . etc. Teachers should use all the
instructional technologies at hand.
The study yielded different results that showed students’ lack of strategy aware-
ness to deal with the assessment of literary texts. The following suggestions for
further research studies are based on the findings and implications of this study:
– The test used in this study was prepared by the researcher. Will the same results
be obtained if a standardized fiction test was used? A further research may find
an answer to this question.
– The current study was applied for one semester. Another research may apply the
strategy for a longer period, for one academic year or more.
Despite the cumulative success achieved over the last five decades, there have been
numerous challenges confronting English language teaching in public schools and
universities. Some of these challenges are illustrated by Al-Hazmi (2007) as follows:
– Many students are not effective in using English for further academic or real-life
purposes after they leave secondary school even though they studied English for
6 years.
– Most first-year undergraduates at our universities need ‘intensive’ or ‘remedial’
English courses and programs before commencing English Department because
of their poor proficiency levels.
Assessing Literature for the Classroom Among Female Learners of English in. . . 229
– Anecdotal evidence suggests that a great deal of newly-graduate EFL teachers
find difficulty in using English effectively in classroom tasks (e.g., lecturing,
giving instructions, or explanations).
Appendix 1: Fiction Achievement Test
DIRECTIONS
1. This is a test of your fiction achievement.
2. You will have (90) minutes to finish it.
3. The test consists of 23 items designed to measure your competence in dealing
with the literary texts of fiction.
4. Part 1 is about the novella “The Old Man and the Sea” by Ernest Hemingway. It
contains ten multiple choice questions. For each question, choose the best
response on the basis of the content of the novella you have studied.
5. Part 2 is about the elements of fiction, it contains six items. You are required to
provide a concise definition for each element.
6. Part 3 contains four items. To answer this section you should fill in a blank story-
element map.
7. Part 4 contains three open-response items. Read each question carefully. Try
planning before you write. Your response should include an introduction, body
or details, and conclusion.
8. You may write corrections or additions nearly between the lines of your
response, but do NOT write in the margins of the lined pages. Illegible answers
cannot be scored, so you must write clearly.
9. If you finish before time is called, you may review your work. Lay your pen
down immediately when time is called.
DO NOT TURN THIS PAGE UNTIL YOU ARE TOLD TO DO SO.
Fiction Achievement Test
Student’s Name: _________________________________ Reg. No. _____________
Part 1 (10 min)
From the novella “The Old Man and the Sea” by Ernest Hemingway, respond to the
following questions choosing the right answer from (A, B, C or D):
1. In what month does the story take place?
(A) September
(B) October
(C) July
(D) May
230 M. Qutub
2. How does Hemingway describe Santiago’s eyes?
(A) They are full of pain
(B) They are blank with defeat
(C) They betray the weariness of his soul
(D) They are the color of the sea
3. Santiago thought the light of what city would guide him home?
(A) Cardenas
(B) San Juan
(C) Havana
(D) Bautista
4. How does Santiago kill the marlin?
(A) With a harpoon
(B) With a club
(C) With a knife
(D) With a gun
5. What type of shark was the first to attack the marlin body?
(A) Tiger
(B) Shovel-nosed
(C) Thrasher
(D) Mako
6. During his great struggle with the marline, what does Santiago wish
repeatedly?
(A) He wishes he were younger
(B) He wishes for better equipment
(C) He wishes that the fishermen who mocked him were present to witness his
victory.
(D) He wishes that the boy, Manolin, were with him.
7. What percentage of the marlin’s flesh was left when Santiago reach the shore?
(A) 75%
(B) 50%
(C) 25%
(D) 0%
8. What was Manolin’s first reaction when he saw the returned Santiago?
(A) To cry
(B) To laugh
(C) To ask about his voyage
(D) To go home and wait for him to awake
Assessing Literature for the Classroom Among Female Learners of English in. . . 231
9. What does Manolin decide to do after speaking to Santiago at the end of the
story?
(A) Leave the shore for the city
(B) Catch his own marlin
(C) Take over fishing to Santiago
(D) Join Santiago to fish with him
10. What is the old man dreaming of at the end of the story?
(A) The boy
(B) Lions
(C) The marlin
(D) A rainfall
Part 2 (10 min)
Give a concise definition for each of the following terms:
11. Plot: _______________________________________________________
12. Irony: ______________________________________________________
13. Character: ___________________________________________________
14. Theme: _____________________________________________________
15. Setting: _____________________________________________________
16. Point of view: ________________________________________________
Part 3 (10 min)
Fill in the spaces in the following story-elements map to display the elements of
“The Happy Prince” by Oscar Wilde.
Part 4 (60 min)
Give a detailed answer to the following questions. Note, your response should
include an introduction, body or details, and conclusion.
232 M. Qutub
21. What are the moral lessons that you can get from “The Necklace” by Guy de
Maupassant? Support your answer with illustrations from the text.
______________________________________________________________
22. Read the following extract carefully, then answer with reference to the context:
“I was startled when the bill of fare was brought, for the prices were a great
deal higher than I had anticipated. But she reassured me. ‘I never eat anything
for luncheon’, she said.”
______________________________________________________________
23. Katherine Mansfield paints a picture of an elderly woman “Miss Brill”, who
believes life is a walk in the park, until a couple leads her to believe life is not
all fantasy. Write a detailed description for Miss Brill’s personality.
______________________________________________________________
Appendix 2: Four-Point Scoring Rubric for Fiction
Achievement Test (Open-Response Questions; Part 4)
Student Name: ______________________________________ Date: __________
Literary
understanding
features 1 2 3 4 Score
Appropriate
interpretations
for the literary
text
– Demon-
strates little
grasp of the
text
– Lacks an
interpretation
or may be a
simple retell-
ing of the pas-
sage
– Lacks tex-
tual examples
and details
– Develops
interpretations
that demon-
strate a limited
grasp of the
text
– Includes
interpretations
that lack accu-
racy or coher-
ence as related
to ideas, pre-
mises, or
images from
the literary text
– Provides
few, if any,
textual exam-
ples and
details to sup-
port the
interpretations
– Develops
interpretations
that demon-
strate a com-
prehensive
grasp of the text
– Organizes
accurate and
reasonably
coherent inter-
pretations
around clear
ideas, premises,
or images from
the literary text
– Provides tex-
tual examples
and details to
support the
interpretations
– Develops
interpretations
that demon-
strate a
thoughtful,
comprehensive
grasp of the text
– Organizes
accurate and
coherent inter-
pretations
around clear
ideas, premises,
or images from
the literary text
– Provide spe-
cific textual
examples and
details to sup-
port the
interpretations
(continued)
Assessing Literature for the Classroom Among Female Learners of English in. . . 233
Literary
understanding
features 1 2 3 4 Score
Pertinent
development
for fictional
narrative
Lacks a
developed
plot line
Provides a
minimally
developed plot
line, including
characters and
setting
Provides an
adequately
developed plot
line, including
major and
minor charac-
ters and a defi-
nite setting
Provides a
thoroughly
developed plot
line, including
major and
minor charac-
ters and definite
setting
Efficiency of
organization
Inadequate or
no organiza-
tion that dem-
onstrates no
evidence of
structure with
no introduc-
tion or con-
clusion. Lack
of traditional
devices
Limited orga-
nization that
demonstrates
evidence of
structure with
an uncertain
introduction
and conclu-
sion. Only
some transi-
tional devices
Adequate orga-
nization that
demonstrates a
generally uni-
fied structure
with a notice-
able introduc-
tion and
conclusion.
Consistent use
of transitional
devices
Effective orga-
nization that
demonstrates a
cohesive and
unified struc-
ture with an
engaging intro-
duction and a
strong conclu-
sion. Effective
use of transi-
tional devices
Effectual Lan-
guage use,
voice, and
style
Inadequate
language use,
voice, and
style. The
response dem-
onstrates
unclear and
incoherent
language and
word choice,
no awareness
of audience,
and major
errors in sen-
tence structure
and usage
Limited lan-
guage use,
voice, and
style. The
response dem-
onstrates sim-
ple language
and word
choice, some
awareness of
audience and
control of
voice. Relies
on simple
sentences with
insufficient
sentence vari-
ety and word
choice
Adequate lan-
guage use,
voice, and
style. The
response dem-
onstrates
appropriate
language and
word choice,
with an aware-
ness of audi-
ence and
control of
voice, gener-
ally uses cor-
rect sentence
structure with
some variety
Effective lan-
guage use,
voice, and
style. The
response dem-
onstrates pre-
cise language
and word
choice, a defi-
nite voice, and
a clear sense of
audience, uses
well structured
and varied
sentences
(continued)
234 M. Qutub
Literary
understanding
features 1 2 3 4 Score
Control of
mechanics and
conventions
Inadequate or
no control of
mechanics
and conven-
tions. Errors
so severe in
grammar,
mechanics,
punctuation,
and spelling
that they sig-
nificantly
interfere with
the meaning
of the
massage
Limited con-
trol of
mechanics and
conventions.
Several notice-
able errors in
grammar,
mechanics,
punctuation,
and spelling
that may inter-
fere with the
meaning of the
massage
Adequate con-
trol of mechan-
ics and
conventions.
Some errors in
grammar,
mechanics,
punctuation,
and spelling
that do not sig-
nificantly
interfere
Effective con-
trol of mechan-
ics and
conventions.
Few or no
errors in
grammar,
mechanics,
punctuation,
and spelling
Total score 20
References
Abdul-Jabbar, H. Y. (1990). Drama in the English curriculum: A reconsideration of the principles
covering the course in Arab universities with application to sample dramatic texts usually
chosen for the course. Unpublished doctoral dissertation, Girls’ College of Education, Makkah.
Abdul-Jabbar, H. Y. (2003). Promoting students’ English language through the teaching of
literature: A mini-course guide for the teaching of Hamlet. Mecca: Umm Al-Qura University.
Abdullah, M. S. (2005). The effect of using a multiple intelligences-based training program on
developing English majors’ oral communication skills. MA Thesis, Assiut University, Assiut.
Adam, H., & Harper, L. (2016). Assessing and selecting culturally diverse literature for the
classroom. Practical Literacy: The Early & Primary Years, 21(2), 10–14.
Akyel, A., & Yalçin, E. (1990). Literature in the EFL class: A study of goal-achievement. ELT
Journal, 44(3), 174–180.
Al Saedi, L. (1998). Promoting students’ mastery of English through drama teaching: An eclectic
approach. Master Thesis, Girls’ College of Education, Makkah.
Alderson, J. C. (2000). Assessing reading. New York: Cambridge University Press.
Al-Faki, I. M. (2014). Using literature in EFL classes: Assessing the suitability of literary texts to
secondary school students. European Journal of English Language and Literature Studies, 2
(4), 9–21.
Al-Hazmi, S. (2007). Current issues in English education in the Kingdom of Saudi Arabia. The
Journal of Modern Languages, 17, 129–150.
Anderson, V. B. (1998). Using multiple intelligences to improve retention in foreign language
vocabulary study. Master of Arts Action Research Project, St. Xavier University, Chicago,
Illinois.
Arikan, A. (2008). Using internet groups in the learning of literature. Hacettepe University Journal
of Education, 34(1), 19–26.
Baird, S. R. (1994). Assessing student achievement in the study of literature. The English Journal,
83(8), 55–58.
Assessing Literature for the Classroom Among Female Learners of English in. . . 235
Bassnett, S., & Grundy, P. (1993). Language through literature: Creative language teaching
through literature. Singapore: Longman.
Bernhardt, E. B. (1995). Teaching literature or teaching students. ADFL Bulletin, 26(2), 5–6.
Bredella, L. & Werner D. (Eds.). (1996). Challenges of literary texts in the foreign language
classroom. Tubingen: Gunter Narr.
Brown-Chidsey, R., Johnson, P., & Fernstrom, R. (2005). Comparison of grade-level controlled
and literature-based maze CBM reading passages. School Psychology Review, 34(3), 387–394.
Brumfit, C. J. (1990). Language and literature teaching: From practice to principles (2nd ed.).
Oxford: Pergamum Press.
Carter, R., & Long, M. N. (1991). Teaching literature. Harlow: Longman.
Carter, R., & McRae, J. (Eds.). (1997). Language, literature and the learner. Harlow: Addison
Wesley Longman.
Chang, H. S. (2003). Difficulties in studying and teaching literature survey courses in English
departments in Taiwan. Unpublished doctoral dissertation, University of Texas, Austin.
Collie, J., & Slater, S. (1987). Literature in the language classroom. Cambridge: Cambridge
University Press.
Faber, P. (1998). Through the camera’s lens: An innovative approach to analyzing literature. In
W. Gewehr, G. Catsimali, P. Faber, M. J. Raya, & A. J. Peck (Eds.), Aspects of modern
language teaching in Europe (pp. 83–92). London: Routledge.
Ford, N., & Chen, S. Y. (2001). Matching/mismatching revisited: An empirical study of learning
and teaching styles. British Journal of Educational Technology, 32(1), 74–88.
Gohlinghorst, N., & Becky, W. (2001). Enhancing student achievement in social studies through
the use of multiple intelligences. Social Studies Journal, 34(1), 13–39.
Halasz, L. (1991). Emotional effect and reminding in literary processing. Poetics, 20, 247–272.
Haley, H. M. (2004). Learner-centered instruction and the theory of multiple intelligences with
second language learners. Teachers College Record, 2(1), 163–180.
Hall, G. (2005). Literature in language education. New York: Palgrave Macmillan.
Hirvela, A. (1996). Reader-response theory and ELT. ELT Journal, 50(2), 127–134.
Kauffman, R. A. (1996). Writing to read and reading to write: Teaching literature in the foreign
language classroom. Foreign Language Annals, 29(3), 396–402.
Kim, J. (2002). Teaching culture in the English as foreign language classroom. The Korea TESOL
Journal, 5(1), 27–40.
Kodama, K. (2012). The role of literature in the EFL/ESL classroom revisited: Using literature in
communicative language teaching. Bulletin of the Graduate Schools of International Cultural
Studies, Aichi Prefectural University, 13, 31–56.
Kramsch, C. (1985). Literary texts in the classroom: A discourse model. Modern Language
Journal, 56(2), 356–366.
Lee, C. D., & Goldman, S. R. (2015). Assessing literary reasoning: Text and task complexities.
Theory Into Practice, 54, 213–227.
Maley, A., & Alan, D. (1989). The inward ear. Cambridge: Cambridge University Press.
Nguyen, T. T. (2002). Differential effects of multiple intelligences curriculum on student perfor-
mance. Unpublished doctoral dissertation, Harvard University.
Ogeyik, M. C. (2007). Attitudes of the students in English language teaching programs towards
literature teaching. Eurasian Journal of Educational Research, 27(2), 151–162.
Pickering, J. H. (1998). Fiction 100: An anthology of short stories. Hoboken, NJ: Prentice Hall.
Pope, R. (1995). Textual intervention: Critical and creative strategies for literary studies. London:
Routledge.
Quattrocki, E. (1976). Classroom presentations of Shakespeare. Teaching English in Southern
Ohio. Teaching Shakespeare, 11(3), 26–33.
Quda, M. (1994). Literature is a great vehicle for understanding language and culture (pp. 61–73).
13 National Symposium Proceedings, Cairo, CDET.
Rasmussen, K. L. (1998). Hypermedia and learning styles: Can performance be influenced?
Journal of Multimedia and Hypermedia, 7(4), 425–431.
236 M. Qutub
Riding, R., & Michael, G. (1999). Cognitive style and learning from multimedia materials in
11-year children. British Journal of Educational Technology, 30(1), 65–79.
Savvidou, C. (2004). An integrated approach to teaching literature in EFL classroom. The Internet
TESL Journal, X(12).
Stern, S. L. (1985). Teaching literature in ESL/EFL: An integrative approach. Unpublished
doctoral dissertation, University of California, Los Angeles.
Widdowson, H. G. (1975). Stylistic and the teaching of literature. London: Longman.
Widdowson, H. G. (1990). Teaching language as communication. Oxford: Oxford University
Press.
Zyngier, S. (1994). Literature in the EFL classroom: Making a comeback, PALA. The Poetics and
Linguistics Association. Universidade Federal do Rio de Janeiro, Colgio Pedro, Occasional
Papers No. 6.
Zywno, M. S., & Waalen, J. K. (2002). The effect of individual learning styles on student
outcomes in technology-enabled education. Global Journal of Engineering Education, 6(1),
42–50.
Assessing Literature for the Classroom Among Female Learners of English in. . . 237
Part V
Assessing Reading, Vocabulary
and Grammar
Assessing Two Strategies for Learning
Vocabulary
Manal Sabbah
Abstract The focus of this chapter is on assessing the effects of two vocabulary
learning strategies, guessing the meaning of words from a reading context and using
a monolingual dictionary, on second language learners’ vocabulary knowledge. To
this end, a study was carried out with a sample of 60 female students attending a
4-week intensive English as a second language (ESL) course, at a university, in
Saudi Arabia. They were divided into two groups, group (1) and group (2). Group
(1) was instructed to look up the meanings of words in a dictionary. Group (2) was
instructed to guess the meanings of the words from the context in which they
occurred. Each group was then divided into three levels, according to proficiency,
high, medium and low. One measure was used to determine the effect of the
instruction on the two groups, a pretest and a posttest. A significance level of
( p < 0.05) was accepted. Analyses of the measure indicated that the students, in
group (2), did significantly better than the students in group (1). Results were
analysed through comparing the means of the two groups using a paired T-test
then using a repeated measure ANOVA. The results revealed that the students in the
guessing from context group outperformed the students in the dictionary group.
Proficiency level of the students was also found to have a significant effect on the
results.
Keywords Vocabulary assessment • IELTS • Vocabulary teaching strategies •
ESL
1 Introduction
The study presented in this chapter aims to assess the efficiency of two vocabulary
learning strategies, using a monolingual dictionary and guessing words from
reading contexts. According to the early teaching methods of English Language
M. Sabbah (*)
University of Exeter, Exeter, UK
e-mail: Manaljy@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_12
241
teaching and research, certain components of language, such as grammar and
translation, were of paramount importance. Other components, such as vocabulary,
were “taken for granted” (Read, 1999, p. 99). It was not before the mid-1980s, with
the emergence of the communicative approach to the English Language teaching
and learning, that vocabulary was acquiring importance. This importance was due
to the “primary role” vocabulary played in “communication and acquisition”
(Richards & Renandya, 2012, p. 255). Recently, curriculum designers, theorists
and teachers have been increasingly interested in second language vocabulary
acquisition (Meara & Miralpeix, 2016). This interest has resulted in the emergence
of different definitions, approaches and strategies to vocabulary teaching. Among
the vocabulary approaches are the indirect1
(learning vocabulary as a by-product
though doing other things such as reading or listening) and the direct2
approaches
(teaching vocabulary independently or from a dictionary) (Richards & Renandya,
2012). Indirect learning can occur as a result of different learning strategies. One of
the most widely used is guessing the meaning of words from the context in which
they appear. This strategy is “a complex and often difficult strategy to carry out
successfully” (Hunt & Beglar, 2003, p. 262). However, many researchers (Liu &
Nation, 1985; Nation, 1980; Nation & Coady, 1988; Oxford & Crookall, 1988)
agree that learning words from context is an effective vocabulary learning strategy.
Direct learning can occur as a result of different strategies, as well. For their close
relevance to this study, one direct strategy, using a monolingual dictionary, and one
indirect strategy, guessing words from a reading context, will be discussed.
2 Theoretical Background
The growing interest in vocabulary has resulted in the emergence of different
definitions to explain what a word is. However, many of these definitions are not
sufficient because “words” are usually defined theoretically or in relation to their
applied use. Though almost everyone knows what a word is, it is hard to provide a
comprehensive definition for it (Read, 2005). According to Schmitt (2012, p. 2), a
word is “An item that functions as a single meaning unit, regardless of the number
of words it contains”. However, Read gives words a more comprehensive definition
as “The basic building blocks of language, the units of meaning from which larger
structures such as sentences, paragraphs and whole texts are formed” (2004, p. 1).
He acknowledges that words definitions might differ according to their various
applied purposes. For the purpose of this study,3
words will be defined as lemmas,
the base and inflicted forms of a word. Schmitt (2011) affirms “That knowing a
1
Also called implicit and incidental learning (these terms will be used interchangeably throughout
this chapter).
2
Also called explicit learning (these terms will be used interchangeably throughout this chapter).
3
This is because the study is mainly about vocabulary size.
242 M. Sabbah
word requires more than just familiarity with its meaning and form” (p. 4). How-
ever, as Alderson (2002, p. 99) puts it, “more subtle and informed definitions of
‘vocabulary’ are needed—a much better idea of what it means to know and use a
word”. Qian’s (2002) framework of vocabulary knowledge proposes that vocabu-
lary knowledge comprises four intrinsically connected dimensions. The first is
vocabulary size that refers to the number of words of which a learner has at least
some superficial knowledge of meaning. The second is the depth of vocabulary
knowledge that includes all lexical characteristics, such as phonemic, graphemic,
morphemic, syntactic, semantic, collocational, and phraseological properties, as
well as frequency and register. The third is lexical organization, which refers to the
storage, connection, and representation of words in the mental lexicon of a learner.
The last is automaticity of receptive–productive knowledge, which refers to all the
fundamental processes through which access to word knowledge is achieved for
both receptive and productive purposes, including phonological and orthographic
encoding and decoding, access to structural and semantic features from the mental
lexicon, lexical-semantic integration and representation, and morphological parsing
and composing. Qian (2002) assumes that these dimensions are intrinsically
connected and they interact in every basic process of vocabulary development
and use. He states (2002) that the importance of the different factors in these
dimensions might vary in different processes. Though all four dimensions are
important when vocabulary is concerned, the present study will be confined to
studying vocabulary size and the automaticity of receptive–productive knowledge.
How many words are there in English? How many words does a native speaker
know? And how many words does a speaker of a second language need to know?
Many researchers have attempted to answer these questions. Two independent
studies (Dupuy, 1974; Goulden, Nation, & Read, 1990) have suggested that the
best way to determine the vocabulary size of English is to look at the largest English
dictionary. They found that there are 54,000 word families in English. A word
family, “include[s] a base word, its inflicted forms and a small number of reason-
ably regular derived forms (Bauer & Nation in Schmitt & McCarthy, 2011, p. 8). It
is argued that a university graduate native speaker has a vocabulary size of around
20,000 word families. This means that every year the native speaker is expected to
learn about 1000 words (Goulden et al., 1990). Other researchers, however, suggest
larger vocabulary sizes, according to how a word family is defined (e.g., D’Anna,
Zechmeister, & Hall, 1991).
The number of words a second language speaker needs to have is dependent on
what the learner wishes to communicate. It might be hard for a second language
learner adult to reach 20,000 word families, but s/he can make use of the fact that
the English Language consists of high and low frequency words. With a vocabulary
size of 2000 high frequency words, a second language learner can comprehend up
to 80% of written texts (Waring & Nation, 2004). As far as frequency is concerned,
there are four types of vocabulary that can be found in a text, high frequency,
academic, technical and low frequency words (Nation, 2011). High-Frequency
words are “the most frequent and widely used words of a language” (Nation &
Newton, 1997, p. 238). Frequency counts have resulted in 2000 headwords as the
Assessing Two Strategies for Learning Vocabulary 243
most frequently used words in English. These words are presented in West’s (1953)
General Service List of English Words, a list which contains the 2000 headwords,
with their frequency for example, “a”, “some” and “to”. Without learning these
words, it is “not possible to use English in any normal way” (Nation & Newton,
1997, p. 239). Nation (2011, p. 11) suggests that these words cover up to “80% of
the running words” in a written or a spoken text. Alderson (2002) proposes that high
frequency words cover 80–90% of the running words in a text. Academic words
appear in academic textbooks and generally make up 9% of the running words in a
text (Nation, 2011). Coxhead’s (1998) Academic Word List is one of the well-
known academic word lists. These words are essential for learners of English for
academic purposes. Words like “assess”, “assume” and “concept” are examples.
Technical words are words related to the topic or subject area of the text itself. They
make up 5% of the running words in the text (Coxhead, 1998). They can be found in
technical dictionaries of different subject areas, for example in law “judge”, in
computing “icon” and in electronics “drain”. There are approximately 2000 tech-
nical words (Nation & Newton, 1997). Low-Frequency words form the majority of
words in English. They are words that are “(. . .) of much less value to learners”
(Read, 2005, p. 158). These include ‘rare’, ‘obsolete’ or ‘dialectal’ words in a
dictionary. These words make up more than 5% of words in academic texts (Nation,
2011). Some examples are “curious”, “wing” and “approximately”.
2.1 Receptive–Productive Knowledge
Many researchers agree that there are different degrees of knowing a word, recep-
tive and productive (Nation, 1990, 2011; Hunt & Beglar, 2003; Read, 2005). They
generally connect receptive knowledge with listening or reading while they connect
productive knowledge with speaking or writing. Richards (1976) has made some
assumptions about lexical competence, which Nation (1990) later improved on by
adding other components to specify the learner’s task scope. Though Nation’s
model (1990) appears to cover important components of word knowledge, some
of its components are hard to apply in practical tests (Meara, 1996; Read, 2005;
Schmitt, 1998; Schmitt & Meara, 1997). According to some researchers (e.g.,
Aitchison, 2012; Clark, 1993; Ingram, 1974), receptive vocabulary is considerably
larger than productive vocabulary. Some estimate that receptive vocabulary is
double the productive one in a second language (Aitchison, 2012, pp. 5–7; Clark,
1993; Marton, 1977). Some researchers (e.g., Corson, 1995; Laufer, 1998; Meara,
1990) prefer to use the terms passive (for listening and reading) and active (for
writing and speaking). However, in this chapter, the terms productive (for writing
and speaking) and receptive (for listening and reading) will be used. This is because
experience has revealed that learning and knowledge can be “receptive”, but cannot
be “passive”. Learners are “active” participants in the learning process even if they
are unable to be “productive” at the early stages of the learning process.
244 M. Sabbah
2.2 Vocabulary Learning Approaches
There are two vocabulary learning approaches, the explicit approach and the
incidental approach. There are many strategies listed under each approach. How-
ever, two relevant strategies to my study will be discussed here, dictionary use and
guessing the meanings of words from context. Explicit learning occurs when the
language study is focused on words (Schmitt, 2012). It is more conscious than
implicit learning because, in a search for a structure, the learner makes and tests
hypotheses (Ellis, 1994). Nation (2011) sees that explicit learning and teaching
should be used with beginner and intermediate students to teach the first 2000–3000
words. Students need to be taught the meanings of words, use dictionaries and
complete assignments. He (2011) sees that the word meanings need to be addressed
explicitly while the word forms need to be addressed implicitly. Though this type of
learning is recommended for beginner and intermediate students, there is no
empirical evidence against using it with advanced students as well. Incidental
(implicit) learning occurs when the learner’s attention is directed towards the
language use rather than the learning of the words themselves (Schmitt, 2012). In
order to achieve the desired learning, learners need to get maximum exposure to the
second language. Theories have affirmed that most words are indirectly learned,
through reading and listening extensively (Nagy, Herman, & Anderson, 1985).
Research has stressed this fact, as well (See Chun & Plass, 1996; Day, Omura, &
Hiramatsu, 1991; Knight, 1994). Ellis (1994) sees that implicit learning involves
focus on the stimulus but does not involve any other operations that are conscious.
Though some researchers see that this approach of learning is suitable for interme-
diate and advanced students4
(Schmitt, 2012), many studies examining incidental
learning revealed that advanced students were not any better than low-ability
students at incidental word learning (Schefelbine, 1990; Stanely & Ginther,
1991). In fact, most of the words are incidentally learned in both first and second
languages (Carter, 2012; Read, 2005). Hence, it can be concluded that incidental
learning is suitable for all levels of students.
2.3 Vocabulary Learning Strategies
2.3.1 Dictionary Use
Dictionary use is one of the explicit learning strategies used by vocabulary learners.
Hence, it is usually recommended for learners of beginner and intermediate levels
in particular, who have a vocabulary size below 3000 words, as discussed earlier.
This is not to assume that it cannot be used by advanced students. This strategy can
be used by learners of different levels, however, “expectations of what will be
4
Who have a vocabulary size of over 3000 words.
Assessing Two Strategies for Learning Vocabulary 245
learned about words from dictionary use should not be too high” (Nation, 2011,
p. 296). Scholfield (2011) has differentiated between two types of dictionaries,
dictionaries used for comprehension (reading and listening) and those used for
production (speaking and writing). Dictionaries can also be used for learning
unknown words or partly unknown words (Nation, 2011). The fact that dictionaries
can be used for learning purposes provides further evidence that the dictionary use
strategy is suitable for all proficiency levels. It is so because “vocabulary continues
to be learned throughout one’s lifetime. (. . .) a person is unlikely to ever run out of
words to learn.” (Schmitt, 2012, p. 4).
Dictionaries can be monolingual and bilingual. This study is concerned with the
use of monolingual-learning dictionaries. Hence, there will be more literature on
the history and development of monolingual dictionaries, especially the Oxford
Advanced Learner’s Dictionary, which is used in the current study. Carter (2012)
sees that there are two monolingual dictionaries that have affected the development
and design of non-native English learners’ dictionaries. They also have helped
distinguish between bilingual and monolingual dictionaries. These are the Oxford
Advanced Learner’s Dictionary (OALD, 1974) and the Longman Dictionary of
Contemporary English (LDOCE, 1978). Monolingual dictionaries, designed for
non-native users, aim is to “supply encoding information which will allow for
productive use of language” (Carter, 2012, p. 152). The OALD employs a coding
system to describe syntactic patterns. This system is derived from Hornby’s
research in the 1940s. The OALD (4th ed., 1992) has benefited from the OUP
Lexical Research Unit at the University of Leeds. The OALD (1995) has derived
from the British National Corpus (BNC). The OALD (1995) contains a number of
innovations derived from the 100-million-word Cambridge Language Survey (CIC)
(Carter, 2012, p. 152).
2.3.2 Guessing the Meanings of Words from Context
Guessing from context is an incidental learning strategy. Sternberg (1987) affirms
that learners learn most vocabulary from context through the strategies of inference.
Hulstijn (1992), as well, sees that most vocabulary is learned from context. A
number of questions need to be examined under this title. How does learning
vocabulary from a reading context take place? How many words can be learned
from context? How many words can be guessed from context? Can learners be
trained to guess from context? How can learners be trained to guess from context?
What are the problems of guessing from context?
Several studies that examined second language learners’ approaches in guessing
from context reveal that different learners approach guessing from context differ-
ently (e.g., Bensoussan & Laufer, 1984; Haynes, 1993; Laufer & Sim, 1985; Parry,
1991). Guessing is based on many factors including the different ways (van Daalen-
Kapteijns & Elshout-Mohr, 1981), the different abilities, the knowledge and the
skills that the guessers use (Nation, 2011). Many things can happen to an unknown
word when met in context. Nation (2011, p. 237) summarizes these in four points:
246 M. Sabbah
1. The word is guessed correctly to some degree and at least partially learned. This
may happen to 5–10% of the words.
2. It is guessed correctly to some degree but nothing about it is learned.
3. It is guessed incorrectly.
4. It is ignored, possibly because it is not important for the wanted message in
the text.
Studies with native learners (Nagy et al., 1985; Shu, Anderson, & Zhang, 1995)
have shown that there is a 1 in 10 to 1 in 20 chance of a word to be learned. Learners
learn about 15% of the unknown words met in context (Swanborn & de Glopper,
1999). Studies with second language learners (Day et al., 1991; Dupuy & Krashen,
1993; Pitts, White, & Krashen, 1989) have affirmed that learning from context did
occur. Horst, Cobb, and Meara (1998) have found that about one in every five
unknown words were learned to some degree. In addition, it has been argued that
the learners remember new words more efficiently if the words recur in a text. The
more often an unknown word occurs, the greater the chance of guessing and
learning it.
Different studies presented different results regarding how many words can be
guessed from context. Some studies affirmed that the learner should know at least
95% of the running words in a text to be able to guess the unknown words
successfully (Nation, 2011). Other studies, on second language learners guessing
unknown vocabulary from context, were conducted (e.g., Bensoussan & Laufer,
1984; Horst et al., 1998; Knight, 1994; Parry, 1991). The percentages of how much
successful guessing took place varied,5
but all studies affirmed that learning from
guessing from context did occur. Studies by Gibbons (1940) and Cook, Heim, and
Watts (1963) revealed that there was a wide variation in learners’ abilities to guess
from context. Some learners could guess a large number of unknown words
successfully. In order to train learners to guess successfully, performances of both
the best guessers and averages need to be reported and studied (Nation, 2011).
There are a number of studies that have examined the different approaches of
second language learners to guessing from context (e.g., Haynes, 1993; Laufer &
Sim, 1985; Parry, 1991). In fact, many studies have been carried out to examine
what effect guessing from context has on learning (Carnine, Kameenui, & Coyle,
1984; Jenkins, Matlock, & Slocum, 1989; Morrison, 1996; Pany & Jenkins, 1978;
Pany, Jenkins, & Schreck, 1982). The results have shown that learners became
better guessers after being trained. Hence, this implies that it is possible to train
learners to guess from context. Stahl (1999) suggests the use of productive vocab-
ulary instruction. He suggests that students should be taught word parts, prefixes,
suffixes and root words. He suggests that trainers and teachers should be selective
as to what word parts their students are likely to encounter in texts, such as those in
the list of The Most Frequent Affixes in Printed School English (White, Sowell, &
Yanagihara, 1989). Nation (2011) sees that context and attention-drawing activities
5
The rate ranges from 6 to 70% successful guessing.
Assessing Two Strategies for Learning Vocabulary 247
should be used.6
Schmitt (2012) prefers reading. In fact, he asserts that “[t]here is
plenty of evidence that learners can acquire vocabulary from reading only” (p. 150).
Some researchers see that guessing from context is not always an easy strategy
for certain learners to carry out. According to Laufer (1997), reading comprehen-
sion can face a number of lexical problems. These problems are summarized under
three categories, “Words You Don’t Know”, “Words You Think You Know” and
“Words You Can’t Guess”. Laufer (1997, p. 25) discusses each of these in detail.
She relates the first problem to insufficient vocabulary knowledge, the second to
words that are “deceptively transparent” and the third to words that are unknown to
the reader but s/he cannot guess. Despite the difficulty of guessing from context,
many researchers describe it as one of the most useful resources of vocabulary
learning (Liu & Nation, 1985; Nation, 2011; Nation & Coady, 1988). Hence, it is a
strategy that is worth studying.
2.4 Vocabulary and Reading Comprehension in a Second
Language
Since this study examines vocabulary acquisition through reading texts, it is
important to shed light on the relationship between the two. It is also important to
note that all correlational studies, readability research and experimental studies
have proved that there is a strong and reliable relationship between words difficulty
in a text and the text comprehension (Anderson & Freebody, 1981; Graves, 1986).
There are three hypotheses that are closely related to the relationship between
vocabulary and reading comprehension, the Instrumentalist Hypothesis, the Knowl-
edge Hypothesis and the General Ability Hypothesis (Stahl, 1999). The Instrumen-
talist Hypothesis implies that a “person who knows the words can comprehend the
text, regardless of any other factors” (p. 4). Of course, this hypothesis is a little
general since, as discussed earlier in the academic and technical word types, the
meanings of the same words can differ according to the topic or subject area. As a
result, text comprehension becomes impossible. Though Stahl (1999, p. 7) sees this
hypothesis as “the most important” of all hypotheses, because it implies that
teaching word meanings will improve learners’ comprehension, it appears that
the Knowledge Hypothesis is more comprehensive. It is so because it states that
knowing word meanings is not sufficient for comprehension. In fact, it is important
as well to know the topic of the text itself since word meanings can differ according
to the topic (Anderson & Freebody, 1981). The General Ability Hypothesis sug-
gests that knowing the meanings of words is not enough. Vocabulary knowledge is
associated with the person’s intelligence. In this regard, vocabulary and compre-
hension are both related to the general ability of comprehension. Sternberg (1987)
6
Such as drawing attention to the word, providing access to the meaning and motivating attention
to the word.
248 M. Sabbah
includes, in his model of intelligence of learning from context, the general ability
factor that some people have that allows them to comprehend vocabulary and
information. This hypothesis associates comprehension with intelligence; therefore,
minimizing the effect of training that improves comprehension. It contradicts with
the research discussed earlier that affirms that learners can be trained to improve
their comprehension.
2.5 Assessing Vocabulary
The assessor can design his/her test according to the aspect of vocabulary to be
assessed. There is a number of ready-made tests available that test different aspects
about vocabulary, for example, Nation’s Level Tests, Vocabulary Depth Test
(Read, 1995), a definition completion test (Read, 1995), or a translation test
(Nurweni & Read, 1999). For the purpose of this study, Nation’s 3000 Productive
Word Level will be employed as it is suitable for the level of the students in this
study. Moreover, the reliability and the validity of this test have been established.
3 Research Problem and Rationale
3.1 Research Problem
The purpose of this chapter is to present an experiment to examine which of the two
vocabulary learning strategies is more effective, guessing the meanings of words,
from a reading context, or learning vocabulary directly from a monolingual dictio-
nary. Though it is possible in theory that both strategies can be implemented in the
classroom together, usually in short intensive courses, this option is difficult to
implement because of time restrictions. This is why it is important for the teacher to
opt for the most effective strategy to use with a particular group of students. This
means that every teaching situation is unique and though there are existing evi-
dences for and against the teaching of both strategies, the debate of which strategy
is more effective continues to the present day. Only experimenting with learning
strategies can offer acceptable solutions to this dilemma. As Hunt and Beglar put it,
“Given the continuing debate about the effectiveness of guessing from context,
teachers and learners need to experiment with this strategy and compare it to
dictionary training” (2012, p. 262). In addition, it is important to note that with
the dictionary use group, a monolingual dictionary was used. Although ample
research was done with second language speakers using bilingual dictionaries,
very little research was done with second language speakers using a monolingual
dictionary (Carter, 2012). This makes the results of the study more interesting, since
the participants, like most second language learners, had previously depended on
the use of bilingual dictionaries. Such a dependency tends to hinder the growth of
second language proficiency (Baxter, 1980).
Assessing Two Strategies for Learning Vocabulary 249
3.2 Rationale
This chapter aims to answer the following questions: What effect does guessing the
meanings of words, from a reading context, have on a second language learner’s
vocabulary as opposed to using a monolingual dictionary? How effective is each
method of learning, with the different proficiency levels, high, medium and low?
Which of the two strategies need to be used for the participants in this study? In
order to answer the above questions, two sets of hypotheses were established for
two factors, methods of instruction and proficiency level:
Instruction: H0: The method of instruction has no effect on vocabulary acquisition.
H1: The method of instruction has an effect on vocabulary acquisition.
Proficiency: H0: Proficiency has no effect on the instruction method for vocabulary
acquisition.
H1: Proficiency has an effect on the instruction method for vocabulary
acquisition.
A significance level of p < 0.05 is set for all measures. A pretest/posttest is used
to determine the effects of instruction on the different proficiency levels and to
measure the students’ vocabulary growth.
4 Method
The study was conducted via comparing the means of the two groups on one
measure of proficiency, the pretest/posttest. The participants were attending an
intensive English as a Second Language Course at a university in Saudi Arabia.
The intensive course lasted for a month. The participants were 19–22 years old. One
hundred prospective university students were asked to take the IELTS Placement
Test in the university computer lab. Students scored one point for each correct
answer. The test covered listening, reading, vocabulary and writing skills and the
total grade was 60. All students were allowed 1 h to complete the test. Sixty
students were asked to participate in the experiment. The students who scored
20–60 points were chosen as samples for the dictionary-learning and the learning
from context groups. The students were divided into three levels, according to their
general English language proficiency. In this study, students who scored 20–30 out
of 60 were considered beginners. Students who scored 40–50 were considered
intermediate. Students who scored 51–60 were considered advanced. Each of the
groups, the dictionary-learning and the learning from context groups, had 30 stu-
dents, 10 beginners, 10 intermediate and 10 advanced. Other than making sure
students from all of the three levels were equally present in each group, students
were randomly divided into the two groups.
The eight units of the Increase Your Vocabulary book were covered. Two units
were covered every week. The course took place 6 days a week, for 1 h daily. The
250 M. Sabbah
dictionary-learning group covered all units of the book. The learning from context
group covered all units except Unit 2, which discussed the use a monolingual
dictionary, the Oxford Advanced Learner’s Dictionary (OALD). The dictionary-
learning group was asked to look up the meanings of words in the OALD. The
learning from context group was asked to guess the meanings of the same words
from the context in which they occurred. The learning from context group was also
instructed to read texts from The Penguin Graded Readers. The students were asked
to read six novels, one per week. The purpose was to boost the participants’
incidental learning from context. Both groups were tested two times throughout
the course as follows: They were given the 3000 Productive Word Level Test (See
Appendix 2) as a pretest at the beginning of January, right after they were divided
into three proficiency levels after taking the Online IELTS Placement Test and
before any instruction took place. At the end of the course, students took the 3000
Productive Word Level Test again as a posttest to measure their vocabulary growth.
Comparing the pretest with the posttest determined the participants’ vocabulary
growth.
Although the students were informed that they were participating in a study, they
were not informed that it is directly related to vocabulary to prevent them from
paying special attention to vocabulary. This was also done to eliminate problems
interfering with the study and affecting the results, like the Hawthorne effect. The
course was delivered to all groups by the same teachers. Three teachers taught the
groups in rotation to eliminate both the Halo effect and the participants’ expectancy
effect. In an attempt to control mortality, the researcher asked the students to sign
that once they join the classes, they have to attend and take the required tests. The
students signed and agreed to do that. However, some mortality was still expected.
The researcher’s expectancy effect was controlled through concealing the names
and the groups of students written on the pretest/posttest. These were revealed at the
end of the experiment to analyze the data. Environmental issues such as noise and
lighting were controlled for the two groups. A quiet environment with good lighting
was provided throughout the study. The testees were tested in the university
laboratory. Each student was provided with a PC and an Internet connection. For
each test in this study, an explanation of the test requirements was provided.
4.1 Materials
There are five different materials used in this experiment, The Online IELTS
Placement Test, Increase Your Vocabulary (Lacey, Mahood, Trench, &
Vanderpump, 1991), The Oxford Advanced Learner’s Dictionary (OALD), Seventh
Impression (Cowie, 1992), Nation’s (2011, pp. 425–426), The 3000 Productive
Word Level: Version C, and The Penguin Graded Readers. Table 1 summarizes the
material use for each group.
The Oxford Advanced Learner’s Dictionary (OALD), 7th edition is a monolin-
gual English dictionary is designed to help students to find the meanings of words,
Assessing Two Strategies for Learning Vocabulary 251
their different spellings, pronunciation, parts of speech, usage. . .etc. It is designed
to eliminate difficulties for “foreign students” (Cowie, 1992, p. 7). Increase Your
Vocabulary comprises eight units that are mainly concerned with teaching vocab-
ulary learning skills as well as new vocabulary. It is basically designed to be used
with the OALD but can also be used independently. It is designed for students with
low to advanced level of English. The 3000 Productive Word Level Test was
designed by Nation. It has been used by researchers to estimate the non-native
speakers’ vocabulary size. Meara (1996, p. 38) calls it “the nearest thing we have to
a standard test on vocabulary”. According to Nation (1990), the 3000 Productive
Word Level Test contains the most frequent words that all learners need to function
effectively in English. The test is made of 18 items. A part of each of the 18 words is
written for students to complete it. Each word is put in a sentence to be tested
productively in context. For information on the validation of this test, see Read
(2005). The test was used in this study as a pretest right after the Online IELTS
Placement Test and again as a posttest after instruction took place to measure the
students’ vocabulary growth. The Penguin Graded Readers are simplified novels
that have been written for learners of English as a foreign language. The Advanced
Level, Level 6 (3000 Words), was chosen for the participants in the learning from
context group to read.
4.2 Scoring Procedure
This study uses one measure to examine the students’ vocabulary growth, the 3000
Productive Word Level Test as a pretest/posttest. The participants scored one point
for each correct answer on the test. They lost one point for each mistake or
misspelled word.
5 Data Analysis Procedure
T-test for The Learning from Context Group and The Dictionary-Learning Group
T-test were used to compare the pretest means of the dictionary-learning and the
learning from context groups to determine whether there is any difference between
Table 1 Material use for each group
Group 1 Group 2
The online IELTS placement test The IELTS placement test
Increase your vocabulary Increase your vocabulary (unit 2 excluded)
The OALD The penguin graded readers
The 3000 productive word level
(pretest/posttest)
The 3000 productive word level
(pretest/posttest)
252 M. Sabbah
the means. All requirements of the T-test were met. In addition to the above T-test,
two statistical tests were used to determine the participants’ vocabulary growth.
First, T-test was used to determine the overall vocabulary growth, for the
dictionary-learning and the learning from context groups, on one variable, methods
of instruction. Second, a two-way ANOVA and a Bonferroni posttest were used to
determine each proficiency group vocabulary growth on two variables, proficiency
and method of instruction. A significance level of 0.05% was accepted. All statis-
tical tests were performed using GraphPad Prism version 4.00 for Windows.
6 Results
6.1 T-Test to Compare the Pretest Means of the Dictionary-
Learning and the Learning from Context Groups
The descriptive statistics in Table 2 shows that there is no significant difference
between the pretest means of the two groups. The mean difference is 0.2 at 0.05%.
Hence, the null hypothesis of no difference between the means can be accepted. It can
be inferred from the above that the students in both groups, the dictionary-learning
and the learning from context groups, were of the same level before they received the
different methods of instruction. This is expected because the two groups were
sampled from the same population. The T-test was performed to prove this.
Since it is statistically well-established that both groups were of the same level
before receiving the different methods of instruction, a separate T-test was done to
each group to compare each group improvement.
6.2 T-Test to Compare the Pretest and Posttest Means
of the Dictionary-Learning Group
T-test (paired) was used to compare the pretest and posttest means difference of the
dictionary-learning group to determine whether or not the students’ vocabulary
Table 2 Statistics of pretest for the learning from context group vs. the dictionary-learning group
The learning from context
group
The dictionary learning
group
Mean
difference T P
N 30 30
X 5.3 5.1 0.2 0.3 NS
SD 0.484 0.494
In all the tables presented in this study, the following symbols stand for the followings: “N” stands
for number, “X” stands for mean and “SD” stands for standard deviation
*p < 0.05, df ¼ 58
Assessing Two Strategies for Learning Vocabulary 253
improved after instruction. All requirements of the T-test were met. The descriptive
statistics in Table 3 shows that there is a difference between the pretest and the
posttest means (5.97). This difference is significant at 0.05%. Hence, the null
hypothesis of no difference between the means was rejected and the alternative
hypothesis was accepted. It can be inferred from the above that the students in
the dictionary-learning group have improved after receiving the dictionary use
instruction.
6.3 T-Test to Compare the Pretest and Posttest Means
of the Learning from Context Group
T-test (paired) was used to compare the pretest and posttest means difference of the
learning from context group to determine whether or not the students’ vocabulary
improved after instruction. All requirements of the T-test were met. The descriptive
statistics (Table 4) show that there is a difference between the pretest and the posttest
means (8.17). This difference is significant at 0.05%. Hence, the null hypothesis of
no difference between the means was rejected and the alternative hypothesis was
accepted. It can be inferred from the above that the students in the learning from
context group have improved after receiving the guessing from context instruction.
6.4 T-Test for the Posttest of the Learning from Context
Group Versus the Dictionary-Learning Group
A t-test was used to compare the posttest means of the dictionary-learning and
learning from context groups to determine whether there is any difference between
Table 3 Statistics of pretest vs. posttest for the dictionary-learning group
Pretest Posttest Mean difference T P
N 30 30
X 5.10 11.1 5.97 27.5 *p < 0.001
SD 2.71 2.73
*p < 0.05, df ¼ 29
Table 4 Statistics of pretest vs. posttest for the learning from context group
Pretest Posttest Mean difference T P
N 30 30
X 5.30 13.5 8.17 35.5 *p < 0.001
SD 2.65 3.15
*p < 0.05, df ¼ 29
254 M. Sabbah
the means. All requirements of the T-test were met. The descriptive statistics
(Table 5) show that there is a significant difference between the posttest means of
the two groups (2.40) at 0.05%. Hence, the null hypothesis of no difference between
the means can be rejected. It can be inferred from the above that the students in both
groups, the dictionary-learning and the learning from context groups, were affected
by the different methods of instruction in different ways. The learning from context
group mean shows that the students in this group have benefited from the instruction
more than the dictionary-learning group.
6.5 A Two-Way ANOVA for the Pretest/Posttest
and a Bonferroni Posttest
For the two independent variables in this study, instruction method and partici-
pants’ proficiency level, a two-way ANOVA was conducted. The first variable has
two levels under it, guessing from context and dictionary use. The second has three
levels of participants’ proficiency, high, average and low. There are six levels in this
study to be analysed, A, B, C, D, E and F. They are organized in Table 6.
A two-way ANOVA (with repeated measures) and a Bonferroni posttest were
performed. The two-way ANOVA and the Bonferroni posttest affirmed that the
guessing from context proficiency groups did significantly better on the posttest.
The Bonferroni posttest was used to find out the differences between the pretest and
the posttest means in each proficiency level. The results are presented in Table 7
and Fig. 1.
The results of the Bonferroni posttest show that the differences between the
means of the pretest and the posttest are significant for all levels, A, B, C, D, E and
F. Hence, it can be concluded that all levels have benefited from the different
methods of instruction. This shows that the instruction resulted in vocabulary
growth for all levels. Also, through comparing the means of the pair levels, A
and B; C and D; E and F, it can be concluded that the means difference7
in the
learning from context levels, levels A, C and E are higher than the means in the
dictionary-learning levels, levels B, D, and F. Therefore, it can be inferred that the
students’ vocabulary has grown after receiving the different methods of instruction,
Table 5 Statistics of posttest for the learning from context group vs. the dictionary-learning group
The learning from context
group
The dictionary-learning
group
Mean
difference T P
N 30 30
X 13.5 11.1 2.40 3.16 *p < 0.001
SD 3.15 2.73
*p < 0.05, df ¼ 58
7
Between the pretest and the posttest.
Assessing Two Strategies for Learning Vocabulary 255
in different ways, according to the method of instruction. The low-level E,
improved by 381.82% in comparison to the medium level D, which improved by
89.09%. It can be inferred from the results that the proficiency level variable has an
effect on both methods of instruction. Levels A and E mean differences between the
pretest and the posttest show that they improved more than level C. In the same
way, levels B and F improved more than level D. Figure 1 shows the increase in
each level of the two groups.
Table 7 The Bonferroni results for pretest vs. posttest for proficiency levels
Levels Posttest Pretest Difference
95% CI of
difference T
Improvement
in percentages p
A 17.20 7.90 9.30 8.337–10.26 35.43 117.72 *p < 0.001
B 13.80 7.50 6.30 5.337–7.263 24.00 84 *p < 0.001
C 12.60 5.80 6.80 5.837–7.763 25.91 117.24 *p < 0.001
D 10.40 5.50 4.90 3.937–5.863 18.67 89.09 *p < 0.001
E 10.60 2.20 8.40 7.437–9.363 32.00 381.82 *p < 0.001
F 9 2.30 6.70 5.737–7.663 25.53 291.3 *p < 0.001
*p < 0.05
A B C D E F
0.0
2.5
5.0
7.5
10.0
12.5
15.0
17.5
Pretest
Posttest
Proficiency Levels
Fig. 1 Pretest vs. posttest
for proficiency levels
Table 6 The six proficiency levels involved in the study
Instruction method
Guessing from context Dictionary use
Proficiency levels High A B
Average C D
Low E F
256 M. Sabbah
7 Discussion
The comparison of the two groups overall means using T-test, on the pretest and
the posttest resulted in significant differences between the means. As a result, the
null hypothesis of no difference between the means was rejected. The complete
analysis of the T-tests and results based on the students’ raw scores on the pretest
and posttest (See Appendix 1), revealed that students in the learning from context
group performed better on the posttest than the dictionary-learning group stu-
dents. Hence, guessing from context was found to be a more effective method of
instruction in this study. This result is largely in agreement with theories that
confirm that most words are indirectly learned through reading extensively (e.g.,
Nagy et al., 1985). This result also confirms the results of the previous research
that has stressed this fact, as well (e.g., Knight, 1994; Ellis, 1994; Chun & Plass,
1996; Day et al. 1991) as discussed earlier. Likewise, the two-way ANOVA with
the Bonferroni posttest performed to examine the mean scores for each profi-
ciency level within the dictionary and context groups revealed that when it came
to comparing the two methods of instruction, guessing from context and dictio-
nary use, it appeared that guessing from context, in general, was a more effective
method of instruction. This was determined by the differences of the means
between the pretest and the posttest.
In addition, guessing from context proved to be more significant for the high
and low levels than for the medium levels. In other words, the analysis provides
evidence that the high and the low-level students, in the learning from context
group, have improved their vocabulary more than the intermediate level stu-
dents. This result contradicts some of the previous theories and studies that
suggest that guessing vocabulary from context is more suitable for the high
and medium levels and suggest that low proficiency students learn better from
dictionary or word lists (e.g., Laufer, 1997; Nation, 2011; Schmitt, 2012
discussed earlier). In this study, level D (medium level) had lower mean differ-
ence between the pretest and the posttest than level E (low level). Hence, it can
be inferred from the results that even the low proficiency level in the learning
from context group benefited from the instruction more than the medium level in
the dictionary-learning group. On the other hand, the results of this study are in
agreement with studies that were discussed earlier that revealed that when
incidental learning is concerned, advanced students were not any better than
low-ability students at incidental word learning (See Carter, 2012; Read, 2005;
Schefelbine, 1990; Stanely & Ginther, 1991). The results of this study affirm that
incidental learning is suitable for all levels of students, especially, the high and
low proficiency levels.
There are three possible explanations for the above results. Firstly, it is relatively
a new experience for the dictionary-learning group to use a monolingual dictionary.
All students at the university are used to looking up the meanings of words in a
bilingual dictionary (English-Arabic). As discussed earlier, this is a typical practice
of most second language speakers. Such a practice was found to hinder second
Assessing Two Strategies for Learning Vocabulary 257
language speakers’ proficiency growth as noted by Baxter (1980), but, unfortu-
nately, second language speakers still resort to this practice. Switching to a mono-
lingual dictionary was a dramatic change for the dictionary-learning groups,
especially, the low-level students. Secondly, students in the learning from context
group had the advantage of meeting the new words more than once, because they
used graded readers. This may mean that they met the different meanings of the new
words many times. Students who meet words more than once tend to remember
them better. This theory was discussed in previous research by Horst et al. (1998).
Students in the dictionary-learning group, on the other hand, met the words only
once through brief examples in the dictionary. They met one intended meaning of
each word and did not go through all the different meanings a word may have in
different contexts. Finally, both groups appeared to be enthusiastic about learning.
However, while the students in the learning from context group enjoyed reading the
graded reader novels to see how the events will unfold, the dictionary-learning
group students seemed to be burdened by looking up the meanings of words in the
dictionary. It seemed that they did not enjoy it much.
8 Implications
The study discussed in this chapter has implications for research and pedagogy. As
for implications for research, the study points to the need to conduct studies that
experiment with the different strategies of learning vocabulary. As discussed above,
one of the results of this study confirms that the low-level group has benefited from
the guessing from context strategy. This contradicts previous research results that
suggest that low proficiency groups do not benefit from guessing from context.
Hence, the debate discussed by Hunt and Beglar (2003, p. 262) about the need to
experiment with the two strategies discussed in the theoretical background of this
chapter continues to exist. The need to conduct studies of second language learners
using a monolingual dictionary discussed by Carter (2012) is still valid, as well. As
for pedagogical implications, the results of this study point to the importance of
giving guessing vocabulary from a reading context a priority over using a dictionary
when teachers teach second language learners, especially those of high and low
proficiency levels. This is because learning from a reading context was found as a
more efficient strategy at least in the context of higher education intensive courses
in Saudi Arabia and similar contexts.
9 Conclusion and Recommendations for Future Research
In conclusion, it is clear from what has been discussed so far that there was a
difference between the two groups involved in this study. The pretest/posttest
results affirmed that students who guessed the meanings of words from the context
258 M. Sabbah
in which they occurred have significantly improved their vocabulary in comparison
with the dictionary-learning students. So, it can be concluded that guessing from
context need to be given priority when teaching in at university intensive courses in
Saudi Arabia as the participants’ vocabulary, in this study, has improved using this
method of instruction. However, if enough time is available, for the teacher, it is
probably more comprehensive “that a mixture of approaches should be adopted.”
(Carter, 2012, p. 213). Therefore, it is suggested that this study can be developed in
two ways. First, this study can be developed if a third group is added to the learning
from context and the dictionary-learning groups. The added group can be a third
group that is taught by both methods of instruction in combination, guessing from
context and dictionary use. The comparison of the added group with the two groups
studied already can give a better idea about the influence of a combination of
methods on learning vocabulary. It is true that other researchers might have
conducted such a study with the three groups, but it is always useful to remember
that different participants, in different environment, respond to the different
methods differently. Second, this study can be developed further if the limitations
of the study are eliminated. This can be done in two ways, if participants of both
sexes are involved and if the study lasts for 6 weeks or more. Finally, with regards
to the dictionary-learning group, results showed that the high and low proficiency
levels have improved more than the intermediate proficiency level students, as well.
The results of the low proficiency level students in the dictionary-learning group, in
particular, showed that their vocabulary has improved when they used a monolin-
gual dictionary.8
Such results appear to suggest that though using a monolingual
dictionary might seem harder, for second language speakers, than using a bilingual
dictionary, the improvement achieved is worth the effort. It appears that students
have linguistically matured when they started to use a monolingual dictionary,
because they stopped relying on their native language, to explain the meanings and
started using English only to express themselves. Students were not familiar with
their ability to express themselves in English before this study took place. When
they used to use a bilingual dictionary, they explained the meanings of words the
way they learned them from the bilingual dictionary, in Arabic. Hence, it can be
concluded that the monolingual dictionary use has resulted in vocabulary growth in
the dictionary-learning group. This growth might have been hindered had a bilin-
gual dictionary been used. Again, more studies are needed to examine these
conclusions.
8
Experience has revealed that their proficiency was not improving when they were previously
using a bilingual dictionary.
Assessing Two Strategies for Learning Vocabulary 259
Appendix 1
Appendix 2: Nation’s 3000 Productive Word Level Test
1. He has a successful car___________ as a lawyer.
2. The thieves threw ac__________ in his face and made him blind.
3. To improve the country’s economy, the government decided on economic
ref________.
4. She wore a beautiful green go________ to the ball.
5. The government tried to protect the country’s industry by reducing the
imp______ of cheap goods.
6. The children’s pranks were funny at first, but finally got on the parents’
ner______.
7. The lawyer gave some wise coun__________ to his client.
8. Many people in England mow the la____________ of their houses on Sunday.
9. The farmer sells the eggs that his he______ lays.
10. Sudden noises at night sca______ me a lot.
11. France was proc_________ a republic in the eighteenth century.
Table 8 Students’ raw scores
on the pretest/posttest by
proficiency group
Groups
A B C D E F
Pretest 9 10 7 8 4 4
9 9 7 8 4 4
9 9 7 6 3 3
8 9 6 6 3 3
8 7 6 6 3 3
8 7 6 6 2 3
8 7 5 4 2 3
7 6 5 4 1 0
7 6 5 4 0 0
6 5 4 3 0 0
Posttest 18 18 15 12 13 11
18 17 15 12 12 11
18 16 14 11 12 10
18 14 14 11 11 10
17 14 13 11 11 9
17 13 12 10 10 9
17 13 12 10 10 8
17 12 11 10 10 8
16 11 10 9 9 7
16 10 10 8 8 7
260 M. Sabbah
12. Many people are inj______ in road accidents every year.
13. Suddenly he was thru________ into the dark room.
14. He perc___________ a light at the end of the tunnel.
15. Children are not independent. They are att________ to their parents.
16. She showed off her sle________ figure in a long narrow dress.
17. She has been changing partners often because she cannot have a sta________
relationship with one person.
18. You must wear a bathing suit on public beach. You are not allowed to walk
na________.
References
Aitchison, J. (2012). Words in the mind: An introduction to the mental lexicon (4th ed.). Oxford:
Blackwell.
Alderson, J. C. (2002). Assessing reading. Cambridge: Cambridge University Press.
Anderson, R. C., & Freebody, P. (1981). Vocabulary knowledge. In J. T. Guthrie (Ed.), Compre-
hension and teaching: Research reviews (pp. 77–117). Newark, DE: International Reading
Association.
Baxter, J. (1980). The dictionary and vocabulary behavior: A single word or a handful? TESOL
Quarterly, 14(3), 325–336. https://doi.org/10.2307/3586597.
Bensoussan, M., & Laufer, B. (1984). Lexical guessing in context in EFL reading comprehension.
Journal of Research in Reading, 7, 15–32.
Carnine, D., Kameenui, E., & Coyle, G. (1984). Utilization of contextual information in deter-
mining the meaning of unfamiliar words. Reading Research Quarterly, 19, 188–204.
Carter, R. (2012). Vocabulary: Applied linguistic perspectives (2nd ed.). London: Routledge.
Chun, D., & Plass, J. (1996). Effects of multimedia annotations on vocabulary acquisition. Modern
Language Journal, 80, 183–198.
Clark, E. (1993). The lexicon in acquisition. Cambridge: CUP.
Cook, J., Heim, A., & Watts, K. (1963). The word-in-context: A new type of verbal reasoning test.
British Journal of Psychology, 54, 227–237.
Corson, D. J. (1995). Using English words. Dordrecht: Kluwer Academic Publishers.
Cowie, A. P. (Ed.). (1992). Oxford advanced learner’s dictionary (4th ed.). Oxford: Oxford
University Press.
Coxhead, A. (1998). An academic word list: Occasional publication number 18. New Zealand:
Victoria University of Wellington.
D’Anna, C., Zechmeister, E., & Hall, J. (1991). Toward a meaningful definition of vocabulary size.
Journal of Reading Behavior, 23, 109–122.
Day, R., Omura, C., & Hiramatsu, M. (1991). Incidental EFL vocabulary learning and reading.
Reading in A Foreign Language, 7(2), 541–549.
Dupuy, H. (1974). The rationale, development and standardization of a basic word vocabulary
T-test. Washington, DC: US Government Printing Office.
Dupuy, H., & Krashen, S. (1993). Incidental vocabulary acquisition in French as a foreign
language. Applied Language Learning, 4(1–2), 55–64.
Ellis, N. C. (1994). Vocabulary acquisition: The implicit ins and outs of explicit cognitive
mediation. In N. C. Ellis (Ed.), Implicit and explicit learning of languages. London: Academic
Press.
Gibbons, H. (1940). The ability of college freshmen to construct the meaning of a strange word
from the context in which it appears. Journal of Experimental Education, 9, 29–33.
Assessing Two Strategies for Learning Vocabulary 261
Goulden, R., Nation, P., & Read, J. (1990). How large can a receptive vocabulary be? Applied
Linguistics, 2(4), 341–363.
GraphPad Software, Inc. (2005) GraphPad Prism. Version 4.00 for Windows [computer software].
San Diego, CA: GraphPad Software.
Graves, M. F. (1986). Vocabulary learning and instruction. Review of Research in Education, 13,
49–89.
Haynes, M. (1993). Patterns and perils of guessing in second language reading. In T. Huckin,
M. Haynes, & J. Coady (Eds.), Second language reading and vocabulary. Norwood, NJ:
Ablex.
Horst, M., Cobb, T., & Meara, P. (1998). Beyond a clockwork orange: Acquiring second language
vocabulary through reading. Reading in a Foreign Language, 11, 207–223.
Hulstijn, J. H. (1992). Retention of inferred and given word meaning: Experiments in incidental
vocabulary learning. In P. J. L. Arnaud & H. Bejoint (Eds.), Vocabulary and applied linguis-
tics. London: Macmillan.
Hunt, A., & Beglar, B. (2003). Current research and practice in teaching vocabulary. In C. J.
Richards & W. A. Renandya (Eds.), Methodology in language teaching. Cambridge:
Cambridge University Press.
Ingram, D. (1974). The relation between comprehension and production. In R. L. Schielelbusch &
L. L. Lloyd (Eds.), Language perspective: Acquisition, retardation and intervention. London:
Macmillan.
Jenkins, J., Matlock, B., & Slocum, T. (1989). Two approaches to vocabulary instruction: The
teaching of individual word meanings and practice in deriving word meaning from context.
Reading Research Quarterly, 24, 215–235.
Knight, S. (1994). Dictionary use while reading: The effects on comprehension and vocabulary
acquisition for students of different verbal abilities. Modern Language Journal, 78, 285–299.
Lacey, C., Mahood, J., Trench, J., & Vanderpump, E. (1991). Increase your vocabulary, second
impression. Oxford: Oxford University Press.
Laufer, B. (1997). The lexical plight in second language reading. In J. Coady & T. Huckin (Eds.),
Second language vocabulary acquisition. Cambridge: Cambridge University Press.
Laufer, B. (1998). The development of passive and active vocabulary: Same or different? Applied
Linguistics, 19, 255–271.
Laufer, B., & Sim, D. D. (1985). Taking the easy way out: Non-use and misuse of clues in EFL.
English Teaching Forum, 23(7–10), 20.
Liu, N., & Nation, I. S. P. (1985). Factors affecting guessing vocabulary in context. RELC Journal,
16(1), 33–42.
Marton, W. (1977). Foreign vocabulary learning as problem number one of foreign language
teaching at the advanced level. Interlanguage Studies Bulletin, 2(1), 33–47.
Meara, P. (1990). A note on passive vocabulary. Second Language Research, 6, 150–154.
Meara, P. (1996). The dimensions of lexical competence. In G. Brown, K. Malmkjaer, &
J. Williams (Eds.), Performance and competence in second language acquisition. Cambridge:
Cambridge University Press.
Meara, P., & Miralpeix, I. (2016). Tools for researching vocabulary (second language acquisi-
tion). Buffalo: Multilingual Matters.
Morrison, L. (1996). Talking about words: A study of French as a second language learners’
lexical inferencing procedures. Canadian Modern Language Journal, 53, 41–75.
Nagy, W. E., Herman, P., & Anderson, R. C. (1985). Learning words from context. American
Educational Research Journal, 24, 237–270.
Nation, I. S. P. (1980). Strategies for receptive vocabulary learning. RELC Journal Supplement
Guidelines, 3, 18–23.
Nation, I. S. P. (1990). Teaching & learning vocabulary. Boston: Heinle and Heinle Publishers.
Nation, I. S. P. (2011). Learning vocabulary in another language (6th ed.). Cambridge: Cambridge
University Press.
262 M. Sabbah
Nation, P., & Coady, J. (1988). Vocabulary and reading. In R. Carter & M. McCarthy (Eds.),
Vocabulary and language teaching. London: Longman.
Nation, I. S. P., & Newton, J. (1997). In J. Coady & T. Huckin (Eds.), Second language vocabulary
acquisition. Cambridge: Cambridge University Press.
Nurweni, A., & Read, J. (1999). The English vocabulary knowledge of Indonesian university
students. English for Specific Purposes, 18, 161–175.
Oxford, R., & Crookall, D. (1988). Learning strategies. In J. B. Gleason (Ed.), You can take it with
you: Helping students maintain foreign language skills beyond the classroom. Prentice Hall:
Englewood Cliffs, NJ.
Pany, D., & Jenkins, J. R. (1978). Learning word meanings: A comparison of instructional
procedures. Learning Disability Quarterly, 1, 21–32.
Pany, D., Jenkins, J. R., & Schreck, J. (1982). Vocabulary instruction: Effects on word knowledge
and reading comprehension. Learning Disability Quarterly, 5, 202–215.
Parry, K. (1991). Building a vocabulary through academic reading. TESOL Quarterly, 25,
629–653.
Pitts, M., White, H., & Krashen, S. (1989). Acquiring second language vocabulary through
reading: A replication of the clockwork orange study using second language acquirers.
Reading in a Foreign Language, 5, 271–275.
Qian, D. (2002). Investigating the relationship between vocabulary knowledge and academic
reading performance: An assessment perspective. Language Learning, 52, 513–536.
Read, J. (1995). Refining the word associates format as a measure of depth of vocabulary
knowledge. New Zealand Studies in Applied Linguistics, 1, 1–17.
Read, J. (1999). Assessing vocabulary in a second language. In C. Clapham & D. Corson (Eds.),
Encyclopedia of language and education, volume: Language testing and assessment. Dor-
drecht: Kluwer Academic Publishers.
Read, J. (2005). Assessing vocabulary (5th printing). Cambridge: Cambridge University Press.
Richards, C. J. (1976). The role of vocabulary teaching. TESOL Quarterly, 10(1), 77–89.
Richards, C. J., & Renandya, W. A. (Eds.). (2012). Methodology in language teaching. Cam-
bridge: Cambridge University Press.
Schefelbine, J. (1990). Student factors related to variability in learning word meanings from
context. Journal of Reading Behavior, 22, 71–97.
Schmitt, N. (1998). Tracking the incremental acquisition of second language vocabulary: A
longitudinal study. Language Learning, 48, 281–317.
Schmitt, N. (2011). Introduction. In N. Schmitt & M. McCarthy (Eds.), Vocabulary: Description,
acquisition and pedagogy (11th printing). Cambridge: Cambridge University Press.
Schmitt, N. (2012). Vocabulary in language teaching (13th printing). Cambridge: Cambridge
University Press.
Schmitt, N., & McCarthy, M. (Eds.). (2011). Vocabulary: Description, acquisition and pedagogy
(11th printing). Cambridge: Cambridge University Press.
Schmitt, N., & Meara, P. (1997). Researching vocabulary through a word knowledge framework:
Word associations and verbal suffixes. Studies in Second Language Acquisition, 19, 17–36.
Scholfield, P. J. (2011). Vocabulary reference works in foreign language learning. In Schmitt and
McCarthy (Eds.). Vocabulary: Description, acquisition and pedagogy (11th printing). Cam-
bridge: Cambridge University Press.
Shu, H., Anderson, R. C., & Zhang, Z. (1995). Incidental learning of word meanings while
reading: A Chinese and American cross-cultural study. Reading Research Quarterly, 30,
76–95.
Stahl, S. A. (1999). Vocabulary development. Mayfield, PA: P. A. Hutchison Co.
Stanely, P., & Ginther, D. (1991). The effects of purpose and frequency on vocabulary learning
from written context of high and low ability reading comprehenders. Reading Research and
Instruction, 30(4), 31–41.
Sternberg, R. J. (1987). Most words are learned from context. In M. G. McKeown & M. E. Curtis
(Eds.), The acquisition of word meanings. Hillsdale, NJ: Lawrence Erlbaum Associates.
Assessing Two Strategies for Learning Vocabulary 263
Swanborn, M., & de Glopper, K. (1999). Incidental word learning while reading: A meta-analysis.
Review of Educational Research, 69, 261–285.
van Daalen-Kapteijns, M. M., & Elshout-Mohr, M. (1981). The acquisition of word meanings as a
cognitive learning process. Journal of Verbal Learning and Verbal Behavior, 20, 386–399.
Waring, R., & Nation, I. S. P. (2004). Second language reading and incidental vocabulary learning.
Angles on the English Speaking World, 4, 97–110.
West, M. (1953). A general service list of English words. London: Longman, Green and Co.
White, T. G., Sowell, J., & Yanagihara, A. (1989). Teaching elementary students to use word-part
clues. The Reading Teacher, 42, 302–309.
264 M. Sabbah
Testing Usefulness of Reading Comprehension
Exams Among First Year Students of English
at the Tertiary Level in Tunisia
Yassmine Mattoussi
Abstract This study investigated testing usefulness (Bachman & Palmer, 1996) of
reading comprehension among first year students of English at the tertiary level in an
EFL context. T Data were gathered by means of two questionnaires and a corpus which
included 173 first year students’ graded reading exams. A questionnaire was admin-
istered to 64 students and 21 teachers which was collected at different institutes and
universities. Scores of the reading exams were meant to check the appropriateness of
the construct and rater consistency. Questionnaire results indicated that most of the
reading comprehension examinations had an acceptable level of reliability and a
moderate level of authenticity and interactiveness. It was also revealed that these
exams had a high level of construct validity and practicality. In addition, the findings
indicated that the achievement readingtests had a harmfulimpactonfirst year students,
whereas they had a beneficial washback effect on the teachers. On the other hand, the
results of the reading scores’ analysis using Cronbach alpha to estimate the reading
tests’ reliability showed that there were only three tests among six which had an
acceptable reliability of (/¼.70). Concerning the findings of the construct validity
assessment procedure of the reading exams, they showed that only the reading part in
comprehension, composition, grammar test 1 and the reading section in comprehen-
sion, composition, grammar test 2 were proven to be construct valid. These results
contradicted those of the questionnaires. Actually, the participants did not provide
trustworthy answers in the reliability and construct validity parts. Therefore, most of
the reading midterm and final exams designed by the teachers at the tertiary level in
Tunisia had a low reliability and construct validity, a moderate authenticity and
interactiveness. Nevertheless, the reading exams had a high practicality and a benefi-
cial impact on the teachers, but had a harmful impact on first year learners. The reading
exams had a low, since reliability and construct validity which are two important
criteria were proven to be threatened. The study had implications for test design.
Y. Mattoussi (*)
Department of English, The University of Letters, Arts and Humanities of Manouba, Tunis,
Tunisia
e-mail: yassminematoussi@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_13
265
Keywords Test usefulness • Reliability • Construct validity • Authenticity •
Interactiveness • Impact • Practicality
1 Introduction
This study probed the reading comprehension test usefulness among first year
students at the tertiary level in Tunisia. Testing is an essential component of any
learning or teaching experience as it is mainly associated with decisions making
(Bachman, 1990; Hughes, 1989). Language testing for instance provides a method-
ology for probing and investigating students’ language ability. Bachman and
Palmer (1996) maintain that “the primary purpose of a language test is to provide
a measure that we can interpret as an indicator of an individual’s language ability”
(p. 23). Language ability involves not only linguistic competence but also commu-
nicative performance to make tests resemble as closely as possible to real-life.
Language testing is significant in applied linguistics field. According to Davies
(1990), what makes it so is that unlike other subjects offered to students, a language
has an unclear content. Therefore, assessing language produces complications
regarding what is to be tested and how the testing process is to be done. Language
tests has many purposes. They help to diagnose students’ strengths and weaknesses.
In addition, they help to assess the learners’ progress and achievement. On the other
hand, they help to evaluate the effectiveness of the different approaches to language
teaching and provide feedback on teaching and learning (Brown, 1996). Indeed,
tests enable the teacher to know if his/her teaching was effective and can help
him/her to improve the testing process. From this standpoint, we can consider the
close relationship between teaching and testing. Hughes (1989) states “we cannot
expect testing only to follow teaching. What we should demand of it, however, is
that it should be supportive of good teaching and, where necessary, exert a correc-
tive influence on bad teaching” (p. 2).
2 Theoretical Background
After looking closely to the literature on language testing, it was found that there is
a lack in theoretical testing knowledge among Tunisian teachers and this lack has
led to a poor testing quality. In fact, the scope of education does not prepare
graduates of English to be good test-designers. For instance, there are test-takers
who face problems of bad-formulated instructions. They fail to understand what
they are required to do and apart from a score assigned out of 20, there is no detailed
scoring which make them feel convinced with their scores. That’s why, exams are
perceived as not reliable tools which reflect their abilities truly.
In professional events, such as conferences and workshops, there is rarely any
organized event on testing. In addition, MA and PhD studies handle rarely topics on
testing and assessment. Researchers are not encouraged to work on such important
266 Y. Mattoussi
areas. On the other hand, there has never been any initiative to reform the edu-
cational policy in the country. The Tunisian Ministry of Higher Education does not
make remedial actions yet to the neglected area of assessment which has led to a
worldwide public dissatisfaction with this educational system in general and lan-
guage testing in particular (Hidri, 2015).
In Tunisian universities, English learners are assessed in the four skill areas—
listening, speaking, reading and writing. The literature in the field of reading assess-
ment research is wide and complex. The issue which is of continuing interest for
researchers is the validity of the tests used to assess students’ academic reading
abilities (Taylor & Weir, 2012). Apart from validity, Bachman and Palmer (1996)
suggest that there are other properties which should be considered in designing a test
which are: Reliability, authenticity, interactiveness, impact and practicality. Thus, a
notion of usefulness merged to qualify kinds of tests that should be used in testing.
2.1 Research Problem and Rationale of the Study
The fundamental research problem upon which the present study hinges pertains to
the established fact that the reading comprehension examinations are not well-
designed by many teachers at the tertiary level in Tunisia. The structure inaccuracy
is frequently observed in most of the reading achievement tests administered in
Tunisian universities. They often include unclear questions and bad-formulated
tasks. This might contribute to the students’ failure in the test. In other terms, the
inappropriateness of these tests’ format could be a factor that might influence an
inadequate completion of the test. Indeed, teachers should pay more attentiveness in
designing reading tests to avoid misleading the students.
In addition, many of these examinations do not cover various reading assessment
techniques. Most of them include common tasks such as true/false questions, open-
ended instructions and exercises which are about the identification of unfamiliar
words and referents selected from the text. Besides, the reading achievement tests
are seen to be rarely corrected effectively. In broad terms, the student’s strengths
and weaknesses are rarely mentioned. Consequently, the students would be affected
negatively by the reading examinations for two reasons. First, the identification of
the strengths in their reading abilities which would build their confidence are rarely
done. Second, the weaknesses which would push them to take remedial actions are
not found out clearly on the exam paper. Further to the above, the teachers are not
aware of the notion of usefulness in the reading examinations. Hence, testing of
reading needs thorough investigation.
The reading comprehension test is meant to assess the students’ reading skill. It
determines their actual abilities in understanding passages. The reading exam-
ination that students take at universities is called an achievement test. Its purpose is
to establish how successful they have been in achieving course objectives which are
not frequently taken care of by the teachers at the tertiary level in Tunisia. In spite of
its importance, little is done concerning the reading examinations’ evaluation in
terms of usefulness at the tertiary level in Tunisia. Therefore, the present research
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 267
project endeavours to evaluate particularly first year reading comprehension tests’
usefulness at Tunisian universities. It aims to investigate characteristics of their reli-
ability, construct validity, authenticity, interactiveness, impact and practicality.
The study aimed to answer the following research questions:
1. Are the reading examinations reliable?
2. Are the reading examinations a good measure of the students’ reading ability?
3. What are the teachers’ and students’ perceptions of test usefulness?
3 Method and Data Analysis
This section introduces the participants who took part in this study, the instruments
and the procedures. Since the focus of the study was on the testing of reading com-
prehension among first year students at the tertiary level in Tunisia, so the partici-
pants were teachers who have experienced teaching first year reading course and
first year students of English at the three selected Tunisian public universities:
FLAH, ISLT and ISSH. The teachers were 21 and the students were 64. These
respondents were selected randomly and they were identified according to conve-
nience sampling.
Data were gathered by means of two types of questionnaires and a corpus which
included 173 first year students’ graded exams in the reading comprehension.
Questionnaires (see Appendices 1 and 2) were designed principally on the basis
of Bachman and Palmer’s theory of test usefulness (1996) which has proven to be
sound and comprehensive for a given test usefulness evaluation. They were handed
to the students and the teachers. The following Table 1 illustrated clearly the
division of the examinations.
The table above showed that 99 students’ copies were obtained from FLAH
which corresponded to four different reading comprehension progress tests (see
Appendices E, F, G and H). Concerning ISLT, the reading skill was tested along
with grammar and writing. These three abilities were measured in a test called
comprehension composition grammar (CCG). The comprehension part is meant to
assess the students’ ability to understand a text. The Composition sub-test aims at
measuring the learners’ capacity to write. As to the grammar part within this test, it
intends to test the students’ mastery of the grammatical rules. Seventy-four exams
Table 1 Reading exams
Universities
Reading
tests Students’ reading exams corresponding to each test Total
ISLT 2 CCG test 1 CCG test 2 74
32 42
FLAH 4 Reading
test 3
Reading
test 4
Reading
test 5
Reading
test 6
99
24 28 15 32
268 Y. Mattoussi
were obtained from ISLT which corresponded to two progress CCG Tests 1 and
2. CCG Test 1 (see Appendix C) was made up of reading and grammar solely.
However, CCG Test 2 (see Appendix D) included reading, grammar and writing.
On the basis of the availability of the teachers and the class schedule in the three
selected public universities: FLAH, ISLT and ISSH, date and time were fixed for
the teachers’ and students’ questionnaire survey distribution. Then, the teachers
were directly contacted before their classroom entrance. They were requested to
answer the questionnaire, to administer the students’ questionnaires version to the
class at the end of the course to be filled out and to give them all back. The questions
were explained clearly to the participants for the purpose of a better understanding.
In this study, Cronbach alpha (often symbolized by the lower case Greek letter α)
was used to estimate the reliability of the collected reading comprehension tests.
The reason which lay behind choosing Cronbach alpha was that as Brown (2002)
states, “it is one of the most commonly reported reliability estimates in the language
testing literature” (pp. 17–18). It was computed by the SPSS software. The com-
monly accepted rules for describing reliability using Cronbach alpha were
presented in Table 2.
As it can be noticed according to the table, Cronbach alpha normally ranges from
0 to 1.
In this study, Pearson correlation (r) was used as a statistical technique to
investigate the reading tests and sub-tests construct validity using the SPSS soft-
ware. The Pearson correlation is used to explore the strength of the relationship
between two continuous variables. It indicates the direction, which would be either
positive (þ) or negative (), and the strength of the relationship. A positive correl-
ation indicates that as one variable increases, so does the other. A negative correl-
ation shows that as one variable increases, the other decreases. It ranges normally
from 1 to þ1 (Pallant, 2005).
On the one hand, the assessment procedure of the reading sub-tests in the two
comprehension composition grammar (CCG) tests collected at the ISLT in terms of
construct validity, it is assumed by Henning (1987) that the reading items show
construct validity if each item correlates highly with the reading total score than
with the grammar total score in CCG Test 1 (see Appendix C) and with the
grammar and writing totals in CCG Test 2 (see Appendix D). This might be
expressed mathematically as follows:
Table 2 Rules for Cronbach alpha reliability estimate (Cronbach, 1970)
Cronbach’s alpha Reliability
/  0.9 Excellent
0.7  / < 0.9 Good
0.6  / < 0.7 Acceptable
0.5  / < 0.6 Poor
/ < 0.5 Unacceptable
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 269
CCG Test 1 : 1
ð Þ rRn RT > r RnGT
CCG Test 2 : 1
ð Þ rRn RT > rRn G
: 2
ð ÞrRn RT > rRn WT
r ¼ Pearson correlation
R ¼ Reading
n ¼ a given number of item
RT ¼ Reading Total
GT ¼ Grammar Total
WT ¼ Writing Total
The symbols indicated that the correlation r of each reading individual item with
its own reading total should be > r of the same item with the grammar total in CCG
Test 1 (see Appendix C) and with the grammar and writing totals in CCG Test
2 (see Appendix D).
On the other hand, the assessment of each reading comprehension test collected
at FLAH was processed by correlating its various test tasks with each other. As
suggested by Alderson, Clapham, and Wall (1995), these correlation coefficients
are expected to be higher than 0.5 since these tests are supposed to assess only one
trait which is the reading skill. However, in case these correlations turn out to be
low-possibly in the order of þ.3 to þ.5, then the test is meant to measure different
skills.
The data were analyzed using the SPSS 20.0. The analysis of both questionnaire
versions was quantitative descriptive. It consisted in showing the major differences
and similarities between first year students’ and teachers’ views towards the use-
fulness of the reading examinations that they sat for during the year. This was
processed to pinpoint the problems of these reading achievement tests and present
different solutions to overcome them. In particular, the most essential criteria for
test construction, which are reliability and construct validity, were more investi-
gated through two quantitative approaches. On the one hand, the reliability of the
reading tests and sub-tests was evaluated through computing their Cronbach alpha.
On the other hand, the construct validity of the examinations under focus in this
study was assessed through correlating its various components with each other
using Pearson correlation.
4 Results and Discussion
This section presents mainly the results of Cronbach alpha estimation of the reading
examinations collected at ISLT and FLAH. After computing Cronbach alpha using
SPSS, it was revealed that the reading part in the CCG Test 1 (see Appendix C) and
in CCG Test 2 (see Appendix D) had an / ¼ .721, .7, respectively, which indicated
a good reliability. On the other hand, among the four reading comprehension tests
gathered at FLAH, there was only one test which had a good reliability. The
Reading Test 5 (see Appendix G) had an / ¼ .729. However, the Reading Test
270 Y. Mattoussi
3 (see Appendix E), the Reading Test 4 (see Appendix F) and the Reading Test
6 (see Appendix H) were proven to have an unacceptable reliability. They had an
/ ¼ .363, .49, .201, respectively.
This section is meant to present the findings of the construct validity assessment
of the reading examinations under focus in this study. In order to assess the
construct validity of the reading part of CCG Test 1 (see Appendix C) and CCG
Test 2 (see Appendix D), the correlations between the reading items were computed
according to the theory of Henning (1987). The following results of CCG Test
1 were obtained:
rR1 RT ¼ :782 :540
ð Þ > rR1 GT ¼ :219
rR2 RT ¼ :773 :642
ð Þ > rR2 GT ¼ :291
rR3 RT ¼ :791 :578
ð Þ > rR3 GT ¼ :298
rR4 RT ¼ :651 :360
ð Þ > rR3 GT ¼ :330
As it can be noticed, each of the reading items displayed construct validity since
it correlated more highly with the reading total than with the grammar total. The
coefficients in parentheses represented the value of the correlation after correction
for part-whole overlap. This latter correction is necessary because the reading items
before correction display artificially high correlations with their own reading total
since they include a portion of it. In sum, the reading part of CCG Test 1 was proved
to be construct valid. Concerning CCG Test 2, its results of correlations were
presented below:
rR1 RT ¼ :733 :350
ð Þ > rR1 GT ¼ :319
rR1 RT ¼ :733 :350
ð Þ > rR1 WT ¼ :289
rR2 RT ¼ :538 :310
ð Þ > rR1 GT ¼ :217
rR2 RT ¼ :538 :310
ð Þ > rR1 WT ¼ :221
rR3 RT ¼ :722 :500
ð Þ > rR3 GT ¼ :394
rR3 RT ¼ :733 :500
ð Þ > rR3 WT ¼ :254
rR4 RT ¼ :707 :450
ð Þ > rR4 GT ¼ :356
rR4 RT ¼ :707 :450
ð Þ > rR4 WT ¼ :343
rR5 RT ¼ :280 :170
ð Þ > rR5 GT ¼ :136
rR5 RT ¼ :280 :170
ð Þ > rR5 WT ¼ :116
rR6 RT ¼ :001 :160
ð Þ > rR6 GT ¼ :112
rR6 RT ¼ :001 :160
ð Þ > rR6 WT ¼ :078
As illustrated by the results, the six reading items which were included in the
reading part in CCG Test 2 correlated more highly with the reading total than with
the grammar and writing totals. Therefore, it could be argued that the construct
validity of the reading part in CCG Test 2 was good. On the other hand, the
assessment of the four reading comprehension tests collected at FLAH was com-
puted according to the theory of Alderson et al. (1995). The correlation coefficients
of the various tasks with each other in the Reading Test 3 were presented in Table 3.
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 271
According to Table 3, there were only three correlations which were higher than
.5. The correlation of task 1 and task 2 was high (r ¼ .814). Moreover, the correl-
ation of task 1 and task 5 was .522 and that of task 1 and task 6 was .576. According
to Alderson et al. (1995), since there were only three correlations which were higher
than .5. So, the Reading Test 3 (see Appendix E) was not supposed to measure only
the reading ability, but rather other skills. Therefore, the construct validity of the
Reading Test 3 (see Appendix E) was threatened. For the Reading Test 4 (see
Appendix F), the correlation coefficients of the various tasks with each other within
it were shown in Table 4.
According to the table, only task 1 and task 3 correlated higher than .5 (r ¼ .552),
whereas the rest of the correlations between the tasks were either <.3 or between
þ.3 and þ.5. This paved the way to the conclusion that this test measured other
skills and not only the reading skill. So, the Reading Test 4 (see Appendix F) was
construct invalid. For the Reading Test 5 (see Appendix G), the correlation coeffi-
cients of the various tasks with each other were presented in Table 5.
According to Table 5, there were only two correlations between tasks which
were >.5. The first one was that between task 1 and task 3 (r ¼ .501). The second
correlation was between task 2 and task 4 (r ¼ .664). This suggested that the
Reading Test 5 (see Appendix G) was not construct valid. Lastly, the correlation
coefficients of the various tasks with each other in the Reading Test 6 (see Appen-
dix H) were displayed in Table 6.
Table 3 The correlation coefficients of the various tasks with each other in the Reading Test 3
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6
Task 1 Pearson correlation 1 .814**
.132 .150 .522*
.576*
Sig. (2-tailed) .000 .638 .595 .046 .025
N 15 15 15 15 15 15
Task 2 Pearson correlation .814**
1 .158 .320 .456 .288
Sig. (2-tailed) .000 .575 .245 .088 .298
N 15 15 15 15 15 15
Task 3 Pearson correlation .132 .158 1 .340 .156 .016
Sig. (2-tailed) .638 .575 .215 .579 .955
N 15 15 15 15 15 15
Task 4 Pearson correlation .150 .320 .340 1 .148 .136
Sig. (2-tailed) .595 .245 .215 .599 .630
N 15 15 15 15 15 15
Task 5 Pearson correlation .522*
.456 .156 .148 1 .247
Sig. (2-tailed) .046 .088 .579 .599 .375
N 15 15 15 15 15 15
Task 6 Pearson correlation .576*
.288 .016 .136 .247 1
Sig. (2-tailed) .025 .298 .955 .630 .375
N 15 15 15 15 15 15
*Correlation is significant at the 0.05 level (2-tailed)
**Correlation is significant at the 0.01 level (2-tailed)
272 Y. Mattoussi
As it can be noticed from Table 6, the correlations between the tasks were either
<.3 or between þ.3 and þ.5. This paved the way to the conclusion that this test
assessed skills other than the reading skill. Thus, the Reading Test 6 (see Appendix
G) was construct invalid. Clearly then, it was revealed that only the reading part in
CCG Test 1 (see Appendix C) and in CCG Test 2 (see Appendix D) were reliable
and construct valid. The Reading Test 5 (see Appendix G) was reliable but
construct invalid such as the remaining Reading Tests.
This section is devoted to present a comparison between the students’ and
teachers’ perceptions towards the reading examinations in terms of the six test
qualities. In fact, the students and the teachers provided positive answers to the
questions under the notions of: Construct validity and practicality. Additionally, the
teachers provided positive answers towards the questions under the notion of
impact. However, they presented different perspectives towards the questions
under the test qualities of reliability, authenticity, interactiveness and impact. The
results will be presented as follows (Tables 7, 8, 9 and 10):
The good reliability of the reading examinations [the reading part in CCG Test
1 (see Appendix C), the reading part in CCG Test 2 (see Appendix D) and the
Reading Test 5 (see Appendix G)] reflected that the students ’scores for one and the
same test, though given at different interval, would be approximately the same.
Table 4 The correlation coefficients of the various tasks with each other in the Reading Test 4
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7
Task 1 Pearson correlation 1 .066 .552**
.251 .353*
.027 .023
Sig. (2-tailed) .721 .001 .166 .048 .883 .901
N 32 32 32 32 32 32 32
Task 2 Pearson correlation .066 1 .278 .387*
.370*
.387*
.383*
Sig. (2-tailed) .721 .123 .029 .037 .029 .030
N 32 32 32 32 32 32 32
Task 3 Pearson correlation .552**
.278 1 .223 .436*
.062 .245
Sig. (2-tailed) .001 .123 .220 .013 .737 .177
N 32 32 32 32 32 32 32
Task 4 Pearson correlation .251 .387*
.223 1 .319 .335 .398*
Sig. (2-tailed) .166 .029 .220 .075 .061 .024
N 32 32 32 32 32 32 32
Task 5 Pearson correlation .353*
.370*
.436*
.319 1 .355*
.249
Sig. (2-tailed) .048 .037 .013 .075 .046 .170
N 32 32 32 32 32 32 32
Task 6 Pearson correlation .027 .387*
.062 .335 .355*
1 .295
Sig. (2-tailed) .883 .029 .737 .061 .046 .101
N 32 32 32 32 32 32 32
Task 7 Pearson correlation .023 .383*
.245 .398*
.249 .295 1
Sig. (2-tailed) .901 .030 .177 .024 .170 101
N 32 32 32 32 32 32 32
*Correlation is significant at the 0.05 level (2-tailed)
**Correlation is significant at the 0.01 level (2-tailed)
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 273
Table 5 The correlation coefficients of the various tasks with each other in the Reading Test 5
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6
Task 1 Pearson correlation 1 .117 .501**
.344 .146 .358
Sig. (2-tailed) .554 .007 .073 .459 .062
N 28 28 28 28 28 28
Task 2 Pearson correlation .117 1 .375*
.664**
.442*
.194
Sig. (2-tailed) .554 .050 .000 .019 .321
N 28 28 28 28 28 28
Task 3 Pearson correlation .501**
.375*
1 .461*
.493**
.465*
Sig. (2-tailed) .007 .050 .014 .008 .013
N 28 28 28 28 28 28
Task 4 Pearson correlation .344 .664**
.461*
1 .398*
.042
Sig. (2-tailed) .073 .000 .014 .036 .832
N 28 28 28 28 28 28
Task 5 Pearson correlation .146 .442*
.493**
.398*
1 .130
Sig. (2-tailed) .459 .019 .008 .036 .510
N 28 28 28 28 28 28
Task 6 Pearson correlation .358 .194 .465*
.042 .130 1
Sig. (2-tailed) .062 .321 .013 .832 .510
N 28 28 28 28 28 28
*Correlation is significant at the 0.05 level (2-tailed)
**Correlation is significant at the 0.01 level (2-tailed)
Table 6 The correlation coefficients of the various tasks with each other in the Reading Test 6
Task 1 Task 2 Task 3 Task 4 Task 5 Task 6 Task 7 Task 8
Task 1 Pearson correlation 1 .098 .158 .007 .015 .107 .249 .445*
Sig. (2-tailed) .650 .461 .974 .946 .618 .240 .029
N 24 24 24 24 24 24 24 24
Task 2 Pearson correlation .098 1 .041 .307 .249 .367 .330 .159
Sig. (2-tailed) .650 .847 .145 .240 .078 .115 .458
N 24 24 24 24 24 24 24 24
Task 3 Pearson correlation .158 .041 1 .026 .174 .046 .080 .161
Sig. (2-tailed) .461 .847 .902 .417 .832 .710 .452
N 24 24 24 24 24 24 24 24
Task 4 Pearson correlation .007 .307 .026 1 .219 .099 .075 .220
Sig. (2-tailed) .974 .145 .902 .304 .647 .727 .301
N 24 24 24 24 24 24 24 24
Task 5 Pearson correlation .015 .249 .174 .219 1 .192 .192 .095
Sig. (2-tailed) .946 .240 .417 .304 .369 .368 .658
N 24 24 24 24 24 24 24 24
Task 6 Pearson correlation .107 .367 .046 .099 .192 1 .050 .095
Sig. (2-tailed) .618 .078 .832 .647 .369 .818 .659
N 24 24 24 24 24 24 24 24
Task 7 Pearson correlation .249 .330 .080 .075 .192 .050 1 .034
Sig. (2-tailed) .240 .115 .710 .27 .368 .818 .873
N 24 24 24 24 24 24 24 24
Task 8 Pearson correlation .445*
.159 .161 .220 .095 .095 .034 1
Sig. (2-tailed) .029 .458 .452 .301 .658 .659 .873
N 24 24 24 24 24 24 24 24
* Correlation is significant at 0.05
274 Y. Mattoussi
These examinations can be evaluated to contain essentially an appropriate text,
clear instructions and well-formulated tasks. However, a close look at the unreliable
examinations [the Reading Tests 4, 5 and 6 (see Appendices F, G, H)] revealed that
they had problems. For example, the texts of the Reading Tests 4 and 6 were
presented badly. Indeed, the good presentation should not be neglected. It is
Table 7 Reliability
Reliability
Students Teachers
– Question 3: 39% reported that their teachers
sometimes gave them a detailed scoring key in
the reading examination
– Q2: 75% stated that they often provided a
detailed scoring
– Q3: 50% stated that they did not receive any
type of training in scoring the first year read-
ing test/exam
Table 8 Authenticity
Authenticity
Students Teachers
– Q1/1: 54% mentioned that the reading exam
tasks that they sat for represented sometimes
situations which were similar to what they will
encounter in their lives
– Q2: 75% of the students posited that the
vocabulary task(s) represented question
(s) about the meanings of words or expressions
– Q1/1: 75% ranged between selecting “often”
and “always” in mentioning the frequency of
opting for real-world first year reading
– Q2: 80% ranged between ticking the option
“they represent question(s) about the meanings
of words or expressions” and the option “they
represent questions about the meanings of
words or expressions and paragraphs to be
filled in with given words.”
Table 9 Interactiveness
Interactiveness
Students Teachers
– Q1: 31% reported that answering to the
reading test tasks required “sometimes” their
topical knowledge
– Q3/1: 50% stated that the reading test/exam
tasks depend sometimes on each other
– Q1: 66% stated that their reading exams did
not contain tasks which involve their first year
students’ topical knowledge
– Q4: 50% stated that they sometimes
designed dependent first year reading test
tasks
Table 10 Impact on the students
Impact on students
– Q3: The majority of the learners (76%) stated that their teachers did not often ask them to
suggest appropriate tasks for use in a reading test/exam
– Q4: 87% stated that their teachers did not often ask them to suggest appropriate marking
criteria for use in a reading test/exam
– Q5: 48% said that they received “0 time” remarks about their strengths and weaknesses in the
two midterm tests
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 275
accounted to be a guideline among others to achieve a high degree of reliability
within the tests (Hughes, 2003).
The findings of the construct validity assessment of the reading sub-tests and
tests gathered at ISLT and FLAH, respectively revealed that only the reading part in
CCG Test 1 (see Appendix C) and in CCG Test 2 (see Appendix D) were construct
valid. Nevertheless, the remaining ones were shown to be construct invalid. The
reading sub-tests collected at ISLT contained items which could be evaluated to
measure purely the students’ reading ability and not any other ability. However, the
four reading tests gathered at FLAH included tasks which were not valid devices to
assess only the intended reading ability, but rather other abilities such as the
writing one.
According to the questionnaires, the reading examinations were judged to have
90% of reliability, 95% of construct validity and 100% of practicality. Concerning
reliability, the reading examinations had two shortcomings which centred around
the scoring process. First, a detailed scoring key was not provided in the reading
exams. Second, the teachers at the tertiary level lack trainings in the marking
process of the examinations. On the other hand, a vast majority of the respondents
claimed that their reading exam tasks measured only their students’ ability to
comprehend a text. This result was positive since the ability to comprehend a text
is considered to be the construct which should be assessed in the reading compre-
hension examination.
Indeed, there was a contradiction between what was said about the reading
achievement tests and what was found in them in terms of reliability and construct
validity. In fact, the findings drawn from Cronbach alpha and internal correlations
were more reliable and trustworthy than those drawn from the questionnaires.
In terms of authenticity, first year reading tests and exams had mainly two
problems. On the one hand, the teachers were not aware of the necessity of
including real-world reading tasks in order to make them as authentic as possible.
On the other hand, the teachers did not often insert contextualized vocabulary tasks
in their reading achievement tests. Vocabulary items should be contextualized. In
other words, they should be put in paragraphs. Vocabulary, which is embedded and
context dependent in nature, plays a fundamental role in the assessment of the
learners’ performance (Read & Chapelle, 2001). In sum, it might be argued that the
majority of the reading comprehension examinations designed by the majority of
the teachers in Tunisian universities had a relatively moderate level of authenticity.
This was due to only 50% of the participants’ answers which were positive.
Concerning interactiveness, the results displayed that the reading examinations
had two shortcomings, too. Firstly, the teachers sometimes designed reading tasks
which involved the test takers’ topical knowledge. The originators of this test
quality were Bachman and Palmer (1996) who state that “the engagement of topical
knowledge is [. . .] an important determinant of the relative interactiveness of test
tasks” (p. 143). Secondly, the teachers sometimes included dependent tasks in their
midterm and final reading exams. Bachman and Palmer (1996) state that dependent
questions contribute to the interactiveness of the test. Therefore, the teachers should
take into consideration their point of view and ought to try to design instructions in
the reading comprehension exams which are dependent. In conclusion, it could be
276 Y. Mattoussi
argued that the majority of the reading tests designed by the teachers at the tertiary
level in Tunisia had a moderate level of interactiveness since only 60% of the
respondents’ answers were positive.
Further to the above, the results indicated that the learners were affected
negatively by their reading examinations. This was due to three problems. First,
they reported that they were not often asked to suggest appropriate tasks and
marking criteria for use in a reading test. In addition, the examinees stated that
they never received remarks about their strengths and weaknesses in the reading
skill. Brown (2003) emphasizes the idea that the teachers should comment properly
on every student’s copy. Providing a single numerical score is insufficient. Indeed,
mentioning the strengths contributes to the students’ motivation. In addition,
finding out the weaknesses would be beneficial since they urge the students to
take remedial actions to overcome them.
In sum, most of the reading examinations designed by the teachers at the tertiary
level were assessed to have a low reliability and construct validity, a moderate
authenticity and interactiveness. Additionally, they were evaluated to have a high
practicality and a beneficial impact on the teachers. However, they were judged to
have a harmful impact on first year learners. Therefore, the reading examinations’
usefulness was diagnosed to be low.
5 Conclusion
There were four main limitations in this study to be addressed. First, the number of
participants was limited to 64 first year students and 21 teachers in the capital city of
Tunis. Therefore, the findings could not be generalized to the process of testing of
reading at the tertiary level in Tunisia. Second, the number of the graded reading
exams was restricted to only 100 and 73 copies. It follows then that the results could
not be generalized to the reliability and construct validity of all the reading
comprehension examinations constructed by university teachers in Tunisia. Third,
this study was essentially quantitative in nature. It used classical measurement
theories for the reliability and construct validity assessment. More sophisticated
theories such as Multi-trait-Multi-method (MT-MM) and factor analysis could have
been used for the estimation of these two notions. They could provide more
valuable and in-depth findings about the reading items included in the six
teacher-produced reading examinations. Ultimately, further investigation was
merited for the qualitative analysis of these reading items in terms of authenticity
and interactiveness.
Even if this study has limitations, it has research, pedagogical and methodolog-
ical implications. One major contribution of this study is that it is one of the few
research projects which deal with the issue of language assessment in Tunisia. It
might suggest the need for future studies which can evaluate the usefulness of many
kinds of tests in Tunisia such as the writing and listening examinations. It is
important to note that this study at hand will contribute hopefully to make EFL
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 277
teachers aware of the testing process and they can then remedy the deficiency of the
reading test construction. Furthermore, the research methodology used in this study
may be exploited and developed for future similar studies.
The results of this study may serve as a basis for future research projects. In fact,
the study at hand was conducted with a limited number of teachers and students and
a limited number of reading exams. However, for future studies, a large number of
teachers, students and graded reading exams can be used to generalize the results.
Apart from that, further research is merited for investigating the impact of the
reading tests on educational system. The role of testing should be used to direct the
learning-teaching process for the benefit of not only the students and the teachers,
but also of those outside the classroom, such as the Ministry of Higher Education.
Appendix 1
Students’ Questionnaire
This questionnaire investigates the problems in reading comprehension tests
among first year English students at the tertiary level in Tunisia and tries to find
different solutions to them. This questionnaire may take you about 15 min to fill out.
The information you provide will be strictly confidential and used for academic
purposes only. Your cooperation is very important for this research project. Please,
answer truthfully.
Section A—Background Information
1. Age: 18-22
More than 22
2. Rank the skills below from the most important (1) to the least important (4)
Reading Speaking
Writing Listening
3. What is your level in the reading skill?
Poor Good
Average Excellent
Section B—Test usefulness
Part 1: Reliability
1. Did it happen to you that you got an undesirable mark in the reading test/exam
during this year?
278 Y. Mattoussi
Yes No
If Yes, this occurred because of:
Non-understanding of the text Bad emotional state
No world knowledge for the text Temporary illness
Length of the text Others (please specify)
The topic of the text was boring
2. Which testing techniques did your reading tests/exams include this year?
Multiple-choice questions True/False
questions
Gap-filling
tasks
Information transfer Paraphrasing Summarising
Guessing the meaning of unfamiliar words from
context
Identifying referents [e.g., What does the underlined word ‘it’ (line 25) refer to?]
Short-answer questions (The answers to these questions may vary from a word or
phrase to one or two sentences)
Others (please specify). . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .
3. How often did your teacher give you a detailed scoring key in the reading test
during this year?
Never 0%
ð Þ Sometimes 50%
ð Þ Always 100%
ð Þ
Rarely Often
4. State the extent to which you agree or disagree with the two following statements
[Strongly Disagree (SD); Disagree (D); Undecided (U); Agree (A); Strongly
Agree (SA)]
N
Statements SD D U A SA
1 At least, two different teachers should correct the reading test and
exam paper.
2 In the reading tests, you should become identified by number not
name, like in the final exam.
5. How often did you sit this year for a reading test/ exam in distracting conditions
(e.g., unquiet class, bad conditions of desks and chairs,)?
Never 0%
ð Þ Sometimes 50%
ð Þ Always 100%
ð Þ Rarely Often
6. How do you think of the reading tests/exams that you took this year? They are. . .
Long Neither long nor short Short
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 279
7. Did your teacher permit you to choose between tasks in the reading test/exam?
during this year?
Yes No
8. What do you think of the reading comprehension test/exam tasks that you took
this year? They are. . .
Ambiguous Unambiguous=clear
9. What do you think of the reading test/exam questions of this year? They are. . .
Clear Unclear Guided Vague
10. What do you think of the reading tests/exams that you took this year? They
are. . .
1: Well-presented : Well-typed well-reproduced
2: Badly-presented : Badly-typed poorly-reproduced
11. How many times did your teacher inform you this year about the aspects of the
reading test/exam [format and testing techniques (some of them are mentioned
in page 2)] before you took it?
0 time 2 times 4 times
1 time 3 times
Part 2: Construct validity
1. Do the reading tests/exams that you took this year test only your ability to
understand a text?
Yes No
If No, explain. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . .. . ....
2. What do you think of most of the reading test/exam questions? They are. . .
Difficult to understand
Neither difficult nor easy to understand
Easy to understand
280 Y. Mattoussi
3. Did all your class often take the same reading test/exam on the day of the
examination during this year?
Yes No
Part 3: Authenticity
1. Please, tick (√) one box in each row
N
Questions N R S O A
1 How often did the reading test/exam tasks that you took this year
represent situations, which are similar to what you will encounter in
your life?
2 How often did the text in the reading test/exam have a topic, which is
real and not imaginary?
2. How did most of the vocabulary task(s) in the reading tests/exams that you took
this year represent?
1. They represented question(s) about the meanings of words or expressions
2. They represented paragraph(s) to fill them in with given words
3. They represented question(s) about the meanings of words or expression
and paragraph(s) to fill them in with given words
3. How were the topics in the reading test/exam tasks? They were . . .
Interesting Humorous
Enjoyable Boring
Part 4: Interactiveness
1. How often did responding to the reading test/exam tasks require your topical
knowledge?
Never 0%
ð Þ Sometimes 50%
ð Þ Always 100%
ð Þ
Rarely Often
2. What did the understanding often required in the reading test/exam tasks
involve? It involves...
1. A large knowledge of the English language
2. Neither large nor small knowledge of the English language
3. A small knowledge of the English language
3. Please, tick (√) one box in each row
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 281
N
Questions N R S O A
1 How often did it happen that the reading test/exam tasks depend on
each other?
2 How often did the reading test/exam tasks cause you any emotional
threat?
Part 5: Impact
1. Have you often prepared for the reading comprehension test/sub-test and exam?
Yes No
If No, why?...................................................................................................
............................................................................................................................
2. Did your teacher always inform you this year about the aspects of the reading
test/exam [format and testing techniques (some of them are mentioned in page
2)] before you took it?
Yes No
3. Did your teacher often ask you to suggest appropriate tasks for use in a reading
test/exam?
Yes No
4. Did your teacher often ask you to suggest appropriate marking criteria for use in
a reading test/exam?
Yes No
5. How many times did you receive remarks about your strengths and weaknesses
in the two reading tests that you took this year?
0 time 2 times
1 time
Part 6: Practicality
1. Please, tick (√) one box in each row
282 Y. Mattoussi
Questions N R S O A
1 How often did you complete the reading test/exam within the fixed
time?
2 How often have the reading tests/exams been managed without
problems?
Thank you for your cooperation
Appendix 2
Teachers’ Questionnaire
This questionnaire investigates problems in reading comprehension tests among
first year English students at the tertiary level in Tunisia and tries to find different
solutions to them. This questionnaire may take you about 15 min to fill in. The
information you provide will be strictly confidential and used for academic pur-
poses only. Your cooperation is very important for this research project. Please,
answer truthfully.
Section A—Background Information
1. Name (optional): ...................................................................................................
2. Academic status: Please, tick (√) the appropriate one.
Professor Assistant professor
Secondary school teacher Other please specify
ð Þ . . . . . . . . .
Assistant
3. Teaching first year reading course:
Less than 6 years 11  15 years
6  10 years More than 15 years
4. Training:
Have you received any type of training in testing reading?
Yes No
Please explain whether you answer yes or no ..................................................
................................................................................................................................
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 283
Section B—Test usefulness
Part 1: Reliability
1. Which reading testing technique(s) do you often use in your reading tests/
exams?
Multiple-choice questions True=False questions
Short-answer questions Gap-filling tasks
Information transfer Paraphrasing
Summarizing Identifying referents
Guessing the meaning of unfamiliar words from context
Others please specify
ð Þ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . :
2. Do you often provide a detailed scoring key for the first year reading test/exam?
Yes No
3. Have you received any type of training in scoring the first year reading test/
exam?
Yes No
4. Do you often precise the acceptable responses at the outset of scoring?
Yes No
5. State the extent to which you agree or disagree with the two following statements
[Strongly Disagree (SD); Disagree (D); Undecided (U); Agree (A); Strongly
Agree (SA)]
Statements SD D U A SA
1 Each copy of a reading test and final exam should be scored by at
least two independent scorers
2 In the reading tests, first year students should become identified by
number not name, like in the final exam
6. How are your first year reading comprehension tests/exams? They are. . .
Long short Neither long nor short
284 Y. Mattoussi
7. Do you often allow your first year students to choose between tasks in a reading
examination?
Yes No
8. How often do you inform your first-year students about the reading test/exam
format before finalizing it?
Never Sometimes Always
Rarely often
Part 2: Construct validity
1. Do your first year reading test tasks only measure your students’ ability to
understand a text?
Yes No
If No, explain..........................................................................................................
2. Do you often take into consideration a given level of a students’ category in
designing the reading exam questions?
Yes No
If Yes, what level of students do you often take in mind?
The highest proficient students
The medium proficient students
The least proficient students
3. Do you often give different varieties of examinations to the first-year students on
the day of the reading test?
Yes No
Part 3: Authenticity
1. Please, tick (√) one box in each row
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 285
Questions N R S O A
1 How often do you opt for real-world first year reading exam tasks?
2 How often do you give your first-year students a text which has a real-
world topic in the reading exam?
2. What do the vocabulary task(s) that you often design in the first year reading
tests represent?
1. They represent question(s) about the meanings of words or expressions
2. They represent paragraph(s) to be filled in with given words
3. They represented questions about the meanings of words or expression and
paragraphs to be filled in with given words
Part 4: Interactiveness
1. Do your reading exams always contain some tasks which involve your first-year
students’ topical knowledge?
Yes No
2. Do you often take into consideration first year students’ characteristics (e.gs.,
age, nationality, educational background) in designing the reading exam tasks?
Yes No
If No, why?
...............................................................................................................................
What does the understanding often required in your reading test tasks involve? It
involves. . .
1. A wide range of first year students’ areas of language knowledge
2. Neither wide nor narrow range of first year students’ areas of language
knowledge
3. A narrow range of first year students’ areas of language knowledge
3. How often do you design dependent first year reading comprehension test tasks?
Never Sometimes Always
Rarely Often
Part 5: Impact
1. What do you often assess in your first year reading comprehension test?
Students’ ability to understand a text Students’ other abilities (please
specify)......................................
Students’ ability to write
286 Y. Mattoussi
2. Which type of testing do you often use in the first year reading test?
1. Direct testing: A testing method that closely matches the reading compre-
hension ability being measured
2. Indirect testing: A testing method that measures abilities related to the
reading comprehension ability being tested, rather than this ability itself
3. Direct and indirect testing
3. What is your reading test/exam based on?
Course objectives what is taught in the classroom
Course content Other please specify
ð Þ . . . . . . . . . . . . . . . . . .
4. Do you always familiarise your first-year students with the reading test/exam
format before they take it?
Yes No
Part 6: Practicality
1. Please, tick (√) one box in each row
N
Questions N R S O A
1 How often are administrative details established clearly before the
reading test/exam?
2 How often do your first-year students complete the reading test/exam
within the set time frame?
3 How often is your scoring system for the reading test/ exam feasible
in your time frame?
4 How often have the material resources (space, equipment, and
materials) been provided to the reading test/exam’s design?
5 How often have the material resources been provided to the reading
test/exam’s administration?
6 How often have the reading tests/exams been administered smoothly
(without problems)?
Thank you for your cooperation
References
Alderson, J. C., Clapham, C., & Wall, D. (1995). Language test construction and evaluation.
Cambridge: Cambridge University Press.
Bachman, L. F. (1990). Fundamental considerations of language testing. Oxford: Oxford University
Press.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice. Oxford: Oxford University
Press.
Testing Usefulness of Reading Comprehension Exams Among First Year Students. . . 287
Brown, J. D. (1996). Testing in language programs. Upper Saddle River, NJ: Prentice Hall.
Brown, J. D. (2002). The Cronbach alpha reliability estimate. Shiken: JALT Testing & Evaluation
SIG Newsletter, 6(1), 17–18.
Brown, J. D. (2003). Language assessment-principles and classroom practices. White Plains:
Longman.
Cronbach, L. J. (1970). Essentials of psychological testing (3rd ed.). New York: Harper & Row.
Davies, A. (1990). Principles of language testing. Oxford: Basil Blackwell, Ltd.
Henning, G. (1987). A guide to language testing: Development, evaluation, research. Cambridge:
Newbury House Publishers.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistic, 1(1), 19–43.
Hughes, A. (1989). Testing for language teachers. Cambridge: Cambridge University Press.
Hughes, A. (2003). Testing for language teachers (2nd ed.). Cambridge: Cambridge University
Press.
Pallant, J. (2005). SPSS survival manual: A step by step to data analysis using SPSS for Windows
(Version 12). Australia: Allen & Unwin.
Read, J., & Chapelle, C. A. (2001). A framework for second language vocabulary assessment.
Language Testing, 18(1), 1–32.
Taylor, L., & Weir, J. C. (2012). IELTS collected papers 2 research in reading and listening
assessment. Cambridge: Cambridge University Press.
288 Y. Mattoussi
Techniques in Teaching and Testing
Vocabulary for Learners of English in an EFL
Context
Imen Riahi
Abstract Acquiring an extensive vocabulary is one of the major challenges of
foreign language learners. In many cases, learners struggle with the language
because of their lack of vocabulary. This article explored vocabulary teaching
and testing techniques among Tunisian EFL secondary school teachers. It also
aimed to obtain a better understanding of the teachers’ perceptions of what tech-
niques are effective in presenting and testing unknown vocabulary and consolidat-
ing word knowledge in memory as well as the reasons for their beliefs. Analysis and
discussions were based on the data collected from three research instruments:
Questionnaires (n ¼ 200), observation and interviews (n ¼ 12). The findings
showed that using reading passage (4.07) (SD ¼ 1.18), Example sentences (4.06)
(SD ¼ 1.14), situation examples (4.03) (SD ¼ 1.27), and semantic relations (3.71)
(SD ¼ 1.11) are the most frequent and effective techniques in presenting new
words. However, for consolidating newly taught words, teachers resorted to text
book vocabulary exercises (4.04) (SD ¼ 1.01), writing task (3.98) (SD ¼ 1.25) and
classroom interaction (3.73) (SD ¼ 0.87). Besides, for assessing students’ vocab-
ulary knowledge EFL teachers tied to traditional techniques such as reading
comprehension tasks (3.85) (SD ¼ 1.27), writing tasks (3.64) (SD ¼ 1.37), gap
filling (3.60) (SD ¼ 3.05) and multiple choice (3.55) (SD ¼ 1.28). This study also
addressed a need to examine and improve current vocabulary teaching and testing
techniques of EFL teachers and suggested improvements accordingly.
Keywords Vocabulary • Vocabulary knowledge • Vocabulary teaching
techniques • Vocabulary testing techniques
I. Riahi (*)
Department of English, The University of Letters, Arts and Humanities of Manouba, Tunis,
Tunisia
e-mail: riahi.imen@hotmail.fr
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_14
289
1 Introduction
As a research area, vocabulary was neglected for years in foreign and second
language studies. Recently, the importance given to the role of vocabulary in
second and foreign language learning have grown rapidly as indicated by the
increase in the number of researches and publications related to the study of lexicon
in the context of second and foreign language studies. While Grammar was the
focal study area for second language research up to the 1980s, vocabulary “has
mushroomed enormously” (Meara, 1995, p. 11), and turned to be the center of
research in the last three decades. Although vocabulary research did not bloom until
very recently, the importance of vocabulary was recognized within the heydays of
most language teaching methods.
Vocabulary, and the assessment thereof, has indeed moved from being
peripheral to being regarded as an integral component of L2/FL proficiency
(Daller, Milton, & Treffers-Daller, 2007; Zareva, Schwanenflugel, & Nikolova,
2005). Several researchers such as Gu (2003), Laufer and Nation (1999),
Maximo (2000), and Read (2000) and others have realized the importance of
vocabulary in learning English as a second or foreign language. They have
argued that vocabulary should be considered as an integral part of learning a
language as it leads the way to communication. Learners cannot communicate in
a meaningful way without a large vocabulary. This was supported by the findings
in a study by Haastrup and Phillipson (1983), who found a connection between
communication breakdown and lexical limitations. They states that the main
reason behind communication breakdown is learner’s lexical limitations in both
reception and production. Therefore, vocabulary is a necessary ingredient for all
communication (Wallace, 1982). “No matter how well student learns grammar,
without words to express a wide range of meanings, communication in a foreign
language just cannot happen in any meaningful way” (McCarthy, 1990,
Introduction).
2 Theoretical Background
Vocabulary research has not reached a unified view on the efficiency of one
particular instruction method over another as Beck, McKeown, and Omanson
(1987) argue, “Research has provided much useful information about vocabulary
learning and instruction. What it has not provided is a simple formula for optimal
instruction, because no such formula can exist.” (p. 150) However, there is a strong
debate about whether explicit or implicit instruction should be used for effective
vocabulary learning. Many researchers (Cunningham, 2005; Hiebert & Kamil,
2005; Nagy, 2005; Skehan, 1998; Stahl, 2005) opted for a direct, systematic,
multifaceted instruction with room for multiple exposures, to promote word knowl-
edge as well as its different aspects. Teaching vocabulary explicitly involves a
290 I. Riahi
variety of techniques including word definitions, synonym pairs, word lists, word
associations, the keyword method, semantic mapping and semantic feature analysis
(Duin & Graves, 1987). Other examples of direct instructions involve the use of
realia, pictures, mimicry, explanation, and translation (Harmer, 1991). In order to
examine the most effective and used strategies in vocabulary teaching in EFL
context, Tran (2011) conducted a study on 49 Vietnamese EFL teachers. The
findings of Tran’s study revealed that the participants reported using a large number
of vocabulary learning strategies among which guessing unknown word from
context and learning new words in an English sentences were reported as being
used by the most number of participants. However, these teaching techniques were
severely criticized for the major factor that vocabulary is too broad to be covered
and learned through explicit backgrounds (Nagy, 1997). The alternative for this
explicit vocabulary instruction was to teach vocabulary implicitly. The aim of
indirect vocabulary instruction is to draw the attention of the learners to the target
words, which will be grasped “incidentally” through exposure to a variety of
contexts, reading, listening to a story being told, watching TV or taking part in a
conversation with the focus on language use instead of learning itself. In this
implicit method of vocabulary teaching, Nation and Newton (1997) point out that
“. . .the teacher needs to ensure that learners are being exposed to materials and
activities that will expand their vocabulary in useful ways” (p. 238). All in all, the
implicit instruction requires rich contexts and multiple exposures to the target
words to learn new vocabulary items (Nation, 2001). Many researchers point out
that combining explicit vocabulary teaching with incidental vocabulary learning is
the best for language learners (Folse, 2004; Graves, 2009; Nation, 2001; Schmitt,
2000). Folse (2004) states that learners need to both learn words from context
through reading, as well as get information about words in an explicit way.
According to Schmitt (2000), the best vocabulary learning program comprises of
a “proper mix of explicit teaching and activities from which incidental learning can
occur” (p. 145). Therefore, successful vocabulary teaching requires both explicit
and implicit approaches.
Under the influence of communicative approach, vocabulary items have
changed from de-contextualized tests (i.e., testing words in isolation) to contextu-
alized tests (i.e., embedding vocabulary items in reading comprehension tests).
Read (1997) contends that “vocabulary knowledge may need to be
re-conceptualized within a broader framework of communicative lexical ability”
(p. 318). He further contends that de-contextualized tests such as gap-filling, true/
false and matching will have a negative wash back effect and students will continue
studying words in isolation. Testing vocabulary, therefore, should be concerned
with the learners’ abilities in producing and understanding authentic language in
context, participating in real life situations and learning how to handle receptively
and productively longer pieces of connected language not just single sentences.
Read (2000) proposes three dimensions of vocabulary assessment to broaden the
view of what a vocabulary test is and to include a wide variety of vocabulary
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 291
measures. These three assessment dimensions are relevant to all vocabulary assess-
ments. They can be helpful to teachers in designing their own measures. The first
dimension is discrete and embedded vocabulary measures. The second is selective
and comprehensive measures. The third one is “context-independent” and “context-
dependent” vocabulary measures. All of these three dimensions imply that vocab-
ulary knowledge can be assessed separately or inclusive into the other skills
(listening, reading).
2.1 Research Problem and Rationale
This chapter was designed to investigate EFL teachers’ techniques in teaching and
testing vocabulary to EFL learners at Tunisian secondary schools. It also aims to
obtain a better understanding of the teachers’ perceptions of what techniques are
effective in presenting, consolidating, and testing vocabulary as well as the reasons
for their beliefs. Although research has emphasized that vocabulary is the skeleton
of language and its building block, teachers may not be aware of the various ways to
teach and assess vocabulary. Hence, there is a need to ask teachers as main
vocabulary resources in the EFL classroom, about their vocabulary teaching and
testing techniques. While teaching vocabulary has been discussed frequently in
academic settings, testing vocabulary has received less focus. Knowledge and
understanding of the principles and techniques of vocabulary assessment among
teachers has generally been limited. Hidri (2015) conducted a study on secondary
and university teachers’ assessment conceptions, which clearly shows that there is a
need for further development in the practice of assessment in secondary schools,
thus drawing the attention to the need of further research within the field. Addi-
tionally, in implementing a summative test for final year secondary school learners,
Siala (2010) argued that it was of interest to study assessment methods in Tunisian
secondary school. Therefore, exploring the techniques of testing vocabulary at
Tunisian secondary schools is of great importance. A major rationale behind this
study was to investigate teachers’ techniques in teaching and testing vocabulary and
to address the different assessment problems and suggest improvements accord-
ingly. It follows then that the study aimed to answer the following research
questions:
a. How do EFL teachers present new vocabulary to the students?
b. How do EFL teachers consolidate word knowledge in students’ minds?
c. What are the techniques used in assessing students’ vocabulary knowledge?
d. What are the teachers’ perceptions on the effectiveness of the techniques used
for presenting unknown vocabulary, for consolidating words in memory and for
testing such vocabulary?
292 I. Riahi
3 Method
3.1 Participants, Instruments and Procedure
This section provides a detailed account of the research design which consists of the
setting, the research participants and research instruments. Moreover, it briefly
describes the procedure and the methods of data analysis of the study.
The subjects investigated in this study were 200 Tunisian EFL teachers who
were chosen randomly from different secondary schools (n ¼ 20) to fill in the
questionnaire survey.
Due to the limited time access given by the Ministry of Education (from
20 November to 15 March) only 12 EFL teachers participated in the observation
and interview phase. They were randomly selected from the total population.
Table 1 presents the demographic features of the participants. The 200 sample,
whose participation rate ranged from 6.5 to 12.5%, presented 43.5% of females and
56.5% males with 20% of teachers having a teaching experience that ranged from
16 to 20 years and 6% for teachers who had a teaching experience from 1 to 5 with a
mean of 4.07.
The data collection was composed of three instruments, namely questionnaire
(n ¼ 200), semi-structured interviews and classroom observation (n ¼ 12). In order
to collect data to explore teachers’ practices while teaching vocabulary in English
class, a total of 50 items having both close-ended and open-ended items were
designed for sample teachers. The first four items inquired teachers’ personal
back ground. The second part of the questionnaire included 18 items which were
related to vocabulary teaching techniques for presenting unfamiliar words. The
third part of the questionnaire consisted of 18 items was about techniques for
consolidating word knowledge. The fourth which has 14 items asks teachers
about the techniques of testing students’ vocabulary knowledge. Except the back-
ground of the teacher (4 items), the rest three parts all were based on five point
Likert Scale. Classroom observation was used to identify and describe the tech-
niques used by EFL teachers in presenting and consolidating vocabulary in every
grade in secondary school. A semi-structured interview, as well has its special role
to knowledge as it was conducted in the attempt to obtain additional supplementary
data on the teachers’ perceptions of what techniques are effective in presenting,
consolidating and testing vocabulary knowledge. In light of this, six items of
interview questions were prepared for English teachers.
The procedures of collecting data took nearly 4 months (from 15 November
2014 to March 2015) and about a month to analyze. The questionnaire was
distributed to 200 EFL teachers. The teachers were asked to fill the questionnaire
by putting a tick beside the response they consider appropriate. After administering
the questionnaire, twelve teachers were randomly chosen from the total population
to participate in the observation and interview phase. The observation took place in
seven secondary schools. For each school one or two English teachers were chosen
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 293
randomly to participate in the observation. The information of each participant is
described briefly in the following table 2.
A field note was made in every meeting on whatever happened during teaching
vocabulary including the teachers’ techniques in presenting and consolidating
vocabulary as well as the number of times these techniques were used, without
intervening by any means in the teaching materials or the style of the teachers.
Teachers were not informed beforehand of the available vocabulary teaching
techniques so that observation shows how they normally teach vocabulary in the
classroom. But due to the time constraints and work load of the researcher, each
English teacher was visited ten sessions. Coming to the third phase is a short
interview with twelve teachers who were observed in the second phase. Each
participant was then interviewed separately for about 45 min to investigate their
views on vocabulary teaching and testing techniques and the reasons for adopting
Table 2 Demographic features of the observed participants (n ¼ 12)
Teachers Gender No. of teaching experience Grade taught Region
Teacher 1 Male 10 4th grade Nouvelle Madina
Teacher 2 Male 5 1st grade Megrine
Teacher 3 Female 2 3rd grade Ezzahra
Teacher 4 Female 20 4th grade Rades
Teacher 5 Female 30 2nd grade Hammalif
Teacher 6 Female 3 2nd grade Mourouj
Teacher 7 Male 1 1st grade Megrine
Teacher 8 Female 15 3rd grade Ezzahra
Teacher 9 Male 8 2nd grade Rades
Teacher 10 Male 35 1st grade Hamemlif
Teacher 11 Female 4 4th grade Mourouj
Teacher 12 Female 25 3rd grade Nouvelle Madina
Table 1 Demographic
features of the participants
(n ¼ 200)
Features
Secondary school teachers
Total %
Gender
Female 87 43.5
Male 113 56.5
Teaching experience
1–5 13 6.5
6–10 32 16.0
11–15 38 19.0
16–20 40 20.0
21–25 24 12.0
26–30 28 14.0
More than 30 25 12.5
Mean 4.07
294 I. Riahi
certain techniques. In order to ensure spontaneity of response, the interviewed
teachers were not given the questions in advance.
3.2 Data Analysis
In this study, data analyses were carried out in four phases. The first phase was the
questionnaire survey, which were collected, analyzed and summarized at the very
first stage using Statistical Package for the Social Science (SPSS 17.0) software
analysis. The data were statistically described in terms of standard deviation and
mean. The second phase was the observation. It was analyzed by assembling the
different techniques into a table, adding up the number of times each technique was
employed in order to identify what techniques for presenting and consolidating
vocabulary were used most often. The interviews, on the other hand, were summa-
rized and analyzed thematically to answer the fourth research question.
An explanation of the methodology adapted in the current study was presented in
Chapter “Investigating the Use of an Empirically Derived, Binary-Choice and
Boundary-Definition (EBB) Scale for the Assessment of English Language Spoken
Proficiency” to facilitate the interpretation of the results and subsequent conclu-
sions to be treated in Chapter “Developing a Formative Assessment Instrument for
an In-service Speaking Course”. This chapter’s explanation detailed the nature of
the study’s participants, instruments, procedures, and the ensuing statistical ana-
lyses to be performed. Chapter “The Measurement of Language Ability and
Impairment in Arabic-Speaking Children” will systematically answer each of the
four research questions through the statistical analyses performed on the data.
4 Results and Discussion
This chapter presents the findings that arise from the data collected to answer the
intended research questions. First, questionnaire data was presented and analyzed to
answer the first three research questions. Second, classroom observation gave a
description of the observed techniques that the participating teachers appeared to
employ in presenting and consolidating unfamiliar words in their classrooms.
Finally, teachers’ perceptions of what techniques are effective in presenting, testing
and consolidating words in memory as well the reason for their beliefs were
answered through interviews.
The interpretation of the questionnaire results is divided into three parts based on
the research questions. The first part will include techniques secondary school
teachers use in presenting new vocabulary. The second one will demonstrate
techniques English teachers use in consolidating newly learned words. Finally,
the techniques used in assessing students’ vocabulary knowledge will be
highlighted in the third section.
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 295
There are several techniques that have been agreed on to be mostly used in
teaching new vocabulary in English language classroom. More or less, all tech-
niques above were put into teachers’ lesson plans. However, the frequency of
tending to use each one varies quite clearly. While some techniques are mentioned
to be used frequently, some techniques are explained to be rarely utilized. Regard-
ing the techniques teachers use in teaching vocabulary in secondary school, Table 3
reveals that there are four techniques that English teachers frequently use to present
unfamiliar words. For instance, Table 3 demonstrates that many English teachers
use reading to teach new words with a high mean (4.07) (SD ¼ 1.18). Similarly,
teachers mark that using example sentences to explain new words as a useful
technique with a mean of (4.06) (SD ¼ 1.14). Getting the students to know the
semantic relations of unfamiliar words is another favorable technique that teachers
marked as frequently used (4.03) (SD¼1.27). Another common technique that
teachers agreed on is using situation examples (3.71) (SD ¼ 1.11). However,
some techniques are used rarely in teaching new vocabulary as perceived by EFL
teachers. For example, English teachers do not use mother tongue in presenting new
words or ask student to look up unfamiliar words in English–English dictionary as
the mean show in Table 3 which range between (1.90 and 1.93) with standard
deviation of (0.83–1.01). Also, using songs in teaching vocabulary was rarely
utilized by teachers (2.19) (SD ¼ 1.03). Moreover, Table 3 indicates that few
teachers encourage autonomy in learners (2.17) (SD ¼ 1.18).
In fact, English teachers employ certain techniques for consolidating newly
learnt words. In order to address this issue, it is evident from Table 4 that textbook
Table 3 Techniques of presenting new words
Techniques of presenting new
vocabulary N Minimum Maximum Mean
Std.
deviation
Dictionary 200 1.00 5.00 1.93 0.83
Mother tongue 200 1.00 5.00 1.90 1.01
Word formation 200 1.00 5,00 3.50 1.27
Reading passage 200 1.00 5.00 4.07 1.18
Semantic relations 200 1.00 5.00 4.03 1.27
Pronunciation and spelling 200 1.00 5.00 3.54 1.34
Word parts 200 1.00 5.00 3.23 1.34
Definition 200 1.00 5.00 3.08 1.43
Example sentences 200 2.00 5.00 4.06 1.14
Multiple contexts 200 1.00 5.00 3.53 1.22
Pictures/real objects 183 1.00 5.00 3.03 1.17
Facial expression 200 1.00 5.00 2.67 1.25
Multimedia 200 1.00 5.00 3.00 1.27
Situation examples 200 1.00 5.00 3.71 1.11
Collocation/idioms/phrasal verbs 200 1.00 5.00 2.86 1.36
Encourage autonomy 200 1.00 5.00 2.17 1.18
Valid N (listwise) 183
296 I. Riahi
vocabulary exercises is the most highly appreciated instruments by teachers in
practicing the newly learnt words scoring a high mean (4.04) (SD ¼ 1.01). Another
technique which is frequently used by English teachers is writing tasks (3.98)
(SD ¼ 1.25). In order to emphasize the newly learned words, teachers also use
classroom interaction (3.73) (SD ¼ 0.87). Moreover, the table shows another
preferred technique for consolidating newly taught vocabulary which is brainstorm-
ing with a mean of (3.63) (SD ¼ 1.30). On the other hand, few teachers use dictation
regularly to dictate newly learnt words (2.38) (SD ¼ 1.17). It is clear that using
vocabulary exercises and word list are less preferred techniques for consolidating
newly taught vocabulary with mean of ( 2.75 and 2.57 ) (SD ¼ 1.36 and 1.24).
Assessment techniques are one of learning teaching experience. Assessment is
considered to be an essential tool to check students’ understanding and knowledge
acquisition. Table 5 presents teachers’ perception regarding the techniques used in
assessing vocabulary among secondary school teachers. It is highly obvious that
teachers use reading comprehension tasks in order to assess vocabulary (3.85)
(SD ¼ 1.27). Another technique which is highly appreciated by EFL teachers is
writing tasks (3.64) (SD ¼ 1.37). Likely, as the mean shows in Table 4 English
teachers use fill in the blanks frequently as a way of assessing students’ vocabulary
knowledge (3.60) (SD ¼ 3.05). Besides, another preferred technique for assessing
vocabulary knowledge is multiple choice with mean of (3.50) (SD ¼ 1.28).
However, Table 5 indicates that few English teachers do not emphasize the use of
speaking task (project work, role plays, interviews) in assessing vocabulary knowl-
edge (2.25) (SD ¼ 1.04).
In addition to identifying participants’ perceptions of teaching techniques, the
researcher was also interested in the specific vocabulary teaching techniques that
were being used during a lesson. The following table illustrates the overall
Table 4 Techniques of consolidating learnt words
Techniques of consolidating new
vocabulary N Minimum Maximum Mean
Std.
deviation
Regular dictation 200 1.00 5.00 2.38 1.17
Spelling and pronunciation 200 1.00 5.00 3.04 1.11
Classroom interaction 200 1.00 5.00 3.73 0.87
Reading 200 1.00 5.00 3.59 1.25
Listening 200 1.00 5.00 3.17 1.18
Writing 200 1.00 5.00 3.98 1.25
Role play, presentation, discussion,
project work, etc
200 2.00 5.00 4.04 1.01
Vocabulary notebook 200 1.00 5.00 3.00 1.10
Word list 200 1.00 5.00 2.57 1.24
Games 200 1.00 5.00 3.12 1.04
Brainstorming 200 1.00 5.00 3.63 1.30
Songs 194 1.00 5.00 2.19 1.03
Textbook vocabulary exercises 200 1.00 5.00 4.04 1.01
Valid N (listwise) 200
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 297
techniques that participants were observed using, by the researcher, during the
lesson observation. Each observed teacher has been given a pseudonym.
Table 6 shows the frequency of occurrence of each vocabulary teaching tech-
nique used by the observed teachers. As depicted in Table 5, the data obtained from
classroom observation revealed that in all observed classes teachers were interested
in using contextual strategy of teaching vocabulary. Regardless of whether any type
of information was presented about a word, contextual information was provided.
The most common form of contextual information consisted of example sentences.
Teachers frequently present the target word in example sentences (90 times). The
next most common form of contextual instruction consisted of a passage provided
the context for a target word (85 times). This was most often the case when students
were reading a text and reading was interrupted to teach a new word. In such cases,
the target word has been encountered within the context of the text. Similarly, the
use of situation Examples were also highly used tool for presenting words’ mean-
ings. It occurred 58 times. The following Excerpt is an example of situation
example used by teacher 5 in explaining the difference between “good at” and
“skilled”:
T: I’m an English teacher. Should I be skilled or good at English?
S: Silent
T: I must be skilled at English. To be skilled at a language you must know all the skills and
master all the rules in order to be able to teach. However, when you go to the Souk you find
traders who speak Spanish, English, Italian with tourists (teacher acted like a trader “come,
come, look, look, nice, nice, expensive not”) did you understand the message of the trader?
S: Yes
T: They succeed in communicating, in passing their messages but are these traders skilled at
English? Can they teach English?
S: No they can’t because they have to be skilled to teach a language.
Provision of definitional information was another popular type of instruction.
The most common form of definitional instruction consisted of a definition pro-
vided by the teacher (54 times). After initiating the dialogue by asking “Who knows
Table 5 Techniques of testing vocabulary knowledge
Techniques of testing vocabulary N Minimum Maximum Mean Std. deviation
Multiple choice 200 1.00 5.00 3.50 1.28
Definition 200 1.00 5.00 2.94 1.29
Word parts 200 1.00 5.00 2.30 1.36
Fill in the blanks 200 2.00 5.00 3.60 1.05
Odd one out 200 1.00 5.00 3.13 1.28
Word formation 200 1.00 5.00 3.28 1.25
Matching words with meaning 200 1.00 5.00 2.60 1.31
Reading comprehension 200 1.00 5.00 3.85 1.27
Writing 200 1.00 5.00 3.64 1.37
Listening 200 1.00 5.00 3.38 1.20
Role plays/interviews/project work 200 1.00 5.00 2.25 1.04
Synonyms/antonyms 200 1.00 5.00 3.18 1.33
Valid N (listwise) 200
298 I. Riahi
what [target word] means?” it was most common for the teacher to tell the
definition of the target word. Teachers always attempted to frame their definitions
in language that students could understand. Consider the following example:
T: What is the meaning of desperate?
S: (. . .) (Silent)
T: desperate means feeling sad or upset because of having little or no hope
Definitions were often presented in combination with other types of information
about the target word such as a sentence context, an example, or information
about a relevant affix. The following example illustrates this type of strategy
combination. In this example, the teacher defines the word using words that
students understand. Then, she concludes by giving an example to create a full
picture of what the word means.
T: What is the meaning of penalty?
S: (. . .) (Silent)
T: Penalty is a punishment for doing something against law. For Example, you need to pay
15D if you throw rubbish on the street. 15D is the penalty for throwing rubbish on the street.
As depicted in Table 6, structural and organizational/semantic instructions were
observed less frequently than either definitional or contextual forms of instruction.
Organizational/semantic information was provided about a word, it was usually in
the form of synonyms and antonyms, collocation, idioms and phrasal verbs. Syn-
onyms and antonyms occurred quite rarely without the addition of either defini-
tional or contextual information. Word formation (20 times), collocation, idioms
and phrasal verbs (11 times) were also rarely observed during the observation
sessions.
Table 6 Teachers’ practice of vocabulary teaching techniques in EFL classes
Techniques observed
Teachers MT ED SR P/S WF ES A RP SE M C/I/PV
T1 1 3 4 14 8 6 3 9
T2 5 6 1 3 10 3 7 5 5
T3 4 5 3 14 1 12 1
T4 1 5 3 2 2 6 9 1
T5 1 5 6 5 2 1 5 2 2 1
T6 4 8 3 10 1 4 3
T7 2 5 6 9 8 1 4 7 1
T8 5 6 1 5 9 5 6 5
T9 5 3 9 6 7 3
T10 1 6 5 6 2 10 9 1
T11 5 4 9 2 2 9 10 1
T12 2 3 2 10 8 9 2
Total 20 54 50 35 20 90 29 85 56 13 11
Key: MT mother tongue; ED English definition; SR semantic relations; P/S pronunciation/spelling;
WF word formation; ES example sentences; P/S prefix and suffix; RP reading passage; SE situation
examples
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 299
A close inspection of the use of structural information, as depicted in Table 6,
reveals that it revolved around roots and affixes (29 times), word parts (20 times)
and pronunciation and spelling (30 times). Instruction involving roots and affixes
commonly included the teacher naming an affix and defining it and providing
examples of words in which the affix is found. This type of instruction was usually
accompanied by other types of information such as definitions and contexts. For
example, in teaching the word “transplant”, teacher 5 introduced the meaning of the
prefix “trans” (across; beyond; into another place or state to the students), then, she
asked her students to tell the meanings of the following words with a given context:
Transport, transform, translate, transfer, and transparent. However, pronunciation/
spelling (30 times), were occasionally used by English teachers during the whole
observation. Most of the observed teachers used the same technique in introducing
new words’ pronunciation and spelling. For instance, they usually wrote down the
spelling of the new word on the blackboard and cut it into syllables then they
transcribed it and asked the students to read after them in order to remember the
pronunciation.
Table 7 illustrates the frequency of occurrence of each technique observed in
consolidating learnt words in students’ minds. As shown in Table 7, the use of
vocabulary exercises, learnt words and writing tasks were the most used instru-
ments by teachers in consolidating vocabulary knowledge. The use of vocabulary
exercises seems to be the highest preferable technique among the observed
teachers. It occurred 73 times. All observed teachers used students’ book activities
to provide opportunities for students to practice and review the learnt words.
Students were asked to match words with their definitions or synonyms, fill in a
paragraph with words from the box, guess the meaning of the words from their
definitions, paraphrase expressions using their own words, etc. The second most
Table 7 Techniques
observed in consolidating
new words
Techniques used
Teachers W RP I PW VE G B
T1 5 1 5 10 1 2
T2 8 1 8 1 9 3 5
T3 5 7 5 2 3 6 3
T4 2 3 6 2
T5 4 9 9 6
T6 7 1 8 8
T7 1 2 4 3
T8 3 1 8 5 2 9
T9 9 5 3 2 4
T10 6 10 4 2
T11 5 2 7 4 2
T12 4 5 5
Total 60 16 65 5 73 20 49
Key: W writing exercises; RP role play; I interaction with the
teacher; PW project work; VE vocabulary exercises; G games;
B brainstorming
300 I. Riahi
used technique is interacting with students using the learnt words. This technique
occurred 65 times which is a high score comparing to the other techniques. EFL
teachers usually ended their lessons by an interaction with students to review and
practice the newly learnt words. The following excerpts are examples of interac-
tions occurred with teachers 1 and 2 (the underlined words are newly learnt):
Excerpt 1
An interaction about family relationship with the 1st grade students:
T: How is your relationship with your family?
S1: I don’t have a good relationship with my parents because I don’t agree with them I
belong to different generation.
T: What are the matters that you disagree on?
S1: Internet
T: So you are always quarrelling with your parents about the net.
S1: Yes
S2: I have a good relationship with my parents because they give me everything I want
T: So your parents spoil you.
S2: Yes
T: To whom you resort when you have a problem?
S: Whenever I have a problem I resort to my mother, she advises me.
S: Whenever I have a problem I resort to my sister.
T: You have an understanding mother; you get on well with her
S: Yes
T: Do your parents treat you like mature as an adult or immature as a child?
S: My parents treat me like mature.
T: Are your parents pushy? Do they force you to study?
S: Yes they are very pushy.
Excerpt 2
The following interaction was done with the 4th grade students:
T: What is the reverse brain drain?
S: People are coming back to their home countries
T: What is the purpose of their return?
S: To contribute to the economic growth of their country
T: Which job do they take?
S: Key position
T: Yes or (. . .)
S: High job.
T: Give me examples of key positions?
S: Headmasters, business men, ministries
T: Who are given key position?
S: Skilled people
T: Do they get high salary?
S: Yes
T: Give me the synonym of the word salary.
S: Wage
Furthermore, as can be seen from Table 7 that the frequency of occurrence of
writing tasks is higher either. Many observed teachers preferred to end their lessons
with a writing task, which requires the students to use the newly learnt words. This
technique was used 60 times during the whole observation. Students were asked to
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 301
write letters, paragraphs, short stories, interviews or articles about different topics.
Another popular technique teachers used to check vocabulary understanding was
Brainstorming. It occurred 49 times. This is done by making an arrangement of
words into a picture, which has a key concept at the center and related words and
concepts linked with the key concept. Most observed teachers usually started their
lessons with brainstorming technique in which they triggered their students’ prior
knowledge and make them use it in the new lesson’s content. This may have
provided learners with the opportunity to re-hear words and possibly helped with
the retention of these words in their long-term memory. However, role plays
(16 times), project work (5 times), and games (20 times) were not highly used by
observed teachers as their number of occurrences show.
This section addresses 12 teachers’ perceptions of what techniques are effective
in teaching, consolidating and testing new vocabulary as well as the reasons for
their beliefs. At the end of the interview, teachers share other interesting and useful
vocabulary activities designed by them to teach vocabulary to students.
Question 1
Most of interviewed teachers agreed on the importance of contextual strategy in
presenting unfamiliar words. According to their response, they said that practicing
contextual strategy is helpful for students and for teachers since they can practice it
from their experiences and it is easy to practice in the class. It also helps students to
get a better understanding of words and to know how to use the new word in
different contexts. Interviewed teachers mentioned the use of different sentences,
explanations, contextual examples, synonyms and antonyms, and definition in
presenting new vocabulary as they believe that these techniques help students to
understand different meanings and remember words better. Teacher 5 said: “If my
students really failed to get the meaning by their own, extra examples would be
given to them to guess the meaning instead of giving them the meaning instantly”.
Both teacher 1 and 12 resorted to L1 translation technique in presenting unfamiliar
words. According to them, translation is quite an instinctual technique that is, when
there is no other way to explain the new word and the students show no signs of
understanding, the last resort was translating the new vocabulary into Arabic.
Question 2
Regarding consolidating the newly learned vocabulary, teachers find it useful to
combine certain useful techniques such as using the learnt words in their conver-
sations, checking pronunciation, worksheet exercises, and emphasizing writing
tasks. As quoted by one teacher: “Studying the spelling and pronunciation of a
word with students frequently as well as using vocabulary exercises in workbooks,
worksheets and textbooks could check their understanding and more importantly to
check if they worked hard enough to study the words after class.” In addition, most
of the English teachers have also stressed the importance of using songs to consol-
idate the word meaning in their students’ minds. They believed that using songs is
useful and successful because students would be more motivated. Another tech-
nique teachers used to check vocabulary understanding was to review and recycle
previously discussed vocabulary associated with the Unit of Inquiry at the
302 I. Riahi
beginning of a lesson. They said that this may have provided learners with the
opportunity to re-hear words and possibly helped with the retention of these words
in their long-term memory. As quoted by teacher 4: “At the beginning of each
lesson, I usually asked the students to recall what they have learnt in the previous
lessons as this technique is a good opportunity for less able students and for those
who do not do revision at home to listen to the key words again, so I try to repeat the
learnt words frequently at the beginning or during the lessons.”
Furthermore, teachers 4, 3, 6, and 11 mentioned that using games like word
search or puzzle are very useful to make students discover the meaning themselves
and help them to recall the learnt words. Besides, only teachers 12 and 8 who liked
to get their students to learn new vocabulary through presentations in front of the
class as doing presentation could make sure they knew how to use the learnt words
and could boost their confidence in speaking English.
Question 3
There is no doubt that assessment is essential in any learning process. Specifically,
in the context of second language teaching and learning, assessing the newly
learned word is essential. Stressing assessment process, all the teachers being
interviewed agreed on the importance of assessing vocabulary. Most of them do
it on a regular basis, while some of them integrate vocabulary assessment in their
daily instruction Most of the results from the interviews show that teachers assess
vocabulary by checking to see if student are using the new learnt vocabulary in their
writing task or through answering the reading comprehension passage. Teachers
claimed that students are usually asked to write a paragraph about a particular topic
taken from their textbook units. They state that they also assess vocabulary in
context and this is done in a number of ways like guessing the meaning of a word in
a text, looking for synonyms or antonyms in the passage or finding words that
describe feeling or character.
Another technique that most English teachers agree on is using sentence com-
pletion or filling gaps, which are based on context, that is, the context of the
paragraph or the sentence that enable learners to find the right word easily.
Moreover, all participants stated that the four skills should be integrated in
assessing vocabulary knowledge. For instance, students have a listening test, in
which they listen to an audio recording of a spoken interaction and then indepen-
dently responded to a set of questions (w/h or yes/no questions see). They are also
supposed to have reading comprehension and writing task. Concerning assessing
vocabulary in speaking, some interviewed teachers suggested the use of project
work, role play and presentations in which they assess their students’ abilities in
expressing themselves.
Question 4
At the end of the interview, teachers shared interesting vocabulary activities to
present, consolidate and assess new vocabulary. Concerning presenting new words,
most of the teachers suggested the use of games, songs and videos. Besides, teacher
8 said that instead of explaining the words or translating them for students, she asks
somebody in class to clarify meaning (either explain or translate) for the other ones
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 303
because learning from classmates is more effective than from teachers to some
extent. Students may understand what his/her friends need or how they want the
words to be explained better than teachers do.
For consolidating vocabulary knowledge, all interviewed teachers suggested the
use of role-plays, drama, group discussions, interacting with teacher, as well as
watching videos about different topics. Teacher 6 suggested a very useful and
motivated technique in the writing process which is called “the gallery technique”.
He divided the class into two groups then he distributed white papers for the group
who agree with the topic and yellow papers for those who disagree. Each group
wrote his ideas on the paper then they stuck them on the classroom’s wall. Teacher
6 said that this technique is called the gallery technique because students would
walk around the classroom and choose ideas and use them in their writings.
According to him, this technique is very useful because everybody take part in
the process of writing.
For assessing students’ vocabulary knowledge, all interviewed teachers showed
their desire to implement new techniques in assessing their students but they said
that the Ministry of Education recommends particular tasks that they have to follow
and they are reluctant to apply new tasks believing it is illegal to implement new
testing techniques that the Ministry of Education has not officially agreed on yet.
For this reason, they have to follow a certain model in assessing their students and
therefore they do not have the freedom to create new techniques.
After analyzing the data collected from three research instruments; the ques-
tionnaire, the observation and the interview, it was found out that showing words in
context (i.e., reading passage), giving example sentences, semantic relations and
situation examples are the most frequently used techniques in presenting unfamiliar
words to EFL learners at secondary schools.
Regarding consolidation, most teachers resorted to textbook vocabulary exer-
cises, writing task and classroom interaction. For assessing students’ vocabulary
knowledge, three main assessment techniques seem to be preferred by English
teachers; reading comprehension tasks, writing tasks, gap filling and multiple
choice.
This study revealed that EFL teachers at secondary schools very often employed
direct instruction to teach vocabulary. Providing definitions, example sentences,
synonyms and antonyms were reported to be the highly frequently used activities,
and these activities characterize explicit vocabulary teaching (Duin & Graves,
1987; Nation, 2000; Oxford & Scarcella, 1994; Schmitt, 2000; Schmitt & Carter,
2000). Likewise, writing activities and using context to teach vocabulary were also
significantly reported to be used frequently reflecting explicit vocabulary teaching
(Nagy, 1997). This finding is commensurate with Nelson (2008) study which
examined teachers’ perceptions of their teaching the meaning of vocabulary
word. 15 teachers were selected for interview. The participants mentioned that
direct vocabulary instruction is the secret behind students’ acquisition. They agreed
that the most frequently used approach for teaching words was through the use of
definitions. Furthermore, they find that combining methods of instruction is an
effective way in teaching new words. Not only definitions but also context clues,
304 I. Riahi
word parts, synonyms and antonym should be utilized in teaching vocabulary.
These techniques, however, do not cover all the aspects that are involved in
knowing a word. knowing a word involves not just the ability to know the word
or to match it with its definition, synonyms or antonyms, or put it into context but it
also involves being able to use the word communicatively in any of the four main
language skills. Stahl (2005) stated: “vocabulary knowledge is knowledge; the
knowledge of a word not only implies a definition but also implies how word fits
into the worlds” (p. 15). It is important to create a real life situation in the classroom
communication as it gives students an opportunity to communicate in different
social contexts and in different social roles. In their article, Johnson and Rasmussen
(1998) mentioned six features of effective vocabulary instruction. First, students
should have multiple exposures to new words. Second, new words should be taught
in a meaningful context. Third, when teacher present new words they should help
students activate their own experience and prior knowledge. Fourth, there should be
a relationship established between new words and known words. Fifth, learners
should know how to use context clues and dictionaries to enhance their word
knowledge. Finally, students should be taught to interact with words so deep
processing can occur. Certain features of effective instruction were absent in this
study. For instance, multiple exposures to target words of the type recommended in
the literature was not observed. A second exposure to target words was almost
guaranteed due to workbook activities. Opportunities for practice beyond workbook
activities, however, were rarely observed. When they were, they were typically
textbook vocabulary exercise such as matching words to their definition, synonyms
or antonyms and fill in the blank. These activities can be disadvantageous to students
learning because they rely upon memorization rather than personal connection or
active involvement with words and they manipulate only meaning and/or form, and
thus call for relatively shallow mental processing. In short, the exposures did not
foster deep processing and did not seem to constitute meaningful practice. Schmitt
and Carter (2000) further claim that “Due to the incremental nature of vocabulary
acquisition, repeated exposures are necessary to consolidate a new word in the
learner’s mind”. (p. 4) Furthermore, English teachers rarely teach vocabulary
learning techniques. Dictionary skills especially are never taught and students are
not encourage to keep vocabulary notebooks (3.00) (SD ¼ 1.03). Besides, encour-
aging learners’ autonomy (2.17) (SD ¼ 1.18) was not highly appreciated by teachers
as well as teaching word parts (3.23) (SD ¼ 1.34), collocation, idioms and phrasal
verbs (2.86) (SD ¼ 1.36). Teaching vocabulary learning was not included in the
scheme work of the observed teachers. It has been suggested that teaching vocab-
ulary should not only consist of teaching specific words but also aim at equipping
learners with strategies necessary to expand their vocabulary knowledge (Nation,
2001).
The use of multimedia and especially computer is a new trend in language
teaching all over the world. Numerous research studies suggesting that various
forms of multimedia may provide an environment that fosters the learning of
foreign language vocabulary (Al-Seghayer, 2001; Chun & Plass, 1996; Hulstijn,
2000; Laufer & Hill 2000; Lyman-Hager & Davis, 1996; Lyman-Hager, Davis,
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 305
Burnett, & Chennault, 1993). According to The National Reading Panel (2000), a
number of studies have established that the use of computers in vocabulary instruc-
tion is more effective than traditional methods. In the interview, almost all teachers
agreed on the importance of using multimedia as it motivates students to participate
in the learning activity and increase the interest of learning vocabulary. However,
observational data did not show any evidence of teachers’ use of visual aids.
Questionnaire and interview results revealed that EFL teachers still try to assess
vocabulary, mostly at the end of a unit, using a fill-in-the-blank task, multiple
choice writing task or reading comprehension. These modes of vocabulary assess-
ment are shallow metrics of possible word knowledge (Stahl & Bravo, 2010).
Recent research has shown that vocabulary testing should move away from
multiple-choice, gap-filling and matching testing toward a more communicative
approach. Today’s assessment is not designed to assess whether the learner knows
one correct meaning of a word but rather on “tasks simulating communication
activities that the learners are likely to be engaged in outside the classroom.”
(Read, 2000, p. 5) For instance, the use of project work, role plays, presentation,
discussion, picture elicitation, interviews can provide the opportunities for students
to express themselves and to use vocabulary in a communicative way. These
techniques, however, were not highly appreciated by teachers. This finding is
commensurate with the study of Hidri (2014) which showed that secondary school
assessment was generally used in a summative way which fails to provide what
learners can do with language (Richards & Renandya, 2002).
5 Conclusion
There are two limitations to this study. First, due to time limit access (from
20 November to 15 April) a small sample of EFL teachers participated in this
study. Only twelve English teachers were observed and interviewed and 200 EFL
teachers responded to the questionnaire. Second, the number of observations
undertaken during the course of the study was limited to ten classroom observations
per participating teacher. Ten observations may not be enough to reflect the full
extent of the different participants’ vocabulary teaching styles and techniques in
teaching vocabulary. The research is aware of the fact that the result of this study
may not be generalizable to a large population because of the limited number of
subjects participating in this study. Besides, in spite of its limitation, it is hoped that
the result of this study could be a starting point for more thorough investigations on
teaching and testing vocabulary in an EFL context.
The findings in this study have suggested some implications. First, there is a vital
requirement for teachers of English to seek new and innovative techniques to
present and consolidate new vocabulary. When vocabulary teaching instruction is
limited to traditional techniques like writing the word on the board, using L1
translation to explain unfamiliar words, more use of semantic relation [i.e., syno-
nyms, antonyms, etc. (. . .)], correcting the spelling and pronunciation of the new
306 I. Riahi
word, these techniques may result in students’ off-task behavior because of bore-
dom. In addition, observational data did not show evidence of teachers’ use of
visual aids, especially head-projectors, songs, videos, word games to present and
practice the newly taught vocabulary. It is advisable for EFL teachers to use these
materials in teaching vocabulary as much as possible because they provide effective
and memorable learning and they help learners to guess the meaning of unknown
words. Moreover, secondary EFL teachers should pay more attention to the needs
of their students and their background in general. Students’ interest, level, and
difficulty have considerable impact on the effectiveness of the teaching techniques.
Having understood those factors, teachers can have more reliable criteria to base on
so that to prepare and perform better. Furthermore, it is worthwhile to teach
vocabulary learning techniques to students, especially when they are weak in
vocabulary and only know limited words. Therefore, Focus should be put on
teaching students the skills of guessing from contexts in order to prepare them for
the assessment. When students have the opportunities to learn vocabulary learning
techniques, they can grasp the skills to learn vocabulary well and improve their
English proficiency. These techniques are helpful in learning vocabulary.
An eventual recommendation concerns teacher development training. As suc-
cessful learning depends a great deal on effective teaching, the Tunisian Ministry of
Education is well-advised to conduct workshops and seminars for EFL secondary
school teachers aimed at enlightening the teachers about effective techniques of
vocabulary instruction. The Ministry of Education should also give its EFL sec-
ondary school teachers more freedom in implementing new techniques for
assessing vocabulary knowledge.
On the basis of the findings and limitations of the study, the following sugges-
tions are made for further research. Since this study was just carried out on a small
number of teachers with a period of nearly 3 months, it would have been better if
further researches had been done on a large number of participants for a longer
time. As there is not much study on vocabulary teaching and testing techniques in
Tunisian secondary schools up until now, this study is the elementary step to
explore how Tunisian secondary school teachers present, consolidate and assess
vocabulary knowledge at secondary schools. Researchers interested in conducting
further studies regarding vocabulary teaching and testing techniques in EFL class-
rooms could involve a larger sample size of the observed participants in order to
make a more reliable generalization and design more longitudinal (case or general)
studies in which participants are observed over the course of at least on academic
year rather than one semester. Moreover, the results, discussed in this study, have
prompted many questions that are interesting subjects for further studies within the
field. The first is the need for further investigation of English test vocabulary
teaching and assessing in Tunisian context. Future research could focus on stu-
dents’ perceptions of what techniques are effective in teaching vocabulary. There-
fore, by considering the importance of formative assessment in ESL/EFL teaching
and learning, the difficulties and complexities teachers face in applying formative
assessment in their classes, and lack of studies on this area, scholars should also
focus their attention to the problems of making classroom assessment as formative
as possible and suggesting ways to improve classroom assessment.
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 307
References
Al-Seghayer, K. (2001). The effect of multimedia annotation modes on L2 vocabulary acquisition:
A comparative study. Language Learning & Technology, 5(1), 202–232 A.
Beck, I., McKeown, M. G., & Omanson, R. C. (1987). The effects and uses of diverse vocabulary
instructional techniques. In M. G. McKeown & M. E. Curtis (Eds.), The nature of vocabulary
acquisition (pp. 147–163). Hillsdale, NJ: Erlbaum.
Chun, D. M., & Plass, J. L. (1996). Effects of multimedia annotations on vocabulary acquisition.
The Modern Language Journal, 80(2), 183–198.
Cunningham, A. E. (2005). Vocabulary growth through independent reading and reading aloud to
children. In E. H. Hiebert & M. L. Kamil (Eds.), Teaching and learning vocabulary: Bringing
research to practice. Mahwah, NJ: Erlbaum.
Daller, H., Milton, J., & Treffers-Daller, J. (2007). Editors’ introduction: Conventions, terminol-
ogy and an overview of the book. In H. Daller, J. Milton, & J. Treffers Daller (Eds.), Modelling
and assessing vocabulary knowledge (pp. 1–32). Cambridge: Cambridge University Press.
Duin, A. H., & Graves, M. F. (1987). Intensive vocabulary instruction as a prewriting technique.
Reading Research Quarterly, 22(3), 311–330.
Folse, K. (2004). Vocabulary myths: Applying second language research to classroom teaching.
Ann Arbor, MI: The University of Michigan Press.
Graves, M. (2009). Teaching individual words: One size does not fit all. Newark, DE: International
Reading Association.
Gu, Y. (2003). Fine brush and freehand: The vocabulary learning art of two successful Chinese
EFL learners. TESOL Quarterly, 37, 73–104.
Haastrup, K., & Phillipson, R. (1983). Achievement strategies in learner/native speaker interac-
tion. In C. Faerch & G. Kasper (Eds.), Strategies in interlanguage communication
(pp. 140–158). London: Longman.
Harmer, J. (1991). The practice of English language teaching. New York: Longman.
Hidri, S. (2014). Developing and evaluating a dynamic assessment of listening comprehension
inan EFL context. Language Testing in Asia, 4, 4. https://doi.org/10.1186/2229-0443-4-4.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hiebert, E. H., & Kamil, M. L. (Eds.). (2005). Teaching and learning vocabulary: Bringing
research to practice. Mahwah, NJ: Lawrence Erlbaum.
Hulstijn, J. H. (2000). The use of computer technology in experimental studies of second language
acquisition: A survey of some techniques and some ongoing studies. Language Learning and
Technology, 3, 32–43.
Johnson, A. P., & Rasmussen, J. B. (1998). Classifying and super word web: Two strategies to
improve productive vocabulary. Journal of Adolescent & Adult Literacy, 42(3), 204–209.
Laufer, B., & Hill, M. (2000). What lexical information do L2 learners select in a CALL dictionary
and how does it affect word retention? Language Learning and Technology, 3(2), 58–76.
Laufer, B., & Nation, P. (1999). A vocabulary size test of controlled productive ability. Language
Testing, 16, 33–51.
Lyman-Hager, M., & Davis, J. N. (1996). The case for computer-mediated reading: Une Vie de
Boy. The French Review, 69(5), 775–790.
Lyman-Hager, M., Davis, J. N., Burnett, J., & Chennault, R. (1993). Une vie de boy: Interactive
reading in French. In F. L. Borchardt & E. M. T. Johnson (Eds.), Proceedings of the CALICO
1993 annual symposium on assessment (pp. 93–97). Durham, NC: Duke University.
Maximo, R. (2000). Effects if rote, context, keyword, and context/keyword method on retention of
vocabulary in EFL classroom. Language Learning, 50(2), 385–412.
McCarthy, M. J. (1990). Vocabulary. Oxford: Oxford University Press.
Meara, P. (1995). Single-subject studies of lexical acquisition [Special issue]. Second Language
Research, 11(2).
308 I. Riahi
Nagy, W. (1997). On the role of context in first- and second-language vocabulary learning. In
N. Schmitt & M. McCarthy (Eds.), Vocabulary: Description, acquisition and pedagogy
(pp. 64–83). Cambridge: Cambridge University Press.
Nagy, W. (2005). Why vocabulary instruction needs to be long-term and comprehensive. In E. H.
Hiebert & M. L. Kamil (Eds.), Teaching and learning vocabulary: Bringing research to
practice (pp. 27–44). Mahwah, NJ: Lawrence Erlbaum. Retrieved August 18, 2009, from
PsycINFO database.
Nation, P. (2000). Learning vocabulary in lexical sets: dangers and guidelines. TESOL Journal, 9,
6–10.
Nation, I. S. P. (2001). Learning vocabulary in another language. Cambridge: Cambridge
University Press.
Nation, P., & Newton, J. (1997). Teaching vocabulary. In J. Coady & T. Huckin (Eds.), Second
language vocabulary acquisition (pp. 238–254). Cambridge: Cambridge University Press.
National Reading Panel. (2000). Report of the National Reading Panel: Teaching children to read.
Washington, DC: National Institute of Child Health and Human Development.
Nelson, K. (2008). Teaching vocabulary to primary grade students within a school reform project.
Unpublished doctoral dissertation, The University of Utah.
Oxford, R., & Scarcella, R. C. (1994). Second language vocabulary learning among adults: State of
the art in vocabulary instruction. System, 22(2), 231–243.
Read, J. (1997). Vocabulary and testing. In N. Schmitt & M. McCarthy (Eds.), Vocabulary:
Description, acquisition, and pedagogy. Cambridge: Cambridge University Press.
Read, J. (2000). Assessing vocabulary. Cambridge: Cambridge University Press.
Richards, J. C., & Renandya, W. A. (2002). Methodology in language teaching: An anthology of
current practice. New York: Cambridge University Press.
Schmitt, N. (2000). Vocabulary in language teaching. Cambridge: Cambridge University Press.
Schmitt, N., & Carter, R. (2000). The lexical advantages of narrow reading for second language
learners. TESOL Journal, Spring, 4–9.
Siala, M. (2010). Implementing a summative test for final year secondary Tunisian learners.
E-Teacher Professional Development Workshop University of Maryland Baltimore County
and University of Oregon, Summer 2010.
Skehan, P. (1998). A cognitive approach to language learning. Oxford: Oxford University Press.
Stahl, S. A. (2005). Four problems with teaching word meanings (and what to do to make
vocabulary an integral part of instruction). In E. H. Hiebert & M. L. Kamil (Eds.), Teaching
and learning vocabulary: Bringing research to practice (pp. 95–114). Mahwah, NJ: Lawrence
Erlbaum Associates.
Stahl, K. A. D., & Bravo, M. A. (2010). Contemporary classroom vocabulary assessment for
content areas. The Reading Teacher, 63(7), 566–578.
Tran, T. H. (2011). EFL teachers’ perceptions about vocabulary acquisition and instruction.
Unpublished doctoral dissertation. San Diego, CA: Alliant International University.
Wallace, M. (1982). Armaments and escalation. International Studies Quarterly, 26(1), 37–56.
Zareva, A., Schwanenflugel, P., & Nikolova, Y. (2005). Relationship between lexical competence
and language proficiency: Variable sensitivity. Studies in Second Language Acquisition, 27(4),
567–595.
Techniques in Teaching and Testing Vocabulary for Learners of English in an. . . 309
Testing Grammar in an EFL Context
Yasmine Chniti
Abstract The testing of grammar has an important role in learning and teaching
second and/or foreign language(s). This study aimed to investigate grammar testing
in an EFL context, putting the emphasis on teachers’ and students’ perceptions of
test specifications (specs). The study used different methodologies whose specific
aim was to provide empirical evidence on how ‘fair’ a grammar test can be shaped
at universities and how test designers work to write a well-constructed and balanced
test. In this study, data were collected from the Faculty of Human and Social
Sciences of Tunis, Tunisia (FHSST), University of Humanities at Tunis. It included
20 EFL teachers, 104 students, and 100 grammar test scores. The framework of this
study followed a triangulation of quantitative as well as qualitative data collection
methods. Concerning the quantitative method, a students’ questionnaire was
conducted. For collecting data qualitatively, a set of interviews were performed
with English university teachers to seek variables in their perceptions of the
grammar test specs. Added to that, grammar tests and test scores were investigated.
Results from the quantitative analyses indicated that there were similarities and
differences in terms of conceptions among first year students of English. The
statistical findings from the ANOVA test proved that there was mostly agreement
between EFL teachers in the main principles of creating a grammar test.
Keywords Grammar • EFL • Testing • Teachers’ and students’ perceptions •
Quantitative • Qualitative data collection
1 Introduction
This study deals with grammar testing in EFL context and specifically it treats the
different perceptions about the test specs. In this context, the thesis is written for
EFL teachers who are responsible for designing grammar tests and for other pro-
fessionals who may need to set up language tests. Grammar gives some needed
Y. Chniti (*)
Faculty of Human and Social Sciences of Tunis, Tunis, Tunisia
e-mail: chnitiyasmine@yahoo.fr
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_15
311
structure to transmit ideas in a coherent way. It also helps to reach sense and it
provides clear messages. Thus, it sets the ground for more efficient communication
(Purpura, 2004). In addition, it makes the content more understandable and clear for
both the writer and the receiver. Correct English grammar is the tool that opens the
door to speak and write in an appropriate way. So, grammar is useful in learning a
language and essential in the real-world settings communication. Hence, it is
important in learning, teaching and testing any language.
In addition, the testing of grammar is a major part in the testing field. For
instance, testing grammar means diagnosing the system of rules to build meaningful
sentences and paragraphs with language. As De Saussure (1913) affirmed, grammar
is a system of relationships between words. This latter involves phonemes (sounds)
and morphemes (the smallest part in a word). They are combined altogether to form
meaningful words and sentences. In fact, these sentences are the outcomes of a
specific order of words (noun + verb + complement, etc.). Each word corresponds to
a particular function: Subject, verb, and object. Thus, testing has a paramount
significance of rules. Maybe for that reason, many students of English dislike to
be tested on grammar, for rules are not easy to tackle. Linguists, postgraduate
students and researchers may also need tests for their research. Moreover, there are
people who just want to understand the testing procedure to fathom the assessment
process. So, in this study, the process of test design at university from test speci-
fications’ drafting to test scores would be examined.
In the 1970s and 1980s, Communicative Language Teaching (CLT) began to
spread enormously. Thus, testing grammar has become a crucial field to investigate.
The 1980s witnessed expansions of the interest in language testing and applied
linguistics in general. Many linguists, such as Canale and Swain (1980) focused
particularly on the issue of language ability in relation with meaning. Moreover,
Shohamy (1997) targeted the genuineness of language tests and their communica-
tive effects in real-life settings. Nowadays, both test designers and test-takers strive
to shape genuine tools that better emulate reality. A collection of interesting
publications on language testing were introduced that year (1990) including
works by Bachman (1990) and Weir (2005) which have had a high impact on
language testing studies. Hence, perceptions on language testing started to alter.
Before studying the testing field in depth, applied linguistics (AL) should be studied
first. To start with, the term ‘applied linguistics’ came to the fore in the late decades
of the twentieth century when the discipline comes to highlight the study of foreign
and second languages. Later, many sub-disciplines or ‘sub-fields’ as called by
Bachman (1990) have been included in this broad field. After that, Bachman and
Palmer (1996) state that testing appears as an independent field on its own. It is no
longer considered as a subfield within AL. For instance, Brumfit (1997) believe that
applied linguistics is the embodiment of real world issues through language. Thus,
the most important feature of applied linguistics is its high interest to solve ‘real-
world’ language problems. It is worth mentioning that language assessment has a
crucial role in the field of applied linguistics, since it provides its scholars and
researchers with information for language analysis and language use. Thus, it is a
main field in AL (Bachman & Cohen, 1998).
312 Y. Chniti
Brumfit (1997, p. 93) defines the area of AL stating that it is “the theoretical and
empirical investigation of real-world problems in which language is a central
issue”. Besides, McNamara and Roever (2006) claims that AL translate the testing
behavior. By this, he means that testing is a main part of applied linguistics. Hence,
it becomes a crucial language activity that carries real-world issues. In this context,
the link between AL and testing has been so close since both of them concentrate on
the same topics of interest of language problems.
In this regard, educational testing helps test developers to recognize the way
students are learning. The terminologies “assessment”, “testing” and “educational
measurement” are used commonly in the linguistic field to stand for a system in
which applied linguists and educators use students’ answers to get inferences about
test takers’ performances (Popham, 2000). What is worth mentioning here is that
applied linguists in the research literature prefer the use of the term ‘assessment’
rather than ‘test’ to give a much broader and clearer scene of students’ perfor-
mances. By so doing, they intend to elicit that testing field does overcome the
traditional instruments of paper-and-pencil and multiple-choice questions.
First, each assessment has its unique theory about learners’ acquaintances.
Second, every assessment focuses on the various kinds of tasks and exercises to
better elicit learners’ skills. Third, scholars and testers try to shape meaningful
observations and useful inferences about students’ knowledge and potentials.
Apparently, all these aspects impact test design, use, scoring procedures and
results’ reporting.
2 Theoretical Background
2.1 Defining Grammar as a Construct
Purpura (2004) defines the grammar of the English language as a construct. He says
that grammar is a set of basics which are used to shape well-structured sentences in
the language (p. 11). These sentences contain words. These words form phrases.
And these phrases are interrelated to generate sentences. This is called syntax or
grammar. This approach is basically interested in forms and structures. Other
disciplines are concerned with meaning and contexts of sentences. Purpura
(2004) affirms that traditional grammar is considered one of the earliest approaches
to treat the form of language.
Many studies (e.g., Clapham & Corson, 1997; Nunan, 2003; Purpura, 2004) gave
much importance to the study and assessment of grammar. In this regard, grammar
was assessed in different methods. Thus, to test grammar and to get a set of reliable
information, many researchers (e.g., Purpura, 2004; Rothwell, 1996) adopted dif-
ferent survey instruments (questionnaires and interviews). For instance, they used
interviews to achieve an ongoing conversation between two parts to manage a
qualitative data. In this context, interviewing gave a rich background for analysis
Testing Grammar in an EFL Context 313
and it offered the opportunity to combine personal reports and interpretations to
agreed points and facts. Moreover, the benefit of interviewing was that it helped to
reveal interviewee’s answers and disclose the roots and depths of the treated matter.
2.2 Test Validation
Messick (1989) maintained that validation is one of the primary concerns in
developing a grammar test. Thus, in designing a grammar test not only did test
scores have to be reliable but also the inferences of these scores have to be valid.
For instance, Messick (1989, p. 33) assumes that validity is “an integrated evalu-
ative judgment of the degree to which empirical evidence and theoretical rationales
support the adequacy and appropriateness of inferences and actions based on test
scores”. Seen from this angle, validity is always in relation with the interpretations
made from these scores. Also, test validity does not only focus on test scores’ uses
and inferences. It also preoccupies of test takers’ performances in the grammar test.
Besides, validity does not only reside in grammar test content. It can be seen also
through data of the test. Test scores, for instance, which are the results of students’
performances in particular subject mirrors test validity. In order to determine
scoring validity, test developers make sure that the given test scores are to some
degree error free. Thus, they could make suitable inferences about learners. So,
validity does not have just one face. It is multi-faced and multi-directional.
2.3 Test Specs
A test is defined as a set of items, questions or problems that are well designed to
identify students’ ability or knowledge on a particular subject. The major element
of a list of test specifications is the test type for it helps to identify the grammar test
objectives and items. Tests can be categorized through different criteria to many
types.
To start with, diagnostic test is an intensive assessment process with a detailed
coverage of a subject, generally advanced by a specific problem, and performed to
recognize the learners’ educational levels. It is always used to diagnose the stu-
dents’ strengths and weaknesses in a skill. And it is firstly used to stress special
requirements: Identify students’ deficiencies. It is a test prepared to recognize the
learner’s ‘readiness to learn or potential capacities for future success in a particular
subject when the appropriate education is provided. An IQ test is a famous example
of an aptitude test for measuring general academic abilities.
Purpura (2004) claims that proficiency test is a type of test used to find out
students’ language abilities. Also, proficiency tests are not based on specific
language program. They are rather designed to test the different linguistic skills
and abilities of learners with different training procedures. Besides, they are
314 Y. Chniti
constructed to make sure that students have a well-established ability to be able to
use language in a specific area such as academic study. Besides, placement test
places students in a group or educational level according to their performances.
This type of tests could be based on the principles of the curriculum taught at the
educational institution concerned. In some institutions, for instance, the placement
test’s goal is to see if the students require any more tuition especially at the
beginning of the year.
The consequences of a diagnostic test are used to discover the weaknesses and
prescribe solutions to better enhance the test takers’ abilities and performances. The
achievement test, for instance, is designed to measure the extent students succeed to
acquire particular skills or abilities that had been taught in class or as a part of some
other training elsewhere. They are prepared for the close of the course in contrast
with the progress test that is usually provided with various steps throughout the
semester to discover the information students have learnt.
It should be noted that the content of these tests is generally dependent on the
course textbook and the course curriculum. In addition, aptitude test is a kind of
exam designed to measure the ability of an individual to develop abilities, specific
skills or obtain some knowledge. Test essays, on the other hand are another kind of
testing where objectivity is not a fixed criterion. The student is expected to write a
paragraph or an essay following the instructions of the test. It should be highlighted
here that the scoring method will vary from the objective test to the essay exam.
Consequently, applied linguists and test authors can differentiate between different
kinds of assessments.
Bachman and Palmer (1996) assume that personal characteristics are not related
to students’ language abilities but they may influence their performances on the test.
Many researchers, for instance, did investigate these issues, like Shehan (1989) and
Brown (1994). Cohen (1994) for example, puts forth test-takers’ features including
age, gender, native language, foreign language level, multi-lingual competency,
personality, socio-psychological factors, social position, family position (Cohen,
1994, p. 74). Also, Cohen (1994) maintains that designing an appropriate test, test-
makers should make a list including all of these characteristics of the individuals to
assure test usefulness. He claims that knowledge of the test-taker’s first language,
‘age’, ‘level of education’ helps test-makers create test tasks that better correspond
with these personal characteristics. Hence, they would design a ‘useful’ and
‘fair’ test.
Bachman and Palmer (1996) assume that characteristics of the test-takers can be
physical, psychological or educational. The physical features can be seen as a kind
of illnesses such as problems of sight or hearing. Some students suffer from
dyslexia. Thus, they cannot speak in a normal way. These permanent ailments
can negatively affect the learners’ performances in the test. Besides, the students
can have flu, headache, earache or some other kinds of unpredictable illnesses
during the exam that may cause some obstacles when tackling the grammar test.
Thus, the test takers do not perform well in the exam.
Even though test developers make their best to collect such features, they would
not recognize the test takers’ cognitions or psychological states. In reality, such
Testing Grammar in an EFL Context 315
characteristics do remain ambiguous and hidden. They are too personal to be
known. The students can face social, financial problems that hinder them from
concentrating or even preparing for their tests. Also, many learners suffer from
psychological illnesses such as panic, stress, lack of confidence that could stand as
real obstacles when a student sits for the exam. For that reason, Bachman and
Palmer (1996) estimate that making a complete list of learners’ features is such a
problematic task. According to them, only some obvious characteristics can be
listed in developing a test (age, gender, nationality, native language). Other features
which are mainly psychological seem difficult to acknowledge simply because they
are hidden in the personality of the learner. The teacher, for instance, cannot grasp
the emotional state neither the degree of concentration of the test taker.
Another main feature of test-takers is topical knowledge. Topical knowledge or
real-world knowledge is known as the extent areas of language knowledge that is
available for a test taker. This is another characteristic required to be present to deal
with a test task. This latter makes test authors to recognize some information about
learners that clarify not only the language use but also identify learners’ real world
in relation to test use. Actual knowledge is a way to facilitate the performance of
test takers. And it could be difficult for those who do not acquire any knowledge.
There is no doubt that these characteristics are essentially taken into consideration
to develop a grammar test.
The difference between actual knowledge and topical knowledge is that learners
have this knowledge even before studying. Topical knowledge is acquired after
studying grammar. According to Purpura (2004), only then, the teacher can deter-
mine to what extent learners have mastered grammatical rules during a specific
period of time. This is what Purpura (2004) calls “construct definition” (p. 159).
Also, he considers it as an interesting element in a test design process because for
him it is “the basis for test construction, score interpretation and test validation”
(p. 159). For example, in a writing test, the student is expected to write within the
content requirements and at the same time write accurately using correct grammat-
ical abilities. So, in the test design, the test maker highlights both content and
grammar. And the test-taker receives test score according to the mastery of the two
elements together. Overall, designing a grammar test could not be fulfilled unless a
list of test specifications is well-written.
In this section, Bachman (1991) elicits that test developers are required to decide
what format the test will take. This would help them accomplish the test purposes.
For instance, many teachers opt for multiple-choice items in assessing grammatical
abilities for they have many advantages for both the teacher and learner. This
section also seeks to answer in details the definition of test specifications and
introduce those who need them. It shows also how test specifications should appear
in a grammar test and how testers prepare them.
It is true that multiple-choice item requires a great deal of time to be prepared,
yet it needs short time when scoring the grammar test. Added to that, they are easy
to mark and most importantly are objective tests. For the students, on the other
hand, multiple-choice items are much easier and quicker than any other kind of
questions. Test developers decide what content to be tested. The content is defined
316 Y. Chniti
as the construct through which learners and test users are expected to cope with in
the testing situation. This content is introduced with a specific format and with
different characteristics: Language, length, etc. Before sitting for the test, students
should have an adequate and sufficient content background to deal with the
construct properly. For that reason, educators are supposed to focus on the content
and form when teaching test-takers. On that basis, they are going to recognize the
learners’ skills and abilities. To start with, Bachman (1990) states that test speci-
fications usually called ‘specs’ tend to tell test-makers what a test tests and how it
tests it. It should be noted that the specifications are the test blueprint followed by
test makers when writing a grammar test. According to Bachman and Palmer
(1996), this syllabus is a simple, detailed, and public statement which informs
both test takers and test writers about the test content. It is also prepared for test
users who are required to make decisions on the basis of test grades, and for those
researchers who are concerned with reviewing grammar tests and producing reports
related to them.
2.4 Research Problem and Rationale of the Study
A large body of data concerning testing grammar as well as teachers’ and students’
perceptions of test specifications has been reported. Test specifications differ from
one teacher to another and from one curriculum to another. Some teachers follow
the educational curriculum without paying attention to students’ needs. Others, on
the contrary, take students’ abilities and requirements into consideration. Yet, in
both cases, problems with test takers and grammar tests are mostly dominant at
universities. The students may find difficulties when dealing with the exam in terms
of vocabulary, test questions or test content. Thus, they fail to give appropriate
answers. Some learners of English for example just do not like to be tested on
grammar, since they believe that a grammar test is hard to accomplish. The problem
here does not stop at the student failure in the grammar test but it leads also to the
test-taker’s failure in other subjects for grammar is in every test. Again, the
relevance between grammar teaching and grammar testing comes to the fore.
Most importantly, the issue of the grammar test specifications is raised. However,
this view is challenged by recent data showing the intricate link between teaching,
testing, and communication in real-life settings. Hence, additional studies are
needed.
Testing grammar practices at the university level has always been overlooked
and neglected. For that reason, there is always a lack of awareness in terms of
testing and assessment among students and teachers of English. In this regard,
students consider grammar testing as a ‘failure’ and blame test designers for their
lack of interest in grammar tests in such an important level of education. Also, in the
local context, there is a scarcity of research on grammar both at the secondary and at
university level that should be treated. For instance, Hidri (2015) maintained that
Testing Grammar in an EFL Context 317
“test impacts and uses have been overlooked in the Tunisian context” (p. 21). The
present study addressed three basic questions:
1. I s grammar testing given its importance in the international linguistic research?
2. What are the test specifications that should be followed to create a useful
grammar test?
3. What are the teachers’ and students’ perceptions of grammar tests?
3 Method
One hundred and four first year university students at the Faculty of Human and
Social Sciences of Tunis ranging in age between 18 and 22, had been targeted in this
study though, the number seems to be limited, these students represented a sample
of Tunisian students for they came from different cities of Tunisia, had received the
same secondary education and all set for the national Baccalaureate exam before
they had attended universities. Further, all of these individuals received at least
7 years of English courses before entering universities.
Questionnaire survey was one of the main methods of collecting data in this
study. On the other hand, the interview included questions regarding test planning,
test scores and decisions, ‘fairness’ as well as ethical matters. Added to that, EFL
teachers were required to answer these questions according to their perceptions and
opinions (Table 1).
The second measure is interviewing EFL teachers. All interviews were regis-
tered and marked down later. Every interview lasted for about an hour. Interview
responses, afterwards, were compared to each other to obtain comprehensive
feedback.
In fact, both methods of questionnaires and interviews were not enough to cope
with such broad issue of grammar testing. For that reason, analyzing grammar tests
and test scores were needed to collect more trustworthy and reliable feedback. For
that reason, 100 test scores were used as an instrument to investigate the subject of
grammar testing in more depth. In the current study, the method of test scores was
used to investigate the reliability, validation, authenticity, interactiveness,
Table 1 Gender of
respondents (N ¼ 104)
Features Frequency Percentage
Gender Female 57 55
Male 47 45
Learning
experience
in English
8 100
University The Faculty of
Human and Social
Sciences of Tunis,
Tunisia
318 Y. Chniti
practicality and impact of grammar tests at Tunisian universities. It consists of the
obtained scores of first year students of English at FHSST. As a matter of fact, the
investigation of students’ responses in the questionnaire was clarified. Besides,
teachers’ answers in the conducted interviews and the way they did score grammar
tests were compared. Thus, some statistical tests have been used. Data analysis was
accomplished in four phases (Table 2): SPSS v, 22.0, the Student t-test, Correlation
analysis, and ANOVA test.
4 Results and Discussion
Test designers looked for the previous educational level of the target population to
know the different grammar points that could be tackled during the exam. As it is
seen in some grammar tests test designers chose the number of tasks and the time
for each test task. Each test consisted of a maximum of four tasks and lasts for just
1 h. Obviously, teachers’ and students’ perceptions not only differed but also
seemed to be contradictory. In such a milieu, the issue of ethics within the scope
of testing in general and grammar testing in particular appealed for better investi-
gations. Yet, validity per se was neither a sufficient nor a unique quality in a
language test. For instance, there were other important qualities which were
complementary in shaping a grammar test. Other conditions and details besides
this crucial quality could altogether make a balanced grammar test.
Through these various exercises, the teacher became able to know the students’
levels, weaknesses, strengths, interests, and obstacles. Thus, teachers became
conscious of what was supposed to do when designing the exam. Their educational
experiences allowed them to rethink their lessons and tests of grammar. It is worth
Table 2 Data collection and analysis
Phases Participants Instruments Analysis Rationale
1 Test
analysis
First year
students of
English
(n ¼ 104)
Students’
questionnaire
Turn students’ perceptions
to get reliable numbers
To get the possible
variables on test
specifications
2 Score
analysis
1 (n ¼ 100)
Students of
English
Students
t-test
Find out the distinction
between two groups of
grammar scores
To check the rela-
tionships between
the different
variables
3 Score
analysis
2 (n ¼ 200)
Students of
English
Correlation
analysis
Extract the relationships of
grammar scores and vari-
ables between term 1 and
term 2
To check the fac-
tors of variability
between grammar
scores
4 Score
analysis
3 (n ¼ 100)
Students of
English
ANOVA Find out the differences
between male and female
grammar test scores
To check the dif-
ferent factors of
the t test
Testing Grammar in an EFL Context 319
mentioning here that there was a huge difference between theory and practice.
There was a great difference between what students should prepare and what they
performed in the test. Generally speaking, some teachers and students said what
they did not do.
The student’s t-test was a statistical technique to compare a set of different
variables. Particularly, it compared a set of quantitative data from the same popu-
lation. The given samples in this study were gathered from first year learners of
English. Throughout the study, the t-test proved that there was a difference in
variables. Thus, the test gave the average and the calculated variables and the
average errors that address the significant difference between learners (Table 3).
These data (grammar test scores of a group of first year students of English in
two successive terms) were chosen to be analyzed. These methods were called
correlation analysis in addition to the student t-test analysis. In fact, such types of
data analysis were resorted for it facilitated the kind of score given to each test-taker
according to a given hypothesis. It, indeed, provided clear differences in scores
between first year students within the same term and within the two terms.
Very often, teachers tended to construct test items taking into consideration what
was taught in class and try them out with a sample of test-takers and to recognize
and examine the test weaknesses. Another major test quality that was given much
importance by teachers and test developers was test reliability. Hence, they tended
to give reliable scores in each grammar test. In this case, teachers wrote tests with
many items to avoid the problem of low reliability. More particularly, those
students who forgot sometimes to answer a question that they did know its answer
would not fail the whole exam. Generally speaking, test developers who looked for
test reliability always tended to rate objectively. Many teachers often believed that
score judgment in a grammar test was objective. In fact, question five in the
teachers’ interview disclosed that the majority of the interviewees stressed the
importance of objectivity as a main criterion for correctness. They believed that
subjectivity had no place in rating grammar subject. They also assumed that only
essays and interpretive tasks may require a kind of subjectivity. Most of the EFL
teachers assumed that scoring grammatical ability was considered as a challenge
because there were different test items and each grammar task required a unique
criterion for correctness.
Table 3 Interconnection
between first and second
terms
Grammar
test scores
(term 1)
Grammar
test scores
(term 2)
Grammar
test scores
(first term)
Correlation of
Pearson
1 0.868**
Sig. (bilateral) 0.000
Grammar
test scores
(second term)
Correlation of
Pearson
0.868**
1
Sig. (bilateral) 0.000
*
Correlation is significant at 0.5
320 Y. Chniti
However, there was no way to avoid subjectivity in language tests even though
these tests seemed to be objectively put and marked for the test-taker. For instance,
subjective scoring is omnipresent in every language test. And in this situation, each
teacher provided his/her appropriate reasons. For example, Tunisian teachers
asserted that a multiple-choice task, for instance, is objective simply because
there is only one correct answer. Yet, in a composition task, the scoring could be
subjective because the rater would face different grammatical and semantically
aspects. (For further details on such conflicting views, see Hidri, 2015). Thus, it was
not clear which aspects would be taken into consideration or which ones would be
favored for the others.
Many teachers agreed on the idea of training for both test maker and test-taker
before dealing with grammar tests. According to them, training could raise their
awareness in coping with tests and test items. Thus, they received trustworthy and
interesting feedback. In fact, all the mentioned strategies were learned through the
training courses given to instructors to design grammar tests. Fifteen of the
interviewed teachers found that these courses were unnecessary as they believed
that practice makes the best. On the other side, other teachers stated that designing a
grammar test for first year students of English was not that effortless task as many
people believed. They affirmed that this requires a great deal of attention, consid-
eration, and time.
For instance, since the onset of the grammar course at the beginning of the
semester, teachers started to write the test list of specifications according to his or
her objectives. They chose the test items, types of grammar tasks, sequence of tasks,
test content, and the criteria for correctness. For instance, the types of test items
affect students’ answers in the test. For that reason, testers focused mainly on the
length, vocabulary, grammar, and intention of the test question. Grammar tasks
(e.g., multiple-choice, completion, essay tasks) have different features. They
affirmed that there were some learners who can easily manage multiple-choice
items yet they have difficulties in writing essays or paragraphs. In a sense, taking
the test items types together with students’ abilities and needs were necessary to be
taken into consideration. Most importantly, the features of the grammar test can
affect students’ scores. Therefore, instructors’ role in writing a balanced and well-
constructed grammar test was crucial in boosting students to have a clearer view of
the test creation. Moreover, teachers affirmed that learning and assessment were
two interrelated activities. In a sense, testing grammar as a subject allowed teachers
to make progress in teaching the course objectives as it made it easier for them to
make decisions about the learning activities. In fact, teachers’ and students’ per-
ceptions of test specifications in this study were just a trial to design a grammar test
for first year students of English. These perceptions had some similarities and
differences with other studies within the testing and assessment field that need to
be discussed.
Attitudes and perceptions differed from one student to another. For that reason,
the questionnaire survey was such an important tool of investigation to know the
views of learners concerning this issue. Teachers revealed that students who
Testing Grammar in an EFL Context 321
believed that grammar tests did not correspond with grammar courses were those
who rarely or never attended grammar courses.
This study revealed the different misconceptions of teachers regarding this issue
and the disappointments of some students about the results of testing within the
EFL and the Tunisian context. Like other studies (e.g., Brown & Gao, 2015; Hidri,
2015), this study revealed different misconceptions among teachers of English in
designing a test. Teachers of English in the Tunisian context did only consider
learners’ participations in the test design process, but they also ignored their
perceptions and preference. Hence, “in this regard, students’ criticism has been
lodged at teachers whose exams have been perceived as a heavy burden” (Hidri,
2015, p. 34) (Table 4).
In addition, Hughes (1989) affirmed that language testers are always investigat-
ing the best testing methods to avoid some misconceptions. Yet, he assured that
“such questions reveal a misunderstanding of what is involved in the practice of
language testing” (p. 6).
In the Tunisian context, test designers assured that test specifications framework
should not only stress the prescription of a set of test specifications, but it should
rather stand as an evaluative list in which teachers could discuss any change of the
test blueprints according to the context of the grammar test. For instance, EFL
teachers in Tunisian universities proved that a grammar test may be better observed
as a balanced test in a particular context, rather than a general grammar test.
However, some grammar tests did not translate these perceptions. These grammar
tests were not only poorly written but they had poor context to cope with. First and
foremost, the context of the test has to be clearly understood to guarantee the
efficiency of test specifications. Sometimes, test developers failed to reach the
expected results in class especially when the list of test specifications did not
correspond with students’ abilities or when teachers just relied on their last expe-
riences with grammar tests and forget about the context.
In reality, test fairness is a multi-directional issue. It is a collective work.
Fairness should not be confined to content alone or score alone. Rather, it covers
many aspects. In the previous studies, test fairness was not only limited in reliability
Table 4 Students’ perceptions of some test specifications
Frequency Percent Mean SD
Teachers tested what they taught Strongly disagree 10 9.6 1.4 0.9
Agree 10 9.6 1.0 0.9
Disagree 13 12.5 1.0 1.6
Strongly agree 39 37.5 1.0 14.6
Undecided 32 30.8 1.0 9.8
Degree of satisfaction with criteria
for correctness
Very dissatisfied 7 6.7 1.0 0.4
Very satisfied 10 9.6 1.0 0.9
Neutral 12 11.5 1.0 1.3
Satisfied 33 31.7 1.0 10.4
Dissatisfied 42 40.4 1.0 16.9
322 Y. Chniti
and validity. According to Bachman and Palmer (1996), to build a well-constructed
grammar test, testers paid attention to all aspects of a test. And this was a point in
common between this study and other studies (e.g., Bachman, 1990; Brown & Gao,
2015). In other words, test-takers have to sit for the test within the same learning
conditions. For that reason, the concept of test fairness is not at all easy to tackle, or
to agree-upon. For instance, the majority of researchers and test makers seemed to
agree upon the idea of testing students under the same conditions (setting, equip-
ment, etc.).
Teachers in the interviews affirmed that putting all students under the same
umbrella reduces test bias. This embraced ethics in language testing. Besides,
Jensen (1980) stated that the concept of “bias” in the testing field expressed
unfairness and inequity because it simply preferred an individual or a group to
another. It should be noted here that the term of bias was strongly combined with all
aspects of testing. For example, a test was said to be biased when a group of
students who have similar abilities are scored differently merely because they
belong to different groups and have different perspectives. This was not the same
case in this study because most of the test-takers were treated equally in terms of
time, conditions and scoring according to EFL teachers. This, in fact, increased test
validity and reliability. Moreover, Xiaoming (2010) considered fairness as the
equal treatment of all learners under the same educational conditions. This was
another similarity with the current study. According to him, fairness is “the absence
of bias, equitable treatment of all test takers in the testing process” (2010, p. 147).
By the same token, all test-takers should be treated equally in a testing situation. By
contrast, some learners were often complaining about the absence of the equal
treatment of the testing conditions. To better illustrate this idea, some learners said
that the scoring procedure was biased. Seen from this angle, the scoring procedures
according to them were to some extent unreliable. Clashes in perceptions among
teachers and students were still present which called always for deeper
investigations.
Most importantly, construct-irrelevance was another common problem espe-
cially in achievement grammar tests. Construct-irrelevance was that the test no
longer tested what was taught in class. Therefore, there was no longer test validity.
In Tunisian context, many students of English complained about the absence of
validity and the poor quality of some grammar tests. Often, construct- irrelevance
was related to test reliability. Bachman and Palmer (1996) said that reliability was
the consistency of test scores and that these scores mirrored test-takers perfor-
mances. However, in this study, the case was different. Grammar test scores did not
necessarily reflected students’ true abilities because there were many external
factors (e.g., illness, anxiety, readiness, etc.) that could hinder them from achieving
the expected results. In short, construct-irrelevance was considered as a real
problem in designing a grammar test within the Tunisian context.
Maybe, this was the result of the lack of piloting grammar tests and the lack of
awareness of some test designers. Hence, this study raised a new point of difference
with other studies e.g., Purpura (2004) in such an important issue of test specifica-
tions. Clearly then, similarities and differences of this studies with others
Testing Grammar in an EFL Context 323
(e.g., Kunnan, 2000; Purpura, 2004) expanded to widen the scope of responsibilities
of test designers when dealing with a grammar test at higher education. Similarities
and differences in perceptions and opinions in this study stressed the importance of
designing a grammar test and called for the better ways of doing it.
It is true that Kunnan’s (2000) test framework appeared logical and even
inclusive, but it had its drawbacks. First, it was not easy to guarantee all facets of
test usefulness. Moreover, Kunnan developed a set of detailed ideas to build a
theory that could be best understood for both test developers and test-takers. He
summed up his ideas in 2004 to tell that fairness included five characteristics:
‘Validity’, ‘bias-free’, ‘equity in teaching and testing’, ‘test administration’, and
‘social outcomes’. It was true that all these characteristics were so important in
designing a grammar test but in reality, there were not given too much importance
by test developers. For example, Tunisian teachers in FHSST rarely gave the
grammar test to a group of learners or to some other teachers in advance to make
some needed improvements later. In short, designing a well-constructed grammar
test was a work that necessitates mingling a set of criteria within a well-planned
table of test specifications. As far as the issue of piloting was considered, there was
no similarity with other conducted researches (e.g., Bachman & Palmer, 1996).
In addition, test scores given by test developers in some grammar tests were not
absolutely reliable nor absolutely valid (Bachman & Palmer 1996, p. 22). Teachers
concluded that absolute validation does not exist. A grammar test could not be a
perfect design and a test score could be a final judgment of students’ ability. This
would not hinder test makers and test-takers altogether to focus on test consistency.
However, Bachman and Palmer’s (1996) test specifications framework remained
the most comprehensive and contextualized model of designing a test. The clarity
of the courses’ methodology helped students of English to acquire more knowledge
in grammar subject as it invited them to give much importance to grammar tests
(particularly progress and achievement tests). Besides, the EFL courses and tests
offered the opportunity not only to first-year students of English, but also to the
other would-be researchers to receive tremendous training courses to be more
knowledgeable about assessment in general and testing in particular. Hence, EFL
teachers became aware of the methodologies required in class to shape a grammar
test. In fact, this consciousness raised test developers’ awareness to design a useful
grammar test.
This understanding of methods widened the scope of testing, such as the
correlations between methods and the reached results within the EFL context that
can be enriched. To collect data, three different methods were made use of. Each
method provided its unique outcomes that could not be found in another. To cope
with such topics about testing and evaluations, researchers had to broaden their
scope of analysis using a number of effective methods. These methodologies were
no longer limited to first year students of English only, but they could also be useful
to future researchers in increasing their consciousness in writing and producing
better test specifications. Since they were the receptionists of these tests, learners
were in some ways aware of tests’ characteristics. Thus, students themselves can
give helpful suggestions to their teachers concerning the construction of a grammar
324 Y. Chniti
test. Based on what they had seen in class, students gave helpful suggestions to
create a grammar test.
As for the pedagogical implications, this study disclosed EFL teachers’ percep-
tions of the test specifications, which are the outcomes of teaching a particular skill
in an English class. Like other studies, (Brown & Gao, 2015), the present study
showed the crucial importance of teachers’ and students’ conceptions and mis-
conceptions of the grammar test design within the assessment field. In addition, the
current study drew test designers’ and teachers’ attentions to particular procedures
of shaping a grammar test as well as improving their ways of teaching and testing.
Moreover, knowing students’ perceptions of the grammar test specifications and
their obstacles to deal with a language test help teachers to fulfil better results in the
design process. In the research implications, assessment perceptions of teachers and
students addressed more investigations in grammar testing in particular and assess-
ments practices in general. Thus, assessment literacy called for more effort to boost
the educational systems. In view of all the above implications, the following section
attempted to set forth a number of limitations.
5 Conclusion
Specifying some training courses for first year students of English and learners in
general may help in dealing with performing grammar tests. At universities, there
are several students who strive to enhance the qualities of tests and the courses they
encounter. In such a case, teachers tend to provide more test construction rules,
booklets, and workshops. Added to that, testers should provide useful and support-
ive data (clear list of test specifications) for test-takers to make it easy for them to
select the suitable test. Also, test developers should make clear and explicit pro-
cedures for the scoring criteria to make things clear for the students. Added to that,
grammar teachers are recommended not to ignore piloting grammar tests so that
they prevent any misconception regarding test content or format. Thus, learners can
easily tackle the grammar test. Finally, educators have to give much importance to
the testing and assessment fields to enhance the educational levels and develop
modern methods of learning and teaching. It should also concentrate on the
importance of team work in designing a well-designed test addressing all the test
specifications.
This study tried to tackle the issue of grammar testing that has always been
addressed over years. It foreshadowed the required test specifications to create a
balanced grammar test. More generally, teachers concluded that a grammar test
design was proved to be a well-organized process in which the design stages were
not necessarily linear as it was the test specs of Bachman and Palmer (1996).
Most importantly, the process of writing a list of test specifications invited all
individuals’ suggestions to reach an agreement on a specific list that goes hand in
hand with the grammar test objectives and more particularly with the issue of
the grammar test fairness. Most importantly, some grammar teachers did not
Testing Grammar in an EFL Context 325
exactly follow what they had to follow. They created their own curriculum and
drew their own objectives. Popham (2000) invited linguists and test designers
together to create a commitment that can spread good educational testing culture
and convey meaningful and useful programs that can enhance testing and assess-
ment programs.
References
Bachman, L. (1990). Fundamental considerations in language testing. Oxford: Oxford University
Press.
Bachman, L. F. (1991). What does language testing have to offer? TESOL Quarterly, 25(4), 671–704.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice: Designing and developing
useful language tests. Oxford: Oxford University Press.
Bachman, L. F., & Cohen, A. D. (Eds.). (1998). Interfaces between second language acquisition
and language testing research. Cambridge: Cambridge University Press.
Brown, H. D. (1994). Principles of language learning and teaching (3rd ed.). Englewood Cliffs,
NJ: Prentice Hall Regents.
Brown, G. T. L., & Gao, L. (2015). Chinese teachers’ conceptions of assessment for and of
learning: Six competing and complementary purposes. Cogent Education, 2(1–19), 993836.
https://doi.org/10.1080/2331186X.2014.993836.
Brumfit, C. (1997). How applied linguistics is the same as any other science. International Journal
of Applied Linguistics, 7(1), 86–94. https://doi.org/10.1111/j.1473-4192.1997.tb00107.x.
Canale, M., & Swain, M. (1980). Theoretical bases of communicative approaches to second
language teaching and testing. Applied Linguistics, 1, 1–47.
Clapham, C. M., & Corson, D. (Eds.). (1997). Language testing and assessment: Encyclopedia of
language and education (Vol. 7). Dordrecht: Kluwer Academic.
Cohen, A. (1994). Assessing language ability in the classroom (2nd ed.). Boston, MA: Heinle and
Heinle.
De Saussure, F. (1913). Course in general linguistics. New York: Philosophical Library.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hughes, A. (1989). Testing for language teachers (2nd ed.). Cambridge: Cambridge University
Press.
Jensen, A. R. (1980). Bias in mental testing. New York: Free Press.
Kunnan, A. J. (2000). Fairness and validation in language assessment: Selected papers from the
19th language testing research Colloquium, Orlando, Florida. Cambridge: Cambridge Uni-
versity Press.
McNamara, T., & Roever, C. (2006). Language testing: The social dimension. Oxford: Blackwell.
Messick, S. (1989). Validity. In R. L. Linn (Ed.), Educational measurement (3rd ed., pp. 13–103).
New York: Macmillan.
Nunan, D. (2003). Practical English language teaching (pp. 342–353). New York: Mc Grow Hill
Education.
Popham, W. J. (2000). Stopping the mismeasurement of educational quality. School Administra-
tor, 57(11), 12–15.
Purpura, J. (2004). Assessing grammar. Cambridge: Cambridge University Press.
Rothwell, A. (1996). Questionnaire design. In B. Allison et al. (Eds.), Research skills for students
(pp. 69–97). London: Kogan Press.
Shehan. (1989). Improving testing for English language learners. A comprehensive approach to
designing, building, implementing, and interpreting better academic assessments. London and
New York: Routledge.
326 Y. Chniti
Shohamy, E. (1997). Second language assessment. Encyclopedia of Language and Education, 4,
141–49. Springer Netherlands.
Weir, C. J. (2005). Language testing and validation: An evidence-based approach. Basingstoke:
Palgrave Macmillan.
Xiaoming, X. I. (2010). How do we go about investigating test fairness? Language Testing, 27(2),
147–170.
Testing Grammar in an EFL Context 327
Part VI
Assessment Literacy and Test Fairness
Continuous Cumulative Assessment in Higher
Education: Coming to Grips with Test
Enhanced Learning
Ekaterina Popkova
Abstract Cumulative assessment, as part of a broader area of continuous assess-
ment, keeps up with the times, meeting the requirements of increasing learner-
involvement in the educational process. Practice has shown, however, that imple-
mentation of this type of assessment in tertiary education for the time being falls
short of ELT community’s expectations, with final achievement tests, perhaps by
way of tradition, still being a preferred method of assessing student knowledge and
skills. Moreover, those higher educational institutions that do apply continuous
assessment express concerns over its expediency complaining that teaching condi-
tions of tertiary education turn the formative nature of continuous assessment to
summative, thus bringing all its benefits to nought. The study below aims to
examine the effectiveness of continuous cumulative assessment in the context of
higher education. Specifically, it looks at the rationale behind the testing system
applied and investigates its reliability based on the correlation of the grades
received during the course with the final exam grade. The results of the study,
obtained both through qualitative and quantitative data analyses, demonstrate that
as continuous cumulative assessment reveals both formative and summative nature,
its benefits out weigh the drawbacks. The research was conducted in the course of
delivering ESP (Legal English) and EAP courses to 2nd and 4th year undergraduate
law students at one of the leading Russian universities. The principles postulated,
however, are believed to be applicable across disciplines in other educational
contexts.
Keywords Cumulative assessment • Continuous assessment • Tertiary education •
Formative assessment • Summative assessment • Test-enhanced learning
E. Popkova (*)
MGIMO University, Moscow, Russia
e-mail: ekamip@yandex.ru
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_16
331
1 Introduction
The study presented below provides practical cross-disciplinary analysis,
underpinned by a comprehensive theoretical framework, of the positive effects of
introducing continuous cumulative assessment in ESP and EAP teaching in tertiary
settings. Continuous cumulative assessment presents special significance for EFL
teaching in tertiary education. At the tertiary level, foreign languages are, as a rule,
taught in ESP or EAP programmes, which means that educational programmes are
normally based on objectives unique to students’ needs and assessment on such
courses should attend to these needs (Barootchi & Keshavarz, 2002). This implies
that, unlike schools where programmes generally follow the national curriculum,
universities each set their own requirements to which language levels students
should attain and which competencies and skills should be developed. Thus,
distribution of standardised tests is highly problematic, and assessment through a
one-off final exam is ineffective, as EFL teaching in higher education involves a
great many aspects the diversity of which is too difficult to accurately measure in
one final exam. This means that assessment schemes applied in universities should
be comprehensive enough to cater for a variety of educational needs and to provide
for reliability of assessment; their comprehensiveness and reliability being best
ensured by continuous assessment approaches (Beagley & Capaldi, 2016; Cole &
Spence, 2012; Cross & O’Loughlin, 2013; González, Jare~
no, & López, 2015;
Holmes, 2015; Jolliffe, 1978; Khanna, Brack, & Finken, 2013; Nitko, 1995; Trotter,
2006; Tuunila & Pulkkinen, 2015).
However, whereas the issues connected with continuous assessment in higher
education in general have not escaped educational researchers’ attention, studies on
continuous assessment of foreign language learning as incorporated in university
contexts are only scarce, with even fewer of them focusing on cumulative assess-
ment; as observed in other recent studies on the issue (Beagley & Capaldi, 2016;
Khanna et al., 2013). Moreover, previous studies acknowledging the educational
potential of continuous assessment frameworks in terms of improvement in stu-
dents’ learning outcomes have mainly analysed the performance of students within
the same discipline. This took place by introducing an alternative approach, most
frequently in one group to contrast it with other control groups, either simulta-
neously (Barootchi & Keshavarz, 2002; González et al., 2015; Holmes, 2015;
Johnson, 1984; Kerdijk, Cohen-Schotanus, Mulder, Muntinghe, & Tio, 2015;
Khanna, 2015; Trotter, 2006) or in subsequent years of studies after the adoption
of continuous assessment (Beagley & Capaldi, 2016; Cole & Spence, 2012; Khanna
et al., 2013; Jolliffe, 1978; Tuunila & Pulkkinen, 2015). The present study differs
from former research in that it analyses students’ performance as impacted by
continuous assessment in one subject, i.e., foreign language, but two different
disciplines: Legal English and Academic Writing in English. Secondly, it investi-
gates correlation not only between continuous cumulative assessment and exam
performance, but between that with students’ in-course performance. Finally, the
focus of the study lies in application of a particular continuous assessment type,
332 E. Popkova
i.e. continuous cumulative assessment, where testing is conducted in reiterative
cycles through repeated exposure of students to a variety of tasks, each testing a
particular variable and repeated over the course length.
2 Theoretical Background
To help avoid ambiguity in understanding certain key terms of the study, a few
comments should be made about their nature and the meaning ascribed to them.
Despite the commonly accepted and seemingly exhaustive definitions of summa-
tive and formative approaches to assessment as the ones which, respectively, either
measure students’ achievement at the end of the unit/course or collect information
to create feedback to students and enhance learning (Byram & Hu, 2013; Trotter,
2006), a number of publications (Hernández, 2012; Nitko, 1995) point out that the
distinction is, in fact, not so clear cut. Boundaries between formative and summa-
tive assessment can become blurred if assessment is carried out without careful
consideration of the impact it produces (Cross & O’Loughlin, 2013, p. 587) or when
the would-be formative assessment tasks are designed for the mere purpose of
marking key transition points within the program (ibid., p. 590).
It seems reasonable to agree with Hernández (2012) who argues that the key
difference between the two assessment types is “not when they are used but their
purpose and the effect that these practices have on students’ learning” (p. 490).
Indeed, summative assessment, apart from being seen as a single test/examination
upon completion of the discipline, can be conducted in the form of a series of tests
distributed throughout the course (Holmes, 2015), whereas formative assessment
can take the form of a single one-off test distributed incidentally to check under-
standing of a particular learning point without planning further intervention in case
of a positive outcome.
Moreover, there seems to be certain confusion in the literature with regard to the
meaning ascribed to the terms “continuous” or “ongoing” assessment. In fact, as far
back as 1995, Nitko pointed out the confusion existing among concepts associated
with continuous assessment. Since then, the meaning of the term has not been
delineated. For instance, some authors tend to equate the concept with the notion of
“formative assessment” considering them as synonyms (Barootchi & Keshavarz,
2002; Holmes, 2015). Although this may be true for certain teaching contexts, in
general the concepts of “formative” and “continuous” approaches to assessment are
only partially synonymous, with continuous assessment being an umbrella term
which encompasses both assessment types, formative and summative. Thus, there
are situations when ongoing assessment is used for diagnostic purposes and per-
forms an essentially formative function to help plan further teaching and provide
feedback to students (Harlen, 2005). According to Byram and Hu (2013), such
ongoing formative assessment may be of two types—planned or incidental, i.e.,
carried out at a given point of time to assess an instructional task. It seems
reasonable to designate this type of approach to assessment by the term continuous
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 333
formative assessment. On the other hand, information collected during in-course
assessment may be stored to be used at a later stage (Harlen, 2005) or be designed to
assess key transition points within the program (Cross & O’Loughlin, 2013;
Holmes, 2015) thus providing for the aggregate total at the end of the course.
This is where ongoing assessment becomes summative in nature and serves as a
motivational tool urging students to work more consistently throughout the course
(Cole & Spence, 2012; Holmes, 2015; Jolliffe, 1978; Svinivki & McKeachie, 2011;
Tuunila & Pulkkinen, 2015). This type of ongoing assessment may be called
continuous summative assessment. The interrelation between the formative and
summative approaches and their relation to continuous assessment is visualized
on Fig. 1:
However, where tasks which test a variety of aspects with regard to students’
knowledge, skills or competencies are repeated over time with regard to each
aspect, each task and feedback on it will have a combined nature: summative, in
that the task results in a grade and tells the student where they are at a particular
point of time in relation to the target set, and formative, as feedback provided on a
completed task will be used by students to enhance further learning, given that they
know the same type of skill/competency will be tested again. In other words,
students’ focus on feedback rather than their concentration solely on a grade
received are thus ensured by their knowledge that a similar task will be given
again and they can score higher on the following tests if they take into account their
present weaknesses and recommendations on how to deal with them. In this case,
students will acquire much more than merely grades for an aggregate total at the
end of the course.
Therefore, such a type of continuous assessment where a series of tasks with a
summative function, testing a variety of students’ skills and knowledge, are
repeated to contribute to a formative effect can be referred to as continuous
cumulative assessment. On the diagram above, it occupies the upper quarters of
the field. As opposed to this, continuous simple summative assessment is the term
that describes continuous summative assessment of the type where in a series of
tasks each task is designed to test a particular skill/competency/knowledge only
once to add to an aggregate total at the end. The combined nature of the continuous
cumulative assessment is illustrated in Fig. 2.
CONTINUOUS
FORMATIVE SUMMATIVE
ONE-OFF
Continuous
summative
Continuous
formative
One-off
formative
One-off
summative
Fig. 1 Interrelation between formative/summative approaches and the concepts of continuous and
one-off assessment
334 E. Popkova
2.1 Learner Issues Caused by Summative Assessment
2.1.1 Traditional One-Off Summative Assessment
The use of summative one-off assessment raises quite a few issues. Among these,
the most commonly identified issue pertinent to any discipline is that of reliability.
With students usually delaying studying until examination time (Kerdijk et al.,
2015; Svinivki & McKeachie, 2011; Tuunila & Pulkkinen, 2015), they eventually
must cover such a wide range of topics that dealing with any given subject in depth
inevitably becomes impossible (Dixon & Rawlings, 1987) and deep learning is
replaced by cramming. This, in its turn, inevitably leads to surface approaches to
learning, with students only trying “to retain the information until the exam is over
simply to pass the course” (Tuunila & Pulkkinen, 2015, p. 671).
Furthermore, much of the final examination result can depend on a student’s
physical condition at the time of examination, and thus affect the final grade which
otherwise might be different; a fact observed by Dixon and Rawlings (1987), but,
since then, overlooked by researchers. This, however, can have a significant
implication for students’ future attitude to studying: Whereas physical condition
is attributed to external causes affecting exam performance, students who believe
that their discouraging exam outcome depends on uncontrollable factors “will
likely spend less effort studying for subsequent exams, may fall behind in daily
assignments, and lose interest in the class” (Thibadoux & Greenberg, 1987, p. 123).
Apart from the above, another problem, as identified by Glover and Thomas
(1999), lies in summative assessment being “framed by where the pupil should be
going rather than where he is actually going” (p. 121), which leads to students’
misconceptions being internalized. According to the researchers, depriving students
of formative feedback on their understanding of the learning material, teachers may
fail to recognize the “point of divergence” in their learners, i.e. “the point where a
learner is about to go down a path that may eventually. . . lead to an internalized
misconception” (ibid.). In terms of foreign language teaching, the above view is
reflected in the concept of fossilisation of learners’ errors, an interlanguage phenom-
enon in which an underdeveloped linguistic form or construction becomes a perma-
nent feature of a learner’s interlanguage and shows resistance to correction (Han,
2013; Thornbury, 2011); one of the theories which aim to explain the phenomena
attributes fossilisation to the lack of corrective feedback (Thornbury, 2011).
repeated
CONTINUOUS
SUMMATIVE
FORMATIVE
incident
l
simple
planned
Continuous
cumulative
assessment
Fig. 2 Combined nature of the continuous cumulative assessment
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 335
2.1.2 Continuous Summative Assessment
Whereas a plethora of researchers and ELT professionals point out the advantages
that continuous assessment frameworks can bring into the teaching and learning
process as compared to one-off final tests (Cross & O’Loughlin, 2013; Holmes,
2015; Jolliffe, 1978; Nitko, 1995; Svinivki & McKeachie, 2011; Tuunila &
Pulkkinen, 2015), when it comes to continuous summative assessment, some
authors express a note of caution about, or even question, its use; among the reasons
named being the additional burden on students’ workload which may be a disin-
centive for some due to high pressure of course demands that not everyone is able to
keep pace with (Cole & Spence, 2012; Trotter, 2006).
Yet, others consider that continuous summative assessment reduces the potential
for providing effective feedback due to learners’ failing to act on feedback, since
they pay more attention to grades received rather than to what they need to improve
(Hernández, 2012). In other words, when feedback is accompanied by a grade, any
comments are eventually ignored by students who tend to only note the grade
(Hernández, 2012) and see no point in retaining what they see as no longer relevant
information. This is inevitably connected with a surface approach to learning.
At the same time, for some of particular concern is test anxiety induced by
frequent testing, especially so considering the high-stakes context of tertiary edu-
cation (Khanna, 2015). Interestingly, a study by Holmes (2015) established reduced
test anxiety level on a continuously assessed course; the strength of her argument
being supported by other related studies as cited by her.
The above considerations connected to the negative aspects of continuous
summative assessment are of affective nature. However, often this approach to
assessment merely represents a negative backwash of a teaching/learnings process
in which it is a rise in student test scores that is put in the spotlight. In terms of
foreign language teaching, this usually takes place in preparing students for stan-
dardized tests, such as FCE, IELTS, TOEFL, or others. At the same time, ELT
professionals acknowledge that the rise in scores does not necessarily mean the rise
in achievement, especially given the fact that teachers are able to train students to
pass practically any test (Harlen, 2005, p. 209).
2.2 The Case for Continuous Cumulative Assessment
In spite of the above identified issues connected to both types of continuous
assessment, this assessment approach is largely seen as ensuring far better learning
outcomes as compared to one-off final tests, with Dixon and Rawlings (1987)
already three decades ago maintaining that “in a system which allows coursework
to count, real standards rise” (p. 26). One of the major factors accounting for the
benefits of continuous assessment is, inter alia, connected to its partly formative
nature: This assessment type prevents students’ misconceptions from becoming
336 E. Popkova
cemented by monitoring development of their concepts over the long term, signal-
ling potential inappropriacy and improving understanding (Holmes, 2015). The
view is supported by Svinivki and McKeachie (2011) who note that assessing
students throughout the course helps diagnose misunderstandings and problems in
their learning “while they are still remediable” (p. 83). The cumulative type” of
continuous assessment provides even further benefits: Not only does it synthesize
feedback into the ongoing teaching and learning process (Cross & O’Loughlin,
2013, p. 587; Hernández, 2012; Nitko, 1995), it also makes feedback meaningful
for learners ensuring that it will be necessary for future learning.
On the other hand, this approach to assessment also takes into account the
beneficial features of its summative nature connected to grading students’ achieve-
ments. First of all, empirical experience of a great many members of the ELT
community proves that learners tend to use a strategic approach in allocating time to
homework mainly focusing on assessed tasks (Cole & Spence, 2012; Holmes,
2015). Consequently, continuous summative assessment by raising the worth of
the in-course work promotes the desired level of students’ involvement throughout
the course (Cole & Spence, 2012; Trotter, 2006; Tuunila & Pulkkinen, 2015) and
ensures the style of learning teachers expect from students (Svinivki & McKeachie,
2011). This results in more consistent attendance rates (Holmes, 2015; Khanna,
2015), which means, in its turn, as observed by Cole and Spence (2012, p. 516),
more consistent exposure of students to the course content during the semester. As a
logical consequence, such assessment schemes in education lead to better knowl-
edge retention and improvement in exam results (Jolliffe, 1978; Kerdijk et al.,
2015; Tuunila & Pulkkinen, 2015).
It is true that some studies demonstrate no significant differences between exam
performance of students who scored high on procrastination in relation to exam
preparation and that of more conscientious learners (Pychyl, Morin, & Salmon,
2000), but empirical evidence suggests that introducing continuous assessment
practices in the form of summative tests does produce a positive impact on students’
final learning outcomes (González et al., 2015; Jolliffe, 1978; Trotter, 2006). A
possible explanation of the fact is given by Kerdijk et al. (2015) whose analysis of
the difference in exam performance between the cumulative assessment group and
the end-of-course assessment group reveals that the former performed better with
regard to the content of the last 2 weeks of the course, which also led to slightly
higher examination scores. The authors of the study attributed the result to the lack
of exam preparation time on the part of the end-of-course assessment group who
concentrated on revising the course material from the start of the course while
disregarding what they thought was quite fresh in their memories. According to
Beagley and Capaldi (2016), the positive effect of cumulative assessment on final
examination scores is stipulated by a so-called “spacing effect”, i.e., students’
recurrent exposure to information at intervals. The spacing effect is believed to
increase long-term retention of information (Beagley & Capaldi, 2016; Kerdijk
et al., 2015) and, ipso facto, makes continuous assessment schemes more effective
in the long run. Similarly, benefits of repeated testing and retrieval-based learning
are emphasized in other studies (Karpicke, 2012; Khanna et al., 2013; Soderstrom,
Kerr, & Bjork, 2016).
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 337
Yet another noticeable benefit offered by continuous assessment is that it enables
teachers to provide a variety of assessment schemes to the students, thus providing
more reliable evidence to determine the overall grade by encompassing assessment
of students’ knowledge, skills, and abilities from multiple perspectives, i.e. by way
of data triangulation (Barootchi & Keshavarz, 2002; Nitko, 1995; Svinivki &
McKeachie, 2011); importance for assessment to be “based on multiple data
sources of information” being also emphasised in a comprehensive study by Kuh
et al. (2015, p. 2). Moreover, Tuunila and Pulkkinen (2015) argue that the variety of
tasks provided by continuous assessment helps to cater for students’ different
learning styles, thus ensuring that more students are able to demonstrate their real
level of knowledge and skills. The last statement is supported by the findings of
Carswell, Primavesi, and Ward’s (1987) study. Dixon and Rawlings (1987) also
established an apparent correlation between continuous assessment marks and
examination performance.
2.3 Challenges of Continuous Assessment in Tertiary
Education
Despite the evident benefits of continuous assessment, a prevailing majority of
tertiary educational institutions are still reluctant to incorporate it in their assess-
ment schemes. The literature analysis shows that the reasons behind this may be
established both at individual and institutional levels.
At the macro level, it is possible to speak about the so-called externally driven
compliance culture, a concept introduced in Kuh et al. (2015) to refer to the
necessity of most colleges and universities to comply with the demands of external
bodies which often appear to have no concerns over the usefulness of assessment
for learners and improving students’ performance. This fact, in its turn, urges
institutions to promote assessment practices that would simplify accounting pro-
cedures, which, as a rule, can usually be ensured by a traditional formal achieve-
ment test of a summative nature.
Another consideration because of which managerial requirements to measuring
students’ success at the end of the course are mainly centred around delivering a
formal achievement test is connected to the fact that management may dread
integration of less conventional assessment approaches within extremely high-
stakes tertiary contexts. Such approaches may, as noted by Cross and O’Loughlin
(2013), raise concerns over reliability and validity of assessment.
It should be noted that not only does the position of authority let tertiary
management impose their view on assessment upon course designs, it also results
in the fact that such “institutionalized assumptions on the role and purpose of
assessment permeate through to individual practices and beliefs” (Cross &
O’Loughlin, 2013, p. 586). The latter fact reveals itself in that the staff who are
used to conventional one-off summative approaches to assessment often find it
338 E. Popkova
difficult to adjust to a different pedagogical mode finding its demands challenging
(Cross and O’Loughlin, 2013, p. 592; De Lisle, 2015), or even requiring courage to
switch to learner-oriented assessment practices (Harlen, 2005, p. 210). In this
connection, Kuh et al. indicate the necessity for more professional development
opportunities for faculty as one of the priorities that institutions need if they intend
to advance assessment work (2015, p. 13).
Surprisingly, even those teachers who do not mind trying out new methods and
techniques in their teaching are not always enthusiastic about the new assessment
schemes. The major reason for dissatisfaction in such cases is a great amount of
workload associated with continuous assessment (Cole & Spence, 2012; De Lisle,
2015; Hernández, 2012; Johnson, 1984). Therefore, some researchers underline the
necessity to take into account difficulties experienced by individual teachers when
engaging with continuous assessment in practice: Thus, Cross and O’Loughlin
(2013) argue that “less is more, and programs would benefit from a reduced
assessment load” (p. 592). This, in their view, would allow teachers to allocate
more time to giving feedback on areas identified as weaknesses instead of having to
move quickly from one task to the next in the restricted time frame in a typical
university semester.
Closely connected to this is the issue of “initiative fatigue”, a term offered by
Kuh et al. (2015) to designate “a syndrome that commonly develops when cam-
puses are swamped by the competing demands of multiple initiatives” (p. 5) which
external bodies pile on faculty and eventually make them “feel overwhelmed by the
sense of “one more thing” (ibid.).
2.4 Research Problem and Rationale
Analysis of the literature has helped to establish the fact that although resources
investigating continuous assessment in general are sufficient, studies that focus on
continuous assessment of foreign language teaching in higher education are not
numerous. What is more, while some attention has been paid to investigating
continuous summative assessment, hardly any was devoted to research of contin-
uous cumulative assessment practices in the meaning of the term as delineated in
2. Indeed, no studies have been found that would explore the effect and reliability of
repeated testing of a variety of low-stakes tasks and its impact on the quality of
students’ in-course and exam performance, as well as their motivational levels. This
means that the mode of assessment in question remains understudied and its
potential benefits to the tertiary system of foreign language teaching may be
overlooked. The present study endeavours to fill this important gap by addressing
the following research questions: (1) Does continuous cumulative assessment in
tertiary education provide a more effective framework for foreign language teach-
ing in terms of learner outcomes as compared to other assessment types? (2) Is there
any correlation between continuous cumulative assessment grades and students’
exam performance? (3) How does the proposed assessment scheme impact on
students’ motivational levels?
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 339
3 Method
3.1 Participants and Setting
The study conducted in the 2015–2016 academic year involved 117 bachelor
students who studied in the 2nd and 4th years at the law faculty of the National
Research University Higher School of Economics, one of Russia’s leading univer-
sities. The 2nd year students (N ¼ 53) were divided into groups according to their
proficiency level: Groups 32 and 42 being diagnosed as B1+-B2 learners, groups 12
and 22 being diagnosed as lower B1–B1 and also being smaller in size. All the 2nd
year groups were doing an optional Legal English course. The 4th year students
(N ¼ 64), who were divided into 4 groups irrespective of their level of English, were
doing a compulsory course on Academic Writing. The choice of students from
different years of studies and different disciplines was to ensure comparability of
results across different age groups and disciplines, with all pertaining to teaching a
foreign language.
Eighty-minute classes in all groups took place once a week starting in
September. However, the 2nd year students were examined at the end of December,
whereas the course in the 4th year lasted until mid-March. A 10-point assessment
system was applied, with grades 8–10 corresponding to an excellent mark, grades
6–7 to a good one, grades 4–5 to a satisfactory mark, and those from 3 and below
leading to a non-pass.
Assessment schemes in the 2nd year included a variety of tasks within the
course, all of which contributed to a cumulative grade (CG) at the end; grades
(G) for different tasks being given different weights according to the following
formula:
CG ¼ 0; 4  GHOMEWORK
ð Þ þ 0; 3  GMID-TERM TEST
ð Þ
þ 0; 1GPRESENTATIONÞ þ 0; 2  GLMS TESTS
ð Þ:
ð
where LMS—learning management system.
The grade for homework was formed by a mean for a number of tasks aimed to
check the students’ preparation of the home task and included a variety of tested
activities, such as testing vocabulary, grammar or reading skills, as well as Russian-
English translation. The mid-term test was conducted in the middle of the semester
and focused on checking the students’ knowledge of content and legal terminology.
The presentation was a structured 8-min talk, and the LMS tests checked vocabu-
lary knowledge, listening and reading skills.
At the end of the course students had to pass a final exam which checked the
students’ knowledge of content through a series of content-related questions and
their Russian-translation skills. In combination with the cumulative grade, the
examination grade formed a total course aggregate where the cumulative grade
contributed to 60% of the final mark, and 40% of the total course aggregate was
formed by the exam grade.
340 E. Popkova
As for the 4th year course, the total course aggregate (TCA) was calculated on
the basis of the formula below:
TCA ¼ GWRITING-RELATED COMPONENT  0:6
ð Þ þ GPRESENTATION  0:4
ð Þ
where the writing-related component (WC) was, in its turn, calculated as follows:
WC¼ 0:4GCLASSWORK
ð Þþ 0:3GMID-COURSE TEST
ð Þþ 0:3GPROJECT PROPOSAL
ð Þ
“Classwork” in the above formula in fact implied a number of incidental tasks
aimed to check the students’ preparation of the home task. These were asked from
randomly selected students and included a variety of tasks, such as testing vocab-
ulary and writing clichés, knowledge of APA style formatting rules and of the
project proposal structure. Needless to say, due to the time constraints and groups’
size, and given the amount of explanations that the teacher had to do, it was just
natural that not every student was asked in class. The mid-course test took the form
of paragraph writing based on the presented materials; the students had to demon-
strate their paraphrasing and summarising skills, as well as their knowledge of
citation and quotation rules. The project proposal meant a mini research paper that
the students had to submit 2 weeks before the end of the course.
3.2 Data Collection
First of all, it should be noted that in terms of the Legal English course, where the
students’ retention of the material was tested at the end-of-course exam, the data
collection was aimed at establishing a potential correlation between cumulative
grades and exam grades. At the same time, given that the course grade for
Academic Writing was a sum of four graded tasks, each checking specific skills,
an assessment scheme was designed to check the effect of continuous cumulative
assessment on the quality of the students’ performance rather than its correlation
with the exam marks, as the case was on the Legal English course.
Another important consideration to be highlighted is that the assessment
schemes were initially based on the continuous simple summative approach, and,
due to the institutional requirements, the formula of calculating the cumulative
grade, as well as that of the aggregate course total, could not be changed. Therefore,
to test the potential effectiveness of continuous cumulative assessment, the core
principle of which lies in the combination of both formative and summative
functions, the nature of the tested tasks and feedback on them was significantly
amended rather than the schemes themselves. The general principle in teaching
both disciplines was to provide feedback to the students right upon completion to
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 341
increase testing effect benefits. In addition, the teaching process incorporated a
series of graded tasks repeated for each component throughout the course, each one
being low-stakes given the proportion of a grade assigned for it in the overall grade.
Regarding the components of the Legal English course, the LMS tests were tuned
to provide correct answers to the students once a test was finished; besides, they
were dispersed throughout the semester with equal time spacing. Class work was
designed to regularly include a series of tests on vocabulary, grammar and reading,
delivered repeatedly to the whole class, with each subsequent test including part of
the language tested before. To increase personalisation and effectiveness of feed-
back on Russian-English translation tasks, a two-stage assessment procedure was
introduced: First, translations were prepared by the students in pairs and peer
reviewed, after that the students had to rewrite their translations, taking into account
comments made by the classmates, and submit the final version for assessment by
the teacher. Although the first translation version did not carry any marks, the
students were penalised for failure to submit it by their grade for subsequent
translation being lowered. In order to introduce a formative component into assess-
ment of presentations given by the 2nd year students, each student was required to
deliver two presentations instead of one. The first presentation was peer assessed
based on the assessment criteria grid; the second presentation was assessed by the
teacher. It was the mean grade for both presentations that contributed to the
cumulative grade. At the end of the course, the mean cumulative and mean
examination grades were calculated; the results were analysed quantitatively to
establish the relation between the two. Attendance rates were analysed as well. As
for the qualitative aspect of the data analysis, it was ensured through the teacher’s
evaluation of the quality of the students’ progress in completing language tasks
throughout the course.
In terms of assessment of the 4th year students’ performance on the Academic
Writing course, an experimental group was chosen (N ¼ 9), the students in which
were required to regularly submit small pieces of writing that gradually formed the
body of their work. A two-stage assessment procedure was applied where the
students first had their pieces of writing peer reviewed, and then presented a revised
version to the teacher. Measures were taken to ensure timely submission of the first
version: Whenever a student failed to submit the first version of the assignment,
he/she was penalised by their grade for the second version of writing being lowered.
Moreover, the students in the experimental group gave a preliminary presentation
before being assessed to help them see potential points for improvement. At the end
of the course, the performance results and attendance rates of the experimental and
control groups were analysed quantitatively. Quantitative data analysis was based
on the teacher’s evaluation of the quality of the writing produced by the experi-
mental group.
342 E. Popkova
4 Results and Discussion
The results yielded by the data analysis are grouped below on the basis of the tested
variables according to the disciplines taught and the year of studies. In terms of the
grades received by the 2nd year students on their Legal English course, it was
established that the cumulative finals had a high correspondence with the exam
grades, irrespective of the group’s proficiency level, as illustrated by the data in
Table 1.
As is seen from the table, the variances between the cumulative and examination
grades fall within the range 0.1–0.4, with the total means being absolutely the same.
The exam grades of groups 22 and 32 were slightly lower than their cumulative
grades, whereas groups 12 and 42 scored slightly higher at the exam. Interestingly,
the attendance rates of groups 22 and 32, as shown in Table 2, were almost twice as
low than those of the other two groups.
The data collected with regard to the performance on the Academic Writing
course has revealed that overall performance on the writing-related component,
which was practiced during the longest part of the course, yielded significantly
higher results in the experimental group as compared to the control groups: The
variance between the means constituted 2.4. At the same time, the variance between
the means for presentations, which were practiced by the students in other subjects
apart from their English course, was considerably smaller, amounting to 0.5. The
grades for the presentations to a certain extent levelled the variance in the total
course aggregate between the control and experimental groups, with the final
cumulative and total aggregate variance being only 0.7, as compared to the enor-
mous discrepancy in the writing component means (Table 3).
Analysis of attendance rates has also shown a direct relation between cumulative
assessment and the students’ attendance; as outlined by the data in Table 4.
Table 1 Correlation between cumulative and examination grades of the Legal English course
Group №
Cumulative grade
Exam
grade
means for each component
Mid-term
test Presentations LMS tests Homework
Cumulative
final
12 15 5.7 6.7 9,5 6.7 7 7.3
22 10 5.3 5 5.5 6.8 5.9 5.4
32 12 6.5 6.8 7.2 6.7 6.5 6.1
42 16 7.7 7.7 7.6 7.7 7.5 7.8
Total
means
53 6.3 6.6 7.5 7 6.7 6.7
Table 2 Attendance rates on the Legal English course compared
Groups 12 22 32 42
Percentage of skipped classes (%) 14.3 24.3 33.3 17.4
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 343
As can be seen, the attendance in the experimental group was almost 3,5 times as
high as in the control groups. Perhaps not surprisingly, the quality of the writing
works produced by the experimental group in their mid-course tests, as well as the
quality of their project proposals presented later in the course, was noticeably
higher. Moreover, their works also required less marking, and the checking process
was less time-consuming for the teacher.
The obtained results have confirmed the great educational potential of continu-
ous cumulative assessment, both in terms of its reliability and in terms of its impact
on the quality of students’ performance. First and foremost, analysis of the results
yielded by the 2nd year groups demonstrates direct correlation between the cumu-
lative and exam grades; the variances between the variables generally being
insignificant. With that in mind, and given that the exam tested what was taught
in class, the small discrepancy between the variables, which did not reach even half
the measuring point, proves reliability of the continuous cumulative assessment
scheme. These findings go in line with the results obtained by other researchers
(Dixon & Rawlings, 1987; González, Jare~
no, & López, 2015). Moreover, it may be
worthwhile to consider a point of view advanced by Carswell et al. (1987) who,
noting that students’ in-course performance scores were successful predictors of
their failure in the final examination, went as far as to question the necessity of
major final exams.
Table 3 Correlation between mean grades between control and experimental groups on the Legal
English course
Group
№ of
students
Means for writing-related component
Presentation
Total
course
aggregate
Classwork
Project
proposal
Mid-
course
test Total
14 13 5 6.9 3.7 5.2 7.8 6.2
24 11 4.6 5.8 5.1 5 8.8 6.5
34 12 2.9 5.4 3.8 3.5 7.3 5
44 19 4.7 6.4 5.4 5.3 7.6 6.2
Means for
control
groups
55 4.3 6.1 4.5 4.8 7.9 7
Experimental
group (54)
9 7.6 7.9 6.1 7.2 8.4 7.7
Table 4 Attendance rates on
the Academic Writing course
Groups 14 24 34 44
Mean
for
control
groups
Experimental
group
Percentage
of skipped
classes (%)
42.3 42.2 33.3 36.8 38.4 10.3
344 E. Popkova
Having said that, the direction of the measured variances seems to have no
connection to the students’ level of language proficiency, but is rather directly
linked to their attendance rates: Whereas in more or less stressful conditions of the
exam the performance of the 2nd year groups with the highest attendance rates
turned out to be higher, the groups with the lowest rate of attendance demonstrated
lower results during the exam. This means that the groups that attended classes
more regularly benefited more from the testing effect of the class work which made
them retrieve material more often than just through studying for the final exam.
Moreover, their in-course performance was also much better than that of the other
group of the same proficiency level but with a lower attendance rate; the results that
contradict those yielded in the study by Dixon and Rawlings (1987) who stated that
“course-work marks are on average higher than the examination marks for the
weaker students, and lower on average for the stronger students” (p. 35).
It should be noted that the effect of being regularly exposed to testing would be
even more evident had group 12 not cheated on the LMS tests: Compared to the
results of the others, especially higher level groups, the results produced by group
12 are surprisingly high. Logically, such exceptionally good overall performance in
distantly-delivered tests in only one group, who did not show similar outstanding
results during in-class work, can be attributed to academic cheating. However, the
analysis of such is beyond the scope of this study. It can only be mentioned that if
group 12 had performed less successfully on the LMS tests and had received a lower
cumulative grade, given their mean exam grades the effect of better attendance
would stand out more clearly.
Similar straightforward dependency of the quality of the students’ performance
upon their attendance rates was observed on the Academic Writing course. While
no official examination that would measure the students’ achievement in academic
writing skills was held at the end of the course, with the final grade being composed
of a variety of assessed aspects, the experimental group who attended classes
regularly outperformed the others in a majority of the tested components; the
only exception being the students’ grades for presenting skills which were practiced
in other disciplines as well, including previous ESP courses, and where all the
groups performed generally well, given the extensive practice they had had before.
Besides, the formative potential of the introduced assessment scheme was also
registered in the students’ performance quality; a rise in quality, albeit not very
quick, was observed towards the end of the course across all the tested aspects both
in the 2nd year groups and the 4th-year experimental group. Here, it is possible to
agree with Trotter (2006) who observes that assessment is “process rather than
product and will go a good way towards encouraging a deep approach to learning”
(p. 508). The advantages of the proposed scheme are further highlighted by the
noticeably higher grades received by the 4th-year experimental group in all aspects
of the course as compared to the groups with continuous simple summative
assessment. It was just natural that the latter, given the vague perspectives of
being asked in class, largely disregarded preparation for classes, and their main
efforts were focused on preparation for the few graded components; procrastination
and late submission of assignments being not a rare case.
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 345
As for excessive test anxiety and the adverse effect on students’ level of
motivation that some attribute to frequent testing (Harlen, 2005; Khanna, 2015),
these negative effects may be avoided by making assessment low-stakes. It can
likewise be argued that such detrimental effects are also mitigated by a variety of
tasks applied in continuous assessment of the cumulative type, as it enables students
to catch up with the course grade requirements if they performed unsuccessfully in
one of the tests, or are generally not apt at one form of testing but are more apt at
others.
Similarly, there are concerns about the nature of continuous assessment leading
to surface approaches to learning. Although admitting that such claims are not
unreasonable, it should be noted that there are ways to deflect students’ learning in a
more beneficial direction by ensuring a variety of assessment tasks and adding
creativity and personalisation to testing. Besides, a crucial role in this respect is
played by the nature of feedback: Caution should be taken not to turn feedback into
mainly justification of students’ grades. Rather, feedback should include a “feed-
forward” component, i.e. focus on learners’ responses to feedback (Cross &
O’Loughlin, 2013; Hernández, 2012).
The study, however, is not without limitations. First, it is true that the sample
population for each tested variable was not large enough to enable reliable calcu-
lation of the standard deviation; this means that the established variances may, in
fact, be larger or smaller. Besides, the students’ attitude towards ongoing cumula-
tive assessment were not investigated; isolated comments made by the students
only generally confirmed the usefulness of the approach in terms of long-term
retention of the material. However, the yielded results seem to genuinely reflect
general relations and dependencies between the tested variables, and the overall
reliability of the inferences drawn seems to be hardly affected.
5 Conclusion
The results of the study have important pedagogical and methodological implica-
tions. First and foremost, they reveal the positive effect of the continuous cumula-
tive assessment on students’ extrinsic and intrinsic motivation; the former being
ensured by the frequency of tested tasks that help to keep learners in the “learning
rut”, whereas the latter is secured by the nature and timeliness of feedback provided
and its gradual effect on students’ progress and, therefore, their perceptions of
themselves as learners.
Moreover, given the high correspondence between the cumulative and exam
grades, the question arises about the necessity of the final examination; a proposal
advanced by other researchers of continuous assessment schemes as well (Carswell
et al., 1987; Fook & Sidhu, 2014). Thus, Fook and Sidhu (2014) suggest that all
final examinations be abolished both at the undergraduate and postgraduate levels
in higher education (p. 305).
346 E. Popkova
Another important factor in ensuring that the potential of continuous assessment
is not wasted is defined by Cross and O’Loughlin (2013) who emphasise the value
of greater teacher autonomy and suggest giving greater responsibility for assess-
ment procedures to the classroom-based agent rather than program coordinators or
administrators.
Furthermore, taking into account the nature of foreign language teaching which,
in contrast to disciplines where mainly knowledge acquisition is pivotal, also aims
to develop language skills and competencies, it is possible to argue that continuous
cumulative assessment presents far more benefits to foreign language teachers than
to those involved in teaching mainly knowledge transmitting subjects; as supported
by the findings of certain studies which revealed no impact of continuous assess-
ment on student performance (Johnson, 1984).
All in all, there is sufficient evidence to suggest that the continuous cumulative
approach to assessment, by presenting what Harlen (2005) calls “the synergy of
formative and summative assessment” (p. 220), more effectively supports students’
learning than traditionally applied schemes where the “feed-forward component” is
generally disregarded. Indeed, a series of summative tests of various forms distrib-
uted throughout the course serve multiple purposes: They ascertain students’
progress and level of achievement at different points within the programme,
guide the teaching process by revealing students’ weaknesses that must be worked
on, and ensure the desired level of student engagement on the course. Such an
approach proves that rather than being simply an end-of-course exercise to deter-
mine student grades, assessments can be learning experiences for students (Svinivki
& McKeachie, 2011, p. 73). The variety of tasks and a relatively long span during
which these take place in continuous cumulative assessment provide for the validity
of the cumulative grade. At the same time, its reliability is ensured by the diversity
of tested aspects: Whereas in a one-off summative examination students may get a
higher grade because of merely being lucky to have received a question they know
best or having learned the material by rote without adequately processing it and
having only ensured short-term retention, continuous cumulative assessment takes
account of every topic learnt in all the variety of tasks and, therefore, is a more
accurate reflection of a student’s overall attainment.
Following from the above, it is possible to suggest that further research is
required with regard to proportion of weights given to different tasks in EFL
assessment schemes. Indeed, whereas since the end of the twentieth century many
researchers have expressed concerns over establishing the correct balance between
tasks in the cumulative final, as well as about the relative percentages allotted to the
cumulative and examination grades in the final aggregate (Jolliffe, 1978; Nitko,
1995), no studies have been found that would provide a reasonable basis for
calculation of such in higher education, even more so in case of foreign language
teaching. This presents a perspective for further studies in the area, which, admit-
tedly, should include a comprehensive number of participants. Besides, it would be
worthwhile to explore students’ attitudes to cumulative assessment and to deter-
mine the most adequate time spacing between graded tests, perhaps alternating
them with ungraded ones, which is claimed by Khanna (2015) to lead to higher
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 347
cumulative final exam performance and reap the benefits of test-enhanced learning
without inducing test anxiety (p. 117). The impact of continuous cumulative
assessment on preventing fossilization of EFL students’ mistakes is, as it seems,
yet another promising area for researchers.
To conclude, although practicality of testing can create a concern for many
teachers in case with continuous cumulative assessment, it is believed that advan-
tages of continuous assessment outweigh its disadvantages, and a prudent approach
to test designs and assessment workload, as well as sharing responsibility for such
between a number of teachers on the course, will ultimately benefit teachers as well.
One should accept the fact that there will always be concerns of this or that kind
connected with any approach to assessment, and no approach will become a one-
serve-all solution. The teaching community should aim to find the necessary
balance between different types of assessment based on their estimated effective-
ness factor rather than adopt assessment approaches one simply feels comfortable
with being unwilling to try less conventional practices or putting simplicity of test
delivery at the core of test design.
References
Barootchi, N., & Keshavarz, M. H. (2002). Assessment of achievement through portfolios
and teacher-made tests. Educational Research, 44(3), 279–288. https://doi.org/10.1080/
00131880210135313.
Beagley, J. E., & Capaldi, M. (2016). The effect of cumulative tests on the final exam. PRIMUS, 26
(9), 878–888.
Byram, M., & Hu, A. (Eds.). (2013). Routledge encyclopaedia of language teaching and learning
(2nd ed.). London: Routledge.
Carswell, F., Primavesi, R., & Ward, P. (1987). Qualifying exams for medical students: Are both
major finals and continuous assessment necessary? Medical Teacher, 9(1), 83–90. https://doi.
org/10.3109/01421598709028983.
Cole, J. S., & Spence, S. W. (2012). Using continuous assessment to promote student engagement
in a large class. European Journal of Engineering Education, 37(5), 508–525. https://doi.org/
10.1080/03043797.2012.719002.
Cross, R., & O’Loughlin, K. (2013). Continuous assessment frameworks within university English
Pathway Programs: Realizing formative assessment within high-stakes contexts. Studies in
Higher Education, 38(4), 584–594. https://doi.org/10.1080/03075079.2011.588694.
De Lisle, J. (2015). The promise and reality of formative assessment practice in a continuous
assessment scheme: The case of Trinidad and Tobago. Assessment in Education: Principles,
Policy & Practice, 22(1), 79–103. https://doi.org/10.1080/0969594x.2014.944086.
Dixon, R., & Rawlings, G. (1987). Experiences with continuous assessment. Assessment &
Evaluation in Higher Education, 12(1), 24–36. https://doi.org/10.1080/0260293870120103.
Fook, C. Y., & Sidhu, G. K. (2014). Assessment practices in higher education in United States.
Procedia – Social and Behavioral Sciences, 123, 299–306. https://doi.org/10.1016/j.sbspro.
2014.01.1427.
Glover, P., & Thomas, R. (1999). Coming to grips with continuous assessment. Assessment in
Education: Principles, Policy & Practice, 6(1), 117–127. https://doi.org/10.1080/
09695949993035.
348 E. Popkova
González, M. D., Jare~
no, F., & López, R. (2015). Impact of students’ behavior on continuous
assessment in higher education. Innovation: The European Journal of Social Science Research,
28(4), 498–507. https://doi.org/10.1080/13511610.2015.1060882.
Han, Z. H. (2013). Forty years later: Updating the fossilization hypothesis. Language Teaching, 46
(02), 133–171. https://doi.org/10.1017/S0261444812000511.
Harlen, W. (2005). Teachers’ summative practices and assessment for learning – Tensions and
synergies. The Curriculum Journal, 16(2), 207–223. https://doi.org/10.1080/09585170500136093.
Hernández, R. (2012). Does continuous assessment in higher education support student learning?
Higher Education, 64(4), 489–502. https://doi.org/10.1007/s10734-012-9506-7.
Holmes, N. (2015). Student perceptions of their learning and engagement in response to the use of
a continuous e-assessment in an undergraduate module. Assessment & Evaluation in Higher
Education, 40(1), 1–14. https://doi.org/10.1080/02602938.2014.881978.
Johnson, P. E. (1984). An aspect of continuous assessment. International Journal of Mathematical
Education in Science and Technology, 15(6), 713–714. https://doi.org/10.1080/
0020739840150605.
Jolliffe, F. R. (1978). An evaluation of continuous assessment in statistics courses for social
scientists. International Journal of Mathematical Education in Science and Technology, 9
(2), 177–181. https://doi.org/10.1080/0020739780090206.
Karpicke, J. D. (2012). Retrieval-based learning: Active retrieval promotes meaningful learning.
Current Directions in Psychological Science, 21(3), 157–163. https://doi.org/10.1177/
0963721412443552.
Kerdijk, W., Cohen-Schotanus, J., Mulder, F., Muntinghe, F., & Tio, R. (2015). Cumulative versus
end-of-course assessment: Effects on self-study time and test performance. Medical Education,
49(7), 709–716.
Khanna, M. M. (2015). Ungraded pop quizzes: Test-enhanced learning without all the anxiety.
Teaching of Psychology, 42(2), 174–178. https://doi.org/10.1177/0098628315573144.
Khanna, M. M., Brack, A. S., & Finken, L. L. (2013). Short- and long-term effects of cumulative
finals on student learning. Teaching of Psychology, 40(3), 175–182. https://doi.org/10.1177/
0098628313487458.
Kuh, G. D., Ikenberry, S. O., Jankowski, N., Cain, T. R., Ewell, P., Hutchings, P., & Kinzie,
J. (2015). Using evidence of student learning to improve higher education. San Francisco:
Wiley.
Nitko, A. J. (1995). Curriculum-based continuous assessment: A framework for concepts, pro-
cedures and policy. Assessment in Education: Principles, Policy & Practice, 2(3), 321–337.
https://doi.org/10.1080/0969595950020306.
Pychyl, T. A., Morin, R. W., & Salmon, B. R. (2000). Procrastination and the planning fallacy: An
examination of the study habits of university students. Journal of Social Behavior and
Personality, 15(5), 135–150.
Soderstrom, N. C., Kerr, T. K., & Bjork, R. A. (2016). The critical importance of retrieval – and
spacing – for learning. Psychological Science, 27(2), 223–230. https://doi.org/10.1177/
0956797615617778.
Svinivki, M., & McKeachie, W. J. (2011). McKeachie’s teaching tips: Strategies, research, and
theory for college and university teachers (13th ed.). Belmont, CA: Wadsworth Cengage
Learning.
Thibadoux, G. M., & Greenberg, I. S. (1987). Accounting students’ perceptions of factors
influencing exam performance. Journal of Education for Business, 63(3), 123–125. https://
doi.org/10.1080/08832323.1987.10117292.
Thornbury, S. (2011). An A–Z of ELT: A dictionary of terms and concepts. Oxford: Macmillan.
Trotter, E. (2006). Student perceptions of continuous summative assessment. Assessment & Evalu-
ation in Higher Education, 31(5), 505–521. https://doi.org/10.1080/02602930600679506.
Tuunila, R., & Pulkkinen, M. (2015). Effect of continuous assessment on learning outcomes on
two chemical engineering courses: Case study. European Journal of Engineering Education,
40(6), 671–682.
Continuous Cumulative Assessment in Higher Education: Coming to Grips with. . . 349
Developing Student Teachers’ Classroom
Assessment Literacy: The Ukrainian Context
Olga Ukrayinska
Abstract Nowadays the scope of ESL teachers’ roles has changed. They are
expected not only to teach what is written in textbooks and then make their students
do grammar tests aiming to assess their progress but to assess fairly how successful
their own teaching was and what strengths and weaknesses their students have, to
which extent their progress corresponds to standards. As target language use
situations vary greatly so there cannot be a unique bank of ready-made tests to be
used in any educational environment. Likewise, not all establishments can afford to
hire professionals to develop a test for them. Thus, teachers need to be able
themselves to design their own tasks, tests. Unfortunately, it is common practice
in Ukraine not to enhance student teachers’ assessment literacy. And not all
teachers are creative and competent enough to write items and develop tests.
Moreover, they should know some bases of designing tests. It is not sufficient
just to read a book and then produce an objective test. There should be some
training and, without a doubt, some feedback. The European Union has published
some papers on quality assurance. Amidst the ways to improve quality of education
they mention fair assessment and give some corresponding recommendations.
However, these recommendations are fairly vague. They are not practical tips or
direct guidelines to be implemented. Thus, some guidelines and checklists need to
be developed for teachers to enable them to design purposeful, objective and valid
tools for assessing their students’ progress.
Keywords Classroom language assessment literacy • Designing tests • Student
teachers
O. Ukrayinska (*)
Kharkiv Skovoroda National Pedagogical University, 189, Geroyiv Stalingradu Avenue, appt.
67, Kharkiv 610096, Ukraine
e-mail: ukrolga1981@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_17
351
1 Introduction
The chapter is devoted to speculations on theoretical bases and practical issues for
provision of the development of student teachers’ classroom language assessment
literacy. It monitors an assessment course taught at Kharkiv Skovoroda National
Pedagogical University, Ukraine. Improving quality of education has been becom-
ing a matter of concern throughout the world. In line with the policy of globaliza-
tion of education having been pursued by European countries, standards and
guidelines aiming to guarantee quality assurance in institutions of higher education
have been issued (Standards & Guidelines, 2015). Their implementation is
expected through providing internal and external quality assurance and creating
corresponding agencies.
Ukraine, claiming to join in the process, has already taken some steps. Thus, for
instance, as for the external level, in 2014 the National Agency for Quality
Assurance in Higher Education was created, in 2004 External Independent Testing
(called ZNO) was introduced (and since 2009 for foreign languages as well) for
school-leavers to be passed in order to be admitted to Ukrainian universities. In its
turn, on the internal level quality assurance should be provided through redesigning
programs, carrying out objective assessment of students’ progress and recruiting
competent teaching staff (Standards & Guidelines, 2015, pp. 11–16). However, so
far in Ukraine little has been done in the area of language assessment. The only
groundwork is the appearance of a pilot version of the National Curriculum for
Foreign Language Teacher Education (Methodology) (2016) as an output of the
“New Generation School Teacher” project jointly initiated by the British Council in
Ukraine and the Ministry of Education and Science of Ukraine and launched in
March 2013.
The curriculum is aimed at improving student teachers’ proficiency in method-
ology (for bachelors). The old one, comprising a unit devoted to “Assessment of
pupils’ progress and achievements”, nevertheless, could not be claimed to develop
language assessment literacy of student teachers as it is but just encompassed a brief
overview of some most common types and methods of assessment and pedagogical
conditions of their provision. Whereas the new curriculum includes the unit “Test-
ing and Assessment” with 18 contact hours allocated for it and presents the
corresponding learning outcomes as follows: “Select existing tests to assess
learners’ progress and achievement; design new tests to assess learners’ progress
and achievement; assess and evaluate learners’ reading, listening, speaking and
writing skills using set criteria; identify different types of errors in learners’ spoken
and written language and deal with them appropriately” (Core Curriculum, 2016,
p. 3). Thus, we can assert that it is oriented on developing assessment literacy
in-service and, what is more important, strives for teaching to develop assessment
materials, which is something absolutely new for Ukraine.
It is common knowledge that language assessment gives grounds for teachers to
gather information about their students’ learning progress and, thus, reveal diffi-
culties which they encounter, check efficiency of techniques used, confirm
352 O. Ukrayinska
achieving outcomes stated in the program and, generally, gauge the effectiveness of
their teaching. However, despite the significance of assessment both for students
and teachers, they all consider tests as a nuisance. Such a deep mistrust of tests is
frequently well-founded in Ukraine. It cannot be denied the fact that a great deal of
language tests produced in the country is of very poor quality, irrespective of
whether it is a formative school test, a handbook for preparation for the External
Testing (ZNO) or a postgraduate admission test. Whilst not many Ukrainian
stakeholders do realize that carrying out valid assessment requires special knowl-
edge and skills which means additional training besides the traditional course on
methodology.
With all this, tests are becoming more and more popular in Ukraine as anywhere
else. This tendency necessitates teachers’ awareness of testing techniques and
principles of their application in the learning process to enhance its efficiency and
prepare effectively pupils to pass external independent tests. To meet this need, in
Kharkiv Skovoroda National Pedagogical University at the Foreign Languages
Department the course “Introduction into Testing” was launched in 2009 for
students doing their specialist and master degrees (the 5th year of study). It should
be mentioned that is the first and the only course on language assessment taught in
Ukraine. The course is meant to supplement the basic course of methodology,
which itself contains a module dedicated to classroom assessment. As a matter of
fact, the course is still being designed. Its goal is to increase the professional
competency of student teachers. Meanwhile, its objectives, content and evaluation
methods are under analysis. They are being correlated with new empirical data
gathered after each year of studying. In addition, further research is needed to
specify the range of activities offered to students to facilitate their acquisition of the
subject.
Thus, this chapter explores a course designed to address the assessment needs of
student teachers. Data generated from teaching and analyzing the course contrib-
utes to research about developing classroom language assessment literacy
in-service. The structure and content of the course under analysis is described.
Considerations about modifying the course at its best are outlined.
2 Theoretical Background
Generally, there is a large number of works dedicated to either selection, adaptation
or design of teaching materials, general didactic design. However, both theoretical
bases and practical guidelines are addressed to practicing language teachers and
concern materials used for teaching purposes. Since assessment is part of the
learning process, our belief is, though, that some approaches and guidelines can
be adapted to teaching students to write items or design tests. The point is that it is
assessment, but also it concerns teaching students to assess. In order to design a
good item, apart from the high level of proficiency in English, creative, critical,
analytical writing skills and analytical reading skills are needed.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 353
Books written or edited by Brian Tomlinson are considered to be the cornerstone
reference works for the subject of the production of ELT materials, can be used as a
resource for the stimulus and refreshment of teachers, publishers and applied
linguists in the field. Although covering a wide range of aspects such as: a glossary
of basic terms for materials development, principles and procedures of materials
development, materials evaluation, electronic materials delivery, they are meant for
postgraduates, teachers with at least limited previous experience in the field (e.g.,
Tomlinson, 2011). In fact, the issue of materials evaluation is widely raised as it is a
process where, as in testing, no unique ‘recipes’ can be provided. Howard and
Major (2005) in their guidelines for designing teaching materials to in-service
teachers examine advantages of teacher-produced materials which we can also
apply to teacher-made tests and thus contribute to enhancement of qualities of
test usefulness (Bachman & Palmer, 1996):
• Contextualization, i.e., correspondence to learning outcomes, curriculum
requirements, objectives of a course (to enhance validity and reliability).
• Responsiveness to learners’ individual needs (to enhance authenticity and
interactiveness).
• Personalization, i.e., taking into account interests and learning styles of pupils
(to enhance interactiveness).
• Timeliness, i.e., responsiveness to current local and international events
(to enhance validity, authenticity and interactiveness).
• Availability of resources when ready-made materials are scarce concerning
some target aspect (to enhance validity and reliability).
Another important issue considered in publications on teaching is authenticity
and adaptation of materials. We have found little evidence of adapting authentic
texts in books on testing, though it is essential for the item writers. Despite an
avalanche of sources, it might be problematic to find a text suitable for testing
purposes, responding to the curriculum requirements and assessees’ individual
characteristics. Hence, it becomes a challenge for a non-native speaker item writer
to adapt materials in terms of vocabulary, grammar, nature and length to fit the
targeted level of proficiency of test takers.
In Ukraine and Russia, the issue of developing designing student teachers’ skills
has been widely researched by educationalists. Thus, for instance, Ivashova (2007)
established general didactic basis for developing language teachers’ skills to design
tasks. Klepikova (2009) described the procedure of applying information and
communication technology to developing student-teachers’ skills of pedagogical
design. Meanwhile there are no developed methodologies concerning teaching
assessment literacy to students, test design in particular.
In accordance to the modern educational paradigm the scope of roles of the
teacher has significantly changed. A teacher should be able to carry out continuous
assessment of learners’ achievements and as well design various assessment
methods in line with modern standards, be flexible enough to change the way
he/she teaches in response to assessment results. New requirements to the language
teacher have been implemented in frame documents. In the European Portfolio for
354 O. Ukrayinska
Student Teachers of Languages apart from the skills constituting assessment liter-
acy “I can evaluate and select valid assessment procedures” (Newby et al., 2007,
p. 52), ability to interpret results, report scores and give feedback (Newby et al.,
2007, p. 53), self- and peer assessment (Newby et al., 2007, p. 54), abilities to assess
various aspects (Newby et al., 2007, pp. 55–56) and to conduct error analysis
(Newby et al., 2007, p. 57), we find such skills as “I can design learning materials
and activities appropriate for my learners” (Newby et al., 2007, p. 31) and “I can
design and use in-class activities to monitor and assess learners’ participation and
performance” (Newby et al., 2007, p. 52). This recommendation has been taken into
account in the framework for English teachers at the starting stage in the curriculum
In-service Teacher Professional Development (2012, p. 27) as a skill “can write
simple test exercises”.
A large number of works, predominantly by British and American authors, are
dedicated to test design (e.g., Bachman & Palmer, 1996; McNamara, 1996) and
item writing (e.g., Alderson, Clapham, & Wall, 2005), namely structure of tests,
types of tests and test tasks (e.g., Hughes, 2003), advantages and disadvantages of
various formats, rating scales (e.g., Upshur & Turner, 1995), administration and
scoring procedures, validation of tests, assessing basic communication skills:
assessing writing (e.g., Weigle, 2002), speaking (e.g., Douglas, 1994; Fulcher,
2003; Luoma, 2004), reading (e.g., Alderson, 2000), listening (e.g., Buck, 2001),
and assessing vocabulary (e.g., Read, 2000) and grammar (e.g., Purpura, 2004).
These fundamental works and more recent ones can provide both classroom
teachers and language researchers with comprehensive description of testing basics.
Another massive group of publications presents researches on large-scale interna-
tional proficiency tests (e.g., rating oral interviews in IELTS by Brown, 2007). We
can explain this by commercialization of the field as thousands of candidates over
the globe take these tests to get a certificate confirming their level of English
proficiency which allows them to get a job or apply to universities abroad. To
make these tests more ‘attractive’ for customers researches are mainly done to
increase validity of tests.
Despite the fundamental nature of the works mentioned above, their indisputable
scientific value and practical orientation, they can intimidate by mathematics or
psychometric jargon, metalanguage and generalizability of information, for student
teachers do not have any work experience to be able to differentiate various
learning contexts, to use methodical terminology without confusing it, and, what
is more important, they are intellectually and psychologically immature in perceiv-
ing a large mass of abstract information when analyses and deductions are needed.
A number of books were published to promote the current language policy of the
European Union. They are claimed to help standardize the instruction and build fair
assessment in an easy and practical way. Below there is their short overview.
Common European Common European Framework of Reference for Lan-
guages: Learning, teaching, assessment (2001) contains definitions of competen-
cies of language users, six-level descriptors of language proficiency scales for
productive and receptive skills. It is a helpful tool for designing assessment
materials, foremost for designing analytical rating scales, but requires special skills
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 355
of working with descriptors as their formulation is not detailed and, therefore, they
can be interpreted differently depending on the context of the target language
acquisition.
Insights from the Common European Framework (Morrow, 2007) offers clari-
fication of terminology from the CEFR, updated versions of descriptive scales,
description of the procedure of working with the CEFR for self-assessment and for
test design purposes. The document is claimed to be built in such a way that a
teacher herself can align the content, formats of tests and criteria of evaluation with
the CEFR using a check-list. But it should be born in mind that assessments must be
conducted in the same way by all teachers working in the same context which is not
possible unless they co-operate.
In Developing and assessing intercultural communicative competence: A guide
for language teachers and teacher educators (Lazar, Huber-Kriegler, Lussier,
Matei, & Peck, 2007) a definition of intercultural competence is given, its structure
is described, guidelines to its developing and assessing are formulated. Ukrainian
teachers have not got used to developing and, consequently, to assessing this
competence yet, because it is difficult to build constructs, as not only knowledge
of facts about culture are concerned but also skills to apply this knowledge
effectively to achieve a certain communicative goal.
In Insights from the Common European Framework (Morrow, 2007) those who
are engaged in quality assurance can find practical tips and tasks, the description of
types and methods of assessment, evaluation procedures. The tasks work the way to
make teachers think about applicability of assessment in this or that case, about
interpreting and using scores and about washback.
In Relating Language Examinations to the Common European Framework of
Reference for Languages: Learning, Teaching, Assessment (CEFR) (2009) and its
updated version Relating language examinations to the Common Framework of
Reference for Languages: Learning, teaching, assessment (CEFR). Highlights from
the manual (2011) there are guidelines on how to use the CEFR, description of
procedures and approaches to assessment, specifically writing test specifications,
design and validation of exam tasks and clarifications of how to use descriptive
scales. These documents are meant to provide teachers with necessary knowledge
for making valid and reliable tests.
Pathways through assessing, learning and teaching in the CEFR (2011) focuses
on learners and teachers working jointly and in coordination on developing com-
petences. There is description of aims, functions and types of achievement assess-
ment. This frame document assists teachers in changing their attitude to work and
incorporate the newest educational tendencies into their classes.
Developing classroom tests takes a significant amount of time and human and
material resources, which is a burden on teachers because of scarcity of their time,
but due to standardization of the process of development tests become valid,
objective and reliable. In the Ukrainian context, school teachers have to design
tasks and tests for classroom assessment and for various educational contests. Many
Ukrainian stakeholders argue that tests should be either selected from the current
rich array of test handbooks or produced by methodologists, specialists from local
356 O. Ukrayinska
district public education departments, therefore there is no need to teach students to
design tasks and tests. Then naturally a question arises “Where do these specialists
learn to design tests?”. At universities, teachers themselves write tests. It is com-
mon practice that students of the same year taught by different lecturers in different
groups at the same Ukrainian university get different tests to take. Classroom tests
are often called low-stake. One, two or a series of low-quality tests seemingly will
not affect pupils or students, but, if just to think about it in the long run, conse-
quences can be disastrous. For this reason, numerous publications address the issue
of classroom assessment.
Originally, the term ‘assessment literacy’, coined by Stiggins, stands for knowl-
edge and skills to plan assessments, administer them, interpret and apply the results
of assessments accurately and effectively. Furthermore, Weigle (2007, p. 194)
claims that “teachers frequently need to prepare their students for externally
mandated large-scale writing assessments, and thus they need to have an under-
standing of the uses and misuses of such tests” and she provides suggestions for
how to incorporate considerations about assessment into a course on teaching
writing or as a stand-alone course. Taylor (2009) insists on all stakeholders getting
assessment literacy, mentions the roles of exterior organizations in it and considers
assessment standards documents with a range of skills for teachers. Fulcher (2012)
offers an updated definition of assessment literacy:
The knowledge, skills and abilities required to design, maintain or evaluate, large-scale
standardized and/or classroom based tests, familiarity with test processes, and awareness of
principles and concepts that guide and underpin practice, including ethics and codes of
practice. The ability to place knowledge, skills, processes, principles and concepts within
wider historical, social, political and philosophical frameworks in order understand why
practices have arisen as they have, and to evaluate the role and impact of testing on society,
institutions, and individuals. (p. 13)
Beziat and Coleman (2015) add that teachers also need to be able to discuss the
results of assessments with parents and students. Hidri (2015) provided evidence
that high level of assessment literacy of stakeholders can lead to improvement of
accountability. The authors cited above emphasize that courses aimed to develop
assessment literacy should focus on classroom-based practices, reflect needs and
capabilities of practicing teachers and prepare student teachers for a variety of
classroom and assessment contexts.
At the symposium Enhancing Language Assessment Literacy: Sharing, Broad-
ening, Innovating held on 16–17 September 2016 at Lancaster University pre-
senters addressed various themes of enhancing assessment literacy and
considered practices related to it but differently across contexts. Having analyzed
the abstracts, we have found the following considerations crucial in the context of
our research: by Fulcher about operationalization of assessment literacy in learning
contexts for teachers (Enhancing Language Assessment Literacy, 2016, p. 8), by
Malone about enhancing students’ understanding of language assessment literacy
(Enhancing Language Assessment Literacy, 2016, pp. 9–10), by Kiddle about
online professional development of assessment literacy through virtual learning
teams (Enhancing Language Assessment Literacy, 2016, p. 11), by Tsagari, Green,
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 357
Vogt, Csepes, Sifakis urging the need to develop an efficient, relevant and sustain-
able LTA training infrastructure for ELTs to help them develop relevant assessment
literacy (Enhancing Language Assessment Literacy, 2016, pp. 11–12), by
Eberharter about fostering language assessment literacy through teacher involve-
ment in high-stakes test development (Enhancing Language Assessment Literacy,
2016, p. 15), by Westbrook about the project, aimed at developing assessment
literacy among university lecturers and school teachers in the Russian Federation
(Enhancing Language Assessment Literacy, 2016, p. 16). As it can be seen, little or
no attention is given to preparation of pre-service teachers.
2.1 Research Problem
Bare theoretical knowledge of basic terminology, main test types and task formats
is not evidently sufficient both for students and even practicing teachers for
designing tests or writing items of high or even satisfactory quality. Thus, the
problem arises concerning organization of efficient preparation, 18 contact hours
long on the 4th year and 16 contact hours long on the 5th year, for students to be not
only assessment literate but also be able to design classroom tests. Constant
research is being done to shape the course. Empirical data is gathered to enhance
the content and teaching techniques. The process is impeded with a number of
issues. The researcher herself is considering various solutions to which aspects to
teach, that is to say, which skills constitute classroom language assessment literacy
and which of them are crucial for students to learn while at university, how to
develop them within a limited period of time, how to evaluate levels of assessment
proficiency and how to make learning more student-friendly but not at the expense
of quality bearing in mind that a certain level of maturity is required from students
to profit from the knowledge they obtain.
Many Ukrainian scholars do not support the idea of teaching students to design
tasks explaining that this target group does not have any practical work experience
and hence cannot appreciate all the nuances of the real-life context. Consequently,
according to them, this competency must be formed during in-service teacher
professional development. However, our belief is that student teachers will be
more competent assessors having acquired at least some initial preparation than
those who have not got any. Furthermore, there is another point to support this idea.
When students are taught at universities they get unified knowledge and develop a
unified perception of the nature of assessment and conditions of its provision. This
contributes to certain standardization. As a result, such competency could greatly
benefit not only students themselves but also their potential pupils and the system of
assessment in Ukraine as a whole. Moreover, the appearance of numerous European
frame publications on assessment makes it difficult for teachers to keep up with the
newest tendencies and understand how to implement all those guidelines in a
meaningful context to fit the requirements to a modern teacher.
358 O. Ukrayinska
The analysis of the mentioned European frame documents allows to make a
conclusion that each tertiary level establishment should take into account the
standards, pursue the development plurilingualism and assist in raising professional
level of teaching staff. Researchers of the field of assessment are faced with new
objectives such as developing the construct for assessing the intercultural commu-
nicative competence, to ensure learning autonomy of students, organizing learning
in the target language, relating tests to the CEFR, standardizing valid and objective
assessment of students’ achievements and using various forms and instruments of
assessment.
Summing up what has been mentioned above, we can define factors which
necessitate developing Ukrainian student teachers’ assessment literacy as follows:
requirements of the new curriculum of methodology and of the curriculum of
in-service teachers’ professional development, recommendations in European
frame documents and a range of real-life teachers’ responsibilities. Initial training
can be provided at pedagogical universities where the syllabus of the special course
must reflect all the relevant aspects.
Alongside with the difficulty to create a theoretical framework for the course
there is another more practical-oriented problem. Cheating is commonplace in
Ukraine especially now with the appearance of mobile phones with built-in cam-
eras. It is challenging both for school teachers and university lecturers to arrange
valid assessment of their learners’ achievements. It is especially problematic when
it comes to doing objective test tasks where assessees should just select a letter or
write T or F. Furthermore, cheating makes piloting more complicated because tasks
are copied somehow and then passed next year to those who are to take them as
tests. Teachers have to think of various tricks in order to eliminate this problem.
They change the order of tasks per each group of students, make negative True/
False statements positive and vice versa, read tasks when it is possible instead of
giving them in a written form. These and other measures affect reliability.
3 Method
The research under consideration of the nature of assessment literacy and peculiar-
ities of its development is being done on the materials obtained while teaching the
course “Introduction to Testing” taught at Kharkiv Skovoroda National Pedagog-
ical University from 2009 up to present.
3.1 Materials
The materials given to the students to be evaluated as part of their training and
assessment were tasks taken from test handbooks for preparation (e.g., Cambridge
Practice Tests), past test papers (Cambridge exam suite, ZNO tests), random tests
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 359
designed by Ukrainian or Russian teachers. The materials submitted by students to
be evaluated collectively at class or be assessed by the teacher were student-
selected and student-made test tasks. A study guide containing tasks related to the
theory and practice was made by the teachers of the Department (Tuchyna,
Kamynin, & Ukrayinska, 2015).
3.2 Participants
Below we provide the description of the target group of students doing the course.
The average characteristics are taken into consideration.
Number of Students Each year approximately 150 students do this course.
Degree Participants are students who complete their preparation to be either
school (specialist degree, master degree) or university (master degree) teachers of
English and another foreign language.
Academic Year The 5th year of study.
Age Students are 20–21 years old.
Sex Female prevails.
Cultural Background Students are Ukrainians.
Work Experience Some students already work at school and/or English courses,
tutor private pupils, but still there are those who have no teaching experience at all
apart from the teaching practice at schools on the 4th year.
Motivation Most students are not motivated, have low interest in learning anything
related to teaching. In Ukraine, many graduates of pedagogical universities choose
not to teach or if to teach then not at school. Hence in schools there is a lack of
teachers.
Prior Knowledge of the Subject Under Consideration Students have basic knowl-
edge of assessment and corresponding basic skills obtained from the course on
methodology taught on the 4th year.
Procedures To ensure student teachers’ development in terms of building up their
assessment literacy, we made use of a number of research methods.
An exploratory study was conducted to inform the content of the course “Intro-
duction to testing” and to serve as a pilot study for further research on developing
assessment literacy of Ukrainian student teachers. The study included a pre-course
document analysis of fundamental publications on testing to outline the content.
The course comprises four lectures and eight seminars plus 30 academic hours
allotted for individual work. The lectures are devoted to the aspects as follows:
360 O. Ukrayinska
Lecture 1. Evaluation, assessment, testing. (Types of tests. Test structure.
Approaches to testing. Common European Framework of Reference: Learning,
Teaching, Assessment. Main characteristics of tests.)
Lecture 2. Test design and piloting. Testing within the Ukrainian educational
system. (Stages of test construction. People involved in test design. Best-
known test techniques, their advantages and disadvantages. External Indepen-
dent Testing.)
Lecture 3. Testing use of English. Testing receptive skills. (Testing grammar and
vocabulary. Testing listening comprehension. Testing reading.)
Lecture 4. Testing productive skills. (Testing writing. Testing speaking. Rating
scales.)
Afterwards tasks for the study guide to be used at seminars were written. Various
types of tasks were exploited to answer learning objectives better and to familiarize
students with test formats: Matching, multiple choice, True/False, table completion,
answering questions, listing, ordering, filling the gaps, cloze tests, open questions,
error correction, discussion, debates and task design. Students do these tasks
individually, in pairs or in groups. Questionnaires are frequently used to follow
students’ progress in forming assessment literacy. Appendix 1 presents a question-
naire to be filled by students at the beginning of the course. Their answers are then
discussed with the teacher. The aim is to see what students already know about
assessment and make them reflect on what constitutes valid assessment.
Evaluation of assessment tasks selected by students in accordance with a set of
criteria or developed by them during the course allowed data collection and analysis
to coincide with the course requirements. Further stages of the study may include
additional sources of data such as students’ and practicing teachers’ interviews.
Evaluation of student-made tasks was performed with the application of method of
expert judgments. It was implemented by the researcher herself and an experienced
teacher instructor who both had received prior LTA training provided by the British
Council in Ukraine. The judges’ task was to reach agreement on the quality of the
test tasks constructed by students. It contributed to identifying participants’
strengths and weaknesses in constructing test tasks and allowed to make compar-
isons of the tasks of the previous years. Furthermore, observations of students
analyzing ready-made tasks appeared to be fruitful. Tests were used as a formative
and summative assessment method intended to check students’ progress.
Along with the methods mentioned above, piloting of activities offered to
students was carried out. The aim was to see how attractive and helpful they are
for students. Moreover, the results of piloting allowed to widen the scope of tasks,
make them more practical. The author served as a course instructor and worked
regularly with participants in face-to-face format. In the future, it is planned to
introduce on-line format which will allow to save the time and involve students into
working with information technologies.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 361
4 Results and Discussion
The procedures described above have been conducted for 7 years. The experiences
are drawn to adjust the materials and methods each year. Empirical data collected
served to:
– Align the objectives of the course with its content and evaluation methods.
– Communicate our expectations to students.
– Establish a logical sequence of learning milestones.
– Select appropriate teaching techniques.
– Allow us to self-evaluate based on stated expectations.
The content of the course was outlined on the basis of the contents of funda-
mental works on assessment mentioned above and analysis of definitions of assess-
ment literacy. In addition, time limit played its role. Apart from outlining the
content it appeared to be challenging to select task formats to develop the target
competency. Initially only receptive tasks were used but they served solely to help
students remember basic terminology and principles, meanwhile failed to teach
them apply this knowledge. It necessitated introducing a series of productive tasks
alongside with objective ones. Thus, matching tasks are used to teach students to
differentiate basic testing terminology, to familiarize them with basic testing
principles (e.g., Match the terms with the definitions. Match the examples of
assessment with the washback effects. Match parts of the task to their names (see
Example 1 in Appendix 2). Also, ordering task type is exploited (e.g., Order stages
of test development. Order parts of the tasks. Order the descriptors of the CEFR
scales.). Another technique is doing ready-made tasks following the rubrics. This
activity is used to get students familiarized with the formats and help understand
what they check. When doing proofreading tasks, students are asked to classify/
rank mistakes.
When it appeared that students copy homework from students of other groups,
the decision to make learners justify their answers in True/False tasks was taken. In
fact, since then a scope of inference, analogy, follow on and viewpoint questions
have been asked to confirm not only comprehension of the material but also to
check whether students did the tasks themselves. There is a series of activities based
on task analysis. This activity proves to be effective to prepare learners for selecting
tasks according to certain parameters. Students are given examples of some item
formats suggested in language testing handbooks and asked to evaluate them in
accordance with the task evaluation checklists:
(a) • Recognition/production is required.
• Difficulty of construction.
• Difficulty and reliability of marking.
(b) • What aspect it checks.
• Whether it is possible to check it against the key.
• Proficiency level.
• Advantages and disadvantages.
362 O. Ukrayinska
(c) • Communication skill.
• Level of difficulty.
• Type of response (open, guided).
• Quality.
• Correspondence to curriculum, characteristics of pupils.
Students found it to be extremely difficult to identify the level of the texts
according to the CEFR, to match the text types to levels and also to match types
of questions to levels. To eliminate this problem, we attempted to engage students
into retrieving components from the CEFR relying on the school curriculum.
Though it was rather time-consuming and we ended up in making numerous tables,
it not only assisted students in doing the tasks but also made them familiarize with
the core document. Another problematic area for students was to identify the
construct of the tasks. They identified the main skill tested but when asked to tell
which, for instance, reading subskill was tested, most of them failed to do this. And
vice versa, when they were asked to select an item testing listening for detailed
understanding or some other subskill, many were puzzled.
When the course was taught for the first year, the part of the summative
assessment was to develop tasks for schoolchildren. The results were low, not to
say disastrous. Some students brought ready-made tasks, which was quite obvious,
the others had problems with time allotment and complying their tasks with the
curriculum and children’s age characteristics. One student had a question “How old
is your husband?” on the list of interview questions. Another brought six photos of
different accommodations to be contrasted and compared in pair format without
supplying any real-life like situation. Although they were explained how to allocate
time for doing tasks, almost everybody doubled or tripled the actual time needed.
Since then the priority has been changed and students are asked not to design their
items but to bring ready-made tasks in accordance with the set parameters. With
each year taught the results are becoming better and better but still many students
fail to do it successfully. The solution might be to give more ready-made tasks for
analysis, but due to the lack of time it cannot be implemented. Hence the providers
of the course are in the constant search for methods to improve the quality of tasks
submitted by students. In Appendix 2 you can find more examples of activities from
the study guide (Tuchyna et al., 2015).
One of our objectives was to define student teachers’ competency in testing, to
define which skills constitute their classroom assessment literacy. The structure of
the competency allows to shape the content of the assessment course taught and
further plan workshops. Having retrieved components from the definitions of
assessment literacy considered above we obtained the following list of
corresponding constituent skills:
• To plan assessments.
• To design assessments.
• To administer assessments.
• To interpret assessments results.
• To apply the results of assessments accurately and effectively.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 363
• To discuss the results of assessments with parents and students.
• To evaluate classroom based tests.
• To identify the uses and misuses of externally mandated large-scale assessments.
These skills are generalized and reflect main stages/procedures of assessment.
Our strong belief is that it is not sufficient to build the content of an assessment
course on as various numerous subskills underpin each ability. Thus, elaboration of
such subskills must be pursued which will allow to set more concrete objectives to
be attained.
In their presentation Assessment literacy for language teachers Berry and
O’Sullivan (2016) summarized existing constituents of teachers’ assessment liter-
acy and displayed the results of the British Council teachers’ survey. They offer a
wider range of skills, which can be either added to the list of skills we have already
had or be included as their subskills. Thus, “identifying and evaluating appropriate
assessments for specific purposes within specific contexts”, “analyzing empirical
data in order to improve one’s own instructional and assessment practices” and
“integrating assessment and its outcomes into the overall pedagogic process” also
reflect main stages of assessment and can be regarded as skills. Other constituents
given in Table 1 are to be revisited and classified and if needed further developed
according to corresponding skills.
In line with the competence-based approach another objective is to differentiate
skills and subskills according to levels of assessment proficiency. To do this, we
offer to refer to Bloom’s taxonomy of domains of educational activities thought of
as “the goals of the learning process” (1956):
• Creating.
• Evaluating.
• Analyzing.
• Applying.
• Understanding.
• Remembering.
We offer our version of the structure of the competency of classroom assessment
literacy in Appendix 3. It is represented with skills in accordance with the main
stages of test design and provision and subskills of lower and higher levels.
Table 1 Assessment literacy skills by Berry and O’Sullivan (2016)
• Writing items for tests
• Developing core skills tests, basic skills
tests, integrated skills tests
• Defining assessment criteria
• Developing rating scales
• Establishing the reliability of tests
• Collecting validity evidence for tests
• Using basic statistics to analyze tests
• Washback effect in the classroom
• Scoring and/or rating (including rater reliability)
• Evaluating tests for usefulness
• Fairness
• Adapting existing tests to local contexts
• Identifying the construct of test tasks
• Defining assessment objectives
• Awareness of local contexts
• CEFR
364 O. Ukrayinska
5 Conclusion
My observations and experiences as a course instructor provide an invaluable
opportunity to carry out further study in field and, vice versa, implement results
of research immediately, be flexible in achieving outcomes and introduce changes
as needed. The description of the Ukrainian experience may inspire teachers from
other countries where assessment literacy is not taught to start their own investiga-
tion of the problem and try to introduce their own assessment course at universities.
The results of the present study may be implied by applied linguists in further
research of classroom language assessment literacy of student teachers.
Progress has been made in enhancing assessment literacy in the Ukrainian
context, but many important research questions concerning in-service teachers’
preparation remain open. In the present study, a framework of student teachers’
classroom language assessment literacy is suggested with the aim improve the
content of an assessment course, though some subskills need to be further clarified.
The research should also produce tools with a reasonable ease of implementation
for evaluating the efficacy of the course and students’ assessment proficiency.
Hence descriptors for each level of attainment must be developed. For more
effective teaching and research purposes it is necessary to classify tasks used to
develop assessment literacy at universities. The practice of teaching the course in
Kharkiv Skovoroda National Pedagogical University sets the task to search for
corresponding didactic means of confronting cheating and compensating the lack of
students’ motivation. Still the study provided little data contributing to teaching
effectively students to write items and design tests. Attempts were made with the
introduction of analytical tasks but they have been barely fruitful so far. There are
also other issues to be considered as a part of the course. Thus, future research may
consider organizing teaching basic descriptive statistics and bases of computer-
based testing to students.
While there are still many questions left unanswered about developing student
teachers’ assessment literacy, and many possible solutions to be drawn from
considering various contexts, we have aimed in this chapter to establish theoretical
grounds for an assessment course for a limited range of works regarding teacher
education and language testing exists. The present research study was conducted
during the course “Introduction to Testing” taught to Ukrainian students. The
course is designed as an introduction to language assessment and focuses on
helping students develop their language assessment skills. We emphasized that
for quality assurance assessment plays a great role. Moreover, we determined
factors necessitating fostering classroom assessment literacy in the Ukrainian
context. There has been given a short overview of activities used for teaching the
competency ranging from simple tasks for recognition to analytical and creative
tasks. We outlined difficulties that can arise in working with frame documents. The
present study resulted in developing the structure of classroom language assessment
literacy including skills and subskills.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 365
Appendix 1: Lead-in Questionnaire
When teachers assess, they . . . Always Seldom Never
1. Apply different criteria of evaluation
2. Admit that mistakes are an integral part of the learning process
3. Want to see whether their students’ level of achievement
corresponds to the objectives set
4. Strive for revealing difficulties which their students have
5. Check efficiency of the techniques used
6. Inform students about their performance
7. Rely on external standards
8. Try to reduce competition among students
9. Respect personality of students
10. Give their students individual tasks.
11. Take into consideration learners’ needs
12. Take into consideration each learner’s level of proficiency
13. Make students do creative tasks
14. Reinforce their authority
15. Make students respect their teachers
16. Count mistakes
17. Try to use various tasks
18. Introduce new task formats different from those used at classes
19. Administer written tests
20. Tell students beforehand what is expected from them
Appendix 2: Examples of Tasks by Tuchyna et al. (2015)
Example 1
Task structure. Do the matching tasks. In each there is one extra option that you do
not need to use. Discuss the peculiarities of the structure of tasks for testing different
skills. What is the function of each part of a task? Which aspect do the tasks test?
(a) Match the names of the parts of a task (A–D) to the parts (1–3).
A. Visual stimulus
B. Rubric
C. Verbal stimulus
D. Instructions
1. ________
Look at the photos. Discuss the task below with your partner. You have 2 min to find
a solution.
366 O. Ukrayinska
2. ____
You have decided to go on holidays together. Where would you like to go? Why?
3. _____
Photo 1 Photo 2
Example 2
Read the situations. Decide whether it is evaluation, assessment or testing.
(1) A researcher asked your group to fill in a questionnaire. She wanted to find out
according to which criteria your teacher assesses your pieces of writing.
(2) Maksim, a Ukrainian school-leaver, has to pass ZNO tests to apply to a
university.
(3) At the end of the lesson the teacher gave her pupils a text to read and then they
do True/False tasks. She marked the papers. The pupils got their marks for the
lesson.
(4) An official from a local district public education department came to check
school progress records.
Example 3
Types of tests. Read the situations. Identify the type of the tests.
(1) At the first lesson the teacher of the English language school administered a
test. The students wrote an essay, did some grammar tasks and a multiple-
choice task where they chose appropriate vocabulary units.
(2) Vadim wants to get a job in an international company. Recruiters asked him to
bring a certificate confirming his ability to speak English. Vadim has to pass
a test.
(3) You wrote a test on your first year. According to the results of this test your
English teachers divided you into two groups: A and B.
(4) At a university a new course is going to be introduced but the content to be
taught hasn’t been decided on yet. To design the syllabus, the teacher asked her
students to answer a number of questions.
(5) After studying the future tenses pupils had a test and got their marks for this
module.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 367
Example 4
Choosing text types. For testing which listening skills can the following types of
audio texts be used?
(1) The announcement of arrivals at the airport.
(2) Two girls talking about their new hobby.
(3) A theatre critic doing a review of the recent play.
(4) An examiner describing the test procedure.
(5) Political scientists discussing a new political reform.
(6) Sport commentary.
(7) Part of the lecture on medieval art.
(8) The advertisement of some cosmetic.
Example 5
Matching rubrics to skills. Read the rubrics below and decide which listening
skills are targeted in each test task.
(1) Listen to the conversation of the customer discussing hotels with the travel
agent. For questions 1–4, write the name of the hotel next to the picture which
matches the description you hear.
(2) You are going to hear a short radio programme which gives information about
suitcases. Before you listen, look at the questions below very carefully. For
questions 1–5, fill in the missing information in the spaces.
1. The main disadvantages of leather suitcases are that they are both __ and
________.
(3) You are going to hear a runner describing the route he took round the city of
Bristol. As you listen, mark the route which the runner took.
(4) Listen and answer the questions (1–5).
1. Jonathan implies that e-book availability
A is limited compared to printed books.
B is determined by unknown factors.
C is too dependent on technology.
D is not influenced by individual publishers.
Example 6
Analyze the textbook for Form 7. Which writing activities are offered there?
Decide which test format you would use to test writing.
368 O. Ukrayinska
Appendix 3: Classroom Language Assessment Literacy
Skills and Subskills
• To plan assessments:
Lower level subskills: To feel the need for assessment, to set the frequency of
assessment, to set the goal, to gather information about interests of the target
group, to carry out needs analysis of the target group, to take into account
specific local contexts, to identify and evaluate appropriate assessments for
specific purposes within specific contexts
Higher level subskills: To define assessment objectives, to reflect testees’
interests in content of tests, to reflect language needs in content and formats of
tests
• To develop assessments:
Lower level subskills: To select appropriate assessments from ready-made
tests within specific contexts (including: To differentiate various types of test
tasks and test formats, to identify the construct of ready-made test tasks, to
identify the construct of ready-made tests), to adapt existing tests to local
contexts, to define assessment criteria, to select appropriate rating scales, to
adapt appropriate rating scales, to trial tests, to review tests
Higher level subskills: To design assessments: To write constructs complying
with the requirements of curriculums/the frame documents, to write tasks for
tests (to select/write appropriate rubrics, to select materials for listening or
reading in compliance with the construct, to adapt materials for listening or
reading in compliance with the construct (in terms of length/duration, level of
difficulty of grammar/vocabulary, individual characteristics of testees), to select
non-visual/visual inputs, to write items for tasks (ordering, editing, TRUE/
FALSE, matching, gap-filling, multiple choice), to create distractors, to write
keys, to write items for testing speaking, writing, to formulate expected perfor-
mances), to pilot items, to review items; to develop tests: To develop analytical/
holistic rating scales, to use basic statistics to analyze tests, to establish the
reliability of tests, to collect validity evidence for tests, to evaluate tests for
usefulness
• To administer assessments:
Lower level subskills: To apply analytical rating scales, to apply holistic
rating scales, to serve as administrator, interlocutor, assessor (to rate
performances)
Higher level subskills: To serve as interlocutor and assessor
• To mark subjective and objective tests
• To score tests
• To interpret assessments results
• To apply the results of assessments:
Lower level subskills: To report the score, to discuss the results of assess-
ments with parents and students, to integrate assessment and its outcomes into
the overall pedagogic process
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 369
Higher level subskills: To analyse empirical data to improve their own
instructional and assessment practices, to provide positive washback effect in
the classroom, to confront negative washback effect in the classroom
• To evaluate classroom based tests (their own or ready-made)
• To identify the uses and misuses of externally mandated large-scale assessments
• To teach strategies of doing various test tasks
References
Alderson, C. J. (2000). Assessing reading. Cambridge: Cambridge University Press.
Alderson, C. J., Clapham, C., & Wall, D. (2005). Language test construction and evaluation.
Cambridge: Cambridge University Press.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice. Oxford: Oxford University
Press.
Berry, V., & O’Sullivan, B. (2016). Assessment literacy for language teachers. TEA SIG PCE.
Retrieved from http://tea.iatefl.org/wp-content/uploads/2015/10/Vivien-Berry-and-Barry-
OSullivan.pdf
Beziat, T. L. R., & Coleman, B. K. (2015). Classroom assessment literacy: Evaluating pre-service
teachers. The Researcher, 27(1), 25–30.
Bloom, B. S., Engelhart, M. D., Furst, E. J., Hill, W. H., & Krathwohl, D. R. (Eds.). (1956).
Taxonomy of educational objectives, Handbook I: The cognitive domain. New York: David
McKay Co Inc.
Brown, A. (2007). An investigation of the rating process in the IELTS oral interview. In Studies in
language testing: IELTS collected papers (19) (pp. 98–139). Cambridge: Cambridge Univer-
sity Press.
Buck, G. (2001). Assessing listening. Cambridge: Cambridge University Press.
Common European Common European Framework of Reference for Languages: Learning,
teaching, assessment. (2001). Cambridge: Cambridge University Press.
Core curriculum: English language teaching. Methodology. Bachelor’s level. (2016). Retrieved
from http://media.wix.com/ugd/15b470_9539e82cee3b464a809bbca0444343a4.pdf
Douglas, D. (1994). Quantity and quality in speaking test performance. Language Testing, 11,
125–144.
Enhancing Language Assessment Literacy: Sharing, broadening, innovating. (2016, September
16–17). Lancaster University, UK: Language Assessment Literacy Symposium.
Fulcher, G. (2003). Testing second language speaking. Pearson Education: Longman.
Fulcher, G. (2012). Assessment literacy for the language classroom. Language Assessment Quar-
terly, 9(2), 113–132.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary
and university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Howard, J., & Major, J. (2005). Guidelines for designing effective English language teaching
materials (pp. 101–109). Seoul, South Korea: PAAL9, October 2004. In Proceedings of the 9th
Conference of Pan-Pacific Association of Applied Linguistics.
Hughes, A. (2003). Testing for language teachers. Cambridge: Cambridge University Press.
In-service teacher professional development. (2012). Kyiv: British Council in Ukraine, Ministry of
Education and Science of Ukraine.
Ivashova, L.N. (2007). Razvitie umenija proektirovat’ uprazhnenija v processe professional’noj
podgotovki uchitelja inostrannogo jazyka [Development of the skill of designing tasks in the
process of professional training of a foreign language teacher]. Extended abstract of candi-
date’s thesis. Yelets: Lipeckij filial GOU VPO “NGLU im. N.A. Dobroljubova” [in Russian].
370 O. Ukrayinska
Klepikova, A.G. (2009). Podgotovka budushhego uchitelja k ispol’zovaniju pedagogicheskogo
dizajna v professional’noj dejatel’nosti [Preparation of student teachers for using pedagogical
design in their professional activity]. Candidate’s thesis. Belgorod: BGU [in Russian].
Lazar, I., Huber-Kriegler, M., Lussier, D., Matei, G. S., & Peck, C. (Eds.). (2007). Developing and
assessing intercultural communicative competence: A guide for language teachers and teacher
educators. Strasbourg: Council of Europe.
Luoma, S. (2004). Assessing speaking. Cambridge: Cambridge University Press.
McNamara, T. F. (1996). Measuring second language performance. London: Longman.
Morrow, K. (2007). Insights from the Common European Framework. Oxford: Oxford University
Press.
Newby, D., Allan, R., Fenner, A.-B., et al. (2007). European portfolio for student teachers of
languages: A reflection tool for language teacher education. Strasbourg: Council of Europe.
Noijons, J., Beresova, J., Breton, G., & Szabo, G. (2011). Relating language examinations to the
Common Framework of reference for languages: Learning, teaching, assessment (CEFR).
Highlights from the manual. Strasbourg: Council of Europe.
Piccardo, E., Berchoud, M., Cignatta, T., Mentz, O., & Pamula, M. (2011). Pathways through
assessing, learning and teaching in the CEFR. Strasbourg: Council of Europe.
Purpura, J. E. (2004). Assessing grammar. Cambridge: Cambridge University Press.
Read, J. (2000). Assessing vocabulary. Cambridge: Cambridge University Press.
Relating Language Examinations to the Common European Framework of reference for lan-
guages: Learning, teaching, assessment (CEFR). (2009). Strasbourg: Council of Europe.
Standards and guidelines for quality assurance in the European higher education area (ESG).
(2015). Brussels: EURASHE.
Taylor, L. (2009). Developing assessment literacy. Annual Review of Applied Linguistics, 29,
21–36.
Tomlinson, B. (Ed.). (2011). Materials development in language teaching. Cambridge: Cambridge
University Press.
Tuchyna, N. V., Kamynin, I. M., & Ukrayinska, O. O. (2015). Introduction to testing: Guidelines.
Kharkiv: Kharkiv Skovoroda National Pedagogical University.
Upshur, J., & Turner, C. E. (1995). Constructing rating scales for second language tests. English
Language Teaching Journal, 49, 3–12.
Weigle, S. C. (2002). Assessing writing. Cambridge: Cambridge University Press.
Weigle, S. C. (2007). Teaching writing teachers about assessment. Journal of Second Language
Writing, 16(3), 194–209.
Developing Student Teachers’ Classroom Assessment Literacy: The. . . 371
Valid for the Elites? The Trade-Off Between
Test Fairness and Test Validity
Kioumars Razavipour
Abstract The relationship between validity and fairness has been heatedly debated
in the literature (Kunnan, 2010). The orthodoxy is that test fairness should be
subsumed under validity; that is, a valid test ensures fairness. Tracking the test
retrofits of a high stakes national language test in Iran, known as Specialized
English Test (SET) used to determine admission to tertiary English language
programs, and collecting data on test takers’ language learning experiences, this
article argues against the established view that more valid tests would necessarily
promote fairness. Being an achievement measure based on secondary school
English curriculum, the previous version of the SET was widely criticized for its
construct underrepresentation (Farhady & Hedayati, 2009) and fizziness. In its
current version, the SET is more construct representative, for it goes beyond the
high school curriculum and covers more areas of communicative competence.
Data collected from 173 undergraduate students of English translation and
literature in three national universities across the country revealed that an over-
whelming majority of students come from families with high socio-economic
status, with poorer students represented only in low-tire university student popula-
tion. This finding indicates that the improvement in validity has come with a cost in
fairness and social mobility; hence, reproducing existing social order by denying
underprivileged applicants access to quality tertiary education language programs.
The paper further discusses issues of test validity and fairness and calls for
a broader understanding of test consequences within a larger sociocultural
perspective.
Keywords Fairness • Validity • Ethics • Construct representation • Specialized
English test
K. Razavipour (*)
Department of English Language and Literature, College of Literature and Humanities,
Shahid Chamran University of Ahvaz, Ahvaz, Iran
e-mail: K.razavipour@scu.ac.ir
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_18
373
1 Introduction
Tests are one realization of human beings’ long lasting quest for social justice
(Kunnan, 2014; Weir, 2005). In the absence of tests, opportunities and resources
would be allocated based on factors other than ability and achievement, as centuries
of tyrannies across the globe would attest. Nevertheless, as is the case with all
human inventions, tests are not perfect neutral means in fulfilling their promise of
social justice. In fact, evidence suggests that inasmuch as they are good instruments
to foster justice, tests can equally be exploited to prevent social justice from
prevailing. In particular, the mathematical and statistical guise associated with
tests, which gives a false impression of neutrality and impartiality, has been
exploited by the powerful to forward their own agendas, which are often in conflict
with the ideal of social justice (Fulcher, 2009; Hamp-Lyons, 2000; Shohamy,
1998). This packaging of tests in mathematical objective language requires a
collective awareness on the part of all stakeholders as to the motives behind giving
tests, the uses to which they are put, and the processes through which they are
designed. If they are to remain true to their democratic, social justice mission for
which they were invented in the first place, tests in general and language tests in
specific have to be fair to all citizens.
Test fairness has been variously defined as lack of bias, equitable treatment or
equality of opportunity (Kunnan, 2014). Most research in language testing on
fairness revolves around the first two concerns of test fairness. Attempts to keep
test content free of sensitive topics or language and of stigmatizing races, genders
and minorities fall within the purview of bias studies (McNamara & Roever, 2006).
The well-known DIF studies are also motivated by a concern for lack of bias in that
they seek to uncover items or cluster of items which function differentially for
groups of test takers with similar competence in the target construct (ibid). As to
equitable treatment, the very point of standardized testing in its uniform adminis-
tration procedures is about levelling the playing field for all test takers (Davies,
1997; Kunnan, 2014). Regarding the third meaning of fairness, however, there is a
paucity of research on equal access to the learning materials and opportunities that
would contribute to improved competence in the construct being measured by tests
(Kunnan, 2000). It then seems that concerns for fairness have been taken more
seriously in the choice of test items and in test administration than in test use and in
equal access to educational opportunities that need to be in place to render legiti-
mate comparisons of performance on a seemingly uniform test taking procedure
(Bachman, 2000; Shohamy, 2000). A lack of consensus among scholars concerning
the scope of fairness and validity is perhaps the main reason why the latter meaning
of test fairness has remained under-searched. In fact, the dominant view in language
testing is that fairness would be ensured through test validity. Accordingly, it is held
by many that once attempts are made to keep a test free of construct irrelevant
variance and construct underrepresentation (Messick, 1996; Xi, 2010), fairness
would be automatically catered for.
374 K. Razavipour
The present study challenges the view that ensuring adequate content coverage
and purging a test of construct irrelevant variance would necessarily guarantee
fairness. To do so, we will draw on data related to SET, a high stakes examination
annually administered to high school graduates seeking admission to tertiary
education programs in Iran. The remainder of this paper is structured as follows:
After reviewing the pertinent literature on test fairness, SET exam will be briefly
introduced. Afterwards, methods of the study are elaborated on, followed by results
from statistical analyses. The paper concludes with a discussion of the results and
the implications regarding SET in particular and fairness in language testing in
general.
2 Theoretical Background
2.1 Epistemology, Political and Moral Philosophy, and Test
Fairness
In the wake of postmodernism, a heighted awareness of social and political issues
involved in social sciences has given rise to the emergence of critical subfields for
orthodox fields of social science; hence, critical applied linguistics in applied
linguistics (Pennycook, 2001). Informed by a constructivist epistemology, how-
ever, the critical movement may appear self-defeating by maintaining that all
reality is socially constructed, going so far as to believe that even traits thought to
be determined by biology are believed to be constructed by sociocultural factors.
Thus, constructivism in its radical readings goes so far as to hold that one is not born
a woman; one becomes a woman (De Beauvoir, 2014). While the constructivist
thinking is superb in its ambition, it seems to be founded on shaky premises
(Corson, 1997; Fulcher, 2009, 2014, 2015). Applied linguists have either been
reluctant to embrace the critical aspirations or have embraced them with a con-
structivist epistemology that strips the very object of enquiry of any reality.
According to Corson (1997) the urge to being critical has not found appropriate
resonance in applied linguistics. Corson laments that “applied linguistics seems to
have developed and presently operates without any coherent or consistent guiding
philosophy, much less with a general awareness of the emancipatory potential that
is has.” (p. 166). He further warns against adopting conservatism and staying
neutral to the injustices that can be dealt with by applied linguists. Drawing on
ideas of other prominent social theorists such as Bourdieu, Wittgenstein, and
Habermars, he further maintains that the appropriate epistemology for the object
of inquiry in critical applied linguistics is critical realism, which holds that people’s
reasons and accounts of their conditions are real not constructed versions of reality
because only if they are ascertained this way, would it make sense to promote
agency and emancipation. He states that
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 375
Neutrality is always a problematic stance to adopt in applied areas . . . to profess neutrality
often means no more than condoning an unsatisfactory status quo, which may be ‘unsat-
isfactory’ largely because it is not adequately interpreted out of unrealistic respect for an
ideology of neutrality (pp. 166–67).
Corson sees the choice of epistemology an ethical choice because of the rami-
fications that it would have for stakeholders of research. He further asserts that a
critical realism epistemology for applied linguists amounts to contributing “more
directly to improving the human condition” (p. 166). For language testing, which
happens to be the most positivistic among other areas of applied linguistics (McNa-
mara & Roever, 2006), to avoid the “ideology of neutrality” is to be always mindful
of the uncertainty that is inherent in the inferences and decisions that we make
based on test scores. By extension, it also demands that we be mindful of the agency
of tests and that of social structures and the interaction of the two (see Potter &
Lopez, 2001). In other words, neutralizing the influence of no number of construct
irrelevant factors would per se render our tests neutral. “The notion of just a test” is
an impossibility because it is impossible to separate language tests from the
contexts in which they operate (Shohamy, 1998, p. 333). Hanson (1993, cited in
Shohamy, 1998) attributes the power of tests in their recourse to the language of
numbers considered by the public as indexes of objectivity, and the unequal power
relationships between test givers as organizations, and test takers as individuals.
Thus, for question related to test fairness to fall in place, the adoption of the
appropriate epistemology is of vital importance.
In addition to epistemology, the political philosophy has also been found to bear
on how fairness is treated in educational testing. Attempts have been made to link
political philosophies that underpin political systems and their approaches to test
use (Carlsen, 2009; Fulcher, 2009). The rationale behind such attempts is that not
only should we study the effects of tests on society, but we should also be aware of
the effects of societies on how tests are perceived, designed and used. Fulcher
maintains that collectivist and individualistic societies take different approaches to
test use. As such, in the former individual rights and freedoms are likely to be
ignored or violated to establish conformity, control, and surveillance, whereas in
the latter tests are used to promote individual success, independence, and prosper-
ity. In other words, test fairness is more likely to be violated in collectivist societies.
Yet, the predication from political philosophy to test use is yet to be supported by
solid empirical evidence.
The most recent attempt at positioning language test fairness within a broad
moral philosophical model has been made by Kunnan (2014). Drawing on insights
from normative ethics in moral philosophies of Rawls and Sen, Kunnan explains
that the Western moral philosophy has until recently been dominated by the
controversy between the teleological and deontological approaches to morality. In
the former, individual rights should be dumped, if necessary, to bring about the
greatest good for the greatest number of people. As such, it is the consequences of
actions that should guide moral decision making not their inherent rightness. This
view resonates well with the consequential validity movement championed by
Messick (1989, cited in Newton & Shaw, 2014). Building on Rawls’ distinction
376 K. Razavipour
between fairness as being about individuals and justice as being about institutions,
Kunnan derives a set of principles for both justice and fairness in language
assessment. The bottom line in this deontological view of moral behaviour is that
fairness is prior to justice and the institution of testing cannot be just if it seeks to
promote justice through violating fairness principles. Another important implica-
tion that Kunnan derives from the justice as fairness framework is the message it
has for the globalized reach of testing institutions and the need for such institutions
to abandon their parochial attitudes and seek global justice in assessment. This is
especially relevant given the current globalized impacts that international assess-
ments have on national and local discourses around education and assessment (see
Scott, 2016; Song, 2016)
2.2 Test Fairness Research in Language Testing
A relatively long tradition of researching test fairness in language testing is what
has come to be known as differential item functioning (DIF), which seeks “evi-
dence that one group of assessees has performed better than another on one test item
in a way that is inconsistent with their overall test performance” (Green, 2014,
p. 214). There are however a number of problems with DIF studies. For one thing,
they require data from very large number of candidates, limiting their applicability
to national or international standardized tests designed and administered by pow-
erful organizations, which are not always readily open to or tolerant of external
monitoring and research (Farhady & Hedayati, 2009). Secondly, when bias is
detected it is often difficult to interpret; that is, ascribing the differential perfor-
mance to construct relevant or irrelevant factors is not an easy decision (Green,
2014; McNamara & Roever, 2006). Moreover, the range of variables used in DIF
studies to identify groups of test takers whose performance is differentially affected
by individual or cluster of items is limited to those which are easy to identify such
as gender (Ahmadi & Darabi Bazvand, 2016), and ethnicity (Stoneberg, 2004).
Other important variables such as socio-economic status that do not readily lend
themselves to identification do not conventionally feature in DIF studies. Another
problem with DIF approach is its post-hoc nature. Although reviewing tests, items
or tasks prior to test administration may lead to flagging some items or tasks as
having the potential to produce DIF, it is often only after the test is given and scores
are in that is it possible to conduct DIF investigations. In sum, bias detection studies
have served to camouflage other more serious fairness issues by narrowing the wide
range of bias sources to a single source of test items. In the words of Gipps (1994,
p. 149), the psychometric approach to detecting bias
Simply ‘tinkers’ with an established set of items. Focusing on bias in tests, and statistical
techniques for eliminating ‘biased’ items, not only confounds the construct being assessed,
but has distracted attention from wider equity issues such as actual equality of access to
learning, ‘biased’ curriculum, and inhibiting classroom practices.
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 377
One problem which is currently holding back fairness investigations is lack of a
coherent theory of fairness. This is manifested in diverse positions which have
emerged regarding the relationship between validity and fairness in language
assessment (Xi, 2010). On one extreme, are those who see fairness as a higher
order framework under which validity may be subsumed (Kunnan, 2000). On the
other, fairness is seen as a facet of validity or comparable validity for all groups of
test takers (Karami, 2013; Xi, 2010). Fairness is also seen by some as a separate test
quality independent of validity (ETS, 2002). There is also a fourth view according
to which fairness is not a legitimate aspiration to pursue (Davies, 2010).
In the broadened view of fairness, test fairness goes far beyond the test taking
ritual, encompassing wider social, political, economic, cultural and educational
matters. This view has much appeal in critical language testing (McNamara &
Roever, 2006; Shohamy, 2001). As formulated by Kunnan (2000), fairness com-
prises three components: Validity, access, and legal matters. Kunnan maintains that
validity ipso facto is not enough grounds for claiming fairness. One has to make
sure that test takers have had equal access to educational and test preparation
opportunities. Otherwise, it would lead to choosing the elites (Klitgaard, 1985,
cited in Song, 2016).
This broadened view of fairness has proponents in educational measurement as
well. According to Gipps (1994), fairness should not be reduced to a matter of
ensuring uniform test taking conditions or lack of bias determined through DIF
studies. Rather, test fairness concerns have a wider temporal span, extending to
events both prior to and after the testing event. Such a broad definition of fairness
underlies some of the most successful educational systems in the world. One such
system is Norway, where it is believed that social mobility can only be promoted by
aspiring to the principle of result equality (Carlsen, 2009, p. 348).
Drawing on the argument based approach to validation, Xi (2010) argues for the
second view of fairness; that is, fairness must be treated within the argument based
approach to validation by incorporating into it a fairness argument. The strand of
fairness studies known as DIF research subscribe to such an understanding of the
relation of fairness and validity. According to Xi, such an approach to fairness
makes it possible to determine priorities for research on fairness. Nonetheless, the
decontextualized nature of argument in argument based validation has been criti-
cized on the grounds that it fails to take account of the dynamic, complicated facets
of the cultural, political, and ethical contexts wherein tests are used (Kunnan, 2010;
O’Sullivan & Weir, 2011). Accordingly, questioning the necessity or even the
usefulness of Toulmin’s argument structure to test evaluation, Kunnan maintains
that test fairness considerations cannot be adequately addressed within a validity
framework.
To Davies, the search for fairness is an attempt doomed to failure, “a quest for
the grail”, an attempt that is “chimerical” (2010, p. 175), and a “pursuit in vein”
(p. 171). Davies argues that life has never been fair, nor can tests as a facet of life
be. He further explains that while it stands to reason to expect fairness in the process
of testing, it is not plausible to expect fairness in the outcomes. He likens testing to
games, where it is important to be fair in the procedures of the game but not in its
378 K. Razavipour
final outcome. This position is in stark contrast with Carlsen’s account of fairness
which informs educational policy decision making in Norway.
Karami (2013) provides an overview of the major fairness models offered in
language testing and assessment from those which see fairness as ensuring psycho-
metric qualities to models which stretch the domain of fairness to go beyond
validity. He maintains that the two paradigms diverge in their very points of
departure. Adopting an optimistic perspective, the former start with the premise
that all language tests are constructed and used with benevolent purposes in mind
and that whatever negative consequences that may result are unintended. The
critical language testers, on the other hand, see the intentions behind the design
and use of language tests with scepticism. They see language tests as covert means
of control and surveillance in the hands of the powerful to perpetuate the status quo
(Fulcher, 2009; Shohamy, 2001). Approaching language testing from a default
cynical vantage point, critical language testers take the limits of fairness to
extremes inasmuch as to take the language tester responsible for all the intended
and unintended consequences. Karami cautions against such an extreme fairness
agenda, which, to him, would amount to the abolition of the entire business of
language testing. In keeping with Xi (2010), Karami argues that given the broad-
ened scope of validation, fairness concerns should be given top priority and can be
addressed within this broadened scope of validation, a position that Kunnan regards
as inadequate for taking account of fairness (2010).
Others argue that regardless of what measures test designers take to ensure
validity and fairness, larger societal preconditions if not in place, fairness is not
likely to prevail. Results from an empirical study by Byczkiewicz (2004) suggest
that validity safeguards would be inevitably subverted by stakeholders in the
absence of social equity. Analysing three films whose theme centred around
cheating, Byczkiewicz (2004) remarks that ethical decision making is complicated
and embedded within a host of individual, professional, institutional, and social
web of variables. She highlights the unpredictability of ethical behaviour because of
the complexity of decision making in day to day life events. Drawing on Kunnan’s
fairness framework consisting of social equity and equal validity, Byczkiewicz
concludes that “to achieve social equity in our educational systems, especially in
testing, students must first have unfettered equal access to learning opportunities.”
(p. 203). In the absence of such social equity, Byczkiewicz maintains that the
underprivileged would engage in unethical testing and teaching behaviour which
they see as legitimate solutions to the existing hegemonic social structure that has
served to deny them access to equal social and educational opportunities.
Overall, then It seems that unless the limits of test validity and test fairness are
demarcated, consensus regarding the relationship between the two is not likely to
emerge. Kane (2010) states that the relationship between the two concepts depends
on how each one is conceptualized. If validity is conceived of in a broad sense and
fairness in a narrow one, fairness would appear to be a facet of validity. On the
Contrary, if fairness is conceived of in a broad sense and validity in a narrow one,
validity would be an aspect of test fairness. Kane further distinguishes between two
types of fairness namely, procedural and substantive fairness, which are roughly
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 379
akin to Walters’ (2012) technico-formalistic and broad-based approaches to fair-
ness. The former, as Kane suggests, is easier to achieve while the latter is more
challenging to address as it is concerned with broad social and political consider-
ations. Contrary to Walters (2012), who foresees that an agreement regarding the
scope of fairness and validity is on the horizon in a decade, Kane maintains that the
two are difficult to reconcile and none can cover the other, though they remain
related.
The current paper provides evidence in support of the position taken by Kunnan,
and Byczkiewicz concerning the inadequacy of validity to ensure fairness. In
essence, informed by a broad conceptualization of test fairness, the study sought
to see how admission to undergraduate language programs in Iran national univer-
sities is moderated by prior differential access to educational opportunities to
prepare for the Specialized English Test (SET), a description of whose retrofits
and evolution is in order.
3 Method
3.1 Specialized English Test (SET)
Before giving an account of the SET, a brief introduction to high stakes university
admission exams in Iran is deemed necessary (for a comprehensive review of major
assessment policies in Iran (see Farhady & Hedayati, 2009). Admission to Iran
national universities is determined mainly through a suit of annual nationwide
multiple choice exams, known as University Entrance Exam (UEE) or konkoor in
Persian. Given their gate keeping function, stakes are very high for these exams.
UEE exams are given in two categories: Main exams and elective ones. Both the
main and the elective exams have in common a module that is known as the General
test, which is intended to measure candidates’ knowledge of Persian, Arabic,
Islamic thought, and English. Test items on the general test are based on high
school textbooks. The main category of exams includes the math and engineering
UEE, the UEE exam for life sciences, and the humanities and social sciences UEE
exam. Depending on their backgrounds and interests, students may choose to
register for only one of the main exams. The elective category comprises two
exams: One for the visual arts and one for foreign languages. Regardless of their
choice among the three main exams, applicants can choose to take one or both of
the electives as well. The foreign languages exam, of which SET is a subcategory,
serves to screen candidates pursuing undergraduate programs in a number of
languages including English, French, German, Russian, Chinese, Japanese, and
Turkish. Consistent with the global demand for English, the most popular among
the noted tests for foreign languages is the Specialized English Test, which keeps
the gate for those pursuing B.A degrees in English translation or literature. It
comprises two parts: The general part and the specialized section.
380 K. Razavipour
Being an achievement test, the general section of the SET, as noted above,
assesses test takers’ knowledge of Persian, Arabic, English, and Islamic thought.
The English section of the general module is almost exclusively centred around
grammar, passive vocabulary and reading comprehension. The specialized section,
on the other hand, goes beyond high school curriculum by testing a wider range of
English language proficiency constructs. It demands a richer lexical competence,
more advanced grammatical knowledge, better reading comprehension skills and
some oral and discourse competence. The latter is tested via items similar to
discourse completion tests but in a multiple choice format. It should be noted that
prior to the introduction of the SET, high school graduates interested in English
language programs were selected solely based on their scores on the general module
of any of the three exams in the main category of UEEs described above. As such,
they were tested on a limited range of the constructs of the broad communicative
language ability laid out in Bachman (1990). The SET in its current state, however,
captures a larger sample of the constructs underlying English language proficiency.
In validity lingo, compared to its predecessor, the new SET suffers less from
construct underrepresentation.
The data for this study were collected as part of the data I gathered in 2010 for
my PhD dissertation (Razavipour, 2010). That study was primarily concerned with
test washback and fairness was touched upon only in the passing. However, in
retrospect, I found the data quite relevant to a separate fairness consideration. As
such, in the interest of space, I am not going to present a detailed account of all the
procedures followed in that study. Suffice it to note that the participants who
contributed to the data were 173 undergraduate students of English literature or
English translation selected on a convenience basis from three national universities
in Iran. The three universities were different in terms of their national rank (see
Table 1). In the original study, data were collected from a number of sources
including interviews, material evaluation checklists, and a questionnaire. For the
purpose of this study, however, I drew only on a few items in the questionnaires. In
particular, the focus was on questionnaire items related to participants’ language
learning experience prior to college admission and their perception of the fairness
of the SET test results. Regarding the former, the participants were asked whether
they had attended private language schools before their admission to university. In
addition, there were three Likert scale items (with five choices ranging from
strongly agree to strongly disagree) pertaining to participants’ perceptions of SET
fairness. The collected data were subjected to descriptive statistics and a
non-parametric Pearson Chi-square test was run to examine the association between
university rank and students’ taking private language courses. Furthermore, one
sample t-test was used to examine the extent to which participants’ perception of
test fairness deviate from the average neutral value. Finally, to examine the
differences among the three participant groups in their perception of test fairness
a one-way ANOVA procedure was used.
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 381
4 Results and Discussion
Table 1 illustrates the number and percentage of participants who had attended
private language schools prior to admission to college. Looking at the percentages,
a direct positive trend is discernable between the number of students attending
private language schools and the quality of the university to which they have been
admitted. As such, 87% of participants from the first-tier university experienced
English language learning classes in private schools. As we go down the university
rank ladder, so does the number of students with a background in private language
schools. As such, only 50% of participants from the third-tier university reportedly
attended English language courses beyond their regular English learning
curriculum.
To investigate whether the observed differences in Table 1 are large enough to
reach statistical significance, a Pearson Chi-square test was run, which demon-
strated that the observed differences are significant and the two categorical vari-
ables of university ranking and language school attendance are strongly related, X2
(2, N ¼ 173) ¼ 21.13, p ¼ 0.000.
As noted earlier, participants were also asked to indicate their degree of agree-
ment with the fairness of the SET exam on a set of questionnaire items. Table 2
presents the descriptive statistics for participants’ total scores on the fairness scale.
An overall look at the mean score indicates that participants’ perception of the SET
fairness is not high (M ¼ 2.88; SD ¼ 0.97), given that the maximum possible score
on the fairness scale is 5.
To see whether participants’ scores on the fairness scale are significantly differ-
ent from the neutral value of three (See Larson-Hall, 2010), a one-sample t-test was
run. Results revealed that the deviation of scores from the neutral value of 3 was not
significant at the 0.05 level (t ¼ 1.64, df ¼ 186, p ¼ 0.10). However, if we follow
Larsen-Hall’s (2016) advice in avoiding a rigid adherence to the arbitrary p-value,
we may legitimately claim that participants’ perceptions of the SET fairness are
meaningfully below the neutral mean value.
Table 1 Language school attendance across students from the three universities (adopted from
Razavipour, 2010)
University
Language school attendance
Total
Yes No
1st tier university 12 (12.9%) 81 (87.1%) 93 (100.0%)
2nd tier university 13 (32.5%) 27 (67.5%) 40 (100.0%)
3rd tier university 20 (50.0%) 20 (50.0%) 40 (100.0%)
Total 45 (26.0%) 128 (74.0%) 173 (100.0%)
Table 2 Descriptive statistics of perception of fairness
N Min Max Mean SD
Perception of fairness 187 1 5.00 2.88 0.97
382 K. Razavipour
Table 4 illustrates scores on the fairness perception scale across the three
universities. As can be seen, participants from the first-tier university (FTU) had
the most favourable perceptions of SET fairness (M ¼ 2.97; SD ¼ 0.88), followed
by STU participants (M ¼ 2.81; SD ¼ 1.06). Students from the third-tier univer-
sities obtained the lowest score on the perception of SET fairness scale (M ¼ 2.78;
SD ¼ 0.97). Thus, there appears to be a discernable descending trend between the
university rank cline and that of test fairness perception. That is, there is a direct
positive association between the university ranking and perceptions of the test
fairness.
To see if the apparent differences observed in Table 4 are statistically significant,
an ANOVA test was conducted.
Results from the ANOVA test indicate that perceptions of SET fairness across
universities are not statistically significant. In the light of the results from Tables 3
and 4 and the ANOVA results, it can be inferred that all three groups of students
perceive of the SET as a test that does not score high on the fairness scale (Table 5).
This study investigated the fairness of the SET examination from both a social
scientific and a normative moral philosophy perspective. From the former, partic-
ipants’ perceptions of the fairness of SET were elicited. We also investigated
Table 4 Descriptive statistics of scores on the perception of fairness scale across universities
Mean fairness
N Mean SD Std. error
95% confidence interval for mean
Min Max
Lower bound Upper bound
FTU 89 2.97 0.88 0.09 2.78 3.15 1.00 5.00
STU 57 2.81 1.06 0.14 2.52 3.09 1.00 5.00
TTU 41 2.78 1.04 0.16 2.45 3.11 1.00 5.00
Total 187 2.88 0.97 0.07 2.74 3.02 1.00 5.00
FTU First-tier university; STU Second-tier university; TTU Third-tier university
Table 3 One-sample t-test results
Test value ¼ 3
t df Sig. (2-tailed) Mean difference
95% confidence
interval of the difference
Lower Upper
Mean fairness 1.647 186 0.101 0.11765 0.2585 0.0232
Table 5 ANOVA test for perception of fairness across universities
Mean fairness
Sum of squares df Mean square F Sig.
Between groups 1.44 2 0.72 0.755 0.47
Within groups 175.96 184 0.956
Total 177.41 186
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 383
whether there existed a relationship between admission to undergraduate English
language programs in Iranian national universities and attending private language
schools prior to university admission. From the former standpoint, social scientific,
it was found that test takers do not think of the SET as a highly fair test. In fact, their
scores on the fairness perception scale bordered on the opposite. From the latter
perspective, it was found that private language school attendance prior to college
was 90% for the first-tier university, 70% for the second-tier university and 50% for
the third-tier university. Generally speaking, as attested by the Chi-Square test
outcome, there appeared to be a close association between studying English at a
top university and the opportunity of private language school attendance before
college. The question is now whether credit should be given to the SET exam for
successfully telling apart candidates with more English learning experience and
those without or whether a seemingly more valid SET measuring a construct that is
beyond the reach of a majority who lack access to private language school should be
discredited for its unfairness to the majority of test takers.
In the rest of this section, we argue that validity theory is inadequate for
addressing the type of fairness concerns that such findings raise. Viewed from
psychometric validity perspectives, from the narrow psychological to the broad
social conceptions, no objections can be levelled at the function of the SET exam in
its successful screening of the candidates with more English language proficiency.
Defining a valid test as one whose score variations are caused by variations in the
attributes measured by the test (Borsboom, Mellenbergh, & van Heerden, 2004;
Borsboom & Mellenbergh, 2007), then the finding would suggest that SET scores
represent variations in candidates’ language proficiency because those with lan-
guage learning experiences beyond the regular public school curriculum must
possess more of the desired construct, English language proficiency. Likewise,
the above finding attests to the validity of SET exam even from a broad social
understanding of validity that counts test consequences as aspects of validity
(Hamp-Lyons, 1998; Messick, 1996), as one might argue that contrary to many
high stakes exams that frequently result in ethically dubious test preparation
practices, the SET has induced positive washback in driving students to pursue
authentic language learning activities that are both ethically sound and education-
ally defensible (for what counts as ethical test preparation see Hamp-Lyons, 1998;
Mehrens & Kaminsky, 1989; Popham, 1991). The SET exam represents the con-
struct of language proficiency much better than its predecessor (explained above).
But is construct coverage always a good thing even to the detriment of fairness? In
other words, is a more valid test, in terms of content and construct coverage, to be
always preferred? These are questions that do not admit straightforward yes/no
answers. The question then comes down to the conflict between test validity and test
fairness in its broad sense (Gipps & Stobart, 2009; Kunnan, 2014). That is, a valid
test is likely to hamper social mobility and promote social injustice. In the case of
SET, not all students can afford to pay for private language schools. In fact, such
schools are the province of the urban upper class families.
We must not forget that the idea of testing emerged to make it possible for
people of all walks of life and persuasions to transcend their life circumstances
384 K. Razavipour
(Fulcher, 2015); to level the playing field for the rich, the poor, the privileged and
the underprivileged. However, this benign instrument, like all human inventions,
should not be thought to be immune from misuse and abuse. The obvious example
is the eugenics movement in which tests played a major part (Fulcher, 2015;
Goldstein, 2012). Thus, if values that inform test use go unexamined, they are
likely to work for the opposite of what tests were created for in the first place.
Fulcher maintains that the problem with values is that they are so embedded in
culture that they do not lend themselves readily to scrutiny and examination. Values
may take different shapes from scientific to ideological, religious, cultural or
political. The values that justified eugenic testing were couched in the scientific
parlance of the day, which were epitomized by a belief in nature determinism and a
rejection of nurture (Goldstein, 2012). The “reification of test scores” (Fulcher,
2015, p. 179) with disregard to the entire web of social values and forces that
surround the context of test use is likely to result in stifling social mobility and equal
opportunity. The SET exam in its current state within the social fabric of the Iranian
society seems to contribute to the current social class system. In deciding students’
future, the means students have at their disposal should not be “conflated with
ability and achievement” (Moses & Nanna, 2007, p. 58); otherwise, “those who
have adequate amounts of social and cultural capital most likely will perform
better” (Lareau, 2000, cited in Moses & Nanna, 2007, p. 58).
5 Conclusion
Through examining the backgrounds of successful test takers of a national matric-
ulation language test whose validity had been recently enhanced through more
construct representation, the present study found a strong association between test
takers’ economic background and their chances of admission to a prestigious
university. This seems to suggest that the current debate surrounding the relation-
ship between validity and fairness is far from resolved and that the common
validation practices may not be adequate in ensuring test fairness. Although thanks
to Messickian unitary conception of validity, test consequences as aspects of test
validity are today given more attention than the time when the job of the language
tester was complete at the point of producing psychometrically sound indices of
reliability and validity (Shohamy, 2001), still more has to be done concerning test
fairness. To arrive at a theoretical lens through which not only validity but also test
fairness can be addressed, we need to broaden our current understanding of test
impacts and consequences. In the words of Hamp-Lyons (2016, p. 302)
There needs to be a set of conditions and parameters inside which we are sure of the
consequences of our work and we need to develop a conscious agenda to push outward the
boundaries of our knowledge of the consequences of language tests and their practices.
To do so, we must always remain mindful of the fact that tests are produced by
powerful organizations, based in metropolitan areas of capital cities, to measure
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 385
powerless test takers for whom there is no mechanism to have a say in the decision-
making process (Shohamy, 2001). In other words, tests are to be seen as aspects of a
dominant discourse that if go unexamined, are likely to produce more harm than
good. Through the sophisticated and technical language of validity aided by the
alien statistical jargon, the current dominant discourse would have us believe that
tests are infallible and are essentially in the best interest of everyone. The fairness
framework, as laid out by Kunnan (2014), might be the right heuristic device to help
us not to fall prey to the noted discourse through whose power large groups of test
takers are likely to be unrepresented through seemingly more construct
representation.
The trade-off observed between validity and fairness in this case seems to be one
realization of the well-known trade-off between priori principles of ethics (i.e.,
equal treatment, respect for individuals) and posteriori principles of ethics (i.e.,
maximization of good) (Corson, 1997, p. 183). Put it simply, as it pertains to
language testing, the conflict is between the principle of equal treatment and the
consequences of equal treatment. Nevertheless, this logic holds true only if equal
treatment is interpreted in its most superficial sense, that is, if we reduce it to test
administration. The point is that having test takers taking the same items or tasks
does not guarantee equal treatment. More important is to ensure that all test takers
have had equal access to the kind of learning materials and opportunities that are
required to excel on the construct under measurement. This is in fact aligned with
the first subprinciple of the first principle which Kunnan (2014) derived from the
moral philosophy of Rawls and Sen. “an assessment ought to provide adequate
opportunity to acquire knowledge, abilities, or skills to be assessed for all test
takers” (p. 8, original italics). Nevertheless, although the institution of assessment
should do its utmost to eliminate differential access and levelling the playing field
in the conditions proceeding the test occasion is often beyond the power of the test
makers, care for equal treatment must be exercised at the design stage, at which
proper accommodations must be made with an eye to the deleterious impacts of a
superficial interpretation of equal treatment in test administration.
Since the SET is concerned with making inferences about future performance, as
is the case with most tests used for admission to tertiary educational programs, the
construct of interest must be potential or promise in the target domain. In other
words, priority must be given to factors determining future success rather than to
those leading to past achievements. The central tenets of at least two strands of
research in language assessment, namely dynamic assessment (Lantolf & Poehner,
2013) and language aptitude (Stansfield & Winke, 2008) pertain to this futuristic
orientation toward assessment. The core idea in the growing literature in dynamic
assessment is that we must not only measure what learners already can do but also
what they will be capable of doing in future, hence, the idea of the zone of proximal
development (ZPD). What is at issue here is that learners have various zones of
proximal developments which would not be captured by a test of past achieve-
ments. Though issues of reliability, face validity, bias, and practicality keep arising
wherever DA is put into practice (see Hidri, 2014), the DA promise is still worth
pursuing not least for the broader, more humanistic perspective it adopts towards
386 K. Razavipour
test takers and their social context. Moreover, it is not hard to imagine that advances
in technology would in a foreseeable future alleviate the practicality and reliability
concerns associated with DA (see Teo, 2013).
We are not denying that capturing test takers’ current English proficiency can be
a useful indicator of future performance. Rather, we believe that test takers’ current
proficiency is at least partly a function of test takers’ social stratification and
differential access to cultural capital (Gao, 2011). As Zwick (2012, p. 399) states,
“it is legitimate to question the fairness of a system in which some test-takers can
afford coaching and others cannot”. Research in language aptitude is also of
relevance here as introducing an element of language aptitude or verbal intelligence
(Stansfield & Winke, 2008) would increase the trust we can place in the inferences
we are to make of test scores. Such levelling the playing field alterations (Davies,
1997) to the constructs measured by the test would open up to the socio-
economically disadvantaged test takers the possibility to compete with their
upper and middle class peers for a place in quality language programs. This
would be in keeping with the very genesis of tests “as a promising way to allot
educational places and sort students based on individual merit (as based on test
scores) rather than birth circumstances” (Moses & Nanna, 2007, p. 56). As long as
variations in test scores correlate with variations in test takers’ socio-economic
backgrounds, tests would likely contribute to social inequalities (Pan & Roever,
2016) not to offsetting them.
For language test designers on the periphery to be able to design language tests
sensitive enough to the contingencies of local contexts, it is necessary to be mindful
of the legitimate fragmentation (O’Sullivan & Weir, 2011) that has transpired in
language assessment and of the fallacy of the native speaker in language teaching
and testing. O’Sullivan and Weir (2011) argue that the test takers, the local contexts
and what is going to happen to test takers in the aftermath of tests should be at the
centre of all test design stages. They further argue against the common post hoc
approaches to consequential validity. Blindly following the theories and models
that are proposed in the academic north would lead to the production of tests that
may appear more valid but indeed unfair in the local setting (Chalhoub-Deville,
1997; Fulcher, 2010; Weir, 2005). As such, by directly borrowing constructs from
the theoretical models of language ability or proficiency (e.g., Bachman, 1990;
Bachman & Palmer, 1996, 2010) without due account of the context of assessment
would land us tests that seem to suffer less from construct underrepresentation but
more from construct irrelevant variance. For instance, one major function of the
SET is to help select the best prospective translators. English language proficiency
constitutes only part of the construct of translation competence, as today’s theories
of translation competence attest (Angelelli & Jacobson, 2009; House, 2014). Thus,
for SET to be fair the uses that are to be made of test scores and a full account of the
context of test use must guide the test design and score interpretation.
Finally, the debate around whether validity subsumes fairness or whether it is an
aspect of test fairness seems to have been influenced by how division of labour in
language testing is managed in Western Europe and North America where there are
multiple test making bodies that produce tests and the test consumer organizations
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 387
have options to choose among. In other words, in these countries test makers and
test users are not the same. Therefore, it may not stand to reason to take test makers
accountable for wider social issues that might arise by virtue of the uses to which
tests are put. It follows that in such a context, a narrow conception of fairness on the
part of test makers may seem plausible. However, in the Middle East, often decision
making regarding test design, administration, interpretation, and use all lie in the
hands of one elite state-sponsored testing organization. As rights and responsibil-
ities go hand in hand, such elite organizations must subscribe to broader definitions
of test fairness like the one provided in McNamara and Roever (2006, p. 81). Given
the cultural differences among countries concerning test making and test use,
universal demarcation of the boundaries of responsibility that test makers have is
neither possible nor desirable. Powerful state sponsored test making bodies have
open-ended responsibility for all the consequences of their products.
Like other studies, this study has its share of limitations. Similar to most studies
in language testing, test fairness was approached from a philosophical normative
ethics vantage point. Further inquiry informed by other theories of fairness can
inform and enrich our understanding of test fairness. Second, the data that served as
grounds for our arguments were limited to participants’ profiles of language
learning experiences in private language institutes prior to university admission.
Though whether one can afford to attend such courses depends on their cultural
capital, it is not in itself an adequate measure of test takers’ cultural capital. Future
studies using more valid measures of test takers’ cultural capital would shed more
light on the role of test takers’ socioeconomic backgrounds in their access to quality
tertiary language programs. Another line of inquiry is to compare undergraduate
English students’ cultural capital with that of students majoring in other fields,
especially from other language departments such as Persian and Arabic. One even
more serious limitation of the current study is its reliance on data from test takers
who have been admitted to language programs through the SET. As access to the
profiles of students who were admitted to college through the older version of the
test is difficult, if not impossible, it would be illuminating if the findings from this
study could be examined through a cross-sectional study to see if in fact a more
valid language test, SET, would result in choosing the elites (Klitgaard, 1985).
References
Ahmadi, A., & Darabi Bazvand, A. (2016). Gender differential item functioning on a national field-
specific test: The case of PhD entrance exam of TEFL in Iran. Iranian Journal of Lan-
guage Teaching Research, 4(1), 63–82.
Angelelli, C. V., & Jacobson, H. E. (2009). Introduction: Testing and assessment in translation and
interpreting studies: A call for dialogue between research and practice. In V. C. Angelelli &
E. H. Jacobson (Eds.), Testing and assessment in translation and interpreting studies (pp. 1–9).
Amsterdam: John Benjamins Publishing.
Bachman, L. F. (1990). Fundamental considerations in language testing. Oxford: Oxford Univer-
sity Press.
388 K. Razavipour
Bachman, L. F. (2000). What, if any, are the limits of our responsibility for fairness in language
testing? In A. J. Kunan (Ed.), Fairness and validation in language assessment (pp. 39–41).
Cambridge: Cambridge University Press.
Bachman, L. F., & Palmer, A. S. (1996). Language testing in practice: Designing and developing
useful language tests. Oxford: Oxford University Press.
Bachman, L. F., & Palmer, A. (2010). Language assessment in practice: Developing language
assessments and justifying their use in the real world. Oxford: Oxford University Press.
Borsboom, D., & Mellenbergh, G. J. (2007). Test validity in cognitive assessment. In J. Leighton &
M. Gierel (Eds.), Cognitive diagnostic assessment for education: Theory and applications
(pp. 85–118). Cambridge: Cambridge University Press.
Borsboom, D., Mellenbergh, G. J., & van Heerden, J. (2004). The concept of validity. Psycho-
logical Review, 111(4), 1061–1071. https://doi.org/10.1037/0033-295X.111.4.1061.
Byczkiewicz, V. (2004). Filmic portrayals of cheating or fraud in examinations and competitions.
Language Assessment Quarterly, 1(2&3), 193–204. https://doi.org/10.1080/15434303.2004.
9671785.
Carlsen, C. (2009). Crossing the bridge from the other side: The impact of society on testing. In
L. Taylor & C. J. Weir (Eds.), Language testing matters, Studies in language testing 31
(pp. 344–356). Cambridge: Cambridge University Press.
Chalhoub-Deville, M. (1997). Theoretical models, assessment frameworks and test construction.
Language Testing, 14(1), 3–22. https://doi.org/10.1177/026553229701400102.
Corson, D. (1997). Critical realism: An emancipatory philosophy for applied linguistics?
Applied Linguistics, 18(2), 166–188. https://doi.org/10.1093/applin/18.2.166.
Davies, A. (1997). Introduction: The limits of ethics in language testing. Language Testing, 14(3),
235–241. https://doi.org/10.1177/026553229701400301.
Davies, A. (2010). Test fairness: A response. Language Testing, 27(2), 171–176. https://doi.org/
10.1177/0265532209349466.
De Beauvoir, S. (2014). The second sex. New York: Random House.
Educational Testing Service. (2002). ETS standards for quality and fairness. Princeton, NJ:
Author.
Farhady, H., & Hedayati, H. (2009). Language assessment policy in Iran. Annual Review of
Applied Linguistics, 29, 132–141. https://doi.org/10.1017/S0267190509090114.
Fulcher, G. (2009). Test use and political philosophy. Annual Review of Applied Linguistics, 29,
3–20. https://doi.org/10.1017/S0267190509090023.
Fulcher, G. (2010). Practical language testing. London: Hodder Education.
Fulcher, G. (2014). Language testing and philosophy. In A. J. Kunan (Ed.), The companion to
language assessment (pp. 1–17). Boston: Wiley.
Fulcher, G. (2015). Re-examining language testing: A philosophical and social inquiry. London:
Routledge.
Gao, L. (2011). Impacts of cultural capital on student college choice in China. Maryland:
Lexington Books.
Gipps, C. V. (1994). Beyond testing: Towards a theory of educational assessment. London:
The Falmer Press.
Gipps, C., & Stobart, G. (2009). Fairness in assessment. In C. Wyatt-Smith & J. Cumming (Eds.),
Educational assessment in the 21st century (pp. 105–118). Netherlands: Springer.
Goldstein, H. (2012). Francis Galton, measurement, psychometrics and social progress. Assess-
ment in Education: Principles, Policy & Practice, 19(2), 147–158. https://doi.org/10.1080/
0969594X.2011.614220.
Green, A. (2014). Exploring language assessment and testing: Language in action. New York:
Routledge.
Hamp-Lyons, L. (1998). Ethical test preparation practice: The case of the TOEFL. TESOL Quar-
terly, 32(2), 329–337. https://doi.org/10.2307/3587587.
Hamp-Lyons, L. (2000). Fairness in language testing. In A. J. Kunan (Ed.), Fairness and vali-
dation in language assessment (pp. 39–41). Cambridge: Cambridge University Press.
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 389
Hamp-Lyons, L. (2016). Washback, impact and validity: ethical concerns. Language Testing,
14(3), 295–303
Hidri, S. (2014). Developing and evaluating a dynamic assessment of listening comprehension in
an EFL context. Language Testing in Asia, 4(4), 2–19. https://doi.org/10.1186/2229-0443-4-4.
House, J. (2014). Translation quality assessment: Past and present. New York: Routledge.
Kane, M. (2010). Validity and fairness. Language Testing, 27(2), 177–182. https://doi.org/10.
1177/0265532209349467.
Karami, H. (2013). The quest for fairness in language testing. Educational Research and Evalu-
ation, 19(2–3), 158–169. https://doi.org/10.1080/13803611.2013.767618.
Klitgaard, R. (1985). Choosing elites: Selecting the “best and the brightest” at top universities and
elsewhere. New York: Basic.
Kunnan, A. J. (2000). Fairness and justice for all. In A. J. Kunan (Ed.), Fairness and validation in
language assessment (pp. 39–41). Cambridge: Cambridge University Press.
Kunnan, A. J. (2010). Test fairness and Toulmin’s argument structure. Language Testing, 27(2),
183–189. https://doi.org/10.1177/0265532209349468.
Kunnan, A. J. (2014). Fairness and justice in language assessment. In A. J. Kunan (Ed.),
The companion to language assessment (pp. 1–17). Boston: Wiley.
Lantolf, J. P., & Poehner, M. E. (2013). The unfairness of equal treatment: Objectivity in
L2 testing and dynamic assessment. Educational Research and Evaluation, 19(2–3),
141–157. https://doi.org/10.1080/13803611.2013.767616.
Larson-Hall, J. (2010). A guide to doing statistics in second language research using SPSS.
New York: Routledge.
Larson-Hall, J. (2016). Our statistical intuitions may be misleading us: Why we need robust stat-
istics. Language Teaching, 45, 460–474. https://doi.org/10.1017/S0261444811000127.
McNamara, T., & Roever, C. (2006). Language testing: The social dimension. Malden, MA:
Blackwell.
Mehrens, W. A., & Kaminsky, J. (1989). Methods for improving standardized test scores: Fruitful,
fruitless, or fraudulent? Educational Measurement: Issues and Practice, 8(1), 14–22. https://
doi.org/10.1111/j.1745-3992.1989.tb00304.x.
Messick, S. (1996). Validity and washback in language testing. Language Testing, 12(3), 241–256.
https://doi.org/10.1002/j.2333-8504.1996.tb01695.x.
Moses, M. S., & Nanna, M. J. (2007). The testing culture and the persistence of high stakes testing
reforms. Education and Culture, 23(1), 55–72.
Newton, P., & Shaw, S. (2014). Validity in educational and psychological assessment.
Los Angeles, CA: Sage.
Pennycook, A. (2001). Critical applied linguistics: A critical introduction. Mahwah, NJ: Law-
rence Erlbaum.
O’Sullivan, B., & Weir, C. J. (2011). Test development and validation. In B. O’Sullivan (Ed.),
Language testing theories and practices (pp. 13–32). Basingstoke: Palgrave Macmillan.
Popham, W. J. (1991). Appropriateness of teachers’ test-preparation practices. Educational Mea-
surement: Issues and Practice, 10(4), 12–15. https://doi.org/10.1111/j.1745-3992.1991.
tb00211.x.
Pan, Y., & Roever, C. (2016). Consequences of test use: a case study of employers’ voice on the
social impact of English certification exit requirements in Taiwan. Language Testing in Asia,
6(1), 1–21
Potter, G., & Lopez, G. (2001). After postmodernism: The millennium. In J. Lopez & G. Poter
(Eds.), After postmodernism: An introduction to critical realism (pp. 1–18). London:
The Athlone Press.
Razavipour, K. (2010). National matriculation test for English major students: Its impact and
some validity evidence. Unpublished doctoral dissertation, Shiraz University, Shiraz, Iran.
Scott, E. D. (2016). Assessment as a dimension of globalization. In S. Scott, D. E. Scott, &
C. F. Webber (Eds.), Assessment in education: Implications for leadership (pp. 17–52).
New York: Springer.
390 K. Razavipour
Shohamy, E. (1998). Critical language testing and beyond. Studies in Educational Evaluation,
24(4), 331–345. https://doi.org/10.1016/S0191-491X(98)00020-0.
Shohamy, E. (2000). Fairness in language testing. In A. J. Kunan (Ed.), Fairness and validation in
language assessment (pp. 39–41). Cambridge: Cambridge University Press.
Shohamy, E. (2001). Democratic assessment as an alternative. Language Testing, 18(4), 373–391.
https://doi.org/10.1177/026553220101800404.
Song, X. (2016). Fairness in educational assessment in China: Historical practices and contem-
porary challenges. In S. Scott, D. E. Scott, & C. F. Webber (Eds.), Assessment in education:
Implications for leadership (pp. 67–90). New York: Springer.
Stansfield, C. W., & Winke, P. M. (2008). Testing aptitude for second language learning.
In E. Shohamy & N. H. Hornberger (Eds.), Encyclopaedia of language and education, Lan-
guage testing and assessment (Vol. 7, 2nd ed., pp. 2226–2239). New York: Springer.
Stoneberg, B. D. (2004). A study of gender-based and ethnic-based differential item functioning
(DIF) in the spring 2003 Idaho Standards Achievement Tests. Applying the Simultaneous Bias
Test (SIBTEST) and the Mantel-Haenszel Chi Square Test. Paper for EDMS 889 Measurement-
Statistics Practicum, University of Maryland, College Park. Retrieved from http://files.eric.ed.
gov/fulltext/ED483777.pdf
Teo, A. (2013). Promoting EFL students’ inferential reading skills through computerized dynamic
assessment. Language Learning & Technology, 16(3), 10–20.
Walters, F. S. (2012). Fairness. In G. Fulcher & F. Davidson (Eds.), The Routledge handbook of
language testing (pp. 469–478). London: Routledge.
Weir, C. J. (2005). Language testing and validation. Hampshire: Palgrave McMillan.
Xi, X. (2010). How do we go about investigating test fairness? Language Testing, 27, 147–170.
https://doi.org/10.1177/0265532209349465.
Zwick, R. (2012). Admissions testing in higher education. In C. Secolsky & D. B. Denison (Eds.),
Handbook of measurement, assessment, and evaluation in higher education (pp. 382–404).
New York: Routledge.
Valid for the Elites? The Trade-Off Between Test Fairness and Test Validity 391
Student-Related Challenges of Performing
Alternative Assessments from the Perspective
of Kurdish Tertiary TESOL Teachers
Dler Abdullah Ismael
Abstract Most of the student-related challenges of alternative assessments are
perhaps associated with those in which students are the major assessors. Brown
and Hudson (1998) state that portfolios, self-assessment, and peer-assessment can
be relatively difficult to produce and organise. Pedagogically, diagnosing and
tackling the challenges of alternative assessments are potentially important for
developing the teaching, learning, and assessment of English language. This
study investigated the student-related challenges of conducting alternative assess-
ments from the perspective of Kurdish tertiary TESOL teachers. Teachers’ per-
spectives are immensely important because their beliefs influence and shape their
classroom practices (Wang, 2011). This is part of the continuing research “to
understand how cognitive and affective factors interact in shaping what teachers
do” (Borg, 2006). To this end, this study used face-to-face and focus group
interviews as methods of data collection, and embraced an interpretivist and
phenomenological approach, which requires researchers to situate meaning units
in relationship to context and structure (Anderson, 2007). The participants were
12 interviewees from two English departments at a public university in the Kurdi-
stan Region. The findings revealed that, regarding the performance of alternative
assessments, teachers contended that most students had limited knowledge and low
skills, and were unwilling to participate, unresponsive, and uncooperative. They
were primarily interested in being spoon-fed information and using memorisation
techniques, and were preoccupied with passing tests and acquiring certificates
rather than actual learning. They were also often shy and ashamed of their mistakes,
and thus lacked confidence, were unmotivated, not eager to learn, careless,
inattentive to the rules, and subjective, all of which prevented the successful
D.A. Ismael (*)
11 Flamborough St, London E14 7LS, UK
Exeter University, Exeter, UK
Sulaimani University, Sulaymaniyah, Iraq
e-mail: daii201@exeter.ac.uk; dler276@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_19
393
implementation of alternative assessment practices. The implications of the find-
ings of this study could be influential in tackling the aforementioned student-related
challenges of alternative assessments.
Keywords Alternative assessment • Teacher perspective • Student-related
challenges • Student affective aspects • Self-assessment • Peer-assessment
1 Introduction
Assessment has an essential impact on education in general because its results have
powerful implications for improving the quality of teaching/learning processes
(Wolf, Herman, Bachman, Bailey, & Griffin, 2008). Since the 1990s, the core of
assessment development has shifted from traditional assessment to alternative
assessment (Bahous, 2008), a paradigm shift from psychometrics to a wider
model of assessment, in other words, from a testing culture to an assessment
culture, which has led to the evolution of many alternative assessments
(Derakhshan, Rezaei, & Alemi, 2011). It is only recently that studies into alterna-
tive assessments within the ESL/EFL contexts have started; hence, not much is
known about TESOL teachers’ assessment and evaluation procedures at the tertiary
level (Cheng, Rogers, & Hu, 2004). Alternative assessments have recently become
important elements of teaching and learning foreign languages, and are widely used
at the tertiary level (Anusienė, Kaminskienė, & Kavaliauskienė, 2007). Therefore,
there is a significant need for further research in this area because despite teachers’
positive attitudes to and appreciation of alternative assessments, implementation
has posed a major challenge (Wach, 2012; Yu, 2013).
Student-related challenges, especially when they are assessors of themselves or
each other, are pressing obstacles to conducting alternative assessments. Brown and
Hudson (1998) state that producing and organising portfolios, self-assessment, and
peer-assessment, in particular, are relatively difficult. This study explored the
student-related challenges of performing alternative assessments from the perspec-
tive of teachers, which is immensely important because teachers’ beliefs influence
and shape their classroom practices (Wang, 2011). In particular, the study focuses
upon Kurdish tertiary TESOL teachers, and thus contributes to the continuing
research “to understand how cognitive and affective factors interact in shaping
what teachers do” (Borg, 2006, pp. 271–273). Student-related challenges can be
more pressing than those related to other assessment stakeholders because students
constitute the majority stakeholder in education, and represent the purpose of
education in the teaching-learning process. Pedagogically speaking, diagnosing
and dealing with the challenges of alternative assessments are potentially influential
for developing the use of those assessments. TESOL teachers have to adopt an array
of alternative assessments to provide fair assessments for their students (Troudi,
2006) in order to establish a composite picture of students’ proficiency. This is
because alternative assessments enable students to display their knowledge (and
394 D.A. Ismael
abilities) in many ways via a range of assessment methods and multiple measures
(Earl & Katz, 2006). Alternative assessment has a variety of measures that can be
adapted for various contexts (Tannenbaum, 1996).
2 Theoretical Background
Broadly speaking, alternative assessment practices can be seen as alternatives to
conventional evaluation methods (Chirimbu, 2013), since they use
non-conventional strategies as ongoing processes that involve teachers and students
making judgements on the latter’s progress (Hancock, 1994). Alternative assess-
ments are continuous, untimed, and long-term assessments, which are criterion-
referenced, open-ended, formative, and process-oriented (Anusienė et al., 2007).
Constructivism and socio-cultural theory focus on the social side of learning, in
which students play pivotal roles that have possibly led to the evolution of alterna-
tive assessment. In constructivism, teachers should cultivate a safe and encouraging
environment to ensure that their students feel comfortable in expressing their ideas
and developing their concepts; hence, teachers should adjust their teaching methods
to allow students to take a more active role, because constructivism places each
student at the centre of the learning and assessment processes (Grabin, 2009).
Constructivism presupposes that human beings can inherently construct meaning
from experience (Dragemark Oscarson, 2009), and it argues that people cannot
learn by absorbing knowledge passively. The core of learning is the ability of
individual students to interpret meanings, discover, and problem-solve (Grabin,
2009). This is compatible with the features of alternative assessment as it is
formative and process-oriented, and it encourages interactive performances and
contextualised communicative tasks (Anusienė et al., 2007).
Vygotsky (1978) emphasised the effectiveness of discourse with others through
language mediation in order to achieve a higher level of learning and cognitive and
metacognitive awareness (Dragemark Oscarson, 2009). Such meditation strategy
awareness is very essential in carrying out dynamic/alternative assessment (DA).
Alternative assessment seems to be underpinned by socio-cultural theory within
which learning and assessment are contextualised by the social interaction among
learners and contextual conceptual tools, physical tools, and people (Scarino, 2013).
The Zone of Proximal Development (ZPD) is essential in Vygotsky’s socio-cultural
theory, and he defines it as the distance between the level of the actual development
accomplished by independent problem-solving and the level of potential develop-
ment achieved by problem-solving under the guidance of or in collaboration with an
adult or more capable peers (Vygotsky, 1978, p. 86, cited in Shrestha & Coffin,
2012). Including peers in this process leads to a kind of collaboration among them,
which creates a collective ZPD from which students can benefit equally (Turuk,
2008). ZPD and mediation are central to alternative assessment because it is a
process rather than a product. The development-oriented nature of the process of
alternative assessment reveals students’ current abilities, helping them to overcome
Student-Related Challenges of Performing Alternative Assessments from the. . . 395
any problems and realise their potential. Hence, mediation is as integral to alterna-
tive assessment as it is to ZPD (Shrestha & Coffin, 2012).
Constructivism and a socio-cultural approach are arguably the most important
theories that underpin alternative assessment, as they emphasise the social aspect of
learning and students’ active role and participation in the teaching, learning, and
assessment processes. These theories have possibly led to the evolution and adop-
tion of alternative assessment because their characteristics have become the benefits
and features of such assessment, and this constitutes the theoretical framework of
this study.
2.1 Research Problem and Rationale of the Study
Generally speaking, assessment can be an engine of reform (Winke, 2011) as it has
powerful implications for improving the quality of teaching and learning (Wolf
et al., 2008). Black and Wiliam (1998, as cited in Earl & Katz, 2006) reviewed more
than 250 studies, all of which found that the intentional use of classroom assess-
ments to promote learning increased students’ achievements. Hidri (2017) also
concluded that students’ performances were improved by mediation and support
of alternative/dynamic assessment of reading comprehension. Moreover, alterna-
tive assessment results, which are based on learner progress over time, could be
used to improve instruction (Tsagari, 2004). This is because they are integral
components of teaching and learning, in which each individual student can be
approached as a unique person that can be compared to both his/her past perfor-
mances and the intended aims (Ataç, 2012). Overall, alternative assessments are
important for students’ progress in learning and meeting the goals of any education
programme, but the inherent problem is the students’ low ability or unwillingness to
perform these assessments. For example, the student participants of Hidri (2014)
performed well with alternative/dynamic assessment mediation but when the medi-
ators left them alone or reduced help, the students became indecisive and unable to
continue processing the assessment items. This calls for an urgent need for numer-
ous studies to identify the student-related challenges of alternative assessments and
explore the factors behind them for the purpose of finding and then testing solutions
for them. Seeking solutions to student-related challenges of alternative assessments
is the rationale for this study.
Moreover, the advantages of using alternative assessments in tertiary education
is that they establish expectations that are appropriate within the cognitive, social,
and academic developments of students, whilst meeting the individual needs of
learners (Hamayan, 1995). Chirimbu (2013) discovered that employment of alter-
native assessments could lead to the accomplishment of important education steps
such as training needs analysis, identifying the best teaching materials, monitoring
progress, and evaluating the extent to which course objectives are reached. Finch
(2002) explains further:
396 D.A. Ismael
The authentic assessment model is thus particularly suitable for tertiary students, since it
encourages them to gradually assume responsibility for their own learning and for the
assessment of that learning, as the cycle of intention, action and reflection becomes a
personal endeavor, facilitated by: Portfolios, projects, self- and peer-assessment, learning
conversations, and reflective journals. (p. 5)
Finch’s explanation of the necessity and importance of alternative assessments is
mostly related to such activities in which students are assessors, including portfo-
lios, projects, self-assessment, peer-assessment, and reflective journals. These types
of alternative assessment are not performed adequately by the students discussed in
this paper, who study in two English departments at a public university in the
Kurdistan Region, which is the focus of this study. This is mainly because of several
student-related challenges of alternative assessments.
The basic rationale for conducting this study is to diagnose these challenges in
many dimensions in order to identify their associated factors. This is immensely
important because student-related challenges, as this current study reveals, are the
most pressing obstacles to the successful implementation of alternative assess-
ments. Finding solutions for these challenges, which possibly result in developing
the implementation of alternative assessment, is another significant aspect of the
rationale for this research. This is because through alternative assessment activities
such as portfolios, projects, self-assessment, peer-assessment, and reflective
journals, students become active assessors, and thus are given institutional trust
and respect. Through such activities, a student becomes “an active and socially
responsible agent, fully capable of needs analysis, goal setting, and assessment of
achievement” (Finch, 2002, p. 5). Thus, this current study can be pedagogically
significant because it possibly leads to performing alternative assessments more
successfully and sufficiently by EFL students.
In order to explore the student-related challenges of alternative assessments in
the two English departments under study, this research seeks credible answers to the
following four research questions from the perspective of Kurdish tertiary TESOL
teachers:
1. To what extent are EFL students able to perform alternative assessments?
2. Are EFL students willing to perform alternative assessments?
3. What is the affective aspect of EFL students regarding the performance of
alternative assessments?
4. To what extent are EFL students’ alternative assessments valid and fair?
3 Method and Data Analysis
The informing paradigm of this study is interpretivism, with the general ontological
assumption that social reality is the “product of individual consciousness” and is
created by individual minds (Cohen, Manion, & Morrison, 2007, p. 7), along with
the understanding that humans are the initiators of their own actions, and thus enjoy
Student-Related Challenges of Performing Alternative Assessments from the. . . 397
“free will and creativity” in producing their environments (Cohen et al., 2007, p. 8).
Following this assumption, this study investigated the student-related challenges of
conducting alternative assessments from the perspective of Kurdish tertiary TESOL
teachers in the context of two English departments in a selected public university in
the Kurdistan Region. Interpretivism focuses on context because things give dif-
ferent meanings to different people affected by their cultures, genders, and contexts
(Pine, 2009), as well as their thoughts, feelings, and beliefs (Wilson, 2009).
Epistemologically, this research commits to the interpretivist assumption that
approaching knowledge should be “personal, subjective and unique” (Cohen
et al., 2007, p. 7) because it requires an understanding of the complex world of
teaching experience from the perspective of those teachers who experience it
(Mertens, 2010).
This study adopted a phenomenological approach in order to see things from
people’s point of view. Educationally, the focus is on how teachers and academics
understand their world of education by attempting to interpret sense data (Briggs &
Coleman, 2007). Crotty (1998) explains that “we are rooted in this world, and in us
humans the world has come to consciousness” (p. 149). Phenomenologically,
human consciousness is an active intervention into reality (Crotty, 1998). This is
compatible with the aims of this study, which explores, from the perspective of
teachers, the student-related challenges of alternative assessments, regarding the
EFL students’ abilities, willingness, and affective conditions towards the perfor-
mance of alternative assessment, and whether their conduction of alternative
assessment practices are valid and fair.
In this study, I investigated specifically the student-related challenges of alter-
native assessments, and I used the data I collected in my recent action research
project (Ismael, 2016). Its interviews and focus group interviews prompted
responses to the challenges of alternative assessments in general and those related
to students in particular. Interpretivists pursue rich data from interviews to under-
stand what is happening from the perspective of participants (Radnor, 2002), as
they can yield many types of data: Personal perceptions, experiences, opinions,
preferences, and ideas (Wallace, 1998). Therefore, this study utilised semi-
structured interviews and focus group interviews with 12 in-service Kurdish tertiary
TESOL teachers, six from each of the two selected English departments. Semi-
structured interviews enable several questions to be posed to the participants but in
a natural manner, as the interviewer does not follow a specific predetermined order
(Savenye & Robinson, 1996). After the 12 interviews were completed, two subse-
quent focus group interviews were held with the same 12 interviewees (six per
group), in order to discuss the challenges of performing alternative assessments
more thoroughly, including the student-related challenges.
In the interviews and focus group interviews, each question addressed a dimen-
sion of the TESOL teachers’ beliefs, supplemented with some follow-up questions
to obtain optimal responses from the participants (Turner, 2010). All the partici-
pants of this study are volunteers, and signed the consent form prior to participating
in the two research instruments. Of the 12 participants, six have PhDs and six have
MAs, and all their personal data were protected by providing anonymity and
398 D.A. Ismael
confidentiality throughout the processes of reporting the data findings and conclu-
sions. The interviewees can be representative of the academic population of the two
selected English departments, as they constitute half of the EFL teaching staff.
After the 12 interviews and the two focus group interviews were transcribed
fully, I started coding, categorising, and thematising the transcripts accurately and
thoroughly. Throughout the process of data analysis, the seven steps of interview
research outlined by Kvale were followed: Designing, interviewing, transcribing,
analysing, thematising, verifying, and reporting (Knox & Burkard, 2009). Coding,
which “involves attaching one or more keywords to a text segment in order to
permit later identification of a statement” (Kvale, 2007, p. 105), is necessary for the
analysing, thematising, and verifying stages. I segmented the data into crucial
analytical components (Grabin, 2009) by using NVIVO, and used words and
short phrases in the coding process (Salda~
na, 2012). For categorising the codes
that conceptually reflect sub-categories, I benefited from Strauss and Corbin’s
explanation of open coding as “the process of breaking down, examining, compar-
ing, conceptualizing and categorizing data” (Strauss & Corbin, 1990, p. 61, cited in
Kvale, 2007). For doing thematisation, which involves thematic content analysis,
which is a descriptive presentation of qualitative data, interview transcripts, or other
relevant identified texts (Anderson, 2007), I benefited from Anderson’s 15 steps for
thematic content analysis of interviews and questionnaire open-ended questions
(Anderson, 2007). Finally, for reaching the findings and conclusions, I followed the
phenomenologic approach, focusing on how teachers and academics understand
their world of education by attempting to interpret sense data (Briggs & Coleman,
2007). Phenomenological research requires researchers to situate meaning units in
relationship to context and structure (Anderson, 2007).
4 Results and Discussion
This section is separated into four sub-sections, to address: (1) The low ability of
EFL students to perform alternative assessments, (2) students’ unwillingness to
perform alternative assessments, (3) the affective side of EFL students towards the
performance of alternative assessments, and (4) the validity and fairness of EFL
students’ alternative assessments.
4.1 The Low Ability of EFL Students to Perform Alternative
Assessments
This sub-section answers the first research question, which explored the extent to
which EFL students are able to perform alternative assessments. Almost all of the
interviewees (11 of 12) believed that the knowledge and skills of most students
Student-Related Challenges of Performing Alternative Assessments from the. . . 399
were not enough to enable them to perform alternative assessments successfully.
However, some interviewees thought that a few students could potentially perform
them adequately: “Few numbers of students can do this [alternative assessment]
which are at a very higher level; the others cannot . . .” (Ashty). İşlek and Ortaokulu
(2012) revealed that unlike their fellow students, low-achieving students could not
learn more by portfolio activities (alternative assessment practices), while average
and high achieving students learned more. The participants of this current study
thought that performing alternative assessments is currently difficult for most of
their EFL students, since these assessment activities require a lot of effort from
them: “Students need more effort. . . You are going to give more tasks to the
students (. . .)” (Mardan). This supported the work of Bahous (2008) who revealed
that student participants tasked with conducting a portfolio assessment were
objected to the workload they were supposed to do as they were engaged thought-
fully and deeply in assessment practices. As a result, ELLs could become bored by
the repetition of self-assessment and peer-assessments (Lim, 2007), which are
important types of portfolio work and alternative assessment in general, and in
which students are the assessors.
The participants’ beliefs about the difficulties of alternative assessments and the
ease of traditional testing matches reports in the extant literature due to the
considerable amount of thought, space, logistics, and training required by alterna-
tive assessments (Law & Eckes, 2007). When utilising alternative assessments,
students must perform, create, and produce, using tasks that represent meaningful
instructional procedures. Alternative assessments also use human judgments and
require instructors to play new instructional and assessment roles (Derakhshan
et al., 2011). Adequate implementation of alternative assessments poses significant
challenges that need to be addressed urgently. Nonetheless, regarding peer-
assessment, an important alternative assessment mostly done by students, Roskams
(1999), Cheng and Warren (2005), Grami (2010), and Azarnoosh (2013) revealed
that students experienced peer-assessment as not difficult and not boring.
4.2 Students’ Unwillingness to Perform Alternative
Assessments
This sub-section answers the second research question, which addresses whether
EFL students are willing to perform alternative assessments. Almost all the partic-
ipants (11 of 12) believed that students are not eager to learn, prefer spoon-feeding,
and mostly rely on memorisation: “Most of our students are memorising rules,
structures. . . even the analysis, so they just memorise things.... copying and pasting
in the exam” (Mardan). Thus, “they [students] want handouts for memorisation”
though “handouts have been prohibited by the Ministry” (Goran). According to the
400 D.A. Ismael
focus group interviews, the reason students desire handouts was that they enabled
them to prepare easily for their exams. Many of the one-on-one interviews revealed
the belief among tertiary TESOL teachers that students mostly depend on tests to
obtain certificates: “Students just want to get high degree and get marks and leave”
(Ara). This shows that their EFL students were perhaps test- and certificate-
oriented, as they prepare for tests in order to obtain a BA certificate rather than
focusing on their learning processes. For this situation, the assessment system and
the teaching staff of the English department could be blamed, as Nazanin noted:
“We depend on tests, and what the focus of the learners is, they are studying for the
tests, and they get good marks, and that is all.” Nazanin continued by saying that the
students “don’t care so much about these assessments [alternative assessments].”
This argument is convincing because around 90% of the assessment of students is
accomplished through tests (Ismael, 2016).
Nearly half of the participants (5 of 12) believed that alternative assessments
would not be successful enough with their students because they were mostly tired,
passive, unresponsive, uncooperative, and careless in their attitude towards these
assessments, and they mostly do not want to participate in them. In this regard, Ara
said: “They are careless and they don’t want to learn, they just want to keep silent.
So, how many activities you can do and there are no responses from the students?”
This might be demotivating to the TESOL teachers because students’ interest in and
response to learning, and their recognition and appreciation of their teachers’
efforts, are sources of motivation to teachers that encourage them to invest more
time and effort into their teaching, and make them more passionate about it (Zafar
Khan, 2011).
In the focus group, some interviewees (4 of 12) believed that students were not
eager to learn because they were not motivated as Goran said: “They are not
motivated (. . .). They don’t have enough interests to increase and further their
knowledge.” Three of these interviewees attributed the lack of motivation to
external factors such as political and economic instability in the Kurdistan Region:
“There is political instability and there is economic instability, so such problems are
expected always, outside factors” (Goran). None of the focus group interviewees
rejected those same outside factors when two of the participants, Aram and Naska,
discussed them. Overall, the economic and political crisis in the Kurdistan Region
and its current state of war has affected students.
There might be more potential for alternative assessments to interest students
and make them eager to learn, but perhaps teachers at present are unable to seize
that opportunity adequately. This is because constructivism, an important under-
pinning theory of alternative assessment, guides teachers to adjust their teaching
methods to allow their students to play a more active role in the classroom.
According to constructivism, teachers should cultivate a safe, comfortable, and
encouraging environment for students in order for them to be able to express their
ideas and develop their concepts (Grabin, 2009).
Student-Related Challenges of Performing Alternative Assessments from the. . . 401
4.3 The Affective Side of EFL Students Towards
the Performance of Alternative Assessments
This sub-section addresses the third research question, which focuses on the
affective aspect of EFL students regarding the performance of alternative assess-
ments. Almost all interviewees (11 of 12) believed that their EFL students were
generally shy, unconfident, and afraid of performing alternative assessments in
front of their peers, and some of them did not use English language in the classroom
out of the fear and shame of making mistakes. Roskams (1999) also discovered that
some teachers believed that peer feedback is constrained by several issues in Asian
cultures, including the fear of making mistakes. Regarding students’ confidence,
Lim (2007) revealed that ELLs were not confident in assessing grammatical mis-
takes or faulty pronunciation, either their own errors or those of their peers,
especially if a mistake was made by a more proficient student than the assessor.
Lack of confidence might be caused by the lack of a supportive environment in the
class, as identified by Abbas (2012). Thus, making students feel uncomfortable with
one another, which is one of the major challenges to the implementation of
alternative assessments. Some of the interviewees believed that this was because
the alternative assessment activities were new to their EFL students. Abbas (2012)
also discovered that the unfamiliarity of most Iraqi EFL students to assume
responsibility for their assessment was another challenge to implementing alterna-
tive assessments because students were used to being taught in a traditional teacher-
centred environment. That was also the situation of the students in the two English
departments where this current study undertaken. As a result, those students were
not ready to participate in alternative assessments, while most of them performed
better in paper and pencil tests according to most interviewees: “There are students
who do not participate in the daily activities. They do not have any daily partici-
pation but, in testing, they get very good marks” (Nazanin). This does not match the
current ELLs who are more enthusiastic about their learning (Lin, 2009), and it
decreases altruistic motivation for teaching, as teachers expressed their need for
such motivation: They feel good about themselves when they help students and see
them learn (Zafar Khan, 2011). This is perhaps not found in the participants of this
study, as they mostly did not view their students as eager to learn and appreciative
of teachers’ help. However, the reverse is true for the study of İşlek and Ortaokulu
(2012), which explored teachers’ opinions about students’ attitudes and perfor-
mance in a specific type of alternative assessment: Portfolios and their related
tasks. They revealed that teachers described all students as enjoying portfolio
practices more than traditional ones. The 20 EFL student interviewees who partic-
ipated in Peng’s study (2008) also held positive attitudes to peer-assessment, and
wished to do more peer-assessment in the future. Grami (2010) also concluded that
students discussed and exchanged their ideas more freely and openly while doing
peer-assessment, which had a less formal atmosphere. Furthermore, Lucas (2007),
Bahous (2008), Grami (2010), Hassaskhah and Sharifi (2011), and Azarnoosh
(2013) found that students’ attitudes to portfolio and peer-assessment were positive.
402 D.A. Ismael
Half of the interviewees (6 of 12) believed that students might resist using
alternative assessments. In the focus group interviews, two participants, Naska
and Ara, suggested the possibility of a general student resistance to change and
reform, including the performance of alternative assessments properly. Two par-
ticipants, Lara and Ala, in the one-on-one interviews and four additional focus
group interviewees stated that their students do not follow the rules and regulations
of the English department in their studies. Such resistance can be linked to the large
amount of work students have to accomplish (Bahous, 2008). Also, the repetition of
self-assessment and peer-assessments are likely to make ELLs bored (Lim, 2007).
One interviewee, Ala, experienced how students complained to the English depart-
ment about the devotion of five marks for the completion of alternative assess-
ments. She was obliged to alter the assessment procedure and devote only two
marks out of 20 to the alternative assessment, with the remaining 18 being assigned
to the pen and paper test. Hence, students need personal supervision and clear
guidelines, since certain students are accustomed to traditional language assess-
ments and may resist new alternative assessment practices (Kohonen, 1997, as cited
in Tsagari, 2004).
Two-thirds of the interviewees (8 of 12) believed that because of the importance
and necessity of alternative assessments, students should be taught, motivated,
encouraged, and even obliged to perform such assessments. Teaching students
how to use alternative assessments should include solving students’ subjectivity.
Two participants, Aram and Ala, provided advice to students such as “[you are] not
assessing a human, you are assessing a work, a production” (Aram), or “just cut the
names, just mix the paper, and give it all each. . . to assess” (Ala), in the hopes that
this would make them “learn to be somehow objective” (Aram). This shows that
some of the TESOL teacher participants sensed the necessity of educating and
training their EFL students on the importance of alternative assessments and how to
perform them. This is because students need personal supervision and clear guide-
lines (Kohonen, 1997, as cited in Tsagari, 2004), and generally, there is a pressing
need to educate all learners about the new developments in assessment (Baker,
2010). Suzuki (2009) concluded that L2 writing teachers probably need to guide
their students to enable them to become confident in their self-assessment and L2
ability. In order to teach and train students to use alternative assessments, teachers
themselves need to be qualified in the use of such assessments. As such, teachers
urgently require professional development in assessment in order to occupy leading
roles in training students to use alternative assessments. This is to avoid a situation
in which disadvantaged learners might suffer disproportionately from their instruc-
tors’ lack of experience. A way to eliminate such disparities is to ensure that
learners have been exposed to the desired material (Baker, 2010).
The opinions of the participants of this study on students’ affective conditions
(e.g., shyness, lack of confidence, fear, and shame) indicated that there were few
attempts from teachers to find suitable ways to engage students in alternative
assessments, especially spending time identifying various alternative assessments
that might interest the students. This is because there are a variety of different
alternative assessment practices that can be adapted for different contexts
Student-Related Challenges of Performing Alternative Assessments from the. . . 403
(Tannenbaum, 1996), and because these assessments have the significant benefit of
being context-specific, meaning that assessment methods can be adapted and
tailored to best reflect the purposes of learning (Chirimbu, 2013).
4.4 The Validity and Fairness of EFL Students’ Alternative
Assessments
This sub-section answers the fourth research question, which explored to what
extent EFL students’ alternative assessments are valid and fair. Almost all inter-
viewees (11 of 12) believed that students are subjective in doing self-assessment
and peer-assessment due to their social relationships and the unwillingness to
expose their own or their peers’ mistakes. For example, three of the participants,
Ala, Ara and Lara confirmed that when they asked their students to do peer-
assessment, they gave each other almost full marks, as Ara said: “Later on they
checked the mistakes. . . They gave 9 out of 10, 8 out of 10, etc.” However, the
reverse can also be true: “Sometimes, because they [students] don’t like each other,
they will be very strict; they will be very unfair with each other” (Goran). As a
result, students probably underestimate or overestimate their peers according to the
nature of their relationships, and likewise might overestimate themselves. That is
why three of the interviewees, Ashty, Goran, and Ala voiced the opinion that
student assessments would be never valid, with another adding that their high
marking of “eight out of ten, nine to ten. . . it is not reliable” (Lara).
Thus, students’ propensities to give overly high or low grades compromises the
collaboration among peers, which usually creates a collective ZPD through which
students can benefit equally (Turuk, 2008). A number of those 11 interviewees
related the unjust low or high grades to the nature of the students’ social culture and
society, in which many issues are taken personally: “Because of our culture,
because of the cultural values in Kurdistan, because students still want to help
each other, students betray, students cheat, students are not objective” (Goran), and
they also “take everything personally” (Ala).
In the context of this study, the subjectivity or unfairness in self-assessment and
peer-assessment is associated with social relationships, culture, and the tendency
not to expose mistakes. In other contexts, it is related to a number of other factors.
An exploration of the cultural factors involving performing alternative assessments
in Asian cultures revealed that peer feedback was constrained by several issues
(Roskams, 1999). This is sometimes related to nationality, with Blue (1994, as cited
in Chen, 2008) speculating that nationality can be a crucial factor in self-
assessment, since some nationalities might have a tendency to overestimate them-
selves, whilst others tend to underestimate their levels. Conversely, sometimes it is
associated with the level of students’ proficiency. For example, more able and
experienced students tend to under-mark themselves in comparison with the mark-
ing of their teachers, as many studies on self-assessment reviewed by Boud and
404 D.A. Ismael
Falchikov’s work (1989, cited in Chen, 2008) have concluded. Suzuki (2009) also
showed some studies revealing that more advanced L2 students tend to underesti-
mate their language proficiency. In contrast, Chen (2008) discovered that lower-
ability students were inclined to over-mark themselves. The case of overestimation
and underestimation of students through self-assessment and peer-assessment was
also discovered by Chen (2008), Suzuki (2009), and Khonbi and Sadeghi (2013),
with the latter authors revealing that students were either unconfident or very strict
in peer-assessment. The possibility of overestimation or underestimation of one’s
own or their peers’ knowledge and ability by self-assessment and peer-assessment
clearly denotes students’ subjectivity in alternative assessments. This will generally
create a challenge to the influence of self-assessment and peer-assessment on
fostering students’ learning (Azarnoosh, 2013; Khonbi & Sadeghi, 2013).
The student-related challenges of alternative assessments mentioned above can
be significant obstacles to the successful implementation of alternative assessments
in the context of this study, because such assessments require students to perform,
create, or produce an activity, and utilise tasks that represent meaningful instruc-
tional procedures (Derakhshan et al., 2011). These challenges might have posed
contextual factors that affected the ambition of TESOL teachers for a wider
implementation of alternative assessments. This constitutes a major challenge to
teaching, learning and assessment, as it is claimed that by doing alternative assess-
ments, data can be gathered on some factors that affect achievements. These factors
can be identified in students’ linguistic, cultural, familial, or educational back-
grounds (Tsagari, 2004). This supports the finding of Jia (2009), who explains
that even if language assessment practitioners attempt and wish to follow and
commit to a code, there might be working conditions as challenges to their attempts
that make it difficult for them to fulfil all the items of that code (Jia, 2009).
Fundamentally, all language education contexts require the implementation of
various alternative assessments to meet their actual inferential demands and their
intended consequences (Norris, Brown, Hudson, & Bonk, 2002). That is why
further efforts by researchers and senior administrators are urgently required in
order to address the shortcomings of using alternative assessments.
The student-related challenges of alternative assessments revealed by this study
can also be regarded as drawbacks in the teaching-learning process. This is largely
because they can also cause a significant challenge to the essence of the two main
underpinning theories of alternative assessments, constructivism and the socio-
cultural approach, which are among the most influential theories of learning,
since they demand an active role from students that can be accomplished through
performing alternative assessments and other teaching and learning activities.
According to constructivism, people cannot learn by absorbing knowledge pas-
sively, and so teachers need to adjust their teaching methods to allow students to
play a more active role. This can be achieved by creating a safe and encouraging
environment in which students feel comfortable to express their ideas and develop
their concepts (Grabin, 2009). Moreover, if students are not active then there would
be insufficient mediation in ZPD within the socio-cultural approach framework.
This is because Vygotsky focuses more on the essentiality of discourse with others
and language mediation to reach a higher level of learning (Dragemark Oscarson,
Student-Related Challenges of Performing Alternative Assessments from the. . . 405
2009). Mediation is any human activity through which higher mental functions are
mediated by another human being (Shrestha & Coffin, 2012). Alternative assess-
ments are a collaborative approach to assessment that encourages teachers and
students to interact in the teaching and learning processes (Tsagari, 2004). There-
fore, the Vygotskian notion of the ZPD and mediation are particularly central to
alternative/dynamic assessment (Shrestha & Coffin, 2012). ZPD means that cogni-
tive development takes place through language-mediated activities in interaction
with people who have more advanced cognitive abilities, such as teachers or more
able peers (Vygotsky, 1978, as cited in Ishihara, 2009).
5 Conclusion
To varying numbers and varying degrees, most of the TESOL teacher participants in
this study, in both the one-on-one and focus group interviews, believed that the
knowledge and skills of most of their EFL students were insufficient to enable them
to perform alternative assessments adequately, and thus doing so proved difficult. The
interviewees also believed that students were mostly unwilling and unmotivated to
learn, and preferred spoon-feeding and memorisation in order to pass tests and acquire
certificates. As such, it was suggested that students do not particularly care about
alternative assessments, as most assessments are accomplished through tests.
The participants believed that students were mostly not motivated to learn
because of the political and economic instability in the Kurdistan Region. They
thought that students were mostly unresponsive, uncooperative, tired, passive, and
careless about alternative assessments, and that they did not want to participate in
them. However, when doing so it was found that students mostly assessed both
themselves and each other unfairly and inaccurately. The interviewees also
believed that students were shy, unconfident and afraid of performing alternative
assessments, perhaps due to the fear and even shame of making mistakes. They
believed that their EFL students might resist performing alternative assessments,
along with other changes and reforms, and reject the rules of the department.
Because of the importance and necessity of alternative assessments, the inter-
viewees believed that students should be taught, motivated, encouraged, and even
obliged to perform these assessments, in accordance with the departmental rules.
They believed that students were mostly subjective in doing self-assessment and
peer-assessment because of the social relationships they have and their unwilling-
ness to show their mistakes and avoid losing marks. However, the participants
believed that, in peer-assessment, the reverse might be true as sometimes students
may underestimate their peers, and thus student assessments were not viewed to be
valid and fair. The teachers thought that the situation of underestimation or
overestimation in alternative assessments by students could be related to the nature
of their social culture and society, in which many issues are taken personally.
However, they also believed that there would be some solutions for these chal-
lenges. For example, in order to solve student subjectivity, advising students about
objectivity or anonymizing student names from activity papers can be useful.
406 D.A. Ismael
Due to the existence of student-related challenges, the implementation of alter-
native assessment cannot fulfil its main goal, which is to enhance student learning
of English language through a gradual process of continuous and accurate assess-
ment of proficiency. These student-related challenges also hinder the adoption of
the two main underpinning theories of alternative assessment and their process-
oriented goals, their focus on social aspects of learning, and their emphasis on
students’ active roles in the teaching, learning, and assessment processes.
This study is expected to bring about some pedagogical, research, and method-
ological implications. The limited alternative assessment knowledge and skills of
most EFL students, and the difficulties they encounter, will possibly have a number
of pedagogical implications for English departments to help them identify what
actions should be taken. Firstly, they may be able to increase students’ knowledge
of English language and enhance its four skills, and secondly, they may be able to
help teach and train students how to perform alternative assessments. Unwilling-
ness, a lack of motivation, preoccupation with tests and certificates, and preference
for spoon-feeding and memorisation will probably inform the English departments’
senior administrators about the necessity of increasing the value of alternative
assessments. In turn, this may lead to the allocation of more marks for them to
motivate students to learn through alternative assessments and get enough marks to
pass, thus eliminating spoon-feeding and memorisation. This might also encourage
students to be more responsive, cooperative, interested in, and willing to participate
in alternative assessment activities, which would subsequently result in fairer and
more accurate student assessments. Another implication can be related to the
affective challenges of students (e.g., shy, unconfident, and afraid), as this would
inform teachers to provide advice to their students about the normality of making
mistakes or encountering difficulties in second or foreign language acquisition. This
study could shed more light on the necessity of training students to perform
alternative assessments and eliminating their subjectivity when performing these
assessments.
Based on the findings, conclusions, and implications of this study, I offer the
following recommendations to teachers and senior administrators in the English
departments of the universities in the Kurdistan Region, to help them to eliminate
and resolve the student-related challenges of alternative assessments:
1. English departments should train students in order to enable them to perform
alternative assessments adequately and objectively.
2. English departments and their TESOL teachers should encourage and motivate
their students to be eager to learn rather than spoon-feeding them information to
memorise in order to pass tests.
3. English departments should increase the value of alternative assessments by
allocating more marks for them, creating equal weight for alternative assess-
ments and traditional tests.
4. EFL students should be advised that it is normal to make mistakes when using
English in the EFL classroom.
Student-Related Challenges of Performing Alternative Assessments from the. . . 407
5. English departments should modify their rules and regulations to ensure that
they are compatible with the students’ abilities and interests, which in turn would
encourage them to follow rules and reforms related to assessments.
Finally, this study provides some suggestions for future research in the field of
alternative assessment in the English departments. Because of the immense signif-
icance of alternative assessment and its underpinning theories, constructivism and
the socio-cultural approach to teaching, learning, and assessment of English lan-
guage, each practical challenge of alternative assessments and the factors behind
them necessitates numerous studies to address them. Among these, student-related
challenges are pressing obstacles to alternative assessments, and to the adoption of
constructivism and socio-cultural theory. Therefore, this study recommends the
following areas for further and more in-depth investigations:
1. Each of the student-related challenges this study discovered,
2. External factors (e.g., cultural, social, and economic situations) that affect
students and their work,
3. The factors behind student unwillingness and lack of motivation, along with
their preference for spoon-feeding and memorisation,
4. The effect of students’ affective aspects (e.g., shy, unconfident, and afraid) when
performing alternative assessments,
5. The possibility of student resistance to alternative assessments, along with other
changes and reforms to assessment,
6. The factors behind students’ subjectivity when performing alternative
assessments,
7. The influence of training students to perform alternative assessments, and
8. Potential solutions to eliminate student subjectivity in alternative assessments.
References
Abbas, Z. (2012). Difficulties in using methods of alternative assessment in teaching from Iraqi
instructors’ points of view. AL-Fatih Journal, 48, 23–45.
Anderson, R. (2007). Thematic content analysis (TCA) descriptive presentation of qualitative data
(p. 4). Palo Alto, CA: Institute of Transpersonal Psychology.
Anusienė, L., Kaminskienė, L., & Kavaliauskienė, G. (2007). The challenges for ESP learners:
Alternative assessment of performance and usefulness of class activities. Kalbu
˛ Studijos, 10,
75–81.
Ataç, B. A. (2012). Foreign language teachers’ attitude toward authentic assessment in language
teaching. Journal of Language & Linguistics Studies, 8(2), 7–19.
Azarnoosh, M. (2013). Peer assessment in an EFL context: Attitudes and friendship bias. Lan-
guage Testing in Asia, 3(1), 1–10.
Bahous, R. (2008). The self-assessed portfolio: A case study. Assessment & Evaluation in Higher
Education, 33(4), 381–393.
Baker, E. L. (2010). What probably works in alternative assessment. Report 772 of National
Center for Research on Evaluation, Standards, and Student Testing (CRESST), Graduate
School of Education & Information Studies, UCLA, University of California, Los Angeles.
Retrieved from http://files.eric.ed.gov/fulltext/ED512658.pdf
408 D.A. Ismael
Borg, S. (2006). Teacher cognition and language education, research and practice. London,
New York: Continuum.
Briggs, A., & Coleman, M. (2007). Research methods in educational leadership and management.
London: Sage Publications.
Brown, J. D., & Hudson, T. (1998). The alternatives in language assessment. TESOL Quarterly, 32
(4), 653–675.
Chen, Y. M. (2008). Learning to self-assess oral performance in English: A longitudinal case
study. Language Teaching Research, 12(2), 235–262. https://doi.org/10.1177/
1362168807086293.
Cheng, W., & Warren, M. (2005). Peer assessment of language proficiency. Language Testing, 22
(1), 93–121.
Cheng, L., Rogers, T., & Hu, H. (2004). ESL/EFL instructors’ classroom assessment practices:
Purposes, methods, and procedures. Language Testing, 21(3), 360–389. https://doi.org/10.
1191/0265532204lt288oa.
Chirimbu, S. (2013). Using alternative assessment methods in foreign language teaching. Case
study: Alternative assessment of business English for university students. Scientific Bulletin of
the Politehnica University of Timisoara, Transactions on Modern Languages, (12), 91–99.
Cohen, L., Manion, L., & Morrison, K. (2007). Research methods in education. USA and Canada:
Routledge.
Crotty, M. (1998). The foundations of social research, meaning and perspective in the research
process. London: Sage Publications.
Derakhshan, A., Rezaei, S., & Alemi, M. (2011). Alternatives in assessment or alternatives to
assessment: A solution or a quandary. International Journal of English Linguistics, 1(1),
173–178.
Dragemark Oscarson, A. (2009). Self-assessment of writing in learning English as a foreign
language. A study at the upper secondary school level. G€
oteborg, Sweden: ACTA Universitatis
Gothoburgensis, Geson Hylte Tryck.
Earl, L., & Katz, S. (2006). Rethinking classroom assessment with purpose in mind: Assessment
for learning, assessment as learning and assessment of learning. Winnipeg, MB: Minister of
Education, Citizenship and Youth.
Finch, A. E. (2002). Authentic assessment: Implications for EFL performance testing in Korea.
Secondary Education Research, 49, 89–122.
Grabin, L. A. (2009). Alternative assessment in the teaching of English as a foreign language in
Israel. Unpublished doctoral dissertation, University of South Africa, Pretoria, Gauteng,
South Africa.
Grami, G. M. A. (2010). The effects of integrating peer feedback into university-level ESL writing
curriculum: A comparative study in a Saudi context. Unpublished doctoral dissertation,
Newcastle University, Newcastle upon Tyne, UK.
Hamayan, E. V. (1995). Approaches to alternative assessment. Annual Review of Applied Lin-
guistics, 15, 212–226.
Hancock, C. (1994). Alternative assessment and second language study: What and why? East
Lansing, MI: National Center for Research on Teacher Learning.
Hassaskhah, J., & Sharifi, A. (2011). The role of portfolio assessment and reflection on process
writing. Asian EFL Journal Quarterly, 13(1), 193–231.
Hidri, S. (2014). Developing and evaluating a dynamic assessment of listening comprehension in
an EFL context. Language Testing in Asia, 4(1), 4. https://doi.org/10.1186/2229-0443-4-4.
Hidri, S. (2017). Specs validation of a dynamic reading comprehension test for EAP learners in an
EFL context. In S. Hidri & C. Coombe (Eds.), Evaluation in foreign language education in the
Middle East and North Africa (pp. 315–337). Cham: Springer International Publishing.
Ismael, D. A. (2016). The assessment practices of in-service Kurdish Tertiary TESOL teachers and
their cognitions of alternative assessment (Unpublished doctoral thesis). University of Exeter,
Exeter, UK.
Student-Related Challenges of Performing Alternative Assessments from the. . . 409
Ishihara, N. (2009). Teacher-based assessment for foreign language pragmatics. TESOL Quarterly,
43(3), 445–470.
İşlek, H., & Ortaokulu, M. A. G. (2012). Students’ attitudes and performance on portfolios.
Journal of Educational & Instructional Studies in the World, 2(4), 42–47.
Jia, Y. (2009). Ethical standards for language testing professionals: An introduction to five major
codes. Shiken: JALT Testing & Evaluation SIG Newsletter, 13(2), 2–8.
Khonbi, Z. A., & Sadeghi, K. (2013). The effect of assessment type (self vs. peer) on Iranian
university EFL students’ course achievement. Procedia-Social and Behavioral Sciences, 70,
1552–1564.
Knox, S., & Burkard, A. W. (2009). Qualitative research interviews. Psychotherapy Research, 19
(4–5), 566–575.
Kvale, S. (2007). Doing interviews. London: Sage.
Law, B., & Eckes, M. (2007). Assessment and ESL: An alternative approach. Canada: Portage &
Main Press.
Lim, H. (2007). A study of self- and peer-assessment on learners’ oral proficiency. In CamLing
proceedings of the Fifth University of Cambridge postgraduate conference in language
research (pp. 169–176). Cambridge: Cambridge Institute of Language Research.
Lin, Y. (2009). Enhancing EFL learners’ English reading proficiency through collocation instruc-
tion. English Teaching & Learning, 33(1), 37–71.
Lucas, R. (2007). A study on portfolio assessment as an effective student self-evaluation scheme.
The Asia-Pacific Education Researcher, 16(1), 23–32.
Mertens, M. (2010). Research and evaluation in education and psychology, integrating diversity
with quantitative, qualitative, and mixed methods. California: Sage Publications.
Norris, J. M., Brown, J. D., Hudson, T. D., & Bonk, W. (2002). Examinee abilities and task
difficulty in task-based second language performance assessment. Language Testing, 19(4),
395–418.
Peng, J. C. (2008). Peer assessment in an EFL context: Attitudes and correlations. In Proceedings
of the 2008 second language research forum (pp. 89–107). Somerville, MA: Cascadilla Pro-
ceedings Project.
Pine, J. (2009). Teacher action research, building knowledge democracies. USA: Sage
Publications.
Radnor, H. (2002). Researching your professional practice, doing interpretive research.
Buckingham and Philadelphia: Open University Press.
Roskams, T. (1999). Chinese EFL students’ attitudes to peer feedback and peer assessment in an
extended pair work setting. RELC Journal, 30(1), 79–123.
Salda~
na, J. (2012). The coding manual of qualitative researchers. Los Angeles, London, New
Delhi: Sage Publications.
Savenye, W., & Robinson, R. (1996). Qualitative research issues and methods: An introduction for
educational technologists. In D. Jonassen (Ed.), Handbook of research for educational com-
munications and technology (pp. 1171–1195). New York: Macmillan.
Scarino, A. (2013). Language assessment literacy as self-awareness: Understanding the role of
interpretation in assessment and in teacher learning. Language Testing, 30(3), 309–327. https://
doi.org/10.1177/0265532213480128.
Shrestha, P., & Coffin, C. (2012). Dynamic assessment, tutor mediation and academic writing
development. Assessing Writing, 17(1), 55–70. https://doi.org/10.1016/j.asw.2011.11.003.
Suzuki, M. (2009). The compatibility of L2 learners’ assessment of self- and peer revisions of
writing with teachers’ assessment. TESOL Quarterly, 43(1), 137–148. https://doi.org/10.1002/
j.1545-7249.2009.tb00233.x.
Tannenbaum, J. (1996). Practical ideas on alternative assessment for ESL students. ERIC Digest
Journal, No. ED, 395–500.
Troudi, S. (2006). Empowering ourselves through action research. In P. Davidson et al. (Eds.),
Teaching, learning, leading: Proceedings of the 11th TESOL Arabia conference
(pp. 277–290). Dubai: TESOL Arabia Publications.
410 D.A. Ismael
Tsagari, D. (2004). Is there life beyond language testing? An introduction to alternative language
assessment. Center for Research in Language Education, CRILE Working Papers, 58, 1–23.
Turner, D. W. (2010). Qualitative interview design: A practical guide for novice investigators. The
Qualitative Report, 15(3), 754–760.
Turuk, M. (2008). The relevance and implications of Vygotsky’s sociocultural theory in the second
language classroom. ARECLS, 5, 244–262.
Vygotsky, L. (1978). Interaction between learning and development. Readings on the development
of children, 23(3), 34–41.
Wach, A. (2012). Classroom-based language efficiency assessment: A challenge for EFL teachers.
Wydawnictwo Naukowe UAM, Glottodidactica, 39(1), 81–92.
Wallace, M. (1998). Action research for language teachers. Cambridge: Cambridge University
Press.
Wang, Z. (2011). A case study of one EFL writing teacher’s feedback on discourse for advanced
learners in China. University of Sydney Papers in TESOL, 6.
Wilson, E. (2009). School-based research, a guide for education students. London: Sage
Publications.
Winke, P. (2011). Evaluating the validity of a high-stakes ESL test: Why teachers’ perceptions
matter. TESOL Quarterly, 45(4), 628–660. https://doi.org/10.5054/tq.2011.268063.
Wolf, M. K., Herman, J. L., Bachman, L. F., Bailey, A. L., & Griffin, N. (2008). Recommendations
for assessing English language learners: English language proficiency measures and accom-
modation uses (Part 3 of 3). CRESST Report 732, National Center for Research on Evaluation,
Standards, and Student Testing (CRESST).
Yu, S. (2013). EFL teachers’ beliefs and practices regarding peer feedback in L2 writing class-
rooms. Polyglossia, 24, 74–79.
Zafar Khan, S. (2011). Factors affecting the motivation of expatriate English as a foreign
language (EFL) teachers in the Sultanate of Oman. Unpublished doctoral dissertation, Uni-
versity of Exeter, Exeter, UK.
Student-Related Challenges of Performing Alternative Assessments from the. . . 411
Part VII
Standard Exams and Test-Taking
Strategies
Extending the Scope of the English Exit Exam:
A Study from a Ukrainian Classical University
Olesia Liubashenko and Olga Yashenkova
Abstract The study explores the scenario of the English exit exam that Ukrainian
students are required to take at the end of their BA program, addressing some
challenges related to student exam performance. Such exams are formal and limited
to quantitative assessment, they are traditionally oral and administered by the State
Examination Board. The exam scenario commonly includes only the acts of
sending and receiving information that are programed in advance and expected
by its participants. These acts block the aspiration of examinees to expand their
scope of thinking and narrow it to memorizing and answering according to a certain
pattern. The study, which is based on a sample of 151 students who major in
Ukrainian language and literature and minor in English as a foreign language,
aims to change the existing exam scenario. We see the exam as a macro commu-
nicative act, and its scenario should imply a highly interactive activity of unique
language personalities. The scenario we suggest includes a talk in interaction that
shows symmetry and role reversal, and gives the participants an opportunity for
spiritual and intellectual development. The function of examiners is to facilitate
spoken interactions by getting examinees to think laterally. Results indicate that
lateral thinking is a necessary condition for the successful implementation of the
exam as a macro communicative act and serves as an instrument for ongoing
learning. The study raises a number of topical issues related to the impact of
assessment procedures on learners.
Keywords English exit exam • Macro communicative act • Exam scenario •
Lateral thinking • Ongoing learning
O. Liubashenko (*)
Department of Methodology of Teaching Ukrainian and Foreign Languages and Literatures,
the Institute of Philology, Taras Shevchenko National University of Kyiv, Tarasa Shevchenko
Boulevard 14, office 134, Kyiv 01601, Ukraine
e-mail: telesykivasyk@ua.fm
O. Yashenkova
Department of English Philology and Intercultural Communication, the Institute of Philology,
Taras Shevchenko National University of Kyiv, Tarasa Shevchenko Boulevard 14, office 56,
Kyiv 01601, Ukraine
e-mail: olgayashenkova@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_20
415
1 Introduction
English as a Foreign Language (EFL) degree programs in Ukraine require students
to take English exit exams, which are designed to show what has been achieved,
how effectively learners have met the assessment criteria and graduation standards.
These are high-stakes exams as their results have a significant impact on the life
chances of examinees (e.g., access to further education or employment opportuni-
ties). Because the stakes are so high, taking English exit exams is often a challeng-
ing endeavor. Examinees feel really under pressure mentally or emotionally, which
blocks their aspiration to think creatively, generate and impart ideas. Examiners act
as assessors of a limited number of language skills. There is no free dialogue
between the subjects of pedagogic discourse. The exam scenario includes only
the acts of sending and receiving information programed in advance and expected
by the examiner whose main function seems to control the examinee and calculate
the exam score. In this context, most learners are not able to demonstrate English
language proficiency at their best. Even bright students who typically earn high
grades in their courses may score lower in the exit exam. In this regard, we can
agree with de Bono who argues, “Schools waste two-thirds of the talent in society.
The universities sterilize the rest” (as cited in Balakrishnan, 2007, para. 23). We
would like to extend his words, saying that sterilization of talents mainly occurs
during exams.
To improve the existing exam scenario, we launched a research project in 2014.
We suggested to use an integrated evaluation strategy aimed at getting students to
think laterally and overcome communication barriers by interacting in a free
manner in the exam (Liubashenko & Yashenkova, 2014), and made an attempt to
develop the management model of the English language exam as an integral part of
the continuous teaching-learning process (Liubashenko & Yashenkova, 2015).
The current study deals with the next phase of the research project, focusing on
the English exit exam at the bachelor’s level and addressing some challenges
related to communication and ongoing learning during the exam.
2 Theoretical Background
The study captures the rich view of communication in learning and draws on
multiple theories in an effort to create an effective English exit exam. They include,
first of all, activity theory (Leontyev, 1975; Luria, 1976; Rubinshtein, 1946;
Vygotsky, 1978), dialog theory (Bakhtin, 1981; Jakubinskiy, 1986), speech activity
theory (Leontyev, 2008), and speech communication theory (Krasnykh, 2001;
Yashenkova, 2010), as well as such learning theories and educational philosophies
as andragogy or adult learning (Knowles, Holton, & Swanson, 2005), lateral
thinking (de Bono, 1992), humanistic theory of personality (Maslow, 1954; Rogers,
1969), problem-based learning (Barrows, 1986; Matiushkin, 2009), strategic
416 O. Liubashenko and O. Yashenkova
language learning and teaching (Cohen, 2011; Griffiths, 2013; Liubashenko, 2007;
Oxford, 1990). Many of them overlap and build on each other, providing a diverse
set of principles that can be applied to EFL student assessment. These theories give
the grounds for modelling the English exit exam as an effective macro communi-
cative act that is aimed at attaining a specified goal or outcome through
communication.
By a macro communicative act, we mean an event that happens at a certain time
and place. It is viewed as a scene from life in which real persons act (Krasnykh,
2001, p. 193). “Effective communication, however, occurs when individuals
achieve a shared understanding, stimulate others to take actions, and encourage
people to think in new ways” (Bovée & Thill, 2000, p. 4). To be effective, such a
macro communicative act should be a two-way activity in which both the addresser
and the addressee make efforts and coordinate with one another. Communication
success can be reached only as a result of joint efforts of all the participants
(Yashenkova, 2010, p. 133).
We formulated the following core assumptions that underlie the current
research:
1. The English exit exam is a consciously controlled and goal-directed process that
is jointly projected by all its participants, who employ a variety of strategies to
achieve their goals.
2. The English exit exam is initiated and driven by a sequence of tasks that reflect
real-world uses of language, and is seen as a problem-solving, discovery, and
creative activity, in which the examinee plays a central role.
3. The English exit exam scenario includes a shared dialogue between the exam-
iner and examinee, in which both are constructors of pedagogic discourse and
bear responsibility for its effectiveness.
4. Accomplishing exam tasks requires social interaction and meaningful commu-
nication, which presupposes linguistic creativity and creating meaning as a result
of the collaboration between examiners and examinees as social agents and
unique language personalities.
5. The English exit exam is an integral part of the continuous teaching-learning
cycle. It is not only for grading purposes; it provides motivation for further
learning and gives opportunities for individual development and self-realization.
6. Examinees are autonomous, self-directed, pragmatic, and goal-oriented. They
tend to take the initiative, interact communicatively and purposefully while
engaged in problem tasks, using their prior knowledge and experiences as
resources and developing their own routes to further learning.
7. The main function of examiners is to facilitate spoken interactions by building a
supportive climate and getting examinees to think laterally, encouraging them to
find unconventional ways to perform tasks and letting them offer their own
scenario of the exam.
8. The English exit exam as a process of creative construction considers trial and
error/mistake as a natural part of gaining knowledge or expertise. Examinees are
encouraged to make guesses and learn from their errors/mistakes.
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 417
To sum up, we strongly believe that the English exit exam as a collaborative
problem solving activity implemented through communication should use lateral
thinking as a point of departure.
2.1 Research Problem and Rationale of the Study
Viewing student assessment as “a cornerstone of effective teaching and learning”
(Partnership for 21st Century Skills (P21), 2007, p. 1) necessitates rethinking the
nature of assessment that is “completely artificial” (Chomsky, 2015) and conceived
narrowly. Although experts emphasize the importance of a broader approach to
assessing learning outcomes, which implies multiple assessments and an expanded
set of quality criteria (Brown & Abeywickrama, 2010; O’Malley & Pierce, 1996;
Segers, Dochy, & Cascallar, 2003), most assessments in widespread use today still
measure students’ ability to memorize and recall discrete facts, not the ability to
apply knowledge and complete complex problem-solving tasks (Beacco et al.,
2016, p. 109; P21, 2007, p. 1–2; Simon, Ercikan, & Rousseau, 2013, p. 3). These
assessments do not take into account students’ uniqueness and creativity, nor are
they valid, reliable, and fair (Abedi, 2010, p. 1; Huang, Han, & Schnapp, 2012; The
National Center for Fair & Open Testing, 2007).
Our study addressed critically important matters of assessment effectiveness and
issues related to the impact of assessment procedures on learners, and focused on a
high-stakes EFL exam for BA students, namely the English exit exam.
Despite a growing body of literature on high-stakes EFL exams in a wide range
of national contexts (Benner, 2013; Bøhn, 2015; Damankesh & Babaii, 2015; East,
2015; Hatipo^
glu, 2016; Kazemipour, 2014; Labordaa, Bejaranob, de Juana,
Litzlera, & Rosaa, 2014), there is hardly any study that investigated challenges
encountered in higher school exit exams in Ukraine. We seek to highlight this gap
in the Ukrainian context.
The purpose of the study was to explore the scenario of the English exit exam
taken annually by BA students of the Institute of Philology of Taras Shevchenko
National University of Kyiv, Ukraine, and find some ways to improve the commu-
nication process during the exam to motivate students to learn and help them
discover their potential. A major rationale behind the study was to make the English
exit exam more effective.
Therefore, we attempted to answer the following research questions (RQ):
RQ1: What hinders us from making the English exit exam an effective macro
communicative act?
RQ2: How should we change the exam scenario to have an effective macro
communicative act?
RQ3: How can we use lateral thinking to make changes in the exam scenario?
In the light of the research questions, the study aimed to test the following
hypothesis:
418 O. Liubashenko and O. Yashenkova
The exam scenario, which is based on lateral thinking and includes facilitation,
quantitative and qualitative assessment, transforms the English exit exam from a
blocker of a free dialogue between examiners and examinees into the realm of wide
search and idea exchange, undermines learners’ negative perceptions of exams and
improves their exam performance.
3 Method
3.1 Participants
The study participants were 151 students of the Institute of Philology at Taras
Shevchenko National University of Kyiv. They were native speakers of Ukrainian
or Russian from various parts of Ukraine. Their ages ranged from 19 to 22, with
146 females and 5 males. All of the participants were fourth year BA students who
majored in Ukrainian language and literature, and minored in EFL. They were
taught by Ukrainian teachers and had 4 h of English per week. They were required
to study for 4 years and take their English exit exam at the end of the final year with
a minimum 60% passing score to obtain a BA degree.
The English exit exam was administered by the State Examination Board,
consisting of four female teachers, whose teaching experience at the tertiary level
ranged from 15 to 25 years. The head of the State Examination Board was a teacher
from the National Technical University of Ukraine “Igor Sikorsky Kyiv Polytech-
nic Institute”, while the others were teachers from Taras Shevchenko National
University of Kyiv. All the examiners had a PhD degree in English linguistics or
education.
3.2 Data Collection
The main data presented in this study were collected from the English exit exams
that were held at the Institute of Philology at Taras Shevchenko National University
of Kyiv in 2014–2016. The exam format was determined by the Institute Method-
ology Committee and consisted of three tasks: (a) An oral summary of the English
authentic text based on the reading comprehension activity; (b) a grammatical
analysis of five linguistic units; and (c) a discussion of one of the topics from the
syllabus.
Dictionaries or reference books were not permitted in the exam, but examinees
were allowed to bring and use some audio/visual supporting material prepared and
agreed in advance.
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 419
Each examinee was given 30 min for preparation, after that he or she was
interviewed face-to-face and assessed by the members of the State Examination
Board within 30 min. One examiner took the part of the assessor and the others of
the interlocutors. The assessor focused on pronunciation, grammar, vocabulary,
discourse management, and interactive communication. The interlocutors’ task was
to facilitate a conversation and elicit a range of skills and knowledge from the
examinee. The examinees were expected to ask the examiners questions, share their
experiences, express and verbalize their feelings and emotions, make comments
upon their thinking operations, give alternative answers to the same question,
generate a variety of ideas to solve a problem, and look for unconventional ways
to analyze exam texts.
The researchers were among the members of the State Examination Board and
acted as facilitators and observers at a time, focusing on the factors that effected
exam performance and jotting down notes.
In addition, some data were gathered through two paper-based questionnaires
developed by the researchers to explore students’ attitudes and perceptions of the
English exit exam. The students were asked to respond to a structured sequence of
questions, as well as to make any comment anonymously. The pre-exam question-
naire (Appendix 1) was administered at pre-exam consultations held 3 days before
the exam. The post-exam questionnaire (Appendix 2) was undertaken as soon as the
exam was over. The pre-exam consultations were also used to give oral and written
instructions to examinees and examiners (Appendices 3 and 4).
3.3 Data Analysis
The collected data were examined thoroughly, summarized, and organized
according to the research questions and hypothesis. Both qualitative and quantita-
tive data analyses were conducted to enhance the credibility of the findings. The
quantitative data obtained through two questionnaires were analyzed using descrip-
tive statistics on frequencies and percentages. The qualitative data were derived
from the notes on exam spoken interactions and observations, from answers to
open-ended questions and comments written by examinees in the questionnaires.
The qualitative analysis drew on a mix of framework and thematic approaches,
starting with some preset themes and categories and adding others as they became
apparent after analyzing the collected data. Relevant themes and categories were
selected and coded.
420 O. Liubashenko and O. Yashenkova
4 Results and Discussion
4.1 Pre-exam Questionnaire Results
The pre-exam questionnaire (Appendix 1) was administered to students at the start
of the pre-exam consultation before the researchers gave them oral and written
instructions concerning the English exit exam. The purpose of that questionnaire
was to investigate how students conceived the English exit exam, based on their
previous exam experience, as well as how motivated they were to do their best in
the exam, and if they had any concerns before the exam. The questionnaire
contained multiple-choice questions, and respondents were allowed to select
more than one answer and add comments to any item.
As Table 1 attests, all the students responded that the purpose of the exam was to
grade students through assessing their knowledge and ability to reproduce, use and
apply what was learned during the BA program. In addition, 67.5% of the students
indicated that the exam was to discover their potential, and 33.1% assumed it was to
motivate them to learn. All the students perceived the examiner as a controller-
assessor who focused on mistakes, 66.2% also viewed him or her as a facilitator. All
believed the examinee’s functions included answering questions, completing exam
Table 1 Students’ view of the English exit exam
Questionnaire item Frequency Percent
Purpose of the exam
To assign scores/grades to students 151 100
To discover students’ potential 102 67.5
To motivate students to learn 50 33.1
What is assessed
What materials students have learned and have not learned 151 100
How well students reproduce information they have learned 151 100
How students can use and apply what they have learned 151 100
Examiner’s function
To control and assess students 151 100
To find and correct mistakes 151 100
To facilitate communication 100 66.2
Examinee’s function
To answer examiners’ questions 151 100
To complete exam tasks appropriately 151 100
To think and solve problems creatively 151 100
Good exam performance
Initiating spoken interactions and asking questions 47 31.1
Following all instructions and giving correct answers 151 100
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 421
tasks appropriately, thinking and solving problems creatively. All associated good
exam performance with following instructions and giving correct answers.
Although 31.1% of the students admitted that initiating spoken interactions and
asking questions also could characterize good exam performance, all the students
gave negative answers to the questions related to displaying any initiative or
creativity in their past exams, as shown in Table 2.
Table 3 shows that all the students were motivated to do their best in the exam by
the desire to get a BA degree and apply for a MA program. All expected to pass the
exam with a high score, and 83.4% thought they could gain valuable experience
during the exam.
The survey revealed that all the students experienced anxiety and had some
concerns before the exam, though 15.2% asserted they felt confident. To elicit more
information an open question was asked. Table 4 presents the results.
From Table 4 we can see that some students felt worried (17.2%) and even sick
(8.6%) before the exam. All the students were concerned about doing something
wrong in the exam, and half of them were afraid of going blank on exam questions.
Many students (73.5%) made comments dealing with the problem of communica-
tion apprehension.
Table 2 Past exam experience
Questionnaire item Frequency Percent
Examinee initiative, creativity, and lateral thinking
Using supporting material to perform the exam task 151 0
Proposing one’s own way to perform the exam task 151 0
Offering one’s own scheme of a linguistic analysis 151 0
Making comments upon thinking operations 151 0
Verbalizing feelings and emotions 151 0
Asking the examiner questions on the exam topic 151 0
Asking the examiner to share experiences and views 151 0
Expressing disagreement with the examiner 151 0
Criticizing the examiner’s words 151 0
Table 3 Students’
motivation and expectations
of the English exit exam
Questionnaire item Frequency Percent
Motive
Getting a BA degree 151 100
Applying for an MA program 151 100
Expectations
To pass this exam 151 100
To get a high score 151 100
To gain valuable experience 126 83.4
422 O. Liubashenko and O. Yashenkova
4.2 Post-exam Questionnaire Results
The post-exam questionnaire (Appendix 2) was designed to know how the exam-
inees evaluated the English exit exam overall and their performance in particular. It
also aimed to reveal if our changes in the exam scenario helped students become
active participants of the exam seen as a macro communicative act and provided
motivation for further learning.
Table 5 shows that the student overall exam experience was either excellent
(17.2%), good (50.3%) or fair (32.5%). The large majority of the respondents
(84.8%) asserted the exam was adequate and the examiners were supportive.
Moreover, they answered the exam met all their expectations (65.6%) or even
exceeded them (19.2%). 67.5% of the examinees wrote the exam pleasantly sur-
prised them, first of all, by interesting exam texts, friendly examiners, a semi-formal
exam atmosphere, and a free dialogue with examiners. However, 15.2% rated the
exam as rather difficult and found the examiners strict, 7.2% added the examiners
were critical but fair.
As seen in Table 6, all the examinees argued that nothing impeded their
performance during the exam. The majority of them characterized their communi-
cation with the examiners as dialogic (84.8%) and effective (67.5%), and believed
they performed well enough: Either at the same level as ever (34.4%) or even better
(34.1%). Some acknowledged their communication with the examiners was limited
(17.2%) and rated their performance as worse than ever (32.5). Half of the exam-
inees experienced anxiety, 66.2% felt unsure, and only 17.2% felt confident during
the exam, according to their responses.
In addition, Table 6 shows that while the examinees were engaged in the exam
problem tasks, all of them commented upon their thinking operations, verbalized
their feelings and emotions, initiated questions on the exam topic, and asked the
examiners to share experiences and views. Half of the examinees proposed their
own way to perform the exam task and expressed disagreement with the examiner/
s. 34.4% used their own supporting material. However, only 7.2% offered their own
scheme of a linguistic analysis and criticized the examiner’s words.
Table 4 Students’ concerns and well-being before the English exit exam
Questionnaire item Frequency Percent
Well-being
Confident 23 15.2
Anxious 151 100
Worried 26 17.2
Sick 13 8.6
Concerns
Doing something wrong in the exam 151 100
Going blank on exam questions 76 50.3
Communication apprehension 111 73.5
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 423
Moreover, many respondents shared their ideas and expectations about future
exams, stressing the importance of interesting communication with examiners, as
well as having the opportunity to improve communication and collaboration skills,
fill gaps and learn something new during the exam.
4.3 Observation Results
The exam observations were carried out to see how students really performed, what
effected their exam performance, and what difficulties they encountered during the
exam. Our main observation results can be summarized as follows:
1. The examinees experienced anxiety and looked rather nervous when they talked
to the examiners. We could notice such signs of anxiety as shaky voice,
trembling hands, pale faces or blushing. A few students asked for a glass of
water as they were not feeling well. Some verbalized their feelings, saying,
“Sorry, I am anxious”, or “I am very nervous.”
2. Communication, especially at the beginning of the exam, was tense and limited.
Sometimes the examinees did not know what to say and kept silent. Some asked
to give them a couple of minutes to collect with their thoughts. The examiners
had to make efforts to elicit information from the examinees.
Table 5 Students’ overall evaluation of the English exit exam
Questionnaire item Frequency Percent
Exam experience overall
Excellent 26 17.2
Good 76 50.3
Fair 49 32.5
Exam difficulty
Rather difficult 23 15.2
Adequate 128 84.8
Examiner
Strict 23 15.2
Supportive 128 84.8
Critical but fair 11 7.2
Expectations
It met all my expectations 99 65.6
It exceeded my expectations 29 19.2
It met some of my expectations 23 15.2
Surprise
No 49 32.5
Yes 102 67.5
(interesting exam texts, semi-formal exam atmosphere, free dialogue
with examiners, friendly examiners)
424 O. Liubashenko and O. Yashenkova
3. The students looked extremely frustrated and confused if they made an error/
mistake, failed to recall information or did something wrong. They exhibited
hesitation even when they knew what to say and could easily change their mind
when answering the examiners’ questions. They avoided asking any question if
something was not clear to them.
4. The examinees did not tend to take the initiative. They recognized the key role of
the examiners and waited for them to give instructions or ask questions.
5. When we encouraged the examinees to take a more active role in the exam and
managed to get them to think laterally to complete tasks, they were eager to
make suggestions, share ideas and opinions, and offer their own way of solving a
problem. Many students actively discussed the exam materials and exchanged
their experiences after the exam.
The questionnaire and observation results were integrated to answer the research
questions and test the hypothesis. With regard to RQ1, we argue that the students’
narrow view of the English exit exam is one of the main obstacles to effective
communication during the exam. Most Ukrainian BA students do not consider the
exam part of the teaching-learning process, being convinced that its primary
Table 6 Students’ performance in the English exit exam
Questionnaire item Frequency Percent
Barriers
No 151 100
Communication
Limited 26 17.2
Dialogic 128 84.8
Effective 102 67.5
Examinee
Confident 26 17.2
Unsure 100 66.2
Anxious 76 50.3
Exam performance overall
Better than ever 50 33.1
The same as ever 52 34.4
Worse than ever 49 32.5
Initiative, creativity, and lateral thinking
Using one’s own material to perform the exam task 52 34.4
Proposing one’s own way to perform the exam task 76 50.3
Offering one’s own scheme of a linguistic analysis 11 7.2
Making comments upon thinking operations 151 100
Verbalizing feelings and emotions 151 100
Asking the examiner questions on the exam topic 151 100
Asking the examiner to share experiences and views 151 100
Expressing disagreement with the examiner 76 50.3
Criticizing the examiner’s words 11 7.2
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 425
purpose is to grade students, and the main role of the examiner is to control, look for
mistakes and assign a score. Although many students admit the exam ought to
discover the potential of examinees who should not only reproduce learned infor-
mation effectively but also think and solve problems creatively, they associate good
exam performance, first of all, with strictly following the examiner’s instructions
and giving correct answers to his or her questions. They do not perceive the English
exit exam as a shared communicative act, but rather recognize the dominant role of
the examiner in spoken interactions, and view any deviation from the exam scenario
as a drawback.
According to the examinees’ responses, nothing impeded their performance
during the English exit exam. However, the pre-exam questionnaire and exam
observation results show that the examinees had to overcome various obstacles,
such as exam anxiety or even stress, communication apprehension, fear of going
blank on exam questions or doing something wrong (including making errors/
mistakes), fear of deviation from the traditional scenario of the exam, fear of asking
questions and losing face.
As far as RQ2 is concerned, we strongly believe that to have an effective macro
communicative act, the exam scenario should undergo certain changes. According
to these changes, examinees are expected to:
• Initiate a dialogue and involve the examiner in discussion;
• Propose their own way to perform the exam task, generate a variety of ideas, and
look for alternative or unconventional ways to solve a problem;
• Use their own supporting material to complete the exam task;
• Ask the examiner questions on the exam topic and get him or her to share
experiences and views;
• Agree or disagree with the examiner, express their opinions on the exam text and
give reasons;
• Comment upon their thinking operations, verbalize their feelings and emotions;
• Verbalize the difficulties they may face when engaged in problem task, and ask
the examiner for assistance;
• Determine the types and causes of their errors/mistakes and discuss the ways of
avoiding them with the examiner.
To answer RQ3, it is worth noting that lateral thinking, which means “exploring
multiple possibilities and approaches instead of pursuing a single approach”
(de Bono, 1992, p. 54), is directly concerned with such processes as change,
innovation, and creativity. As our study results show, lateral thinking is one of
the most effective ways to change the exam scenario and extend the scope of the
traditional exam.
We can change the exam scenario if both the examiner and examinee will strictly
follow two important rules:
1. Active criticism is more desirable than passive acceptance.
2. There is no single, unambiguous right answer to the question.
426 O. Liubashenko and O. Yashenkova
As shown in Fig. 1, the exam scenario that we suggest involves four corner-
stones: Exam as a free dialogue, lateral thinking as an assessment criterion, asking
the examiner questions as an advantage, and errors/mistakes as a natural part of
learning.
In this context, the main task of the examiner is to expand the opportunities for a
free dialogue with the examinee, stimulate his or her lateral thinking, and scaffold if
needed. The examinee should feel that he or she is treated as a unique personality
and is allowed to collaborate with the examiner on equal terms. The examinee
should be encouraged to express his or her opinions, share experiences, use prior
knowledge or imagination to generate new ideas, ask questions and give sugges-
tions, explore various approaches to completing exam tasks, make guesses and
learn from errors or mistakes.
The study results show that the effective implementation of our exam scenario
leads to positive changes in examiner and examinee behaviors and perceptions,
improves exam performance, and creates a feeling of satisfaction. It follows from
this that the hypothesis put forth at the start of our study is accepted.
5 Conclusion
The findings from our current study revealed the necessity of extending the scope of
the English exit exam and giving up the strict traditional exam scenario. To improve
exam performance and reduce the discrepancies between classroom and exam
performance scores, it is necessary to change students’ views, attitudes, and per-
ceptions of the exam, which can be realized by means of fundamental changes in
the existing exam scenario. Judgments and comments as a form of assessment, an
error or mistake as a cognitive operation, asking questions as an indicator of
Fig. 1 Cornerstones of the exam scenario
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 427
progress in learning, teacher/student role reversal in the process of communication,
and lateral thinking must become an integral part of every exam. We argue that the
English exit exam as a collaborative problem solving activity through a continuous
free dialogue is sure to give its participants an opportunity for creativity and self-
realization. We strongly believe that the exam according to our changed scenario
will not be defined as alternative, but it will be a common practice and part of
ongoing learning.
The study raised several implications. In the research implications, the study
stressed the importance of having a rich view of language and language assess-
ment, based on a variety of theories, to elaborate the conceptual framework for the
effective EFL exam. The study touched upon the critical issue of the paradigm
shift from measuring students’ ability to memorize and recall information to the
exam as a macro communicative act, which implies extending the scope of
assessment criteria, discovering students’ full potential, and motivating them to
acquire new knowledge and skills. The methodological implications called for
using both qualitative (spoken interactions in the exam, exam observations, and
open questions) and quantitative (questionnaires) instruments and a mix of
approaches within the qualitative method (framework and thematic analyses).
Multiple research methodologies provided opportunities to gather mixed level
data and enhance the credibility of the findings. With regard to the pedagogical
implications, the study addressed the need to raise teachers’ awareness of an
assessment culture (Segers et al., 2003, pp. 1–3) and presented some practical
steps how to improve the English exit exam to make it more effective and less
stressful for students.
We can also see that more in-depth research is needed to investigate challenges
encountered in higher school English exit exams in Ukraine, and a bigger number of
study participants, including EFL teachers and students of diverse backgrounds,
should be involved. It would be interesting to know how students majoring in EFL
and other fields conceive the English exit exam, and what concerns and difficulties
they have during their exams. It would be useful to explore how Ukrainian EFL
teachers perceive changes in assessment practices. Future research should further
develop the management model of the English exit exam as an integral part of the
continuous teaching-learning process and determine a variety of appropriate tactics
and techniques that could help implement the integrated strategic assessment we
suggest.
428 O. Liubashenko and O. Yashenkova
Appendix 1: Pre-exam Questionnaire
______________________________________________________________________________
ENGLISH EXIT EXAM: PRE-EXAM QUESTIONNAIRE
______________________________________________________________________________
Dear student:
Thank you for agreeing to participate in this survey, which aims to improve the English
exit exam for BA students on an EFL program at the Institute of Philology, Taras Shevchenko
National University of Kyiv. Your thoughtful and objective responses to questions are a
highly valued part of the assessment improvement process.
All your responses will remain confidential and anonymous. Do not write your name on
the questionnaire.
For each question, please tick (√) the appropriate box and/or write a comment. You can
tick as many boxes as needed.
______________________________________________________________________________
1. What is the purpose of the English exit exam?
To assign scores/grades to students
To discover students’ potential
To motivate students to learn
Other (please, specify) _____________________________________________________
2. What does the English exit exam assess?
What materials students have learned and have not learned
How well students reproduce information they have learned
How students can use and apply what they have learned
Other (please, specify) _____________________________________________________
3. What is the function of the examiner?
To control and assess students
To find and correct mistakes
To facilitate communication
Other (please, specify) _____________________________________________________
4. What is the function of the examinee?
To answer examiners’ questions
To complete exam tasks appropriately
To think and solve problems creatively
Other (please, specify) _____________________________________________________
5. What characterizes good exam performance?
Initiating spoken interactions and asking questions
Following all instructions and giving correct answers
Making suggestions, expressing emotions and opinions
Other (please, specify) _____________________________________________________
6. What did the examiners ask you to do in the past exams?
To bring supporting material to perform the exam task
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 429
To propose your own way to perform the exam task
To offer your own scheme of a linguistic analysis
To make comments upon your thinking operations
To verbalize your feelings and emotions
Nothing of the above
Other (please, specify) _____________________________________________________
7. What did you do in the past exams?
Asked the examiner questions on the exam topic
Asked the examiner to share his or her experiences and views
Expressed disagreement with the examiner
Criticized the examiner’s words
Nothing of the above
Other (please, specify) _____________________________________________________
8. What motivates you to perform at your best in the exam?
Nothing
Getting a BA degree
Applying for a MA program
Other (please, specify) _____________________________________________________
9. Is there anything that disturbs you before the exam?
Nothing
Doing something wrong in the exam
Going blank on exam questions
Other (please, specify) _____________________________________________________
10. How do you feel before your exam?
Calm
Confident
Anxious
Other (please, specify) _____________________________________________________
11. What are your exam expectations?
To pass this exam
To get a high score/grade
To gain valuable experience
Other (please, specify) _____________________________________________________
12. Do you have any other comments or concerns?
No
Yes (please, specify) ______________________________________________________
Thank you for completing the questionnaire.
Please make sure that you answered all items
430 O. Liubashenko and O. Yashenkova
Appendix 2 : Post-exam Questionnaire
______________________________________________________________________________
ENGLISH EXIT EXAM: POST-EXAM QUESTIONNAIRE
______________________________________________________________________________
Dear student:
Thank you for agreeing to participate in this survey, which aims to improve the English
exit exam for BA students on an EFL program at the Institute of Philology, Taras Shevchenko
National University of Kyiv.
As you were involved in a new exam scenario, we ask you to share your impressions
and suggestions in order to make the English exit exam more useful for further learning. Your
thoughtful and objective responses to questions are highly appreciated.
Your responses will remain confidential and anonymous. Do not write your name on the
questionnaire.
For each question, please tick (√) the appropriate box and/or write a comment. You can
tick as many boxes as needed.
______________________________________________________________________________
1. How did you rate your exam experience overall?
Excellent
Good
Fair
Other (please, specify) _____________________________________________________
2. How difficult was this exam for you?
Rather hard
Adequate
Quite easy
Other (please, specify) _____________________________________________________
3. How did you feel in the exam?
Confident
Unsure
Anxious
Other (please, specify) _____________________________________________________
4. How would you characterize the examiners?
Strict
Hypercritical
Supportive
Other (please, specify) _____________________________________________________
5. How would you determine your communication with the examiners?
Limited
Dialogic
Effective
Other (please, specify) _____________________________________________________
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 431
6. Was there anything that impeded your exam performance?
No
Yes (please, specify) ______________________________________________________
7. How do you think you performed on this exam?
Better than ever
The same as ever
Worse than ever
Other (please, specify) _____________________________________________________
8. To what extent did the exam meet your expectations?
It did not meet my expectations.
It met all my expectations.
It exceeded my expectations.
Other (please, specify) _____________________________________________________
9. Was there anything that surprised you in the exam?
No
Yes (please, specify) ______________________________________________________
10. What did you do during the exam?
Used supporting material to perform the exam task
Proposed your own way to perform the exam task
Offered your own scheme of a linguistic analysis
Commented upon your thinking operations
Verbalized your feelings and emotions
Asked the examiner questions on the exam topic
Asked the examiner to share his or her experiences and views
Expressed disagreement with the examiner
Criticized the examiner’s words
Nothing of the above
Other (please, specify) _____________________________________________________
11. What do you expect from your future exams?
Interesting communication
Acquiring some new knowledge
Improving communication and collaboration skills
Filling your gaps and clarifying ambiguities
Other (please, specify) _____________________________________________________
12. Do you have any other comments?
No
Yes (please, specify) ______________________________________________________
Thank you for completing the questionnaire.
Please make sure that you answered all items
432 O. Liubashenko and O. Yashenkova
Appendix 3: Examinee Instructions
Dear Examinee!
1. Learn by taking your exam. The exam is not the end of the learning
process. The exam questions are your learning materials.
2. Initiate interaction. Ask the examiner’s opinion. A few strategic questions
can demonstrate your intelligence.
3. Think verbally. Verbalize your doubts and hesitations. Your approach may
not always be right.
4. Cover the issue from various perspectives. There can be no unambiguous
answer to the question. Finding alternative ways of analyzing the text
means thinking laterally.
5. Trust examiners. The examiner is your partner-constructor of the success-
ful learning process. Take his or her remarks as advice for further self-
development.
Appendix 4: Examiner Instructions
Dear Examiner!
1. Learn from examinees. The exam is not only aimed at revealing exam-
inees’ knowledge and skills, but also at your own pedagogic achievements.
Take the failures of examinees as your own.
2. Use a variety of evaluation techniques. Combine the ways of testing—
from control questions and correcting mistakes to adding the answers.
Evaluate examinees’ suggestions to analyze the text from a different
perspective.
3. Use the form of dialogue to express approval or disapproval of examinees’
performance. Take examinees’ verbal thinking as the material for analysis.
4. Encourage the attempts of lateral thinking.
5. Trust examinees. The examinee is your partner-constructor of the exam
discourse. Search for the ways of strengthening examinees’ faith in their
abilities.
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 433
References
Abedi, J. (2010). Performance assessments for English language learners. Stanford, CA: Stanford
Center for Opportunity Policy in Education.
Bakhtin, M. M. (1981). The dialogic imagination: Four essays (C. Emerson & M. Holquist,
Trans.). In M. Holquist (Ed.). Austin: University of Texas Press.
Balakrishnan, A. (2007, April 24). Edward de Bono: ‘Iraq? They just need to think it through.’
The Guardian. Retrieved from https://www.theguardian.com/education/2007/apr/24/
highereducationprofile.academicexperts
Barrows, H. S. (1986). A taxonomy of problem-based learning methods. Medical Education, 20,
481–186.
Beacco, J.-C., Fleming, M., Goullier, F., Thürmann, E., Vollmer, H., & Sheils, J. (2016). A
handbook for curriculum development and teacher training: The language dimension in all
subjects. Strasbourg: Council of Europe.
Benner, A. D. (2013). Exit examinations, peer academic climate, and adolescents’ developmental
outcomes. Journal of School Psychology, 51(1), 67–80. https://doi.org/10.1016/j.jsp.2012.09.
001.
Bøhn, H. (2015). Assessing spoken EFL without a common rating scale: Norwegian EFL teachers’
conceptions of constructs. Sage Open, 5(4), 1–12. https://doi.org/10.1177/2158244015621956.
Bovée, C. L., & Thill, J. V. (2000). Business communication today (6th ed.). Upper Saddle River,
NJ: Prentice Hall.
Brown, H. D., & Abeywickrama, P. (2010). Language assessment: Principles and classroom
practices (2nd ed.). White Plains, NY: Pearson.
Chomsky, N. (2015, January 16). Calls to action: Noam Chomsky on the dangers of standardized
testing [Video file]. The Progressive Magazine. Retrieved from https://www.youtube.com/
watch?v¼9JVVRWBekYo
Cohen, A. (2011). Strategies in learning and using a second language (2nd ed.). Harlow:
Longman.
Damankesh, M., & Babaii, E. (2015). The washback effect of Iranian high school final examina-
tions on students’ test-taking and test-preparation strategies. Studies in Educational Evalua-
tion, 45, 62–69. https://doi.org/10.1016/j.stueduc.2015.03.009.
De Bono, E. (1992). Serious creativity: Using the power of lateral thinking to create new ideas.
New York: Harper Business.
East, M. (2015). Coming to terms with innovative high-stakes assessment practice: Teachers’
viewpoints on assessment reform. Language Testing, 32(1), 101–120. https://doi.org/10.1177/
0265532214544393.
Griffiths, C. (2013). The strategy factor in successful language learning. Bristol, UK: Multilingual
Matters.
Hatipo^
glu, Ç. (2016). The impact of the university entrance exam on EFL education in Turkey:
Pre-service English language teachers’ perspective. Procedia—Social and Behavioral Sci-
ences, 232, 136–144. https://doi.org/10.1016/j.sbspro.2016.10.038.
Huang, J., Han, T., & Schnapp, K. (2012). Do high-stakes test really address English language
learners’ learning needs?—A discussion of issues, concerns, and implications. International
Journal of Learning & Development, 2(1), 499–508. https://doi.org/10.5296/ijld.v2i1.1472.
Jakubinskiy, L. P. (1986). Izbrannye raboty: jazyk i ego funktsionirovanie [Selected works:
Language and its functioning]. Moscow: Nauka.
Kazemipour, S. (2014). Comparing the outcomes of two types of corrective feedback on EFL
classes’ final exam. Procedia—Social and Behavioral Sciences, 98, 876–881.
Knowles, M. S., Holton, E. F., III, & Swanson, R. A. (2005). The adult learner: The definitive
classic in adult education and human resource development (6th ed.). Amsterdam: Elesvier.
Krasnykh, V. V. (2001). Osnovy psykholingvistiki i teorii komunikatsii. [Fundamentals of psy-
cholinguistics and communication theory]. Moscow: Gnozis.
434 O. Liubashenko and O. Yashenkova
Labordaa, J. G., Bejaranob, L. G., de Juana, N. O., Litzlera, M. F., & Rosaa, M. M. (2014). A
comparison of pre-service towards testing: The spanish baccalaureate general test and the
american OPI. Procedia—Social and Behavioral Sciences, 116, 2386–2391. https://doi.org/10.
1016/j.sbspro.2014.01.578.
Leontyev, A. N. (1975). Dejatelnost, soznanie i lichnost [Activity, consciousness, and personal-
ity]. Moscow: Politizdat.
Leontyev, A. A. (2008). Jazyk, rech, recevaja dejatelnost [Language, speech, and speech activity]
(5th ed.). Moscow: LKI.
Liubashenko, O. (2007). Linhvodydaktychni stratehiji: proektuvannija protsesu navchannija
ukrajinskoji movy u vyshchej shkoli [Linguodidactic strategies: Ukrainian language learning
design in higher education]. Nizhin: Aspect-Polihraf.
Liubashenko, O., & Yashenkova, O. (2014). Getting students to think laterally as an evaluation
strategy at the classical university. In W. Malec & K. Drabikowska (Eds.), Book of abstracts of
linguistics beyond and within 2014: Challenging ideas and innovative applications
(pp. 69–70). Lublin: KUL.
Liubashenko, O., & Yashenkova, O. (2015). Getting students to think laterally as an evaluation
strategy at the classical University. Studies in Linguistics and Methodology, 10, 77–94.
Luria, A. R. (1976). Cognitive development: Its cultural and social foundations (M. Lopez-
Morillas & L. Solotaroff, Trans.). In M. Cole (Ed.). Cambridge, MA: Harvard University Press.
Maslow, A. H. (1954). Motivation and personality. New York: Harper and Row.
Matiushkin, A. М. (2009). Psikhologija myshlenija. myshlenie kak razreshenie problemnykh
sytuatsij [Psychology of thinking. Thinking to solve problems]. Moscow: KDU.
O’Malley, J. M., & Pierce, L. V. (1996). Authentic assessment for English language learners:
Practical approaches for teachers. New York: Longman.
Oxford, R. L. (1990). Language learning strategies: What every teacher should know. Boston,
MA: Heinle & Heinle.
Partnership for 21st Century Skills. (2007). 21st century skills assessment. Retrieved from http://
www.p21.org/storage/documents/21st_Century_Skills_Assessment_e-paper.pdf
Rogers, C. R. (1969). Freedom to learn. Columbus, OH: Merrill.
Rubinshtein, S. L. (1946). Osnovy obschej psihkologii [Fundamentals of general psychology] (2nd
ed.). Moscow: Uchpedgiz.
Segers, M., Dochy, F., & Cascallar, E. (Eds.). (2003). Optimising new modes of assessment: In
search of qualities and standards. Dordrecht: Kluwer Academic Publishers.
Simon, M., Ercikan, K., & Rousseau, M. (Eds.). (2013). Improving large-scale assessment in
education: Theory, issues, and practice. New York: Routledge.
The National Center for Fair & Open Testing. (2007). The dangerous consequences of high-stakes
standardized testing. Retrieved from http://www.fairtest.org/sites/default/files/Dangerous-Con
sequences-of-high-stakes-tests.pdf
Vygotsky, L. (1978). Interaction between learning and development. In M. Gauvain & M. Cole
(Eds.), Readings on the development of children (pp. 34–40). New York: Scientific American
Books.
Yashenkova, O. (2010). Osnovy teoriji movnoji komunikatsiji. [Fundamentals of speech commu-
nication theory]. Kyiv: Academia.
Extending the Scope of the English Exit Exam: A Study from a Ukrainian. . . 435
Washback on Language Skills: A Study of EFL
Multi-Exam Preparation Classes
Irini-Renika Papakammenou
Abstract This chapter is part of a case study to study the washback effect of exam
preparation classes on teaching practices in Greece. In particular, its focus is on two
different exam preparation classes with distinctive characteristics and exam focus,
the multi-exam and one-exam classes. The study, even though it focuses on
teaching practices teachers’ use when teaching exam preparation classes, it reveals
interesting and valuable information on the washback effect on language skills. The
findings reveal not only the way teachers deal with the skills and sub-skills in exam
classes but also their decisions and the teaching practices they use on teaching
skills. By comparing the two exam classes which prepare students for different
exams and different exam formats it elucidates the influence of exams on skills. The
wide range used in the study comprised interviews with teachers, classroom obser-
vations of both types of classes and follow-up interviews. Washback on teachers’
teaching practices in relation to the skills and sub-skills they teach to prepare
students for exams are addressed as well as the factors that determine whether
and to what degree washback occurs. It will conclude by offering research and
pedagogical recommendations about the ways teachers can enhance washback
awareness related to the skills taught in exam preparation classes.
Keywords Washback effect • Teaching practices • Skills • Assessment of skills •
Language abilities • Teacher training
1 Introduction
EFL exam preparation classes in Greece have revealed interesting information on
the washback effect on language skills and how teachers deal with the skills and
sub-skills in such classes. Greece is one of the most EFL certificate obsessed coun-
tries which creates students the need to learn a foreign language with the aim of
getting a certificate more than learning the actual language. This is more obvious
I.-R. Papakammenou (*)
University of Cyprus, Papastratou 39 and Valtou, Agrinio, Greece
e-mail: renikapap@yahoo.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_21
437
regarding the fact that 21 English language examinations are currently administered
for B2 level of the CEFR. These exams are recognised by the Greek state and
approved by ASEP (Civil Service Staffing Council) as professional qualifications.
These exams present variations in terms of content and format. Most of these exams
consist of various sections evaluating all language skills in balance: Oral produc-
tion; written production; listening and reading comprehension; while some of them
emphasize specific skills.
2 Theoretical Background
Adopting Hughes (1989) definition of washback effect as “the effect of testing on
teaching and learning” (in Bailey, 1996, p. 258) then undoubtedly there is an
influence of exams on the content of teaching. According to Glover (2006, p. 28)
content ‘refers to what is taught in a programme, structures, functions, vocabulary
and so on’ and it ‘also relates to skills, materials and to activities that are carried out
in class’. Undeniably, there is a washback effect on skills since the content of
teaching in most studies has received the most intensive washback effects
(Alderson & Hamp-Lyons, 1996; Cheng, 1997; Li, 2009; Saif, 2006; Tsagari,
2012; Wall & Alderson, 1993).
Teachers in many studies following books which resembled the test and were
structured in the same way as the exams such as past papers and practice tests
books, they restricted their teaching on what it was tested in the exams (Aftab,
Qureshi, & William, 2014; Li, 2009; Wall & Alderson, 1993). Studies have also
shown contradictory results concerning the skills taught in exam preparation
classes. In some studies, teachers focused mainly on skills that were tested in the
exams with a little emphasis or not at all on skills that were not tested in the exam
(Azadi & Gholami, 2013; Cheng, 1999; Wall & Alderson, 1993). In other cases,
teachers did not spend time on certain skills because they did not know the scoring
criteria or the scoring method of the exam and for this reason sometimes neglected
the writing and listening skills (Watanabe, 2000). Mickan and Motteram (2004)
who investigated in detail the skills taught in an IELTS preparation class found that
the teachers taught all skills following the IELTS structure but there was a separate
treatment of skills when assessing them.
Findings of the studies show that skills are taught separately and are influenced
by the skills tested in the exam. Also, the studies found that the choice of the skill
which will be taught in exam preparation classes depended mainly on the exam
whereas the emphasis given on some skills depended on the teacher. In some cases,
some skills were totally neglected since the exam did not test them. Indeed, exams
seem to conquer the skills taught in exam preparation classes and most exam pre-
paration classes are organized taking into account the skills tested in exams.
Washback studies on exam classes provide significant information with respect
to the washback on skills but they fail to provide more specific information on what
teachers use and how they use it, in other words the teaching practices they use, to
438 I.-R. Papakammenou
teach skills. In addition, the factors influencing teachers’ decisions on the teaching
practices they use can provide valuable information on the assessment of abilities.
Washback models on teaching (Glover, 2006; Tsagari, 2009) emphasize the impor-
tance of looking at what teachers teach, how teachers teach and teacher’s beliefs as
they interact with each other. However, concerning teaching practices previous
studies fail to distinguish them properly since there is a variation in the terminology
used in the studies, referring to how teachers teach, such as ‘methods’, ‘methodol-
ogy’, ‘teaching practices’, ‘tasks’, ‘activities’, ‘techniques’, ‘strategy’. This study
combines skills with teaching practices bringing together and drawing a distinction
between the terms used (see Rationale of the study).
2.1 Research Problem and Rationale of the Study
Most studies (Alderson & Wall, 1993; Alderson & Hamp-Lyons, 1996; Green,
2006; Mickan & Motteram, 2004; Read & Hayes, 2003; Tsagari, 2012), however,
that have investigated the teaching content and methods teachers employ to teach,
have repeatedly indicated that exams may affect the content taught to students,
while failing to see any specific impact testing might bear on the teaching methods
employed within the foreign language classroom in order to teach the skills and
sub-skills that are tested in the exam(s) (Cheng, 2004; Wall & Alderson, 1993).
This study returns to the impact high-stake exams might bear on teaching and
specifically in the multi exam teaching environment, to revisit washback in relation
to teaching methods teachers use to teach skills and sub-skills.
In order to understand the intricacies of language abilities in exam preparation
courses not only research on skills and sub-skills in needed but also on how lan-
guage abilities are taught. This study focuses on teaching practices in relation to the
skills teachers teach to determine the existence of washback on both skills and
teaching practices. It is worth mentioning that Glover (2006) highlights the need for
further research on the washback effect on how teachers teach to fill in the gaps in
the existing literature. The problem might arise from an inability to address and
adequately describe core concepts used in the relevant literature (Alderson &
Hamp-Lyons, 1996; Aftab et al., 2014; Azadi & Gholami, 2013; Cheng, 1997,
1999, 2004; Hayes & Read, 2004; Mickan & Motteram, 2004; Tsagari, 2012;
Watanabe, 2008). The present study draws a distinction between ‘teaching strat-
egies’, ‘activities’ and ‘tasks’ which teachers use to teach skills and bring these
terms together under the umbrella term ‘teaching practices’ (Papakammenou, 2016).
In response to the need for more information coming directly from the teacher,
together with the lack of relevant research, this study breaks ground in that it uses
post-observational interviews to provide insights on what leads teachers to make
specific pedagogical choices. It aims to foster a greater understanding of the rela-
tionship among teacher cognition, teaching practices used to teach skills and high-
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 439
stakes exams. Pivotally, it investigates washback within a highly under-researched
context—namely, the multi-exam classroom—and, in relation to a combination of
exams that are either under-represented in the relevant literature on washback (i.e.,
ECCE) or not represented at all, perhaps due to their novelty (i.e., TIE, PTE
General).
Given that it is a relatively new territory, the majority of the studies on washback
have focused on one-exam rather than the multi-exam context creating a need for
further research and specifically on multi-exam classes. At the heart of this problem
is that multi-exam preparation classes have created a more rigorous situation for
teachers’ preparation for and the alignment of their teaching practices with the
various tests. Thus, a highly test responsive teaching and learning environment has
been created where teachers, students and administrators find themselves ‘trapped’.
Thus, the consequences of multi-exam classes on teachers need to be explored since
it can provide valuable information on the different way to assess language ability,
skills and sub-skills.
The general purpose of the study is to investigate Greek B2 level exam classes
and the impact of the multi exam context on teachers’ perceptions, on their curri-
culum planning and, finally, on their teaching instruction, with an emphasis on
teaching practices. This study further seeks to distinguish the types of teaching
practices are employed by teachers for each skill and sub-skill within the multi
exam classes. Most importantly, this study aims to explore the nature and scope of
the phenomenon of the washback effect on multi exam classes and to identify the
factors that contribute to or inhibit the intended washback of multi exam classes.
This study, therefore, deals with the washback effect of multi-exam classes on
teachers’ teaching practices on skills and sub-skills. Teaching practices refer both to
“tasks” and “activities”, and “language strategies” (Papakammenou, 2016, p. 121).
“Activities” are what students do to practice language, “tasks” refer to final pro-
ducts which require a series of activities, “exam related activities” are work plans
which make students aware of to deal with exams and finally “teaching strategies”
are what teachers use to manage the class and teach such as use of first and target
language, feedback, organizational patterns and teachers’ questions and explan-
ations (Papakammenou, 2016, p. 121).
The results of the study will recommend new and appropriate methodologies to
assess language abilities for teachers and effective ways to tackle exam-related
demands. Teachers will, therefore, be able to balance teaching and learning with
exam preparation and introduce activities and/or tasks focusing more on language
learning rather than on exam preparation per se. It follows then that the study
endeavored to answer the following research questions.
1. What kind of teaching practices do teachers use in multi-exam classes? Are there
any differences between the 1st and the 2nd term of the school year?
2. Does washback in a multi-exam context exist? If it does, what is its nature and
scope in terms of teaching strategies, activities, and tasks?
3. How do teachers decide on the teaching practices to use in multi-exam classes?
440 I.-R. Papakammenou
3 Method
EFL preparation classes are mainly done in privately owned language institutions
which are called frontistiria. Frontistiria prepare students for any of the 21 available
language certificates dedicating an entire year for exam preparation especially in B2
level and above. Having that many language certificates and taking into consider-
ation the gradual tendency noted for students to opt for participating in more than
one exam in the same exam period an interesting class environment has been
created. Teachers prepare students for two or even more exams, leading teachers
to strive for a teaching formula that would effectively incorporate all the different
exams and exam formats selected by the students. The term ‘multi exam’ has been
chosen to signify this teaching context which is geared towards a diverse set of
exams. For the purposes of this study, the term ‘multi exam context’ regards
exclusively contexts in which a variety of combinations of the 21 exams are taught.
However, in spite of the strong examination culture in Greece and this rather
interesting exam preparation context there is not much research on the influence of
exams. Preparing students for many exams which have different formats and
evaluate different skills in the same class can provide valuable information on
how language abilities, skills and sub-skills are assessed. This study explores this
education context emphasizing on language skills in combination with the teaching
practices teachers use in order to teach them in exam preparation classes.
3.1 Data Collection
The present study took place at frontistirio of foreign languages in a provincial
town in Greece which offered both multi-exam and one-exam B2 level preparation
classes. Both classes were selected in order comparisons between the classes to be
made and both teachers who taught both classes were the participants of the study.
In the multi-exam class, the ESOL examinations the students opted for were the
Pearson’s PTE General Exam Level 3, University of Michigan (ECCE) and the Test
of Interactive English (TIE). In the one-exam class, the students were interested in
the TIE examination.
Data collection took place during both school terms—namely, in November for
the first term and in April for the second term. In the first term, data collection
started with interviews and then classroom observations and follow-up interviews
(stimulated recalls), and the second term started with observations and follow-up
interviews. Then, the last interview followed and finally the questionnaires were
completed (Table 1).
The general perspective of the teachers was considered to be of particular
importance. The first interviews with the two teachers were conducted prior to
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 441
the observations according to a specific schedule (Papakammenou, 2016, p. 123)
and the aim was to generate some raw data and general beliefs about their teaching
and exams for both terms. The purpose of the final interviews was to reassess
teachers’ beliefs about the exams and their teaching practices, as well as the
research procedure as a whole.
Two teachers teaching both the multi-exam and the one-exam classes during the
same period were observed. A total of 25 lessons were observed, almost 12 lessons
for each teacher in each term. 12 lessons were observed in the first term, while a
total of 13 lessons were observed in the second term, due to increased repetition in
the material covered. During each observation, real time field notes were taken
using an observation scheme (Papakammenou, 2016, p. 124) and comments were
made on the pre-designed coding sheets. For each observation, a digital camera was
set up before the lesson in a corner of the classroom to minimize any interruptions
and disturbance to the lessons and to ensure normal teacher and student interaction.
Follow-up interviews with the two teachers used in order to confirm, clarify or
further explore the findings from the classroom observations. Teachers were asked
to justify their actions and explain the rationale behind their actions as well as their
feelings, beliefs and perspectives. Follow-up interviews were used to investigate
the factors that affect teachers’ choices and how they decide to teach the way they
do. Follow-up interviews were held right after each class.
Table 1 Data collection stages
442 I.-R. Papakammenou
3.2 Data Analysis
Data analysis mainly involved (1) transcriptions of interviews with both teachers in
the beginning and the end of the study, (2) transcriptions of the classroom obser-
vations, (3) transcriptions of the follow-up interviews and (4) qualitative analysis of
the responses of all the above using Atlas.ti (Muhr & Freise, 2004).
Methods of analysis of the interview and the observational data differed
although both sets of data were deemed as qualitative. Topic/thematic coding was
used to analyze the interviews. The data was later collated into broad themes based
on the interview questions and the framework, and these themes were then divided
into sub-themes and sub-categories. The analysis of classroom observations com-
prised four categories covered in the coding sheets: Teaching strategies, washback,
activities and tasks. A coding scheme was prepared to compare data and search for
any commonalities or differences between the lessons. Some more codes were also
added since new information emerged in the transcriptions. A coding scheme that
was prepared for observations, together with descriptive and thematic coding, was
used for the analysis of the follow-up data.
4 Results and Discussion
4.1 Washback on How Skills Taught in Exam Preparation
Classes
The analysis of observed classes and follow-up interviews with teachers revealed
interesting information about the washback effect on each skill and the teaching
practices used in each skill. The results showed that the washback effect on skills
and teaching practices varied among the skills, the type of classes, the teachers and
the terms throughout the preparation year.
The skills taught in both one- and multi-exam classes in both terms related to the
use of teaching materials. In the first term both teachers used course books while in
the second term both teachers used past papers and/or exam practice tests. How-
ever, the types of books differed a lot since in multi-exam class teachers used a
general B2 exam book whereas in one-exam class teachers used a TIE focused
book. This means that the general B2 level book included all the skills and a range
of tasks and activities in all skills whereas the TIE focused book contained mainly
tasks and activities related to the TIE examination. The test of interactive English
(TIE) examination is a task-based exam which tests students only in speaking and
writing and asks students to choose and prepare an investigation topic, a book and a
news story for the exams. So, teachers in one-exam class used other teacher-made
or student-made materials even in the first term since the exam asks students to have
their own materials in a logbook.
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 443
In the first term teachers in multi-exam class followed the book faithfully so they
taught all skills and did a range of activities and tasks for every skill. Information
from interviews and follow-up interviews revealed that there are a lot of factors that
influence teachers’ choices regarding materials. Teacher 1 points out that ‘we have
to finish the books because parents expect that since we buy the books, we have to
finish them’. So, she mentions parents’ pressure to teach the whole book and that is
how she explains the extensive use of the course book and the fact that she does all
the skills in the first term. Teacher 2 refers to multi-exam class and she mentions
another factor for using course books saying that ‘the book contains tasks and acti-
vities which students can find in the exams and they can have a go at various exam
like activities before they decide which exam to take’. She believes that the book
prepares students for any B2 level exams so it is helpful since they practice in all the
skills until students choose which exam they want to. Therefore, teachers were not
influenced by the exams in the first term regarding the skills they taught but rather
by the course books and the fact that they should do it all.
In one-exam class though teachers taught the book and prepared extra materials
with the students for the exam, too. In this case teachers did not do everything in the
book such as listening skills and they sometimes did not emphasize on reading
skills as in multi-exam class since they were not included in TIE exam. Teachers in
one exam-class practiced only speaking and writing skills with the extra materials
that they prepared with students to be included in the logbook. The skills taught in
the first term in one-exam class were more influenced by exams than in multi-exam
class.
In the second term in both classes teachers taught the skills that the exams tested.
One-exam students practiced mostly speaking and writing whereas students in
multi-exam class did all skills because PTE General examination and Michigan
examination included all skills. So, there is a stronger washback on skills since the
teachers use materials which resemble exams and adopt an exam oriented method-
ology in the second term (Azadi & Gholami, 2013; Aftab et al., 2014).
However, the study indicated that parents and not only exams or materials, play
an important role in teachers’ choice to follow the textbook faithfully and cover
everything in it. Also, the type of exam is significant on the number of skills taught
and the quality of supplemented materials since teachers follow the exams. In the
case of TIE exam in one-exam class students use more materials such as a reader
which is a positive rather than a negative washback effect even though they do not
practice all skills which is a negative washback effect. The number of exams plays
an important role whether all skills are taught in exam preparation classes since if
an exam does not include a skill the other might test it.
4.2 Speaking Skill
Teachers used a wide range of speaking activities in both classes. What differed
though is the kind of activities and their aims, as can be seen in Table 1. There was a
444 I.-R. Papakammenou
range of speaking activities used that involved answering questions, practice
sentence structure, describe pictures, role plays, discussions on topics and practice
fluency. However, teachers mainly got students to answer questions in both classes
in order to practice speaking. In multi-exam class teachers got students not only to
answer questions but also to discuss various topics regardless of the exam choice
but on topics that the exams have. In one-exam class teachers also did role plays
since the last part of the speaking test of TIE examination requires students to
discuss on pictures but they strongly practice TIE orals, which indicates a strong
exam influence (Table 2).
Follow-up interviews include useful information on how and why teachers did
speaking activities. In the first term in both classes teachers cared more for language
learning and getting students to speak. Teacher 1 for example mentions that ‘I want
them to start talking as much as possible without thinking’. Teachers did not
emphasize on correction and feedback and if so it is more general since they believe
that in the first term they just want to get students to talk. However, in multi-exam
class they did not always focus on a specific exam but they did activities that all
exams ask for such as interviewing the students and answering personal questions.
In one-exam class, contrary to multi-exam class, teachers in the first term did not
focus on exams at all and they only tried to get students to talk, forget about their
fear and practice language. Teacher 2 explains that ‘I wanted to get them speaking
and try and to forget the problems that grammar can create. Just get them use
English’. Teachers did not provide feedback and corrections because the level of
the students was very low and they believe that it is more significant to get them to
speak and use what they have learnt. However, comparing the first and second term
Teacher 1 explains what she did saying that ‘I’m guessing those are the questions
the candidates are going to be asked, so, we are preparing these from the class.
Only what is in TIE exams. They have to be prepared as much as possible’. In the
second term, though, both teachers in both classes did exam-oriented speaking
activities in the form of mock tests. Students in multi-exam class worked indi-
vidually as the format of the exams they sit for but on the one-exam class they worked
in pairs mostly as the TIE speaking test requires. Unlike in the first term, this time
teachers correct students and give them a lot of exam tips.
Table 2 Speaking activities
in both multi-exam and
one-exam classes
Multi-exam
class
One-exam
class
Discussion 4 0
TIE orals 0 5
Practice fluency 1 0
Role play 1 2
Describe pictures 0 2
Practice sentence
formation
1 0
Answer questions 5 4
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 445
In both classes teachers also used pair and group work to teach speaking how-
ever for different reasons. The teacher in multi-exam classes tried to combine the
exams and find similarities to get students to work together. In addition, the teacher
did group work to change the routine of the class and involve students and make
them participate. She believes that pair work is a more interesting work mode than
teacher to class one for teacher 1, it is fun for students that helps students learn from
each other. Teachers used pair work in one-exam class because in TIE exam
students are tested in pairs for this reason teachers continue using pair work even
in the second term. Instances of laughter were noticed while students worked in
pairs or groups and both teachers confirm that these interaction patterns can relax
students and make the lesson more interesting and fun.
There is a washback effect on how teachers teach the speaking skill in both
classes but not in both terms. Teachers create a more relaxing atmosphere for
students in the first term without providing extreme error correction and feedback
in order to allow students to practice spoken language. However, the situation
changes in the second term since teachers provide feedback and error correction
on exams which indicates high exam washback. Despite doing a wide range of
exam like activities and tasks in the multi-exam class teachers do not refer to exam
specifications and requirements. In one-exam class, though teachers in the first term
are influenced by the level of the students without caring for the exams.
The format of the exam plays an important role on the washback effect of inter-
action patterns in speaking skills since the data show that both classes are influ-
enced by the exams and specifically by the exam format in the second term. The
multi-exam class is more teacher-dominated than the one-exam class because of
format of the exams (Michigan and PTE General). This is in line with results of
previous studies which found the exam preparation lessons to be teacher-controlled
(Cheng, 1999; Hayes and Read, 2004; Pan, 2011). However, one-exam class use
more pair/group and students-to-class interaction patterns since TIE examination
tests the interaction between students and this signifies how influential can the
format of the exam be.
4.3 Reading Skill
There is a variation of reading activities in the multi-exam class as Table 3 shows.
The teacher did a lot of different reading activities in multi-exam class. The
multiple-choice reading activity is more frequently used because both exams
(PTE and Michigan ECCE) include it. So, the teacher follows not only the course-
book but the exams as well when she teaches reading activities in multi-exam class.
In one-exam class she translated mainly reading texts which are used in the
TIE coursebook as model texts for the writing part of the exam.
In a multi-exam class, the teacher used skimming techniques since the exams’
reading activities ask for it but she also taught students reading techniques. In fact,
as observations showed the teacher, in an effort to teach reading skills appropriate
446 I.-R. Papakammenou
for the exam, combines the activities such as a skimming activity with a multiple-
choice one because, as she said, ‘skimming is what the students need in order to
complete the specific exam task within the time limit’. Teacher 2 points that ‘with
the Michigan exam your pressure is time so students have to learn to work with the
passages very quickly’.
What is very interesting about the reading activities is the teacher’s perception
about the use of the students’ L1 (in this case, Greek) during these activities. The
teacher got students to translate by reading aloud in both terms. Teacher 1 believes
that ‘translation is a good teaching method, because if you can translate it, you
know what you are doing’ that is why the teacher used translation to teach reading
skills to students when preparing them for exams. She believes that translation is an
effective tool and it helps students to understand the text better. It is teachers’ edu-
cational beliefs than the exams, since they do not ask for translation that make them
use translation to teach reading to students even in both terms.
Since reading is not a component of the TIE exam, the choice of the reading
activities in one-exam class was not influenced by the exam. It was motivated by the
teacher’s desire to expose students to vocabulary and teach them how a text can be
organized and analyzed in the English language mainly for the writing section of
the exam. In the case of one-exam class the teacher did reading in the first term in
order for students to collect information from model essays to use in the exams but
in the second term she tended to ignore the skill since it is not in the examination.
This extensive use of translation in the case of the one-exam class aimed at not
only speeding up exam preparation, as in the case of the multi-exam classes, but
also at dealing effectively with the students’ low level. This trend of associating the
language level of students with L1 use in the classroom has been presented by
Kourou (2008, p. 6). Kourou (2008, p. 6) suggests that teachers may make use of the
L1 according to the learners’ specific needs and language level. Also, the teacher
used reading activities and translation in order to control the class and check
students’ attention. Exams did not have any influence on the use of reading
activities and translation in the one-exam class.
It can be deduced, therefore, that the influence of the course book on the
selection of reading activities was quite strong, and the reading skills taught were
geared towards the successful completion of the relevant exam task in multi-exam
classes but not in one-exam classes. So, regarding reading activities and how they
Table 3 Reading activities in
multi-exam class
Multi-exam class
True/False 1
Answer questions 1
Reading techniques 2
Translate 3
Scanning 1
Read aloud 3
Skimming 2
Multiple choice 4
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 447
are taught there is a variation concerning the factors influencing reading activities
which differentiate between terms, classes and the teacher herself.
4.4 Writing Skill
Observations showed that teachers combine writing activities with other skills in
order for students to practise the English language, which means that teachers
approach writing in such a way so as to first and foremost promote language
learning. In the first term, both teachers used different teaching strategies to teach
writing to students. They did so in order for students to understand, keep the notes
for future reference, because the activities are totally different from their Greek
lessons and unknown to students, or because they wanted to show students how to
work and think when they have to write something. Also, for the reason that the
level of the students is low requires more explanation which the use of first lan-
guage and translation can provide.
However, in the second term, teachers got students to practice writing either in
the form of homework or alone unsupervised in the form of mock tests. There were
some few cases, though, where teachers taught writing as in the first term for
different reasons which aim at the exams and the success of the students in the
exams.
Exam influence on writing in the second term is more evident than in the first
term in both the content and on the reasons why teachers use specific teaching
strategies to teach writing. Teachers did writing tasks that resembled the exams and
the way they teach writing aimed at helping students to succeed in the exams.
However, even if teachers’ final aim is students to succeed in the exams the way
they teach the writing skill is because they believe that can help students to under-
stand and learn better. So, teachers’ factors such as their educational beliefs and
language learning are combined with exam success in the second term.
4.5 Listening Skill
Observations showed that listening activities conducted only in the multi-exam
class. The TIE examination did not test the listening skill and this is the reason why
teachers did not teach listening in the one-exam class. Listening activities in multi-
exam class either strictly followed those in the course book, in the first term or they
were drawn from past papers and practice tests, especially during the second term.
Follow-up interviews give valuable insight into the kind of activities both
teachers used. More specifically, in multi-exam classes, in the first term, a variety
of activities is conducted closely following the course book, which was of a general
B2 level, including listening activities which conformed to the exam specifications
of various B2 level exams. Teacher 2 explains that ‘We will, we will do the listening
448 I.-R. Papakammenou
skill. It’s maybe necessary because they change their minds further down’. This
shows why teachers follow the book and do all the listening activities they find in
the book.
So, teachers did listening activities although they were not in the exams targeted
because they followed the book. In addition, teachers had to cover all possible exam
task types in case students changed their minds later in the school year about which
exam to sit for. This forced the teachers to do all the listening activities that are in
the course book.
In the first term, teachers used translation to teach listening to the students.
Teacher 2 used translation in the listening activities because she believed that
students should understand most of what the listening section asks for and practice
step by step before letting them do the listening by themselves, which is done in the
second term. Specifically, she explains ‘So that they will build up their vocabulary
gradually to a point where listening becomes much easier to them. Preparing them
little by little, to train them to do listening with things that they know and then
eventually during the test—when they get ready for testing—we have to let them go
and they have to be on their own after that’.
Comparing to multi-exam classes, the listening activities were completely
absent in one-exam classes, where the teachers spent time in other skills. This
absence of the listening skill in one exam classes especially in the second term was
highly due to the test as the teachers teach the materials assigned and considered in
the final tests. In multi-exam classes, though teachers following the book taught any
listening activity since students have not decided yet what exam to sit for and they
wanted to do a wide range of listening activities to prepare students properly for the
exams.
4.6 Grammar and Vocabulary
Grammar and vocabulary activities were commonly used activities in multi-exam
class since the examinations, especially the Michigan exam, have parts that test
grammar and vocabulary. For this reason, grammar and vocabulary activities were
used in a great extent multi-exam class and especially in the first term. In the second
term though, teachers got students to do grammar and vocabulary activities in the
form of mock tests. On the contrary, in one-exam class teachers did minimal
grammar and vocabulary activities and only in the first term.
In the follow-up interviews, teachers confirmed that the frequent use of grammar
and vocabulary activities in multi-exam class was to a large extent for the sake of
preparing their students for the Michigan exam. However, there is a difference
between Teacher 1 and Teacher 2 when they explain why they do grammar and
vocabulary activities. Teacher 1 is more influenced by the exams than Teacher
2 who says that activities do not strictly adhere to the exams but she believes that
are necessary for students in order to practice and learn more vocabulary since they
are weak. Teacher 1 refers that ‘I didn’t have the exams; I wouldn’t spend so much
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 449
time on it. I wouldn’t over-analyze it so much. . .Maybe I would left out the exercise’
whereas Teacher 2 points out that ‘Grammar and vocabulary activities do not
strictly adhere to the exam as students need more extensive practice by being
exposed to a number of activities even if these are not included in the exams’.
In one-exam classes teachers believe that grammar is important because students
need them both in order to write and speak better. Teacher 1 does grammar activ-
ities in the first term because she believes that ‘It’s important because they have to
use grammar to actually talk, to write, to do the writing or the orals’. The influence
of exams is much more evident in one-exam classes in the second term where
teachers used fewer grammar and vocabulary activities because the TIE exam tests
only speaking and writing. Teachers did not do any grammar and vocabulary activ-
ities in the second term because the exam does not test them.
The differences between teachers and between terms regarding the factors
affecting the choices in grammar and vocabulary activities are obvious. Teachers
in the first terms are affected by the level of the students, the fact that students sit
sooner for the exams and somewhat by the exams. In fact, there is a difference
between multi-exam and one-exam classes in the first term showing that one-exam
class is influenced more by the exams even in the first term. There is not any vari-
ation between teachers and classes in the second term since both are influenced
greatly by exams.
There a number of teacher-direct and teacher-indirect factors that influence how
teachers teach skills in exam preparation classes (Papakammenou, 2016, p. 130).
Course books, students and language learning are factors that influence tasks and
activities to a great extent but mainly in the first term. Exams play a very important
role for both activities and tasks in all language skills but there is a big difference
between groups on the importance and priority it has.
Student and teacher factor play a pivotal role in teachers’ decision regarding
teaching practices. These factors affected teachers mostly in the first term rather
than the second in both classes as it pertains to the choice of the exam and the
formation of the groups. Getting closer to the exams, the type of class the teachers
teach (one exam or multi exam), the format of the exam and the number of the
exams are some exam factors that may influence teaching practices.
5 Conclusion
Teachers who play a vital role be well-versed in the exams they teach in terms of
exam requirements, marking criteria and test-taking strategies, among other things,
so as to be able to support their students, especially in a multi-exam context, where
the students’ familiarization with a variety of test formats might seem overwhelm-
ing. Handling multi-exam classes entails the teachers’ ability to combine the
requirements of different exams and use the skills and sub-skills and the activities
450 I.-R. Papakammenou
and tasks of one exam in favor of the other. This necessitates a strategic action on
behalf of the teachers to prepare accordingly for different exams. A multi-exam
context allows students to practise language since exam constructs are different in
different exams and thus a greater variety of language abilities, activities and tasks
is available. Multi-exam contexts seem to provide more opportunities for language
learning, which might suggest that simultaneous exposure to a variety of test for-
mats and requirements might foster a richer learning context.
The use of new methodologies and communicatively oriented language oppor-
tunities should be applied in exam preparation classes in order to make the lesson
more effective, interesting and thus more student-centered and less routinized.
Using more communicative teaching strategies (i.e., pair/group work), teachers
can make the lesson more interesting and teach skills more effectively even in
exam preparation classes. Teachers in the study were in favour of assessment
especially in the language skills in which they used different assessment techniques
to evaluate students (Hidri, 2015). Also, teachers should integrate proper assess-
ment of skills into the teaching and learning process, and be able to design appro-
priate assessment tasks in order to provide quality feedback and therefore minimize
negative exam influence. As Turner points out there should be an “alignment across
CBA and large-scale assessments” (Turner, 2012, p. 68). These require teachers to
become ‘exam-literate’ which means that teachers should have the appropriate and
required knowledge and skills to handle exam classes and their requirements. Thus,
training related to teaching practices and how to deal with skills in exams is
required.
In order to engineer positive washback, not only teachers but other stakeholders
such as exam constructors should focus more on test design and pay attention to
reducing construct-irrelevant variance. Test designers should incorporate all skills
which must be assessed with activities and tasks that promote language learning in
exams and teacher trainers should train teachers to favour activities and tasks that
require more student participation. This study further highlights the need for the
design and use of formative assessment and the consideration of other elements of
language education tests which include all the dimensions of language performance
and abilities.
In addition to teachers, test designers and teacher trainers, this study has poten-
tial implications for researchers too. Firstly, this thesis highlighted the importance
of direct observation of teachers and learners as well as the wider educational con-
text. Direct class observation to a certain extent guarantees more accurate and
transparent conclusion to be drawn, as it captures actual and not assumed teaching
practices. The study proposes a self-reflection of investigating washback combining
teacher research. Pivotally, this study offers a framework for investigating wash-
back on teaching strategies, which could inform further washback studies given the
prominence of the teacher factor in most washback models.
Further research is needed to identify how teachers can be involved in exam
procedure. Teachers’ role is significant since they can provide additional knowl-
edge about what happens in classes. Teachers as one of the main parties who can
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 451
provide additional knowledge on students’ needs and the washback effect in
exam preparation classes should be involved in high-stakes language testing.
In addition, there is a need to identify which forms of formative assessment and
communicative language teaching can teachers adopt to teach language skills in
exam preparation classes. Teacher-direct factors such as students, teachers and lan-
guage learning influence how teachers teach language skills directly. With studies
on all these factors being observed, it will be possible to enrich the data, as the
influence between factors can generate important information on the how language
skills can more effectively being assessed.
Another idea that might enhance future research into teaching strategies and that
would help capture the dynamics of the classroom would be the use of learning
records, together with an examination of English proficiency test scores. These
might provide more concrete evidence to support more detailed accounts of the
relationship among teachers, students and exams. Test scores could provide more
information about the effectiveness of these types of classes related to language
learning and exam success.
The study emphasizes the importance of reconfiguring how teachers teach lan-
guage skills in exam preparation classes with the aim of promoting effective
assessment environments that can result in greater improvement in language learn-
ing. A collaboration between teachers, testers and material designers is needed to
design effective assessment procedures and well-designed exams to promote lan-
guage learning and positive feedback. Acknowledging how language skills are
taught in various exam preparation courses and the factors affecting teachers’
choices can result in more effective assessment practices. Furthermore, for teachers
to be actively involved in assessment, it is essential that they become ‘exam literate’
that is, that teachers have the appropriate and required knowledge and skills to
handle exam classes and their requirements.
Further training related to teaching practices is required in order the immediate
needs of teachers to be catered. Such training should include the appropriate selec-
tion of tasks and activities which entails an appropriate balance of practical skills
and teaching practices. Further training in the four skills and specifically the
integration of these skills is needed so that teachers will be better equipped with
the confidence to apply them in class. Teaching practices which enhance language
learning and student participation should be included in such programs. Through
training teachers will gradually develop their teaching practices providing effective
feedback, through the use of peer assessment as well as use more student-centered
strategies, such as group and pair work. It is the hope of the writer that the present
study has raised awareness about the need for exam and assessment literate
teachers, as Vogt and Tsagari (2014) and Khadijeh and Amir (2015) also support,
in the hope that it will lead to greater awareness of how language skills should be
taught in exam preparation courses and promote further research in the field.
452 I.-R. Papakammenou
References
Aftab, A., Qureshi, S., & William, I. (2014). Investigating the washback effect of the Pakistani Inter-
mediate English Examination. International Journal of English and Literature, 5(7), 149–154.
Alderson, J. C., & Hamp-Lyons, L. (1996). TOEFL preparation courses: A study of washback.
Language Testing, 13(3), 280–297.
Alderson, J. C., & Wall, D. (1993). Does washback exist? Applied Linguistics, 14(2), 115–129.
Azadi, G., & Gholami, R. (2013). Feedback on washback of EFL tests on ELT in L2 classroom.
Theory and Practice in Language Studies, 3(8), 1335–1341.
Bailey, K. M. (1996). Working for washback: A review of the washback concept in language testing.
Language Testing, 13(3), 257–279.
Cheng, L. (1997). How does washback influence teaching? Implications for Hong Kong. Lan-
guage and Education, 11(1), 38–54.
Cheng,L.(1999). Changingassessment: Washback onteacher perspectivesand actions. Teachingand
Teacher Education, 15(3), 253–271.
Cheng, L. (2004). The washback effect of a public examination change on teachers’ perceptions
toward their classroom teaching. In L. Cheng, Y. Watanabe, & A. Curtis (Eds.), Washback in
language testing; Research context and methods (pp. 147–170). Mahwah, NJ: Lawrence
Erlbaum Associates, Inc.
Glover, P. (2006). Examination influence on how teachers teach: A study of teacher. Unpublished
doctoral dissertation, Department of Linguistics and Modern English Language, Lancaster Uni-
versity, Lancaster, England.
Green, A. (2006). Washback to the learner: Learner and teacher perspectives on IELTS prepar-
ation course expectations and outcomes. Assessing Writing, 11, 113–114.
Hayes, B., & Read, J. (2004). IELTS test preparation in New Zealand: Preparing students for the
IELTS academic module. In L. Cheng, Y. Watanabe, & A. Curtis (Eds.), Washback in lan-
guage testing; Research context and methods (pp. 19–36). Mahwah, NJ: Lawrence Erlbaum
Associates, Inc.
Hidri, S. (2015). Conceptions of assessment: Investigating what assessment means to secondary and
university teachers. Arab Journal of Applied Linguistics, 1(1), 19–43.
Hughes, A. (1989). Testing for language teachers. Cambridge: Cambridge University Press.
Khadijeh, B., & Amir,R.(2015). Importance of teachers’ assessmentliteracy.International Journal of
English Language Education, 3(1), 139–146.
Kourou, P. (2008). The use of L1 in the foreign language teaching process. ELT News, 4–6.
ELT Press.
Li, H. (2009). Are teachers teaching to the test? A case study of the College English Test (CET) in
China. International Journal of Pedagogies and Learning, 5, 25–36.
Mickan, P., & Motteram, J. (2004). An ethnographic study of classroom instruction in an
IELTS preparation program. IELTS Research Reports, 8, 17–43.
Muhr, T., & Freise, S. (2004). User’s manual for Atlas.ti 5.0 (2nd ed.). Germany: Scientific Soft-
ware Development.
Pan, Y. (2011). Teacher washback from English certification exit requirements in Taiwan.
Asian Journal of English Language Teaching, 21, 23–42.
Papakammenou, I. (2016). A washback study of the teaching practices used in EFL exam prepar-
ation classes in Greece. In D. Tsagari (Ed.), Classroom-based assessment in L2 contexts
(pp. 118–137). Newcastle upon Tyne: Cambridge Scholars Publishing.
Read, J., & Hayes, B. (2003). The impact of IELTS on preparation for academic study in
New Zealand. IELTS Research Reports, 4, 153–205.
Saif, S. (2006). Aiming for positive washback: A case study of international teaching assistants.
Language Testing, 23(1), 1–34.
Washback on Language Skills: A Study of EFL Multi-Exam Preparation Classes 453
Tsagari, D. (2009). Revisiting the concept of test washback investigating FCE in Greek language
schools. Research Notes, 35, 5–10.
Tsagari, D. (2012). Research on English as a foreign language in Cyprus (Vol. II). Nicosia:
University of Nicosia Press.
Turner, C. (2012). Examining washback in second language education contexts: A high stakes
provincial exam and the teacher factor in classroom practice in Quebec secondary schools.
International Journal of Pedagogies and Learning, 5, 103–123.
Vogt, K., & Tsagari, D. (2014). Assessment literacy of foreign language teachers: Findings of a
European study. Language Assessment Quarterly, 11(4), 374–402.
Wall, D., & Alderson, J. C. (1993). Examining washback; The Sri Lankan impact study. Lan-
guage Testing, 10(1), 41–69.
Watanabe, Y. (2000). Washback effects of the English section of Japanese university entrance exam-
inations on instruction in pre-college level EFL. Language Testing Update, 27, 42–47.
Watanabe, Y. (2008). Methodology in washback studies. In L. Cheng, Y. Watanabe, & A. Curtis
(Eds.), Washback in language testing: Research context and methods (pp. 19–36). Mahwah,
NJ: Lawrence Erlbaum Associates, Inc.
454 I.-R. Papakammenou
Part VIII
Alternative Forms of Assessment
Performance-Based Assessment: A Shift
Towards an Assessment for Learning Culture
Zineb Djoub
Abstract Assessment of language learning has been the focal concern of several
researchers, teachers, test developers, syllabus designers, etc. Indeed, their aim is to
make from this process a tool to support students’ learning and help teachers
achieve the intended learning outcomes. Achieving this objective has increasingly
been targeted with the current trend emphasizing the relationship between students’
learning and their assessment. In light of such concern has emerged the need to
introduce performance-based assessment to the language classroom. How can then
this assessment approach affect students’ learning views and attitudes regarding
assessment? To investigate this issue, a questionnaire was administered to first year
Master students of Didactics and Applied Linguistics at the Department of English
(Abdelhamid IBN Badis University of Mostaganem, Algeria). The participants,
who were attending the course of TEFL, were assessed only through performance-
based assessment, i.e., doing projects and presenting them in class. To understand
the impact of such assessment approach, the questionnaire also sought to find out
about the participants’ views regarding the way they were assessed during the
Bachelor degree cycle, how such assessment affected their beliefs and attitudes
and how they would assess their learners more effectively in the future. The results
indicated that using exams and regular testing had negative washback effect on the
students’ learning views and attitudes. Whereas, using presentations in class as a
form of alternative assessment had contributed to boosting their motivation, self-
confidence and learning. Still, relying entirely on this assessment approach did not
support these students overcome their anxiety of being assessed.
Keywords Presentations • Assessment • Performance • Learning attitudes • Views
Z. Djoub (*)
Abdelhamid Ibn Badis University, Mostaganem, Algeria
e-mail: zdjoub@yahoo.fr
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_22
457
1 Introduction
The last decade has witnessed an array of changes in language learning goals,
requirement as well as perspectives. The prevailing tendency is to teach learners
how to learn in order to enable them to cope with new technologies and learning
environments. Indeed, assessment of learners needs to contribute to their learning
instead of merely providing data on their accomplishment or learning outcomes. As
Gibbs (2006) states: “Assessment frames learning, creates learning activity and
orients all aspects of learning behaviors” (p. 23). To this end, changing assessment
to amplify “the want to learn” is essential to promote student autonomy in higher
education. This requires “a move away from outcome-based assessment and
towards more holistic, process-based assessment, such as portfolios and personal
development planning” (Clegg & Bryan, 2006, pp. 218–219). To this end,
performance-based assessment has been advocated to assess higher-order thinking
skills and help teachers and principals support students in developing a deeper
understanding of content (Vogler, 2002). However, more studies are needed to
explore the nature of this assessment in EFL context. Therefore, the present case
study was concerned with investigating the impact of this assessment on the
participants’ learning views and attitudes. Before outlining the research findings,
there is a need to clarify first what performance-based assessment means and its
benefits on learning.
2 Theoretical Background
Part of the educational process is the process of assessing learners’ progress to find
out whether teacher’s objectives and expectations have been met. In language
teaching, assessment refers to “the act of collecting information and making
judgments on a language learner’s understanding of a language and his ability to
use it” (Chapelle & Brindley, 2002, p. 267). It is an interpretation of the test taker’s
ability to use some aspects of this language (Bachman & Palmer, 2010). Indeed, it
has been acknowledged that assessment is an integral part of the teaching–learning
process (James, McInnis, & Devlin, 2002). It is thus a vital component of the
educational process which serves a variety of purposes like diagnostic, achieve-
ment, progress, etc. Nevertheless, though assessment is undertaken for a variety of
purposes the primary purpose remains to support learning which occurs when
learners are: “[T]hinking, problem-solving, constructing, transforming, investigat-
ing, creating, analyzing, making choices, organizing, deciding, explaining, talking
and communicating, sharing, representing, predicting, interpreting, assessing,
reflecting, taking responsibility, exploring, asking, answering, recording, gaining
new knowledge, and applying that knowledge to new situations” (Cameron, Tate,
Macnaughton, & Politano, 1998, p. 6).
458 Z. Djoub
An increasing interest in improving assessment procedures has been gaining
ground, generating a plethora of research studies on ESL/EFL teacher’s assessment
practices (e.g., Cheng, Rogers, & Wang, 2008). The emerging educational trends
have emphasized learner involvement in the assessment process to support them to
move towards greater autonomy. Indeed, the literature has witnessed an array of
negative criticism with regard to traditional testing techniques such as multiple-
choice, fill-in-the-gaps, matching, etc. (See for example; Herman & Winters, 1994).
It has been claimed that these techniques though they provide information about the
product, still the learning process remains unveil as Genesee and Hamayan (1994)
stress that
(. . .) tests can be useful for collecting information about student achievement under certain
restricted conditions, but they are not particularly useful for collecting information about
students’ attitudes, motivation, interests, and learning strategies. (p. 229)
In light of these findings has emerged the need to introduce wider variety of
assessment methods or what has been called alternative assessment. The latter has
been defined by Alderson and Banerjee (2001) as follows:
Alternative assessment’ is usually taken to mean assessment procedures which are less
formal than traditional testing, which are gathered over a period of time rather than being
taken at one point in time, which are usually formative rather than summative in function,
are often low-stakes in terms of consequences, and are claimed to have beneficial washback
effects. (p. 228)
It follows from this that through alternative assessment it is possible to gather
information about the process of learning including the factors which can affect its
achievement such as learners’ affective and personality styles, their learning strat-
egies, work habits, social behaviors and reactions to the course (Genesee & Upshur,
1996). As a result, teachers are provided with “data on their students and their
classroom for educational decision-making (. . .)” and administrators as well “can
benefit from the clear information about student and teacher attainment over time”
(Hamayan, 1995, p. 215).
Alternative assessment is based on assessment for learning (AFL) where assess-
ment needs to “serve the purpose of promoting students’ learning (. . .) an assess-
ment activity (is formative) if it provides information to be used as feedback by
teachers and their students” (Black, Harrison, Lee, Marshall, & Wiliam, 2003,
p. 10). The term assessment for learning has been often used for formative assess-
ment as opposed to assessment of learning or summative assessment which focuses
on measuring learning achievement, in order to check quality and compare results
from schools and institutions at national or regional level to set standards (Hedge,
2000). So, alternative assessment focuses more on the process of learning, so that it
is mostly formative.
Performance-based assessment is a type of alternative assessment where stu-
dents are involved in tasks that require demonstrating their knowledge, skills, and
strategies through creating a response or a product (Rudner & Boston, 1994).
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 459
Unlike traditional exams and tests in which students select one of the responses
provided, a performance-based assessment requires students to perform a task or
generate their own responses. In this respect, Jones (1985) offers the following
definition, “an applied performance test measures performance on tasks requiring
the application of learning in an actual or simulated setting” (p. 16). For that
purpose, this assessment approach has been considered as the most authentic type
of student assessment since it can replicate actual performances occurring outside
the classroom learning context (McTighe & Firrara, 1998).
Further, performance based assessment has been described as part of a construc-
tivist philosophy as it involves a two-way interaction between learners and their
environment (Yawkey, Gonzalez, & Juan, 1994). It also requires students to use
higher level thinking skills, place the problem within a real-world context, assess
the process and reasoning used to solve the problem as well as the solution attained
(Kind, 1999). This form of assessment can include portfolios, presentations, etc.
On the benefits of performance-based assessment, Fadel, Honey, and Pasnik
(2007) claim that within a knowledge-based society the workplace will require
“new ways to get work done, solve problems, or create new knowledge, so the
assessment of students will need to be largely performance based so that students
can show how well they are able to apply content knowledge to critical thinking,
problem solving, and analytical tasks throughout their education” (p. 34). Indeed,
Koretz, Mitchell, Barron, and Keith (1996) found that in implementing
performance-based assessment, teachers changed their instructional practice
towards emphasizing cooperative work, focusing more on writing, problem solving,
and real-world, hands-on activities; and deemphasizing rote learning and teaching.
Unlike traditional assessments, performance based-assessment approaches provide
students with opportunities to exercise their higher-level thinking (Dietel, Herman,
& Knuth, 1991). In doing so, they can better measure complex skills and commu-
nication, which are considered important competencies and disciplinary knowledge
needed in today’s society (Palm, 2008).
Calling for students to apply knowledge and skills instead of simply recalling
and recognizing, performance based assessment is likely to provide feedback on
students’ understanding (McTighe & Firrara, 1998). Similarly, Short (1993) states
that alternative assessment, including performance-based assessment, provides a
more accurate demonstration of student ability than traditional assessment.
Research has also shown that using portfolios, for instance, “encourage students
to think creatively, critically, and logically; (. . .) to set goals for their own literacy
learning; and to demonstrate their ability to use reading, writing, listening, and
speaking in an integrated way and for authentic purposes” (Spalding, 1995, p. 2). It
was also found that performance-based assessment can result in positive washback
(Miller & Legg, 1993; Shohamy, 1995). By using a context that is meaningful to the
students, they are more likely to grasp the concepts and may be more motivated to
learn and improve.
460 Z. Djoub
2.1 Research Problem and Rationale of the Study
Relying on assessment for grading culture has been a common practice within the
present research context as one had observed. Indeed, exams and tests are mostly
used to grade students’ work and communicate their progress. Even when the
purpose is to assess students formatively regular testing is the alternative. Conse-
quently, assessment for learning opportunities are not provided for students and
negative washback effect is likely to be engendered.
This study intended to reveal the washback effect of performance-based assess-
ment on the students’ learning views and attitudes. In doing so, the findings are
meant to support teachers understand this assessment approach and how it needs to
be integrated to achieve an assessment for learning culture. More particularly, by
relying on this assessment approach, one sought to compare such views and
attitudes with those generated towards exams and regular tests. The data gathered
are thus likely to provoke teachers’ (and those concerned with students’ assessment)
reflection on how formative and summative assessment should be integrated to
boost students’ learning.
The overarching research question that guided this study was: What are the
students’ views and attitudes regarding performance-based assessment? The main
question was supported by the following sub-questions: How were the participants
mostly assessed during their previous learning experience? What effects did such
assessment have on their learning views and attitudes? Were they aware of what
sound assessment means?
3 Method
To provide empirical evidence of what constitutes students’ views and attitudes
regarding performance-based assessment, a case study has been conducted in which
a questionnaire was administered to a sample of first year Master students of
Didactics and Applied Linguistics. They were 26 females and 4 males whose
ages ranged from 22 to 23 years old. These students were studying at the depart-
ment of English of Abdelhamid Ibn Badis University (Mostaganem, Algeria). After
graduating they were expected to teach English in schools, language institutes or
universities (with a PhD degree). It is worth noting, that within the Algerian society,
English is taught as a foreign language since French is considered the second
language. To provide more details about this study, the research problem, rational,
along with the research questions and method are described below.
A questionnaire (see the Appendix) was handed to the students during the course
of TEFL in the second semester towards the end of the academic year in order to
reveal their views and attitudes regarding their teacher’s use of presentations during
this course as a form of assessment instead of final exams and regular tests. It is
worth noting that this course aims at developing the students’ knowledge and skills
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 461
of teaching English as a foreign language. It was held in a form of sessions during
two study semesters. The students were introduced to topics such as students’
motivation, anxiety and teachers’ role, learning styles and strategies, etc.
With regard to their presentations, the participants were asked to select a topic or
an issue in relation to this field and they were encouraged to read, to conduct
research, collaborate with their teacher and classmates whenever they need support
and present their work in class using the PowerPoint. Most of the students’ pre-
sentations exposed and discussed findings of case studies conducted individually by
the students themselves within the same university. For instance, students’ diffi-
culties with English (the different skills), their attitudes and views towards their
teachers’ feedback, anxiety in taking exams, participating in class were among their
researched topics. There were other presentations which put forward some sugges-
tions or teaching tips such as strategies for effective collaboration, use of game-
based learning, technology in class, etc.
Before involving them in such kind of assessment, the students were provided
with the assessment rubric along with the criteria opted for to grade their pre-
sentations including their answers to the questions addressed by the audience (i.e.,
their teacher and classmates) and ability to explain and share their ideas and
experience. They were allotted 25 min to present their work in class using the
PowerPoint as stated before. To enhance their interaction, the participants were
invited to address questions to their classmates who presented their work for about
15 min in order to understand, discuss their views and learn from each other. After
such discussion, the teacher provided her feedback concerning the presenter’s
language, research methodology, the way s/he used the PowerPoint, etc.
The questionnaire consists of ten questions (two open, three structured and five
semi-structured questions) which can be grouped under three sections. The first
section aimed at obtaining some background information about the participants’
gender and age. The second section attempted to unveil their views regarding the
way they were assessed during the Bachelor’s degree cycle. This is through asking
them about the assessment approach which was mostly relied on and whether they
like it or not. To find out about the effect this assessment had on these students’
learning views and attitudes, they were asked to describe their assessment experi-
ence, i.e., whether it was a source of anxiety, doubting of its reliability, conflict with
teachers or a source of motivation to learn and improve. What its results had
changed was also addressed here regarding their learning attitudes, beliefs about
their teachers, their learning, the educational system as well as their learning plans
and ambitions. Finally, the third section of the questionnaire concerns the students’
views concerning using projects’ presentations or performance-based assessment in
the TEFL course to find out how effective this form of assessment was for them.
Moreover, to know about their knowledge of what constitutes good assessment they
were asked about the effective way they would adopt as future teachers to assess
their learners and help them learn and succeed.
462 Z. Djoub
4 Results and Discussion
When they were asked about the way they were assessed during their Bachelor
degree cycle, the majority (73%) pointed out the use of final exams and regular
tests. Whereas, 33% referred to final exams only and just 7% selected project
presentation. But, no student chose portfolios or added other self-assessment
approaches such as journals, diaries, blogs, etc. Besides, as Fig. 1. below shows
57% stated that they did not like such assessment approach. But, some students
expressed the opposite claiming that it was a source of learning and improving
(by seven students), a support to pass (by two students) and a safer approach for
those who are afraid of taking risk (by one student). There was also one student who
wrote that she liked it to a certain extent and another one said that she wished they
were also assessed via other approaches such as presentations in class.
For those who expressed their dissatisfaction with this assessment approach,
there were six students who stated that it does not reflect the students’ learning since
it is associated with anxiety and stress. In the same vein, another student wrote that
it is a source of anxiety rather than learning. Not communicating constructive
feedback and assessing different skills apart from writing were the reasons behind
the disproval of two students. Besides, two other students claimed that exams and
tests provide opportunities for cheating and succeeding for lazy students while
those who work hard along the year have less chance for success because of exam
stress. In this regard, there was also another student who confessed that it focuses on
the learning outcome while ignoring its process. In addition to that, three students
emphasized the need to include performance-based assessment as follows: (1) “I
like this assessment but I wished we had in addition to it the opportunity to present
our projects”. (2) “I do not like it because we were not given the opportunity to feel
at ease and at least present our work to feel comfortable when facing the audi-
ence”. (3)“If we got familiar with projects’ presentations in the previous years, we
would not have faced the problem of being shy while talking in front of others”.
With regard to the washback effects of final exams and regular tests, most of the
participants (70%) indicated that this assessment approach was a source of anxiety
and fear while 30% denied such effect. But, 57% stated that it was not a source of
questioning and doubting teachers’ fairness. Likewise, 80% expressed that being
assessed in such a way did not contribute to having conflict between teachers and
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
Yes
No
Fig. 1 The students’
preference of exams and
regular tests
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 463
students or between students and the administration staff. There were also 87% of
the students who confessed that it was a source of motivation to learn and improve.
Furthermore, going through this assessment process students’ learning views and
attitudes may change. In this regard, the majority of students’ answers (87%)
pointed out that their learning attitudes, i.e., the way they revised and prepared
for their exams changed as a result of their teachers’ assessment and the marks they
had. Also, 60% referred to the change of their learning beliefs towards their
teachers. Still, only, 37% stated that their learning beliefs towards the educational
system had changed. The same rate was also registered for their learning plans and
ambitions. The results also revealed that a few of them (23%) had changed their
belief towards what language learning is (Fig. 2).
For question seven, the students’ answers show that most of them (90%) liked
their teacher’s use of projects’ presentation in class as a form of assessment. Only
10% disproved their use within this context because of finding it embarrassing, not
interesting and a risk taking experience. But, for those who supported the use of this
assessment approach 53% claimed that it was interesting and they enjoyed listening
to other’s presentations. 73% argued that presenting in class and answering their
classmate’s and teacher’s questions/comments helped them gain much more confi-
dence in their learning abilities. Similarly, the majority (90%) considered it a source
of learning. Still, 53% accepted that being assessed through their projects’ pre-
sentations did not make them feel more comfortable than using final exams (Fig. 3).
To probe students’ answers concerning what effective assessment means an
open question was addressed here. Different answers were provided by the partic-
ipants. 30% of them advocated the need to be creative in assessment through
implementing a variety of assessment approaches. Whereas, 23% valued supporting
learners psychologically to cope with exam anxiety and 13% highlighted the
0.00%
10.00%
20.00%
30.00%
40.00%
50.00%
60.00%
70.00%
80.00%
90.00%
100.00%
Learning attitudes
Learning beliefs towards
your teachers
Beliefs towards learning
Learning beliefs towards
the educational system
Learning plans and
ambitions
Fig. 2 Washback effects of exams and regular tests
464 Z. Djoub
importance of making the assessment criteria explicit for learners. There were also
other respondents (23%) who saw the need to select the right assessment approach
that fits students’ learning styles. Providing positive or constructive feedback was
also put forward by 13% and using self-assessment by the same rate of collected
answers. Other suggestions were also indicated here. Two students referred to the
use of presentations, another one to using and assessing problem solving tasks and
two other students suggested being objective and honest while assessing learners.
From the questionnaire results, it can be concluded that the participants were
mostly assessed via the same tools, i.e., exams and regular tests during their
Bachelor’s degree cycle. There were no alternative assessment approaches such
as self and peer-assessment which can help promote their active involvement and
thus their autonomous learning. Indeed, since such assessment approach is based on
grading and passing/failing culture, it is likely to be perceived as a frightening
experience causing stress and anxiety among students. For this reason, the majority
of the participants showed their dissatisfaction with such approach.
In addition, relying entirely on summative assessment is not likely to reflect
students competence and enhance their reflection on their learning and ability to
plan and take decisions to improve it since “many examinations make considerable
demands on learners’ factual knowledge, which can have the unfortunate side-
effect of encouraging cramming and shallow learning at the expense of that ‘deep’
learning which is HE’s avowed goal” (Brown & Knight, 1994, p. 67). Besides, the
amount of feedback provided out of this assessment form may not help students
make sense of their performance, thereby identifying their learning needs and
strengths.
Hence, one should not negate the importance of summative assessment. As the
findings showed the participants did not question its reliability and negated that it
was a source of conflict between them, their teachers or the administration staff. In
addition, in spite of being associated with anxiety, exams and regular tests helped
these students to learn and improve. This confirmed Brown and Knight’s (1994)
statement that this assessment approach may enhance students’ extrinsic motivation
“a stimulus for understanding to be developed through deep learning” (p. 68).
0%
10%
20%
30%
40%
50%
60%
70%
80%
90%
Feeling
more
comfortable
Getting
interested
Gaining
much more
confidence
Learning
Yes
No
Fig. 3 The students’ views regarding presentations
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 465
With regard to students’ presentations, most of the students liked them as a form
of assessment in the TEFL course. This study also proved that using this assessment
approach was a source of learning from each other, besides helping them gain much
more confidence in their learning abilities and stimulating their interest in listening
to their classmates. These findings confirmed Bossert’s (1988) claim that peer
encouragement may improve task engagement, and engaging in collaborative
learning tasks causes students to shift intentional resources and get more motivated.
However, as the data collected reveal using presentations as a formative assess-
ment form was also associated with anxiety and there were even students who
disproved them because they were afraid of taking risk. This is because the
participants were not familiar with such form of assessment. In addition to that,
this might be due to attributing marks for their presentations in addition to their
teacher’s comments. Being restrained by time to complete their work and present
them in class is also likely to make them anxious. Therefore, teachers need to take
into account the different learning styles of their students. In addition to exams,
different forms of formative assessment such as portfolios, reflective logs, team-
work projects, etc. to assess their students’ process of learning.
It is worth noting that understanding the principles and implications of these
assessment approaches is crucial for their proper use. There is a need to go beyond
viewing assessment as a process of grading students’ work but also as a source of
learning for them, thus keeping students learning as the objective of assessment.
Indeed, their use should be geared to involve students in the assessment process and
help them track their own progress and reflect on their learning needs and wants
instead of implementing them merely for obtaining grades over a period of time as
it is the case with the present learning context.
The data gathered also unveiled that the students’ views regarding sound assess-
ment reflect their lack of awareness of it. This is since few of them pointed to the
need to use alternative assessment. Besides, apart from presentations, they did not
give examples of the assessment tools that can be used to achieve formative
purposes. This could imply that there is a need for training students into assessment
literacy. This literacy is of crucial importance because first it helps learners get
relieved from their fear or anxiety towards the test they take, thereby avoiding
negative washback. Second, because it allows them to get actively involved in the
process of assessment and thus feel motivated by assessment (Watanabe, 2011).
5 Conclusion
Findings from this study show that relying only on exams and regular testing is
likely to engender a negative washback effect among students. This can also
contribute to their lack of awareness of the various alternative assessment
466 Z. Djoub
approaches which they can integrate as future teachers to assess their students
formatively. Also, using presentations as a form of performance-based assessment
had triggered the students’ motivation, self-confidence and enhanced their learning.
Yet, their anxiety to get prepared and present their work in class was still prevailing
as their statements indicated.
There is a need thus to support students experience different forms of alternative
assessments along with their learning process in order to provide them with
opportunities to express themselves, take risk, learn how to overcome their anxiety
and improve their performance. Within such grading culture, maintaining dialogue
with students is crucial where guidance, constructive feedback and encouragement
are provided to close the learning gap. Assessment literacy is also required to equip
them with the necessary knowledge and skills to cope successfully with such
process as students and to assess their future learners, making thus from assessment
a tool to support them learn and improve.
As with any study, there were some limitations that affect the generalizability
of the results. One of the limitations of this study is the small size of the sample
(¼30). Besides, using the questionnaire as the only data collection tool may not
help gain insight into the washback effect of students’ presentations. Yet, the
findings serve as indicator for the need to explore how performance-based assess-
ment should be integrated effectively in EFL classes and also in teacher education
programmes. For instance, how teachers can infuse students’ presentations in
teaching oral classes, what effects they can have on their learning engagement
and English fluency. Additionally, there is a need for further research to examine
how assessment literacy needs to be developed among students and pre-service
teachers.
To conclude, the present research has shown that performance-based assessment
can be a good opportunity for students’ learning where their self-confidence and
learning interest get enhanced. Yet, relying entirely on this form of assessment does
not guarantee reducing students’ anxiety. Indeed, teachers need to use in addition to
summative assessment other alternative approaches which are relevant to the
course’s objective and students’ needs. This could help cater for the different
learning styles found within the same class, besides providing students with mul-
tiple opportunities to succeed and improve their performance. As future teachers,
students need assessment literacy training so that they will be able to assess their
learners more effectively.
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 467
Appendix
Student Questionnaire
Dear Student,
The following questionnaire aims at revealing your learning views concerning your teachers’assessment
practices during both the Bachelor and the Master degree cycles. Would you please provide honest
answers, in order to help your teacher understand the impact of such assessment on your learning and
thus bring about changes to improve this process. Thank you for your collaboration. Note: Mark with (X)
next to your selected answer(s).
1. Male Female
2. Age:
The student’s views concerning their assessment during the Bachelor’s degree cycle
3. When you were a bachelor student, your learning was assessed mostly through:
a) Final examinations only.
b) Final examinations and regular tests.
c) Projects’ presentations.
d) Portfolios.
e) Others? Please mention them……………………………………………
4. Did you like this way of assessment? YES NO
Explain why? .................................................................................................................
5. Along your learning experience, assessment, i.e., tests or exams have been:
a) A source of anxiety and fear. Yes No
b) A source of questioning and doubting teachers’ fairness. Yes No
c) A source of motivation to learn and improve. Yes No
d) A source of conflict between teachers, students, Yes No
and administration.
e) Others? Please mention them…………………………………………………
6. Did the assessment’s results change your
a) Learning attitudes. (e. g,. the way you revise and prepare for your exams)
b) Learning beliefs towards your teachers.(New perceptions about your
teachers as a result of the marks they gave to you)
c) Beliefs towards learning.(Your vision of what language learning is)
d) Learning beliefs towards the educational system.(Studying at university)
e) Learning plans and ambitions. (what you wanted to do in the future)
f) Others?.................................................................................................................
7. Explain your answer……………………………………………………………………
The student’s views concerning their assessment during the Master’s degree cycle
8. What do you think about the way you are being assessed within the Master degree?
………………………………………………………………………………………………………
……………………………………………………………………………
9. Do you like your teacher’s use of presentations as a form of assessment in TEFL course?
Yes No
10. If you say YES, is it because:
a) This assessment has made you feel more comfortable than using final exams.
b) It was interesting; you enjoy listening to other’s presentations.
c) Presenting in class and answering your classmate’s and teacher’s questions/
comments have helped you gain much more confidence in your learning abilities.
d) You have learned from your project and others’ as well.
e) Others? Please mention them………………………………………………….....
11. If you say NO, explain why? ..........................................................................................
12. As a future teacher, how would you make from assessment an effective process, thus helping
your learners learn and succeed?
………………………………………………………………………………………………………………
………………………………………………………………………………………………………………
468 Z. Djoub
References
Alderson, J. C., & Banerjee, J. (2001). Language testing and assessment (Part 1). Language,
Teaching, 34(4), 213–236.
Bachman, L., & Palmer, A. (2010). Language assessment in practice. Oxford: Oxford University
Press.
Black, P., Harrison, C., Lee, C., Marshall, B., & Wiliam, D. (2003). Assessment for learning:
Putting it into practice. Maidenhead: Open University Press.
Bossert, S. T. (1988). Cooperative activities in the classroom. Review of Research in Education,
15, 225–250.
Brown, S., & Knight, P. (1994). Assessing learners in higher education. London: Kogan.
Cameron, C., Tate, B., Macnaughton, D., & Politano, C. (1998). Recognition without rewards.
Winnipeg, MB: Peguis Publishers.
Chapelle, C., & Brindley, G. (2002). Assessment. In N. Schmitt (Ed.), An introduction to applied
linguistics (pp. 267–286). London: Arnold.
Cheng, L., Rogers, T., & Wang, X. (2008). Assessment purposes and procedures in ESL/EFL
classroom. Assessment and Evaluation in Higher Education, 33, 9–32.
Clegg, K., & Bryan, C. (2006). Reflections, rationales and realities. In C. Bryan & K. Clegg (Eds.),
Innovative assessment in higher education (pp. 216–227). New York: Routledge.
Dietel, R. J., Herman, J. L., & Knuth, R. A. (1991). What does research say about assessment?
NCREL, Oak Brook. Available online http://www.ncrel.org/sdrs/areas/stw_esys/4assess.htm
Fadel, C., Honey, M., & Pasnik, S. (2007, May 18). Assessment in the age of innovation.
Education Week, 26(38), 34–40. Retrieved August 26, 2008, from http://www.edweek.org/
ew/articles/2007/05/23/38fadel.h26.html
Genesee, F., & Hamayan, E. (1994). Classroom-based assessment. In F. Genesee (Ed.), Educating
second language children (pp. 212–239). Cambridge: Cambridge University Press.
Genesee, F., & Upshur, J. (1996). Classroom-based evaluation in second language education.
Cambridge: Cambridge University Press.
Gibbs, G. (2006). How assessment frames student learning. In C. Bryan & K. Clegg (Eds.),
Innovative assessment in higher education (pp. 23–36). New York: Routledge.
Hamayan, E. V. (1995). Approaches to alternative assessment. Annual Review of Applied Lin-
guistics, 15, 212–226.
Hedge, T. (2000). Teaching and learning in the language classroom: A guide to current ideas
about the theory and practice of English language teaching. Oxford: Oxford University Press.
Herman, J. L., & Winters, L. (1994). Portfolio research: A slim collection. Educational Leader-
ship, 52(2), 48–55.
James, R., McInnis, C., & Devlin, M. (2002). Assessing learning at Australia universities.
Australia: Center for the Study of Higher Education, The University of Melbourne. Retrieved
from http://www.cshe.unimelb.edu.au/assessinglearning/
Jones, R. L. (1985). Second language performance testing: An overview. In P. C. Hauptman,
R. Leblanc, & M. Bingham Wesche (Eds.), Second language performance testing (pp. 15–24).
Ottawa: University of Ottawa Press.
Kind, P. M. (1999). Performance assessment in science—what are we measuring? Studies in
Educational Evaluation, 25(3), 179–194. Retrieved December 18, 2005, from ERIC database.
Koretz, D. M., Mitchell, K., Barron, S. I., & Keith, S. (1996). The perceived effects of the
Maryland School Performance Assessment Program: Final report (CSE Technical Report
No. 409). Los Angeles: University of California, Center for the Study of Evaluation.
McTighe, J., & Firrara, S. (1998). Assessing learning in the classroom. Washington, DC: National
Education Association.
Miller, M. D., & Legg, S. M. (1993). Alternative assessment in a high-stakes environment.
Educational Measurement: Issues and Practice, 12, 9–15.
Palm, T. (2008). Performance assessment and authentic assessment: A conceptual analysis of the
literature. Practical Assessment, Research & Evaluation, 13(4), 1–11. Retrieved December
29, 2008, from http://pareonline.net/pdf/v13n4.pdf
Performance-Based Assessment: A Shift Towards an Assessment for Learning Culture 469
Rudner, L. M., & Boston, C. (1994). Performance assessment. ERIC Review, 3(1), 2–12.
Shohamy, E. (1995). Performance assessment in language testing. Annual Review of Applied
Linguistics, 15, 188–211.
Short, D. (1993). Assessing integrated language and content instruction. TESOL Quarterly, 27(4),
627–656.
Spalding, E. (1995). The new standards project and English language arts portfolios: A report on
process and progress. Clearing House, 68, 219–223.
Vogler, K. E. (2002). The impact of high-stakes, state-mandated student performance assessment
on teachers’ instructional practices. Education, 123(1), 39–55.
Watanabe, Y. (2011, November). Teaching a course in assessment literacy to test takers: Its
rationale, procedure, content and effectiveness. Research Note, 46, 29–34.
Yawkey, T. D., Gonzalez, V., & Juan, Y. (1994). Literacy and biliteracy strategies and approaches
for young culturally and linguistically diverse children: Academic excellence P.I.A.G.E.T.
comes alive. Journal of Reading Improvement, 31(3), 130–141.
470 Z. Djoub
Utilizing Dynamic Assessment to Activate EFL
Inert Grammar
Mahmoud Ibrahim
Abstract Although the inert knowledge problem in grammar is old in the litera-
ture, research has hardly proposed how the gap—already existing—between inert
and activated types of knowledge can be bridged through clear strategies. The study
proposed uses dynamic assessment (DA) as a means to detect areas of inert
knowledge in the grammatical acquired system of adult Egyptian EFL learners.
Learners’ Zone of Proximal Development (ZPD) is specified for the purpose of
activating grammatical inert knowledge and make it fully acquired in the learners’
interlanguage system. Using the interventionist approach of DA, this study inves-
tigated the effectiveness of DA and ZPD on activating EFL learners’ inert gram-
matical knowledge.
Keywords Inert knowledge • Activated knowledge • Dynamic assessment •
Grammar • Zone of proximal development • Procedural grammar • Declarative
grammar
1 Introduction
Through determining areas of EFL inert grammar and using the interventionist
approach of DA to activate them, instruction can efficiently enhance learners’
grammar acquisition. Adding a fifth language skill to the already well known
four, Larsen-Freeman (2003) was the first one to coin the term “grammaring”. It
is “the ability to use grammar structures accurately, meaningfully, and appropri-
ately” (p. 6). This was an important start point in the English language teaching
research to change the way methodologists and curriculum designers perceive
grammar instruction.
Teaching to achieve grammar acquisition is not always an easy task. Whitehead
(1929) was the one who coined the term “The Inert Knowledge Problem” to refer to
the notorious problem when some learners know the rule and can answer written
M. Ibrahim (*)
University of Jeddah, Jeddah, Saudi Arabia
e-mail: mahmoudahmad@aucegypt.edu
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_23
471
questions about it, but they can hardly apply it in their production. It is the case
when learners know that third person singular “s” is obligatory when the subject is
third person singular, but they hardly ever use it in their communication in a natural
setting although their answers in the written test reveal that they know the rule!
Afterwards, Anderson (1983) developed this notion to make a clear distinction
between two types of knowledge: Knowing language and knowing about language.
While the former is called “procedural knowledge” because it allows the learner to
use the knowledge in the natural settings, the latter is called “declarative knowl-
edge” since it only functions on the memory level. What makes it more dangerous
on the pedagogical level is that declarative knowledge may be too specific to be
transferred from one skill to another (DeKeyser, 2007). Language learners who
only have this kind of knowledge can hardly transfer what they have learnt from
language comprehension to language production. They may also be able to apply
the rule in writing (after struggling), but not in speaking. (Ibid)
Can this gap between inert knowledge and activated knowledge be narrowed?
For many years, research could resolve the argument whether or not declarative
knowledge of language (earned through conscious instruction) can turn into proce-
dural knowledge. However, the argument was resolved by McLaughlin (1993) who
provided empirical evidence that conscious knowledge of language can be routin-
ized into acquired language in a process that he named “automaticity”. Automatic-
ity argues that practice in natural or semi-natural settings created by the teacher can
move the linguistic knowledge from the short-term memory, where conscious
knowledge gained through instruction is stored, to the Language Acquisition
Device (LAD) in the brain, where the language acquired is normally stored.
The study sample, from the English Department at the College of Education in
Alexandria University, had an impressive grammar knowledge. However, the
researcher found out that their oral production lacks accuracy. In other words, the
gap between their inert knowledge and their declarative knowledge was enormous.
Some aspects of grammar were well-acquired and clear in their oral production, but
others were mostly missing. Therefore, in light of the socio-cultural theory, the
researcher suggested to define these areas of deficiency by means of DA and
develop them.
The study was carried out to consider the following research question:
How can dynamic assessment help ensure that grammar teaching achieves acquisition, not
only memorization?
2 Theoretical Background
Teaching grammar has become an integral part of modern language teaching. The
theoretical framework proposed by Canale and Swain (1980, 1981) had three main
components for language competence: Grammatical, sociolinguistic, and strategic
competence. However, in later versions of the model, Canale (1983) moved out
some elements from sociolinguistic competence into a new fourth category:
472 M. Ibrahim
Namely discourse competence. This was the beginning of the revival of grammar as
an integral part of the EFL teaching practices after approximately a century of
banning grammar from being a component of any teaching model/approach
(Fig. 1).
It is true that integrating grammar in the EFL/ESL teaching practices was
considered a taboo for decades. Nevertheless, recent models of English language
teaching (ELT) have been paying more attention to the importance of teaching
grammar (Jing, 2011). Celce-Murcia and Larsen-Freeman (1999) argue that teach-
ing the English language competencies cannot be efficiently achieved without
integrating grammar as part of the teaching practices of ESL/EFL. Other studies
like Wang (2010) and Yu (2008) have surpassed the argument whether or not to
teach grammar to now argue on how to teach grammar efficiently in the ESL/EFL
classroom. Wang (2010) explains that the absence of proper grammar teaching in
the classroom is the reason why learners do not produce proper English utterances
inside and outside the classroom. This is why many high school Chinese students
still say “*I think it won’t raining today” or “*he is study hard”! The same problem
negatively affects university students’ achievements because they cannot analyze
structure, and therefore, cannot deal with reading passages or produce proper
English sentences (Ibid: p. 3).
The meta-analysis conducted by Norris and Ortega (2000) of 49 studies on the
efficacy of foreign language teaching found out that explicit teaching, which is done
via explanation and giving examples of the grammatical structure and displaying
the rule, is much more effective in the process of acquisition of the grammatical
forms than only using implicit instruction. Limakina and Ryshina-Pankova (2012)
stress the effectiveness of teaching meaning through grammar. They argue that
grammar controls meaning in many cases, and that without proper English gram-
mar, meaning is always jeopardized.
The traditional view of assessing grammar revolves mainly around assessing
understanding and application in the form of standardized tests. However, these
tests hardly assess the learners’ ability to use the grammar they have learnt in
communication. Here comes the role DA can play to achieve better learning out-
comes. Vygotsky (1978) perceived human cognition as a two-tiered system in
which mental functions, which are lower-ordered and biologically-based, convert
through psychological tool mediation (e.g., language), into their sociocultural
settings, into mental functions that are higher-ordered and that enable mediation
of social and mental activities (Lantolf, 2000). Vygotsky (1978, p. 86) viewed the
Zone of Proximal Development (ZPD) as the cornerstone that explained how
Communicative Competency
(Acquired Competencies)
Grammatical Socio-linguistic Strategic Discourse
Fig. 1 Canale’s (1983)
model for the components
of communicative
competency
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 473
learning could lead to development. He defined it as “the distance between the
actual developmental level as determined by independent problem solving and the
level of potential development as determined through problem solving under adult
guidance or in collaboration with more capable peers.” This means that the role of
the mediator starts with defining the level of learners and defining where exactly
they need assistance so that they can develop. This is very relevant to the current
study where the role of the teachers starts with defining areas of inert knowledge to
move to the second phase where they provide mediation to activate such inert areas.
DA eventually developed from the theory of mind by Vygotsky and aims at
putting assessment under the service of instruction for promoting learners’ perfor-
mance (Poehner, 2008). In other words, it is different from traditional assessment in
that it does not only measure the final product, but also focuses on both the process
of development and product. This is why traditional assessment is always referred
to as “assessment of learning” while DA as “assessment for learning” (AfL) (Lidz &
Gindis, 2003). This gives teachers and stakeholders the insight of continuously
measuring learners’ development in the light of learning outcomes and the ability to
fix any problems arising. Therefore, with the proper use of adequate material and
instruction, DA is argued to lead to better achievement (Thorne, 2005).
Thouësny (2010) states that the term DA was first coined by Luria (1961), one of
Vygotsky’s colleagues who was primarily interested in children with abnormal
behaviors and learning disabilities, to differentiate between “statistical” and
“dynamic” modes of assessment. The difference is well-explained by Poehner
and Lantolf (2005). They illustrate that statistical assessment “inappropriately
assumes that a learner’s solo performance on a test represents a complete picture
of the individual’s capabilities” whereas DA assumes that “a full picture requires
two additional bits of information: The learner’s performance with assistance from
someone else and the extent to which the learner can benefit from this assistance”
(p. 234). Recent research (e.g., Lantolf & Poehner, 2004; Navarro & Calero, 2009;
Poehner & Lantolf, 2005) transferred the concept of DA to the area of second
language education.
DA can be divided according to the type of mediation offered into two large
categories: Interventionist and interactionist approaches. While the interactionist
approach focuses on a qualitative and interactive mode to assessment, the inter-
ventionist approach is centered around a quantitative and scripted approach to
assessment such as psychometric testing (Poehner, 2008). In the interventionist
approach, which is adopted in this study, the assistance- or mediation—is highly
standardized (i.e., it is arranged in order to provide the learner with mediation
ranging from implicit to more explicit). Poehner (2008) outlines that the
mediators are not free to respond to learners’ needs as these become apparent during the
procedure but must instead follow a highly scripted approach to mediation in which all
prompts, hints, and leading questions have been arranged in a hierarchical manner.
(p. 44–45)
474 M. Ibrahim
In contrast, in the interactionist approach, Lantolf (2009) states that the role of
mediation is “continually adjusted according to the learner’s responsivity” (p. 360).
Thouësny (2010) outlined a clearly-laid table for the difference found in the
literature between the interventionist and the interactionist approaches (Table 1):
There are many reasons why this study adopted the interventionist approach.
First, mediation in the treatment was specifically tailored to meet participants needs
(i.e., to develop their oral accuracy in the specific areas of deficiency deducted from
the pretest). Strategy of the treatment was always viable to change according to
learners’ needs and progress in every stage. Although quantitative analysis was
used to calculate t-test, qualitative analysis of the learners’ production was highly
important since it was the vivid measure of the variable under investigation, which
is participants’ oral accuracy. Finally, assessment was human-based, not
computerized.
3 Method
3.1 Sample
Participants were 22 learners from the fourth year (graduation year) in the English
Department at the Faculty of Education, Alexandria University in the academic
year 2014–2015. Only three of them were males, while the rest were all females.
They all participated in the study because they admitted they had problems in
grammar in their oral production, while their written English was much more
accurate. Participants were all volunteers who took the treatment because they
knew they had a gap between their accuracy in written tests versus in oral
production.
Table 1 The difference between interventionist and interactionist approaches of DA
Interactionist Interventionist
– Quantitative analysis – Qualitative analysis
– Mediation established in advance
– Hints ranging from implicit to explicit
– Mediation tailored to learners’ responsivity
– Large-scale assessment – Small numbers of students, time consuming
– Individual or group settings – Individual
– Psychometric reliability and validity – Psychometric measures not viable
– Written and spoken language – Spoken language
– Computer-based assessment – Human-based assessment
Note: Reprinted from Assessing second language learners’ written texts: An interventionist and
interactionist approach to DA by Thouësny (2010). Copyright 2010 by World Conference on
Education, Multimedia, Hypermedia and telecommunications (ED-Media), Toronto, Canada
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 475
3.2 Instruments
Before the treatment started, all learners had been required to take two tests. One of
them was written to investigate their declarative grammar knowledge. The test was
adapted by Ibrahim (2015) to assess grammatical knowledge for his doctoral thesis
from Philips’ (2001) Longman Preparation for the TOEFL test. The reason why the
test was adapted, not adopted, is that Ibrahim (2015) needed a test to measure the
grammatical accuracy for a sample who have a proficient linguistic knowledge,
while Philips (2001) suits all levels of linguistic knowledge. Therefore, modifica-
tions were necessary to make the test more compatible with the objectives of the
current study.
As for validity of the test, please refer to Appendix 1, for a table illustrating the list
of the grammatical aspects covered in the test. The test and the table was validated by
a list of jurors cited in Ibrahim (2015) (For further details, see Appendix 2 for the
grammar written test). Reliability was calculated using test-retest to obtain Pearson R
correlation coefficient. Ibrahim (2015) gave the test to 20 non-native EFL teachers,
who all majored in English language teaching. The period between test and retest was
around 2 weeks. The Pearson R correlation coefficient obtained was 0.001. The table
below shows the Pearson R correlation coefficient of the test displayed on the SPPS
Statistics Software Version 20 (Table 2).
The other instrument used in the pretest was an oral test that aimed at assessing
language accuracy in the sample’s oral production. The test comprised two sec-
tions. The first one was made up of three questions: The first required the partic-
ipants to describe the best/worst day in their life, the second to describe daily
routine and the third to speak about their plans for next summer. For each question,
learners had to speak for no <30 s. This section of the test aimed at investigating
learners’ accuracy in using tense and aspect in English. The second section was
made up of five sentences to measure different grammar rules. In this section, the
examiner would say a sentence and the examinee would complete it quickly
without much thinking. For example, the examiner says “George Clooney is not
only handsome, but also (. . .)” the examinee is supposed to complete it with the
right answer according the parallelism rule, which is an adjective only (e.g., cute).
Table 2 Correlation
coefficient of the grammar
accuracy test
V3 V4
V3 Pearson correlation 1 0.885**
Sig. (2-tailed) 0.000
N 20 20
V4 Pearson correlation 0.885**
1
Sig. (2-tailed) 0.000
N 20 20
**Correlation is significant at the 0.01 level (2-tailed)
Note: Retrieved from Developing the Pre-service EFL Teachers’
Both Grammatical Accuracy and Constructivist Teaching Skills
for Pedagogical Grammar by Ibrahim (2015)
476 M. Ibrahim
If the examinee answered “he is cute”, this would be counted as a wrong answer.
The five sentences measured five different grammatical skills that are also in the
written test. In another question, the examiner would say “never. . .” and the
examinee would be expected to produce a sentence with inversion (e.g., have I
been to Jordan). Please refer to Appendix 3 for the five oral questions.
The Post-test was made up of one test only: The oral one since it aimed at finding
out the progress learners had made in activating their inert knowledge. The oral test
had similar scope with the same specifications, but comprising different test items
to avoid the effect of the participants’ memory on their answers. Oral tests in the
pretest and posttest were audio-recoded for further analysis and grading.
3.3 Treatment
After analysing pretest results, participants were found out to have a high level of
grammar declarative knowledge, but a poor level of procedural knowledge. That is,
their mean score in the written test was 16.47/20 (with an SD of 1.66) while the
mean score in the oral test was 8.14/20 (with an SD of 1.74).
These findings confirm the enormous gap between learners’ inert and activated
types of knowledge participants had. The study adopted the learning theories and
models mentioned in the literature review section above that confirm that declar-
ative knowledge can become procedural through guided practice upon detecting
points of weakness. The treatment focused on some rules of grammar that turned
out to be inert like tense and aspect in English, parallelism, subject–verb agreement,
and inverted negative structures. These rules were especially selected because
qualitative examination of the oral test revealed they were inert. In other words,
learners answered the questions in the written test correctly but couldn’t produce
correct oral output in the same rule. Instruction was basically guided activities that
were form-focused to ensure that participants practiced these structures intensively
in their oral production. Treatment lasted for 36 h over the period of 6 weeks. As
mentioned, the treatment adopted the interventionist approach where mediation is
highly standardized and recurrently adjusted according to the responsiveness of the
learners. The interventionist approach required teachers to frequently assess
learners’ development in light of the objectives set. In this sense, the treatment
utilized learners’ output as measurements to their oral grammar acquisition.
4 Results and Discussion
Participants’ mean scores in the oral post-test rose dramatically. Mean score for the
oral test increased from 8.14/20 in the pre-test to 15.6/20 in the post-test. This
clearly illustrates that the gap between learners’ inert knowledge and activated
knowledge has been drastically narrowed given that the written test mean score was
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 477
16.47/20. T-test was calculated to find the difference between mean scores in the
pre-test and the post-test. T value was found to be 12.91 with a power of signifi-
cance of 0.0001, which is highly significant at p  0.05. This shows the significant
effect that form-focused instruction, directed by DA, has had on the oral grammat-
ical accuracy of the sample (Table 3).
To sum up, results of the quantitative measures reveal two important findings.
First, instruction that was directed by dynamic assessment had a positive effect on
learners’ active knowledge in that their posttest scores increased dramatically from
pretest ones, which confirms the effectiveness of this form of instruction that
depended mainly on investigating areas of inert knowledge before launching in
instruction to make treatment more effective. Second, this form of instruction
helped activate learners’ inert grammar to be fully acquired in their interlanguage
(IL) system instead of being only memorized in their short-term memory. In this
sense, dynamic assessment can be concluded to help steer instruction of grammar to
be more effective in terms of full acquisition rather than memorization only.
Although Larsen-Freeman’s concept of grammaring is being popular in the
teaching practices day after the other. Research has done very little to contribute
on how grammaring can be achieved in communicative classrooms. This challenge
is being emphasized when knowing that research confirms that the Grammar
Translation Method (GTM) is still being heavily used in the Arab classrooms and
in many classrooms around the world (Assalahi, 2013), which means more efforts
need to be exerted to find real solutions. This explains also why many Arab learners
at university levels can hardly use EFL in communication although their EFL test
scores are exceptionally high. The finding of this study that DA can be used to
enhance learning in general agrees with a huge line of studies such as Lantolf and
Poehner (2004), Lantolf (2009), Lidz and Gindis (2003) and Poehner (2008), which
all confirm the positive effect of DA on EFL learning outcomes.
There are two major contributions this study adds to the literature. First, most of
the studies mentioned above utilize DA to enhance learning through investigating
the effectiveness of the teaching/learning process in light of the objectives already
set. However, the current study takes DA a step further. The current study proposes
a technique by which DA sets learning objectives and determines and how to meet
them. It uses DA as a tool for ensuring acquisition of language, not only memoriz-
ing it, which is the ultimate goal of language classes. Second, while most studies in
the literature adopt an interactionist approach for apply DA in their classrooms, the
current study proposes a feasible framework to apply an interventionist methodol-
ogy that can be used to ensure proper L2 acquisition based on validly diagnosing
acquisition problems in the classroom.
Table 3 Paired sample T-test for the pre-post test scores
Treatment N Mean Std. deviation
Paired differences
Mean
Std. error
of the mean t value
Sig.
( p-value)
Post 21 15.57 3.03 7.43 2.57 12.906 0.0001
Pre 21 8.14 1.74
478 M. Ibrahim
In this sense, this is also diagnostic assessment. Harding, Alderson, and Brunfaut
(2015) assert that diagnostic assessment is necessary for ensuring acquisition.
However, their study focused on a different domain: Namely reading. Although
grammar is pretty related, the steps proposed in the current study are different from
those prescribed by Harding et al. (2015), which suggests that specific precautions
should be considered when dealing with different language skills. The diagnostic
process comprises four stages in Harding et al. (2015) and it tends to be
interactionist; they start with observation, initial assessment, hypothesis checking,
and finally, decision making. Nevertheless, the current study proposes an interven-
tionist approach that uses formal assessment in written and in oral forms to
investigate whether certain aspects of language have been acquired or memorized
only. Moreover, in this kind of intervention, making hypotheses is of limited value.
Lantolf and Poehner (2004) also investigated L2 acquisition using DA. Although
their investigation is rather comprehensive on language development as a whole, it
relies heavily on self-meditation. As advocates for the interactionist approach, they
hardly used interventionist techniques to provide solid evidence on the develop-
ment of language in their learners’ IL systems. This is where the current study adds
value. It does not only provide empirical evidence for development, but also sheds
light on the specific problems instruction needs to solve using the insights gained
from the DA procedures (i.e., oral and written tests). In this context, learners’
meditation will not have a significant contribution because they can hardly decide
on what they have acquired or not without the data taken from the tests.
When teaching grammar, teachers shouldn’t be concerned only with how many
correct questions learners can answer. They should be also more concerned about
learners’ grammatical competence in terms of language production. Oftentimes,
placement tests require learners to answer a set of multiple choice items to inves-
tigate learners’ language, and then, results determine not only learners’ levels, but
also the overall language skills they should be having to be able to participate
actively in their language classes and do assignments. The problem with these
placement tests is that they measure learners’ language knowledge ignoring the fact
that a considerable portion of this knowledge might be inert! This can be treated
through DA approaches designed to spot inert areas and activate them. This also
agrees with Hidri (2014) in that the need to develop the current perspective of
assessment is becoming more urgent day after the other.
5 Conclusion
There are many levels for the implications of this study. First, methodological
implications start with developing EFL teachers’ awareness towards the inert
knowledge problem and how it can be remedied. Before teaching a new grammar
point, teachers need to make sure that the previous grammar was fully acquired, not
only memorized. This can be achieved by simple oral tasks instead of written
discrete test items. Also, allowing enough time and space inside the classroom for
practice where learners apply the rule in communication settings is very important.
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 479
The problem is that some teachers only focus on making students answer questions
about the newly-learnt focus without paying attention to the importance of practice
activities that allow this focus to be integrated into the learners’ Learning Acqui-
sition Device (LAD).
Second, there are important implications on the pedagogical level. When design-
ing learning objectives and course requirements for any EFL programs, stake holders
need to put into consideration the fact that learners’ ability to answer a set of discrete
test items does not necessarily mean they have acquired these language foci. There-
fore, placement tests and the whole decision making process need to be modified to
make sure learners achieve the learning outcomes as they should be. Furthermore,
traditional assessment concepts have a dangerous washback effect that forces
teachers and learners to focus on memorizing rather than acquisition.
Third, research needs to pay more attention to the whole structure of assessing
grammar and vocabulary in institutional tests. Research needs to propose another
alternative method that can be easily adopted and that can assess procedural and
declarative knowledge. This way, washback is likely to force teachers and learners
to focus on practice in the classroom to achieve acquisition.
It is important to mention that this study has limitations too. The most serious
limitation is that the study findings can hardly be utilized unless learners’ already
have some kind of linguistic knowledge. That is, it can be used in a later stage of
instruction, just in case it is needed, but it will be useless to use it before making
sure learners already have linguistic knowledge.
The literature now has surpassed the phase when the question whether or not to
teach grammar was a matter of doubt. However, prescriptions on how to develop
declarative knowledge into procedural knowledge are too few in spite of the belief
that practice can change the former in to the latter. Through DA, the EFL teacher
can diagnose areas of inert knowledge and, with guided practice, this knowledge
can transform into activated knowledge. Such prescriptions are essential especially
at academic environments where the Grammar Translation Method, or other tradi-
tional teaching methods are still used. Teachers and program designers need to
update their perspective of grammar assessment. The traditional grammar tests that
test declarative knowledge, rather than procedural knowledge can no longer serve
the emerging notion of grammaring. Assessment should extend to include the
on-going process of learning as well as the product. Then, when it comes to
assessing the product, assessment should be focused on procedural knowledge,
not only memorized rules.
The study recommends program designers to formally investigate the gap
between inert and active knowledge in all areas of language, especially grammar.
This means that objectives of language courses should be flexible to change based
on learners’ response to instruction. Teachers and program designers should con-
tinuously ask themselves while progressing in their EFL/ESL courses, “How much
have my learners acquired so far?”. The answer needs to be substantiated with
empirical evidence from valid DA tools, not only from learners’ meditation. Also,
the study recommends considering this gap of knowledge in placement tests since
they only measure the memorized language, but they hardly pay attention to
acquired knowledge.
480 M. Ibrahim
Appendix 1
Question Grammatical aspect measured
1 Subject Verb Agreement and Relative Clauses
2 Adverbial Clauses
3 Prepositional Phrases
4 Subject Verb Inversion Because of AP
5 Reduction in Relative Clauses (present participles)
6 The Use of Logical Connectors
7 Appositives
8 Reduction in Relative Clauses (past participles)
9 Superlatives and Adjectival Clause
10 Relative Clauses
11 Logical Connectors
12 Relative Clauses
13 Parallelism
14 Reduction in Relative Clauses and Dangling Modifications
15 Subject Verb Inversion Because of PP
16 Parallelism
17 Countable and Uncountable Nouns
18 Articles
19 Adverbial Phrases
20 Parallelism
21 Subject Verb Inversion Because of Negation
22 Articles
23 Adjectival Phrases
24 Passive Voice
25 Articles
26 Subject Verb Agreement in case of inversion
27 Tense and Aspect
28 Adverbial Phrases
29 Articles
30 Countable and Uncountable Nouns
31 Dangling Modification
32 Adjectival Phrase
33 If Conditional
34 Inversion with Negation
35 Subject Verb Agreement
36 Tense and Aspect
37 Relative Clauses
38 Parallelism
39 Adverbial Phrases
40 Relative Pronouns
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 481
Appendix 2
Grammar Accuracy Test1
1 ………… range in color from pale yellow to bright orange.
(A) Canaries
(B) Canaries which
(C) That canaries
(D) Canaries that are
2 Carnivorous plants…………….insects to obtain nitrogen.
(A) are generally trapped
(B) trap generally
(C) are trapped generally
(D) generally trap
3 A federal type of governments results in………..
(A) a vertical distribution of power
(B) power is distributed vertically
(C) vertically distributed
(D) the distribution of power is vertical
4 Featured at the Henry Ford Museum…………of antique cars dating from 1865
(A) is an exhibit
(B) An exhibit
(C) An exhibit is
(D) which is an exhibit
5 Evidence suggests that half of operations…………bypass a surgery and may be unnecessary.
(A) they involve
(B) involve
(C) involving
(D) which they involve
6 ……………..a tornado spins in a counter clockwise in the northern hemisphere, it spins
in the opposite direction in the southern hemisphere.
(A) However
(B) Because of
(C) Although
(D) That
7 The Caldecott Medal, …………for the best children's picture book, is awarded each January.
(A) is a prize which
(B) which prize
(C) which is a prize
(D) is a prize
1
Adapted from Philips, D. (2001) Longman computer course for the TOEFL test: Preparation
for the computer and paper tests. New York: Longman (p. 331-337). The test is modified by the
Ibrahim (2015)
482 M. Ibrahim
Time: 25 minutes
Part 1- Directions:
(A) sports are involved.
(B) involved in sports
(C) they are involved in sports
(D) sports involve them
9 The Wilmington Oilfield in Long Beach, California, is one of ……….oil fields in
the continental United States.
(A) productive
(B) the most productive
(C) most are productive
(D) productivity
10 Thunder occurs as………through air, causing the heated air to expand and collide with layers of
cooler air.
(A) an electrical charge
(B) passes and electrical charge
(C) the passing of an electrical charge
(D) an electrical charge passes
11 The population of Houston was ravaged by yellow fever in 1839 ……….in1867.
(A) it happened again
(B) and again
(C) was ravaged again
(D) again once more
12 Researchers have long debated………….Saturn's moon Titan contains
hydrocarbon oceans and lakes.
(A) over it
(B) whether the
(C) whether over
(D) whether
13 According to Bernoulli's principle, the higher the speed of a fluid gas,………. the pressure.
(A) it will be lower
(B) lower than the
(C) the lower
(D) lower it is
14 The flight instructor,………at the air base, said that orders not to fight had been issued.
(A) when interviewed
(B) when he interviewed
(C) when to interview
(D) when interviewing
15 In the northern and central parts of the state of Idaho……………and churning rivers.
(A) majestic mountains are found
(B) are majestic mountain found
(C) are found majestic mountains
(D) finding majestic mountains
8 Sports medicine is a medical specialty that deals with the identification and treatment of injuries to
persons…………….
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 483
Questions 1-15 are incomplete sentences. Beneath each sentence you will see four words or phrases marked (A),
(B), (C), and (D). Choose the ONE word or phrase that best completes the sentence. There is only ONE correct
answer for each question.
Look at the following examples:
Example I:
The president……….the election by a landslide.
(A) won
(B) he won
(C) yesterday
(D) fortunately
The sentence should read, "The president won the election by a landslide." Therefore, you should choose (A)
Example II:
When……..the conference?
(A) the doctor attended.
(B) did the doctor attend.
(C) the doctor will attend.
(D) the doctor's attendance.
The sentence should read, "when did the doctor attend the conference?" Therefore, you should choose (B)
Now begin work on the questions on the next page and your timer will begin to count.
Part 2- Directions:
In question 16-40, each sentences has four underlined words or phrases. The four underlined parts of the sentence
are marked (A), (B), (C), and (D). Identify the ONE underlined word or phrase that must be changed in order for the
sentence to be correct.
Example I:
The sentence should read "The four
strings on the violin are tuned in fifths." Therefore, you should choose B.
Example II:
The
sentenc
e
should read "The research for the book Roots took Alex Haley twelve years." Therefore, you should choose C.
Now Begin work on the questions:
The four string on the violin are tuned in fifths.
A B C D
The research for the book Roots taking Alex Haley twelve years.
A B C D
16 The Internal Revenue Service uses computers to check tax return computations, to determine the
A B
reasonableness of deductions and for verifying the accuracy of reported income.
C D
17 Much fats are composed of one molecule of glycerin combined with three molecules of fatty acids.
A B C D
8
A pearl develops when a tiny grain of sand or some another irritant accidentally enters the shell
A
B
C D
of a pearl oyster.
484 M. Ibrahim
19 In the Milky Way galaxy, the most recent observed supernova appeared in 1604.
A B C
20 Although the name suggests otherwise, the ship known as the Old Ironsides was built of oak and
A B C
and cedar rather than it was built of iron.
21 Never in the history of humanity there have been more people living on this relatively small planet.
A B C D
D
22 The lobster, like many other crustaceans, can cast off a damaging appendage and regenerate a
A B C
new appendage to nearly normal size.
D
23 The main cause of the oceans' tides is the gravitation pull of the moon.
A B C D
24 The curricula of the American public schools are set in individual states; they do not determine by
A B C D
the federal government.
25 The fact that the sophisticated technology has become part of revolution in travel delivery systems
A B C
has not made travel schedule less hectic.
D
26 On the floor of the Pacific Ocean is hundreds of flat topped mountains more than a mile
A B C
beneath sea level.
D
27 Because of the flourish with which John Hancock signed the Declaration of Independence, his
A B
name become synonymous with signature.
C D
28 The community of Bethesda, Maryland, was previous known as Darcy's Store.
A B C D
29 Irvin Berlin wrote "Oh How I Hate to Wake up Early in the Morning" while serving in a U.S. army
A B C
during World War I.
D
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 485
D
30 That water has a very high specific heat means that without a large temperature change, water can
A B
add or lose a large number of heat.
C D
31 Being the most popular dramatist and poet in his era, Macbeth surpassed Shakespeare's expectations.
A B C D
32 It is proving less costly and more profitably for drug-makers to market directly to patients.
A B C D
33 Were the U.S. Army Corps not spending millions of dollars each year replenishing eroding
A B C
beaches, the coastline would have been changing even more rapidly.
34 The economic background of labor legislation will not be mentioned in this course, nor unionism will
A B C
be discussed.
D
35 At least one sample of each of the brands contains measurable amounts of aflatoxin, and there is
A B C
three which exceed the maximum.
D
36 Great technical advances in aerial and satellite photography have been made near the end of
A B C D
the Second World War.
37 Underlying aerodynamics and all other branches of theoretical mechanics are the laws
A B C
of motion who were developed in the seventeenth century.
D
38 Some insects bear a remarkable resemblance to dead twigs, being long, slenderness,
A B C
wingless, and brownish in color.
D
39 A food additive is any chemical that food manufacturers intentional add to their products.
A B C D
40 The first Native Americans to occupy what is now the southwestern United States were
A B C
the Big-Game Hunters, which appeared about 10,000 B.C.
D
Good luck
486 M. Ibrahim
Appendix 3
George Clooney is not only handsome, but also. . .. . .. . .. . .
If I had been a sailor, I. . .. . .. . .. . .. . ...
Never. . .. . .. . .. . .. . ..
Every morning, my father. . .. . .. . .. . .. . .. . .
Being a university student,. . .. . .. . .. . .. . .. . .
References
Anderson, J. (1983). The architecture of cognition. Cambridge: Cambridge University Press.
Assalahi, H. M. (2013). Why is the grammar translation method still alive in the Arab world?
Teachers’ beliefs and its implications for EFL teacher education. Theory and Practice in
Language Studies, 3(4), 589–599.
Canale, M. (1983). From communicative competence to communicative language pedagogy. In
J. C. Richards & R. W. Schmidt (Eds.), Language and communication (pp. 2–27). New York:
Longman.
Canale, M., & Swain, M. (1980). Theoretical bases of communicative approaches to second
language teaching and testing. Applied Linguistics, 1, 1–47.
Canale, M., & Swain, M. (1981). A theoretical framework for communicative competence. In
A. Palmer, P. Groot, & G. Trosper (Eds.), The construct validation of test of communicative
competence (pp. 31–36). Washington, DC: TESOL.
Celce-Murcia, M., & Larsen-Freeman, D. (1999). The grammar book: An ESL/EFL teacher’s
course (2nd ed.). Boston, MA: Heinle and Heinle.
DeKeyser, R. (Ed.). (2007). Practice in a second language. Cambridge: Cambridge University
Press.
Harding, L., Alderson, C., & Brunfaut, T. (2015). Diagnostic assessment of reading and listening
in a second or foreign language: Elaborating on diagnostic principles. Language Testing, 32(3),
317–336. https://doi.org/10.1177/0265532214564505.
Hidri, S. (2014). Developing and evaluating a dynamic assessment of listening comprehension in
an EFL context. Language Testing in Asia, 4(4), 2–19. https://doi.org/10.1186/2229-0443-4-4.
Ibrahim, M. (2015) Developing the pre-service EFL teachers’ both grammatical accuracy and
constructivist teaching skills for pedagogical grammar. Unpublished doctoral dissertation,
Alexandria University.
Jing, X. (2011). Applying active learning to grammar teaching for non-native English majors in
EFL class settings. Master’s thesis in the University of Wisconsin-Platteville.
Lantolf, J. P. (2000). Introducing sociocultural theory. In J. P. Lantolf (Ed.), Sociocultural theory
and second language learning (pp. 1–26). Oxford: Oxford University Press.
Lantolf, J. (2009). Dynamic assessment: The dialectic integration of instruction and assessment.
Language Teaching, 42(3), 355–368. https://doi.org/10.1017/S0261444808005569.
Lantolf, J., & Poehner, M. (2004). Dynamic assessment of L2 development: Bringing the past into
the future. Journal of Applied Linguistics, 1(1), 1–49. https://doi.org/10.1558/japl.v1i1.49.
Larsen-Freeman, D. (2003). Teaching language: From grammar to grammaring. Boston, MA:
Thomson Heinle.
Lidz, C. S., & Gindis, B. (2003). Dynamic assessment of the evolving cognitive functions in
children. In A. Kozulin, B. Gindis, V. S. Ageyev, & S. M. Miller (Eds.), Vygotsky’s educa-
tional theory in cultural context (pp. 99–116). Cambridge: Cambridge University Press.
Utilizing Dynamic Assessment to Activate EFL Inert Grammar 487
Limakina, O., & Ryshina-Pankova, M. (2012). Grammar dilemma: Teaching grammar as a
resource for making meaning. The Modern Language Journal, 96(2), 270–289. https://doi.
org/10.1111/j.1540-4781.2012.01333.x.
Luria, A. (1961). Study of the abnormal child. A Journal of Human Behavior, 31(1), 1–16.
McLaughlin, B. (1993). Myths and misconceptions about second language learning: What every
teacher needs to unlearn. Washington, DC: Center for Applied Linguistics.
Navarro, E., & Calero, M. (2009). Estimation of cognitive plasticity in old adults using dynamic
assessment techniques. Journal of Cognitive Education and Psychology, 8(1), 38–51. https://
doi.org/10.1891/1945-8959.8.1.38.
Norris, J., & Ortega, L. (2000). Effectiveness of L2 Instruction: A research synthesis and
quantitative meta-analysis. Language Learning, 50, 417–428.
Philips, D. (2001). Longman computer course for the TOEFL test: Preparation for the computer
and paper tests. New York: Longman.
Poehner, M. (2008). Dynamic assessment: A Vygotskian approach to understanding and promot-
ing second language development. Berlin: Springer.
Poehner, M., & Lantolf, J. (2005). Dynamic assessment in the language classroom. Language
Teaching Research, 9(3), 233–265.
Thorne, S. (2005). Epistemology, politics, and ethics in sociocultural theory. The Modern Lan-
guage Journal, 89(3), 393–409.
Thouësny, S. (2010). Assessing second language learners’ written texts: An interventionist and
interactionist approach to dynamic assessment (pp. 3517–3522). In Proceedings of the World
Conference on Education, Multimedia, Hypermedia and telecommunications (ED-Media),
28 June 2010–2 July 2010, Toronto, Canada.
Vygotsky, L. S. (1978). Mind in society: The development of higher psychological processes.
Cambridge, MA: Harvard University Press.
Wang, F. (2010). The necessity of grammar teaching. English Language Teaching, 3(2), 78–81.
Whitehead, A. N. (1929). The aims of education. New York: MacMillan.
Yu, Q. (2008). On the importance of grammar teaching. Vocational Education, 25, 110–111.
488 M. Ibrahim
Analyzing the Impact of Formal Assessments
on EFL Learner’s Use of Language Learning
Strategies
Misbah Rosheen Khan and Misbah Afsheen Khan
Abstract This study aims to highlight the way assessments affect students in
making choices between various learning strategies. For the purpose, a sample of
Saudi EFL students is selected randomly comprising 60 (female) students of the
English Language Institute (ELI), King Abdulaziz University (KAU), Jeddah. The
survey is conducted using Oxford’s (1996) ‘The Strategy Inventory for Language
Learning’. It is an exploratory–quantitative–interpretive mixed-research design
(Grotjahn, 1987) to keep up with the maximum accuracy of the results; leading to
the affirmation that assessment do affect the students’ choice of Language Learning
Strategies. The collected data is analysed both statistically and descriptively. The
results show the frequency of the use of Language learning strategies made by the
EFL learners which is moderate to low in regular instructional week. However, a
slight increase is witnessed during pre-assessment week. Regarding the individual
strategies; cognitive, metacognitive and social strategies rank higher in both,
regular and pre-assessment weeks. Thus, formal assessment is found shaping
learners’ attitudes towards learning as well as affecting their choice of learning
strategies. In addition, students become autonomous language learners to perform
well in their formal assessments due to the right use of LLSs. To this end,
pedagogical implications and recommendations have been listed stressing upon
strategy training for the students and the significant role of the teacher in facilitating
the selection and use of strategies by the learners.
Keywords Formal assessments • Language learning strategies • EFL learners •
Autonomous language learners
M.R. Khan (*) • M.A. Khan
English Language Institute, King Abdulaziz University, Jeddah, Saudi Arabia
e-mail: edmrk@leeds.ac.uk; misbahkhankau@gmail.com
© Springer International Publishing AG 2018
S. Hidri (ed.), Revisiting the Assessment of Second Language Abilities: From
Theory to Practice, Second Language Learning and Teaching,
https://doi.org/10.1007/978-3-319-62884-4_24
489
1 Introduction
Second language learning is a complex process involving interplay of number of
factors facilitating or impeding the successful acquisition. When it comes to learn a
language in a foreign context beyond the ‘cultural and linguistic milieu of the
second language’ (Brown, 2000, p. 3), the process becomes more challenging, since
the target language input is less likely to be available outside the classroom context.
Same is the case with English language learning in Saudi Arabia. With Arabic as
the official language, English language is rarely used in everyday life. Though, a
change has been witnessed among the Saudi youth who has realized the importance
of English language and is seen to be exerting efforts to learn it. This, however, does
not rule out the impeding effects of the context in which English is learnt. Thus,
Saudi EFL learners are exposed to the language only in the classroom setting and
the chances to use it in real life situations are almost minimal. Eventually, despite of
earnest efforts, many learners fail to acquire the communicative competence and
thus, remain less proficient.
The ELI of KAU offers an intensive English language course as a compulsory
part of Foundation year program like many other Saudi universities. The course
consists of seven teaching weeks and offers four levels: Beginners, Elementary,
Pre-intermediate and Intermediate. It is compulsory for students to sit a placement
test which is conducted to enter each of these levels based on their obtained Test
Scores. Successful accomplishment of all these levels is a key to finish the Foun-
dation Year English Course. The aim of the course is to help learners to achieve an
overall English language proficiency. Furthermore, it leads a beginner to be an
independent user of language; defined as B1 level on the Common European
Framework of Reference for Languages (CEFR). Thus, its main concern is to
make students develop discourse skills, express ideas, and help one another in
solving problems in situations where they meet unpredictable and infinite language
patterns.
Hence, at the ELI, assessment is a crucial component of English language
program either formative or summative. The successful achievement of the course’
objectives heavily rely on the scores the students gain at the end of each module.
From Elementary to Pre-intermediate level, successful completion of each level
gives students the necessary credits to meet the Foundation Year English require-
ment. Moreover, the successful accomplishment of English Language course is
prerequisite to get admission in the Graduate courses in the university.
The assessment tends to be quite stressful experience for majority of students.
Along with their pros and cons, assessments tend to affect both learners and their
ways to learn. According to Boud and Falchikov (2007), assessment is possibly the
single biggest influence on how students approach their learning. To this end,
summative or formal assessments are found to be more intimidating process for
the Foundation year students. As researched by Huberty (2010), the assessment’s
stress may have “significant negative” effects on the students’ ability to perform at
optimal level; with some students failing the assessment despite knowing the
490 M.R. Khan and M.A. Khan
material. Similarly, Gass, Behney, and Plonsky (2013) commented ‘testing is one
particular situation where we expect to find higher levels of anxiety’. Consequently,
students need to adopt learning strategies to overcome anxiety connected to formal
assessments and thereby, give better performance as ‘more anxious the learners, the
poorer their performance’ (Gass et al., 2013, p. 463) For the ESL/EFL learners,
effective use of language learning strategies (LLSs) facilitate learners in the lan-
guage learning process and ultimately enable them to secure good grades in exams.
There is no doubt a huge number of studies on second language learning
strategies. However, to the best knowledge of the researcher, there isn’t a single
research work which explores the impact of assessments on the choice and use of
LLSs by the EFL learners. Thus, there is a dire need to fill this gap in the repertoire
of knowledge and to enumerate the practical ways to help learners chose effective
strategies to prepare and perform well in their formal assessments.
2 Theoretical Background
A major change has been witnessed in the field of language education, both as a
second and foreign language learning. The focus has been moved from the teacher
to the learners in terms of curriculum, language tasks, classroom instructions and
individual differences to name few. Studies have been conducted to comprehend
the process through which the learners learn the target language, learners’ language
needs, and their uniqueness as individuals. Thus, number of theories have been
postulated to facilitate language learning by identifying the factors which help L2
learners in acquiring the language competence and enable them to use the language
effectively. One such development is the exploration of the ways EFL/ESL learners
learn the language. These ways are termed as Language Learning Strategies
(LLSs). “Language learning strategies are the specific actions taken by the learner
to make learning easier, faster, more enjoyable, more self-directed, more effective
and more transferable to new situations” (Oxford, 1990, p. 8). Studies have shown
that the language learners who know and employ language learning strategies
become proficient users of the target language. It can also be stated that good
learners are found to be using effective language learning strategies to enhance their
learning. Besides facilitating language learning, the strategies make the learners
autonomous, self-motivated, and enable them to regulate their learning. When
students take more responsibility of their learning, more learning occurs, thereby
making both teachers and learners feel more successful (Oxford, 1990).
Language learning strategies are necessary to develop communicative compe-
tence because they provide learners with active and self-directed guidance (Peng,
2001). Moreover, a significant correlation is found between the use of LLSs and
learners’ L2 achievement (Chang & Liu, 2013; Chang, Liu, & Lee, 2007; Cheng &
Chang, 2015; Riazi, 2007; Wu, 2010). All these studies have found that the learners
who used language learning strategies demonstrated higher achievement. LLSs
learning are equally important for the English language teachers who can observe
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 491
and assess their students’ learning styles and strategies. The understanding of
learning strategies used by the good language learners enable teachers to modify
their teaching to address the needs of less proficient learners. Chamot and Kuper
(1989) stress that language learning strategies should be taught to the unsuccessful
L2 learners. Hence, if the learners are aware of the language learning strategies,
their selection and the use, according to the learning context and their proficiency
level, they become expert L2 users and produce good results when assessed.
A major contribution in the field of LLS has been made by Oxford (1990) in the
form of taxonomy of language learning strategies. She has divided LLS into two
main groups; direct and indirect. Both classes are further divided into six types.
Oxford’s taxonomy of language learning strategies is given below:
• Direct strategies
– Memory
• Creating Mental Linkage
• Applying images and sounds
• Reviewing well
• Employing action
– Cognitive
• Practicing
• Receiving and sending messages strategies
• Analyzing and reasoning
• Creating structure for input and output
– Compensation strategies
• Guessing intelligently
• Overcoming limitations in speaking and writing
• Indirect strategies
– Metacognitive strategies
• Centering your learning
• Arranging and planning your learning
• Evaluating your learning
– Affective strategies
• Lowering your anxiety
• Encouraging yourself
• Taking your emotional temperature
– Social strategies
• Asking questions
• Cooperating with others
• Emphasizing with others
492 M.R. Khan and M.A. Khan
The salient features of each strategy and its role in language learning is briefly
described below:
• Memory Strategies are based on very basic principles, like arranging things in
order, making associations and reviewing. As stated by Oxford (1990), “For the
purpose of learning a new language, the arrangement and associations must be
personally meaningful to the leaner, and the material to be reviewed must have
significance”. Despite of the contribution memory strategies make towards
language learning, some studies have shown that these strategies are rarely
used by the learners (Alhaisoni, 2012; Razak, Ismail, Aziz, & Babikkoi,
2012). The possible reason behind this finding is the lack of awareness among
the learners regarding how often they actually employ memory strategies.
• Cognitive Strategies, with all the variety, are unified by a common function:
Manipulation or transformation of the target language by the learner. Strategies
to practice language are the most important ones among the other types of
cognitive strategies. It’s quite difficult for each learner to have sufficient practice
during the lessons and therefore, extra practice outside the classroom setting is
indispensable to develop target language proficiency.
• Compensation Strategies include using a wide variety of linguistics and
non-linguistic clues to use the target language for either comprehension or
production despite of limitations (Oxford, 1990). Good language learners are
found to be using compensation strategies in understanding and production of
new language and become proficient users of the target language due to the
continual practice. Learners skilled in such strategies sometimes communicate
better than learners who know many more target language words and structures
(Oxford, 1990).
• Metacognitive Strategies are actions which go beyond purely cognitive devices,
and which provide a way for learners to coordinate their learning process
(Oxford, 1990). Use of metacognitive strategies enable learners to take respon-
sibility of their learning, plan and seek opportunities to practice new language,
know their learning styles, identify the areas to improve and evaluate their
learning. In an EFL context, learners crucially need to seek opportunities to
practice the language input received in the classroom due to the fact that English
language is rarely used in everyday life and chances to have interaction with
English language users are very low.
• Affective Strategies play significant role in facilitating language learning. Lan-
guage learning is a complex phenomenon and is affected by numerous factors.
As a result of this interplay of cognitive, psychological, and social factors, the
learning process may get hampered. The affective side of the learner is probably
one of the biggest influences on language learning success or failure (Oxford,
1990). This can be resolved by employing affective strategies which enable the
learners to develop positive attitude towards learning and to control their
emotions. Thus, using appropriate affective strategies, EFL learners can lower
their language earning anxiety, encourage themselves, and make language
learning process an effective and enjoyable experience by developing positive
attitudes and emotions towards learning the new language
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 493
• Social Strategies involves interaction with other people which can be in the form
of asking questions, cooperating with peers and proficient users of language and
becoming aware of other’s thoughts and feelings through empathizing. Thus, use
of affective strategies is essential in language learning process.
The choice of language learning strategies is affected by number of factors, like,
degree of awareness, level of proficiency, task requirements, teacher expectations,
age, sex, nationality/ethnicity, general learning style, personality traits, motivation
level, and purpose of learning the language (Oxford, 1990). As proved in a research
study (Dearaúz, 2009), language learning strategies are teachable and format of
instruction has to be direct and integrated. The strategy use instruction includes
provision of hands-on practices and reinforcement opportunities in strategy use.
Besides improving learners’ language performance and proficiency, it provides a
meaningful way to focus the teaching efforts in facilitating the acquisition of a
foreign language.
In order to assess the use and effectiveness of language learning strategies,
Oxford (1990) developed a questionnaire which is named as “Strategy Inventory
for Language learning (SILL)”. SILL has been extensively used by the researchers
in many different global contexts (Alhaisoni, 2012; Al-Otaibi, 2004; Cheng &
Chang, 2015; Haken, Aydin, & Bulent, 2015; Khalil, 2005; Razak et al., 2012) to
analyze the use of language learning strategies and their impact on L2 achievement
with regards to proficiency level of learners, gender and academic level. However,
there is dearth of work in the area in Saudi context. There are only two studies
(Alhaisoni, 2012; Al-Otaibi, 2004) which have investigated the use of LLS by
Saudi EFL learners. However, none of these addressed the potential effects of
formal assessments on the choice and use of EFL learners. In other words, the
study is needed to know whether the Saudi EFL learners employ LLSs to prepare
well for their formal assessments.
2.1 Rationale of the Study
The present study intends to work on the following research objectives:
1. To find overall frequency of LLS adopted during the regular instruction and the
pre-assessment week;
2. to know the effects of exams on the choice of LLS and
3. to compare the frequency of LLS adopted during regular instruction and
pre-assessment week.
Therefore, keeping the background of the study in view, it is to, firstly, see the
overall frequency of LLS used by the Foundation year Saudi EFL learners during
the regular instructional week and the week before the formal assessments. Sec-
ondly, it tries to analyse the effects of formal assessments on the EFL learners’
choice of LLS. Thirdly, the present study intends to identify the difference between
494 M.R. Khan and M.A. Khan
frequency of Language learning strategies used in regular instructional and
pre-assessment days respectively.
3 Method
The participants of the study were 60 foundation year (female) students studying in
pre-Intermediate 103 level at ELI of KAU. Mixed method research design was used
which included both quantitative and qualitative pattern. As stated by Dornyei
(2007), a mixed-method research design may help researchers to overcome the
weaknesses in each qualitative and quantitative paradigm, and, therefore, can
strengthen the impact of their research outcomes. Hence, this study is exploratory–
quantitative–interpretive mixed research design (Grotjahn, 1987).
The quantitative data was collected through “The Strategy Inventory for Lan-
guage Learning” (Oxford, 1996). Necessary changes were made in the question-
naire to suit the purpose of the study, the context and the competence level of the
participants. The questionnaires were distributed to the pre-intermediate students
during the regular English Language class under the supervision of the class
teachers. Besides assuring the confidentiality of their responses, the students were
briefed about the questionnaire.
As the study was intended to analyze the impact of formal assessment on the
choice of language learning strategies, the survey was conducted twice in Module
1. The first set of numerical data was collected during week 2 of Module 1, which
was the regular teaching week. The second set of data was collected in week 4, 2
days before the Mid-Module Exam was scheduled to take place. As the name states,
Mid-Module exam (CBT—Computer Based Test) is conducted in the middle of an
academic module at ELI and assesses students’ learning in terms of Grammar,
Vocabulary, Reading and Listening. During the exam week, students were advised
to practice and prepare for their upcoming exam. The qualitative data was collected
through the semi-structured interview. The interviewees were ten of the participants
of the study who were selected through random sampling. The interviews were
conducted during week 4 when the students were preparing for their
Mid-Module exam.
4 Results and Discussion
By using SPSS 15.0 version, the data collected through the questionnaires were
statistically analyzed. First, Descriptive Statistics shows the analysis of the quan-
titative data collected during the regular instructional week of the academic module
1. This analysis highlights the frequency of Language learning strategies use by the
EFL learners of Foundation Year during regular instructional week. Second time,
the data collected during the pre-assessment week is analysed descriptively to
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 495
discover the students’ choice of Language learning strategies in order to prepare for
their formal assessments.
As indicated in Table 1, the overall frequency of language learning strategy used
by the participants of the study is moderate to average with metacognitive and
social strategies ranking higher than the rest. Whereas the affective strategies were
the least to be employed by the participants of the study.
Table 2 displays the Descriptive analysis of language learning strategies used by
the participants during the exam week, 2 days prior to their Mid-Module exam. As
seen in the table above, once again the use of LLS by the participants of the study is
moderate to average as measured in the pre-assessment week. Similarly, among the
six categories, metacognitive and social strategies were used more than the rest.
Considering the objectives of the study, Paired Samples Test was used to
compare the two sets of data collected through SILL questionnaire during the
regular instructional week and pre-assessment week. As presented in the table
above, little difference is found between the overall frequencies of language
learning strategies used at different points of time. As far as individual raking of
six LLSs is concerned, cognitive, metacognitive and social strategies still rank
higher than others. A significant correlation can also be seen between the same three
strategies. Moreover, a slight increase can be seen in the frequency of LLSs use
during the assessment week as shown below in Table 3.
The comprehensive data collected through the semi-structured interviews
supplemented the quantitative results by shedding light on the Foundation year
Table 1 Frequency of LLSs use in regular teaching week
N Minimum Maximum Mean Std. deviation
Memory 60 2.22 4.67 3.4389 0.52373
Cognitive 60 2.14 4.50 3.5702 0.59493
Compensation 60 2.17 4.50 3.4444 0.57789
Metacognitive 60 1.89 5.00 3.7000 0.76825
Affective 60 1.67 4.50 3.1833 0.70691
Social 60 1.33 5.00 3.5750 0.89434
Average regular week 60 0.29 0.52 0.4182 0.06129
Valid N 60
Table 2 Frequency of LLSs use in pre-assessment week
N Minimum Maximum Mean Std. deviation Ranking
Memory 60 2.22 4.44 3.2889 0.47854 5
Cognitive 60 2.29 4.50 3.6655 0.56481 3
Compensation 60 2.33 4.33 3.4778 0.48950 4
Metacognitive 60 2.22 5.00 3.9481 0.63367 1
Affective 60 2.00 4.33 3.2167 0.63506 6
Social 60 2.00 5.00 3.8139 0.77149 2
Average 60 0.32 0.52 0.4282 0.04955
Valid N 60
496 M.R. Khan and M.A. Khan
Table
3
Comparison
between
the
frequencies
of
LLSs
used
in
week
1
and
4
Paired
samples
test
Paired
differences
Mean
Std.
deviation
Std.
error
mean
95%
Confidence
interval
of
the
difference
t
df
Sig.
(2-tailed)
Lower
Upper
Pair
1
Memory–Memory
0.15000
0.10396
0.01342
0.12314
0.17686
11.176
59
0.000
Pair
2
Cognitive–Cognitive
0.09524
0.06791
0.00877
0.11278
0.07769
10.863
59
0.000
Pair
3
Compensation–Compensation
0.03333
0.19601
0.02530
0.08397
0.01730
1.317
59
0.193
Pair
4
Metacognitive–Metacognitive
0.24815
0.21782
0.02812
0.30442
0.19188
8.825
59
0.000
Pair
5
Affective–Affective
0.03333
0.78162
0.10091
0.23525
0.16858
0.330
59
0.742
Pair
6
Social–Social
0.23889
0.20439
0.02639
0.29169
0.18609
9.053
59
0.000
Pair
7
Average
regular
week–Average
week
4
0.00998
0.01968
0.00254
0.01506
0.00490
3.928
59
0.000
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 497
EFL learners’ perspective on the choice and use of language learning strategies. So,
the qualitative data was coded according to the themes addressing the research
questions. Hence, the qualitative data exhibits that majority of participants have no
awareness about LLS and their use. Even they are never given any training related
to it. Most of the participants also shared that they practice their English Language
skills (especially writing skill) right before their final assessment, at first, through
various online soft wares and then by taking help from their elder siblings to correct
them as well as guide them. Furthermore, half of the participants claim that their
teachers are of great help both during and after office hours. As far as practicing of
speaking skill (before spoken assessment) is concerned, friends are a great helping
hand for majority of the participants. Though, formal speaking assessment is a
source of anxiety for half of the participants still they do not need any external help
to reduce it as they can overcome it by themselves. Finally, almost all of the
participants expressed that they cannot spare much time to prepare English subject
as they spend most part in preparing the science subjects.
The statistical analyses of the quantitative data describe that how frequently
LLSs are used by the Saudi EFL students studying an intensive English Language
course in Foundation year. Based on the objectives and the research design of the
current study, the first set of data collected during the regular instructional week,
depicts that the learners’ use of is from low to moderate, whereas, cognitive,
metacognitive and social strategies are used more frequently than rest of the
LLSs. This finding is in line with the results found in the studies by Al-Otaibi
(2004) and Alhaisoni (2012). The Saudi EFL learners in both studies were found to
be average users of LLS. In fact, Alhaisoni (2012) found his learners to be less
‘sophisticated language learning strategies users’.
The possible reason behind the average use of LLSs by the foundation year EFL
learners is due to the lack of input-rich environment. This is due to the fact that
Saudi EFL learners are not necessarily required to use English language outside the
classroom setting. Therefore, they do not feel the need to adopt certain ways to
enhance their English language learning and to exert their efforts beyond the level
needed for the successful completion of the course. Another major reason can
possibly be the intensiveness of the English language course itself. As mentioned
earlier, the students are required to pass all four levels in order to meet the entry
requirement of Graduate course they want to join. Both, teachers and students, are
found to be running against time in order to complete the syllabus specified for the
assessments. As a result, it becomes almost impossible for teachers to give explicit
training to the learners regarding LLSs and their effective use to facilitate language
learning and to perform well in their exams. As far as the use of specific strategies is
concerned, cognitive, metacognitive and social strategies are used more frequently
(this finding is parallel to the study conducted by Al-Buainain, 2010; Alhaisoni,
2012; Khalil, 2005; Riazi, 2007 whose participants were found to be using cogni-
tive, metacognitive and social strategies more than memory, compensation and
affective strategies). In general, cognitive strategies are the most popular strategies
with language learners and are essential in learning a new language (Oxford, 1990).
The use of social strategies by the Saudi female students.
498 M.R. Khan and M.A. Khan
It is good to see that the metacognitive strategies are the most frequently used
strategies along with social ones. This finding is consistent with the results reported
by Alhaisoni (2012) in his study conducted in Saudi EFL context, where the
participants were found to be using cognitive, metacognitive and social strategies
more frequently than the other three LLSs. This leads to the fact that the Saudi EFL
learners take the responsibility of their learning and devise the ways to learn the
language. Strategies like organizing, setting goals and objectives, considering the
purpose, and planning for a language tasks, help learners to arrange and plan their
language learning in efficient and effective way (Oxford, 1990). For example, as
shared by the interviewees, they practice for their writing assessment by using a
software which allows them to type in the composition on different topics. More-
over, they seek their teachers’ guidance to get their mistakes in writing corrected.
They also practice speaking skill with their friends prior to final speaking assess-
ment. Seeking opportunities to practice English language becomes more important
in EFL context because of the limited use of the language outside the classroom
setting. Thus, metacognitive strategies are of crucial importance for Saudi EFL
learners to develop communicative competence and to perform well in the formal
assessments.
With regard to the second objective of the current study, a slight increase in the
use of learning strategies by the foundation year EFL learners is evinced. The data
collected in pre-assessment week displays higher frequency of overall use of LLSs
as compared to week 2 which is a regular teaching week. As long as the frequency
of individual strategy use is concerned, Table 2 presents the difference between the
use of cognitive, metacognitive and social strategies in week 2 and week 4. This
shows Saudi EFL students’ choice of learning strategies is not necessarily affected
by the prospect of assessments. In other words, high-frequency use of cognitive,
metacognitive and social strategies is evident prior to formal assessment. The main
reason behind this finding may also be related to the course itself. As discussed
earlier, English language course offered at Foundation year is very intensive.
Besides, students are required to take other subjects of their interest or they wish
to take at graduate level. Thus, without any prep leave or revision time, they sit their
exams and therefore, do not necessarily employ specific learning strategies.
5 Conclusion
Based on the findings, there is a dire need to make EFL learners aware of language
learning strategies and to help them choose the appropriate mix of different
strategies according to their learning styles, context, purpose of learning and
proficiency level. Several studies have found that students who frequently use
LLSs are significantly related to L2 achievement. To this end, teachers should
have clear understanding of Language learning strategies and their effective use by
the learners. This can be achieved through conducting training sessions for English
language teachers. As Dornyei (2001) put forward that teaching learners about
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 499
various learning strategies builds their ‘confidence in their learning abilities’
(p. 97). Another essential task for the teachers is to know their students, their
language needs and their learning styles.
In order to foster overall use of LLSs, training should be integrated into the
curriculum and classroom instructions. Research has proved that EFL learners need
explicit instructions regarding the efficient use of strategies in order to improve
language learning and performance (Cohen, 1998; O’Malley & Chamot, 1995). As
Plonsky’s (2011) study found ‘strategy instruction benefits mainly in case of
reading, speaking, vocabulary and pronunciation’. Teachers should give explicit
and implicit language learning strategies training to the learners. This can be done
by incorporating activities, which focus on and need certain strategies to be
employed, into regular lessons. After assessing students’ proficiency level and
learning styles, teachers should tailor their teaching accordingly. Learners at dif-
ferent levels have different needs in terms of teacher intervention in the learning
process. For low-proficiency learners, the teacher needs to be explicit in developing
declarative and procedural knowledge that helps to heighten understanding of the
‘what’ and ‘how’ of successful language learning (Alhaisoni, 2012).
Teacher’s role as a facilitator is essential in the encouragement of learning
strategy use by the learners, especially before formal assessments. Teachers can
help students to employ strategies, direct and indirect both, to plan and prepare for
their exams. As maintained above, assessments do affect students’ learning and
how they approach it. Thus, intelligent choice of LLSs and appropriate mix of
different types result into better performance. For example, use of memory strate-
gies is more effective when the learner simultaneously uses metacognitive strate-
gies, like paying attention, and affective strategies, like reducing anxiety through
deep breathing (Oxford, 1990). Employing affective strategies is particularly
important prior to assessments as the fear of securing low grades and the appre-
hension connected to exams may overshadow the performance of competent
students. As put by Ellis and Shintani (2014):
‘Awareness—raising activities hold out more promise as they are not designed to change
learners’ ‘actions’ but to encourage them to subject their beliefs about language learning to
critical scrutiny’ (p. 314).
As part of the recommendations is the set of three requirements needed to be
considered in order to hold a strategy training which is based on the precepts of
Sociocultural Theory, as proposed by Ellis and Shintani (2014). These requirements
are presented as under:
(a) The strategy has to be explained ‘Scientifically’. It is not enough to tell learners
to ‘use context to infer the meaning of a word’; they need to be informed of the
exact procedure they will need to follow to execute this strategy as in Nation’s
(1990) proposal for teaching this strategy.
500 M.R. Khan and M.A. Khan
(b) The use of the strategy must be embedded in an authentic instructional task, not
just in decontextualized exercises designed to practice the strategy.
(c) Learners must be encouraged to verbalize their application of the strategy as
they perform the instructional task. (p. 314).
This approach aims ‘to help learner achieve self-regulated use of the strategy’
(Ellis & Shintani, 2014).
Similarly, compensation strategies will enable the less-proficient learners to
improve their performance in their speaking and writing assessments by making
up for the inadequate knowledge of the language. Equally effective is the use of
strategy of “guessing intelligently” to guess the meanings of unknown words in a
reading comprehension or to produce spoken or written expression in the target
language without complete knowledge.
The effective use of metacognitive strategies should also be fostered among the
students to review and practice language prior to assessments. Based on the
interviewees’ responses, there is a lack of planning and preparation on students’
part. Though some of them said to have used some practice activities, yet the
frequency of strategy use is still moderate to low. Furthermore, “confusion about
the overall progress is made worse by academic grading system, which generally
rewards discrete-point rule-learning than communicative competence. These prob-
lems can be ameliorated by using metacognitive strategies of self-monitoring and
self-evaluating” (Cited in Oxford, 1990).
To conclude, this study is an in-depth analysis of Language learning strategies
(LLSs) used by the Saudi (female) EFL learners studying an intensive English
language course in Foundation year program. Moreover, the study attempts to
explore the impact of formal assessments on the choice of LLSs used by the
learners. The statistical analysis of numerical data establishes the fact that language
learning strategies are not frequently used by the Saudi EFL learners as the overall
average ranks between moderate to low. However, a slight increase in the use of
LLSs can be seen in the pre-assessment week. Whereas the ranking of individual
LLSs use remains same in both weeks; week 2 (regular instructional week) and
week 4 (pre-assessment week). In other words, cognitive, metacognitive and social
strategies are more frequently used than memory, compensation and affective
strategies. The quantitative results have been supported and supplemented by the
qualitative data thereby, the possible reasons behind the use of specific strategies by
the EFL learners have been unveiled. Thus, formal assessments are found to
influence the learners’ attitudes towards learning and use of language learning
strategies. Based on results, some practical ways have been enumerated to develop
awareness about the language learning strategies among the language teachers and
the learners. The explicit and implicit strategy training has also been proposed with
special emphasis on the right choice of LLSs to prepare for formal assessments.
Moreover, teacher’s role as a facilitator has been highlighted to foster use of
language learning strategies by the EFL learners.
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 501
References
Al-Buainain, H. (2010). Language learning strategies employed by English majors at Qatar
University: Questions and queries. ASIATIC, 4(2), 92–120.
Alhaisoni, E. (2012). Language learning strategy use of Saudi EFL students in an intensive English
learning context. Asian Social Science, 8(13), 1911–2025. https://doi.org/10.5539/ass.
v8n13p115.
Al-Otaibi, G. (2004). Language learning strategy use among Saudi EFL students and its relation-
ship to language proficiency level, gender and motivation. Unpublished doctoral dissertation,
Indiana University of Pennsylvania.
Boud, D., & Falchikov, N. (2007). Introduction: Assessment for longer term. In D. Boud &
N. Falchikov (Eds.), Rethinking assessment in higher education: Learning for the longer
term (pp. 3–13). New York: Routledge.
Brown, H. D. (2000). Principles of language learning and teaching (4th ed.). New York:
Longman.
Chamot, A. U., & Kupper, L. (1989). Learning strategies in foreign language instruction. Foreign
Language Annals, 22, 13–24.
Chang, C. H., & Liu, H. J. (2013). Language learning strategy use and language learning
motivation of Taiwanese EFL university students. EJFLT: Electronic Journal of Foreign
Language Teaching, 10(2), 196–209.
Chang, C. Y., Liu, S. C., & Lee, Y. N. (2007). A study of language learning strategies used by
college EFL learners in Taiwan. JMGE: Journal of MingDao General Education, 2, 235–258.
Cheng, H. Y., & Chang, N. Y. (2015). Effects of language learning strategies and learners’
motivation on students’ learning achievement. IJELE: International Journal of English Lan-
guage Education, 3(2). https://doi.org/10.5296/ijele.v3i2.8276.
Cohen, A. D. (1998). Strategies in learning and using a second language. New York: Addison
Wesley Longman.
Dearaúz, O. C. (2009). Language learning strategies and its implications for second language
teaching. Revista de Lenguas Modernas, 11, 399–411.
Dornyei, Z. (2001). Motivational strategies in the language classroom. Cambridge: Cambridge
University Press.
Dornyei, Z. (2007). Research methods in applied linguistics. New York: Oxford University Press.
Ellis, R., & Shintani, N. (2014). Exploring language pedagogy through second language acqui-
sition research. London: Routledge.
Gass, S. M., Behney, J., & Plonsky, L. (2013). Second language acquisition: An introductory
course (4th ed.). New York: Routledge.
Grotjahn, R. (1987). On the methodological basis of introspective methods. In C. Faerch &
G. Kasper (Eds.), Introspection in second language research (pp. 54–81). Clevedon: Multi-
lingual Matters.
Haken, K., Aydin, A., & Bulent, A. (2015, February). An investigation of undergraduates’
language learning strategies. 7th World conference on educational sciences, (WCES). Sym-
posium conducted at Novotel Athens Convention Center, Athens, Greece. https://doi.org/10.
1016/j.sbspro.2015.07.388
Huberty, T. (2010). Test performance and anxiety. Principle Leadership, 10(1), 12–16.
Khalil, A. (2005). Assessment of language learning strategies used by Palestinian EFL learners.
Foreign Language Annals, 38(1), 108–119. https://doi.org/10.1111/j.1944-9720.2005.
tb02458.x.
O’Malley, J. M., & Chamot, A. U. (1995). Learning strategies in second language acquisition.
Cambridge: Cambridge University Press.
Oxford, R. L. (1990). Language learning strategies. What every teacher should know? Boston,
MA: Heinle and Heinle.
Oxford, R. L. (1996). Language learning strategies around the world: Cross-cultural perspectives.
Honolulu, HI: University of Hawaii Press.
502 M.R. Khan and M.A. Khan
Peng, I. N. (2001). EFL motivation and strategy use among Taiwanese senior high school learners.
Unpublished master’s thesis, National Taiwan Normal University, Taiwan.
Plonsky, L. (2011). The effectiveness of second language strategy instruction: A meta-analysis.
Language Learning, 61(4), 993–1038.
Razak, N. Z. A., Ismail, F., Aziz, A. A., & Babikkoi, M.A. (2012). Assessing the use of English
language learning strategies among secondary school students in Malaysia. The 8th Interna-
tional conference for specific purpose (LSP) seminar-aligning theoretical knowledge with
professional practice. https://doi.org/10.1016/j.sbspro.2012.11.226
Riazi, A. (2007). Language learning strategy use: Perceptions of female Arab English majors.
Foreign Language Annals, 40(3), 433–440. https://doi.org/10.1111/j.1944-9720.2007.
tb02868.
Wu, K. H. (2010). The relationship between language learners’ anxiety and learning strategy in the
CLT classrooms. International Education Studies, 3, 174–191.
Analyzing the Impact of Formal Assessments on EFL Learner’s Use of. . . 503
