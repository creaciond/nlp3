1.3. STATISTICAL MODELS 19
We have already seen a simple means of modeling p.s; t/: probabilistic synchronous grammar (re-
call that these grammars deﬁne a probability distribution over derivations; this implicitly deﬁnes
a distribution over string pairs since for any string pair we can obtain its probability by summing
up the probabilities of derivations yielding that pair).
Given a probabilistic SCFG or STSG (or data from which to estimate one) we could stop
at this point. However, probabilistic synchronous grammars have been found to make poor trans-
lation models on their own. Instead, generative statistical machine translation models generally
follow the noisy channel approach⁴ from information theory. Returning to the chain rule (Equa-
tion 1.1), if we switch the s and t variables and rearrange terms, then we get
p.s; t/ D p.sjt/p.t/: (1.4)
Substituting into Equation 1.1, we arrive at
p.tjs/ D
p.sjt/p.t/
p.s/
(1.5)
and our objective function becomes
t
D arg max
t
p.sjt/p.t/: (1.6)
Equation 1.5 is known as Bayes’ rule and lies at the heart of the seminal IBM word-based models
[Brown et al., 1993] and early phrase-based models [Koehn et al., 2003, Marcu and Wong, 2002].
It may not be immediately obvious how applying Bayes’ rule can help. After all, for sentences s
and t in two arbitrarily chosen natural languages, there is no reason to think that modeling p.sjt/
is likely to be any more straightforward than modeling p.tjs/, and now there is an additional term,
p.t/. e motivation for this approach lay in its successful use in the ﬁeld of automatic speech
recognition, where a distribution analogous to p.t/ had been modeled using a n-gram language
model. Intuitively, it might seem that modeling two distributions, p.sjt/ and p.t/, instead of
one, p.tjs/, will lead to a weaker model due to the increasing scope for the introduction of errors.
Instead, automatic speech recognition had shown that modeling two components independently
led to a more robust model. In the absence of a reliable model of the translation distribution
(either p.tjs/ or p.sjt/), the addition of a language model component proves essential for ﬁnding
well-formed translations. e language model can be seen as a ﬁlter that weeds out ill-formed
candidate strings.
It is at this point that the formulation of word-based, phrase-based, and syntax-based mod-
els diverges. Let us continue with the syntax-based model by Galley et al. [2006], which as-
⁴Taken literally, the noisy channel approach interprets translation as the task of recovering a corrupted message: the string
s is taken to be a version of t that has been garbled during its transmission across a noisy channel (i.e., its expression in
the source language). Given a source string, s, the task is to recover the most probable target string, t, given what we know
probabilistically about the action of the noisy communication channel and what we know about the probability distribution
of strings in the target language.
20 1. MODELS
sumes that we are using a STSG-based translation grammar⁵ (for presentations of word-based
and phrase-based models, see Brown et al. [1993] and Koehn et al. [2003]).
Applying Bayes’ rule splits our model into two components, a translation model, p.sjt/,
and a language model, p.t/. Galley et al. introduced a third component, p.jt/, which we will
call the tree model. ey do this by splitting the translation model through the application of the
sum rule:
p.sjt/ D
X

p.sj/p.jt/; (1.7)
where  is any target-side derived tree occurring in the set of all derivations in G. (Recall that in
STSG a derived tree is distinct from a derivation tree.) Notice that the term p.jt/ is exactly the
distribution that is modeled by a probabilistic parsing model, such as a PCFG. e motivation
for adding this term is the same as for the introduction of the language model: to increase the
robustness of the overall model by injecting a source of external modeling knowledge. Loosely
speaking, the role of the tree model is to ensure that, all else being equal, translations for which
there are plausible parse trees get higher probabilities than translations for which the parse trees
are ill-formed.
Now, what about the term p.sj/? In general, a single target-side derived tree  can result
from multiple STSG derivations, even if the source sentence s is ﬁxed. If we write D.s; / to
denote the set of all derivations in G for which the source yield is s and the target derived tree is
 then
p.sj/ D
X
d
p.dj/; (1.8)
where d 2 D.s; /. To model the term p.dj/, Galley et al. [2006] make the assumption that a
derivation’s rules are independent of one another. is is the standard independence assumption
that we encountered in Section 1.2.4.
p.sj/ D
1
Z
X
d
Y
r
p.˛jˇ/; (1.9)
where r 2 d and ˛ and ˇ are the source and target components of rule r. Notice the introduction
of the term 1=Z. is is a normalization factor, which is required to ensure that
P
s0 p.s0
j/ D 1
(where s0
is any source sentence in G).
Galley et al.’s presentation goes on to propose two methods for estimating p.˛jˇ/ from
data, one based on relative frequency estimation and the other based on expectation maximization.
In a typical training scenario, estimation is non-trivial since only sentence pairs are provided, not
their derivations. For the remaining details of the model, we refer the reader to the original paper.
e example here demonstrates the distinguishing features of generative syntax-based mod-
els.
⁵Technically, Galley et al. [2006] assume a tree transducer, but the distinction is unimportant here.
1.3. STATISTICAL MODELS 21
1. e probability distribution over translations, p.s; t/, is broken down into a function of dis-
tributions over smaller units. is is primarily to facilitate the reliable estimation of statistics
from data. In the example model, the language model distribution, tree distribution, and
rule probability distribution are much more amenable to machine learning approaches than
the original distribution.
2. e model combines independent distributions that each serve a distinct role in discrimi-
nating between translations: the language model discriminates for ﬂuency, the tree model
for syntactic well-formedness, and the translation model for translational equivalence.
1.3.2 DISCRIMINATIVE MODELS
In contrast to generative approaches, which model p.tjs/ indirectly in terms of p.s; t/, discrim-
inative approaches to statistical machine translation directly model the conditional distribution.
We will focus on the log-linear formulation, which was proposed for use in machine translation
by Och and Ney [2002], and is now the most widely used approach in both phrase-based and
syntax-based approaches.
Let us begin by deﬁning the conditional log-linear model in general.
Deﬁnition 1.26 A conditional log-linear model is a conditional probability distribution of the
form
p.xjy/ D
exp
PM
mD1 mhm.x; y/

P
x0 exp
PM
mD1 mhm.x0; y/
; (1.10)
where h1; : : : ; hM are real-valued functions, often referred to as feature functions, and  D
1; : : : ; M are real-valued constants, often referred to as weights or scaling factors.⁶ e de-
nominator is a sum over every event x0
that is possible given y.
Typically, a conditional log-linear model p.xjy/ is used to approximate a conditional dis-
tribution p.xjy/ that cannot be estimated directly. In principle, the feature functions, h1; : : : ; hM ,
can be any real-valued functions: the task of deﬁning them is the responsibility of the model de-
signer. Once the feature functions have been deﬁned, the scaling factors 1; : : : ; M are deter-
mined by a learning algorithm that attempts to ﬁt the model to data (according to some metric
of model ﬁt).
In statistical machine translation, a log-linear model is used to deﬁne the conditional prob-
ability of a translation, p.tjs/, in terms of the probabilities of individual derivations. Plugging s
⁶We will use the term “scaling factor” to avoid any confusion with rule weights.
22 1. MODELS
and t into Equation 1.10 we get⁷
p.tjs/ D
exp
PM
mD1 mhm.s; t/

P
s0 exp
PM
mD1 mhm.s0; t/
: (1.11)
In a syntax-based system, every possible source and target string is the yield of some derivation
in a synchronous grammar. By marginalizing over derivations, we get
p.tjs/ D
P
d exp
PM
mD1 mhm.d/

P
d0 exp
PM
mD1 mhm.d0/
; (1.12)
where d 2 D.s; t/ and d0
2 D is any derivation with source yield s. In the context of search, the
denominator is ﬁxed and can be dropped (just as we dropped the p.s/ term in the generative
approach).
t
D arg max
t
X
d
exp
M
X
mD1
mhm.d/
!!
; (1.13)
where d 2 D.s; t/. For models based on SCFG or STSG, the number of derivations in D.s; t/ is
exponential with respect to the source sentence length. Calculating the outer sum by enumeration
is therefore intractable, but depending on the choice of features, it may be possible to perform
the calculation in polynomial time using dynamic programming. Unfortunately, this turns out
to be too computationally expensive for models that include a n-gram language model (we will
discuss the computational cost of including a n-gram language model in Chapter 4). In practice,
therefore, the search objective is usually approximated using a single derivation:
d
D arg max
d
exp
M
X
mD1
mhm.d/
!
; (1.14)
where d is any derivation with source yield s. is is sometimes referred to as the max-derivation
objective, in contast to the max-translation objective of Equation 1.13. Mathematically, the max-
derivation objective is an extremely crude approximation to the max-translation objective, taking
the largest element of the sum as an approximation of the whole sum. However, the use of better
(and more computationally expensive) approximations has so far only led to minor improvements
in translation quality in practice.⁸
We can make a further simpliﬁcation: since the exponential function is monotonic,
arg maxx ef .x/
is equivalent to arg maxx f .x/ for any function f .x/. Applying this to Equa-
⁷We will drop the p.tjs/ notation and write p.tjs/ from here on.
⁸See, for example, Blunsom and Osborne [2008] and Arun et al. [2009].
1.3. STATISTICAL MODELS 23
tion 1.14 gives our ﬁnal version of the objective search function:
d
D arg max
d
M
X
mD1
mhm.d/
!
: (1.15)
Having deﬁned the general form of the model, we can turn to the deﬁnition of feature
functions. One possibility is to deﬁne three functions that are analogous to the translation model,
tree model, and language model components of Galley et al. [2006]’s generative model. e ﬁrst
(the translation model) is the log probability of the derivation given the target-side tree:
h1.d/ D log p.dj/ D
X
r2d
log p.˛jˇ/; (1.16)
where  is d’s target-side derived tree and r 2 d is a rule h˛; ˇi. e log of the probability is used
for two reasons: ﬁrst, since the calculations typically involve extremely small numbers, summing
log probabilities is less likely to lead to arithmetic underﬂow than multiplying probabilities; sec-
ond, applying a nonlinear transformation to the probability (of which the logarithm is just one
possibility) makes the function more amenable to learning from data.⁹
e second feature function (the tree model) is the log probability of the target-side tree
given the target-side sentence:
h2.d/ D log p.jt/; (1.17)
where t is the target-side yield of d. e third (the language model) is the log probability of the
target-side sentence:
h3.d/ D log p.t/: (1.18)
Slotting these in to Equation 1.15 gives
d
D arg max
d
1 
X
r2d
log p.˛jˇ/ C 2  log p.jt/ C 3  log p.t/
!
: (1.19)
An important advantage of this model over a generative model is that the scaling factors can be
tuned to increase or decrease the inﬂuence of each component. Scaling of this kind is crucial in
a model where the component distributions are poor approximations of the true distributions
making it likely that some are more reliable than others. A further advantage of the log-linear
model is that there is (at least mathematically) no limit to the number of feature functions. Since
the functions are not required to be statistically independent of each other, the scaling factors can
serve to counter redundancy if there is a strong correlation between feature functions.
⁹See Clark et al. [2014] for a detailed discussion of feature non-linearity in log-linear SMT models.
24 1. MODELS
Dense Features
Most syntax-based models include a small number of core features that are analogous to features
used in phrase-based models. ese are sometimes called dense features (for reasons that will
become clear once we describe sparse features). ey include:
• log p.djds/ and log p.djdt /, the log-probabilities of the derivation conditioned on the
source or target derivation tree. ese are usually the sums of rule-level log-probabilities.
Our earlier example used the target-conditioned form only, but empirically it has been
found that including both improves translation quality;
• log p.tjs/ and log p.sjt/, word-level translation probabilities, in both directions. ese fea-
tures are sometimes called lexical smoothing features since they are estimated from richer
statistics than the rule-level translation probabilities and can compensate for sparsity-related
issues that arise in the estimation of rule probabilities;
• log p.t/, the language model log-probability of the target string;
• jtj, the length of the target string. is allows the model to correct any bias toward producing
overly long or short sentences. For instance, the language model will favor short sentences
over long sentences on average, and a strongly-weighted language model might result in
words being dropped; and
• jdj, the number of rules in the derivation. is allows the model to learn a preference for
using large or small rules. Large rules have the advantage that they include more contextual
information, but usually at the cost of using less reliable statistics.
Other possible dense features include a penalty for rule rarity and the probability of the derived
tree. e dense feature set may also include model-speciﬁc features. We will say more about dense
features when we discuss how rules are learned from data in the next chapter.
Sparse Features
Dense features tend to resemble scores: they provide real-valued answers to questions like: how
probable is the target sentence? How many rules does the derivation use? In contrast, sparse
features answer binary questions about the individual rules of a derivation. ese questions may
be highly speciﬁc: does the target-side of the rule include the word “archival’? Does the source-
side tree fragment include the production NP ! NP PP? In any given derivation, the answer to
most questions of this type is no, so that these features are most eﬃciently represented via sparse
data structures.
Individual sparse features are usually instantiations of a general template feature. For in-
stance, one such template may be:
h.r/ D
(
1; if the target-side of r contains the terminal w
0; otherwise:
1.4. A CLASSIFICATION OF SYNTAX-BASED MODELS 25
From this template, there could be a sparse feature for every possible target-side word w (or
perhaps the most frequent 100 or 1000). As such, a model may include hundreds, thousands, or
even tens of thousands of sparse features.
Unlike dense features, there is not a common core of sparse features (or feature templates).
For a good example of the types of feature used in a real model, we refer the reader to Chiang
et al. [2009]. While the use of sparse features has been shown to signiﬁcantly improve translation
quality for some models, their adoption has been patchy. Frequently cited problems include the
overﬁtting of scaling factors during tuning and a lack of generalization across language pairs. For
a discussion of these issues (in the context of phrase-based statistical machine translation), see
Green et al. [2014].
1.4 A CLASSIFICATION OF SYNTAX-BASED MODELS
Syntax-based models can be classiﬁed along multiple dimensions. Unfortunately, diﬀerent re-
searchers choose to distinguish among models using subtly diﬀerent criteria, sometimes using
the same names in diﬀerent ways. As a result, the terminology in the literature can be inconsis-
tent and confusing. Here we will present our own take on classiﬁcation. ree of the names we
use—string-to-tree, tree-to-string, and tree-to-tree—are widely used (albeit not entirely consis-
tently). e other—string-to-string—is somewhat rarer.¹⁰ Our usage is consistent with much of
the literature, but unavoidably there are cases where our usage disagrees with that of other authors.
Our classiﬁcation rests on whether the source derived tree, the target derived tree, or both,
are intended to resemble a parse tree as observed in the training data. is single (sometimes
fuzzy) criterion is applied regardless of whether source or target syntactic annotation is used in
the rules.¹¹
Of course, there is little to be gained from obsessing over terminology. e main purpose
of this section is to introduce, at a high-level, the diverse family of syntax-based approaches and
to highlight and contrast the distinguishing features of diﬀerent approaches.
1.4.1 STRING-TO-STRING
String-to-string models produce derivations in which there is no requirement or expectation that
the derived trees should resemble linguistic parse trees. Naturally, this category includes models
that use unlabeled hierarchical phrases where there is no representation of linguistic categories.
is category can also include models that do make limited use of syntactic annotation and that
use linguistic parse trees in training. In this case, the crucial distinction is in how the rules are
learned and applied: unlike in the other categories, there is no requirement that a string-to-string
model can reproduce the parse trees observed in the training data.
¹⁰We can recall at least two occasions where this term has arisen in discussion and the researchers involved have failed to agree
on what it means. is, we think, reﬂects the subtle diﬀerences in usage of these names.
¹¹To instead diﬀerentiate models based on the presence of syntactic annotation in the rules, we suggest using the convention
proposed in the Moses Syntax Tutorial [Moses Contributors] and avoiding the X-to-Y names.
26 1. MODELS
String-to-string models are almost universally based on SCFG, usually in a highly restricted
form with limits on the size of a rule and the number of non-terminals it can use. Decoding is
based on parsing the input string.
1.4.2 STRING-TO-TREE
String-to-tree models produce derivations in which the target-side derived trees resemble the
linguistic parse trees observed in the data. Galley et al. [2006]’s model, which we used as our
example in Section 1.3, is an archetypal string-to-tree model: the rules contain tree fragments on
the target side, which are ﬁtted together during decoding and scored according to a model that
takes into account the probability of the resulting derived tree.
String-to-tree models are either based on STSG with tree fragments on the target-side or
alternatively they use a ﬂattened SCFG form (produced by discarding the internal tree structure
of the tree fragments). When SCFG is used, the grammar rules are usually less constrained than
in string-to-string models. Like string-to-string models, decoding is based on parsing the input
string. To make parsing tractable, the grammar rules are often binarized prior to decoding.
1.4.3 TREE-TO-STRING
Tree-to-string models produce derivations in which the source-side derived trees resemble the
linguistic parse trees observed in the data. is conﬁguration makes the model amenable to an
alternative two-phase decoding algorithm: the input string is ﬁrst parsed by the same statistical
parser that was used to parse the training data and the resulting parse tree then becomes the input
to the second phase, which is sometimes called tree parsing. In tree parsing, the source-sides of
rules are matched against the input tree and synchronous derivations are constructed by combining
the rules’ target-sides. Search in the second phase is therefore restricted to derivations in which
the source-side derived tree matches the input tree. is substantially reduces the search space and
in practice tree-to-string decoding is usually much faster than string-to-string or string-to-tree
decoding.
Most tree-to-string models are based on STSG, with tree fragments on the source-side.
Decoding is usually the two-phrase tree decoding approach we have just described, although
sometimes the string parsing and tree parsing phases are performed jointly.
1.4.4 TREE-TO-TREE
Tree-to-tree is the least used of the four classes. e requirement that rules can produce lin-
guistically well-formed derived trees on both sides turns out to be extremely limiting due to the
non-isomorphism of source and target parse trees in practice (which can result from arbitrary
diﬀerences in the annotation applied to the two languages, or may be due to deeper linguistic
diﬀerences). Non-isomorphism can in theory be handled by STSG rules provided it is localized
to the rules’ fragments. Nonetheless, learning rules from data has proven to be problematic in
practice due to the increased data sparsity arising from this constraint.
1.5. A BRIEF HISTORY OF SYNTAX-BASED SMT 27
As a result of the sparsity issue, the pure tree-to-tree approach is usually eschewed in favor
of one of the previous three approaches, but with “soft” or “fuzzy” use of syntax on the string
side (or both sides in the string-to-string case). In soft syntactic approaches, the matching of
linguistic categories at non-terminal substitution sites is no longer a binary decision, but is always
permitted, with the similarity (or dissimilarity) of the non-terminals rewarded (or penalized) by
the statistical model. In line with our previously stated classiﬁcation criterion, if a model, for
example, uses soft syntactic constraints on the source side and hard syntactic constraints on the
target side, then we will classify it as string-to-tree model and not a tree-to-tree model.
Decoding in tree-to-tree models can use the eﬃcient tree-decoding approach of tree-to-
string. e diﬀerence is that the target sides of rules must have matching non-terminals at the
root and substitution site if they are to combine to form a derivation.
1.5 A BRIEF HISTORY OF SYNTAX-BASED SMT
is section provides a brief history of syntax in statistical machine translation, with pointers to
many of the papers that underlie the work presented in this chapter.
e use of syntax in machine translation extends back to its earliest days. Weaver [1955]
speculated about the utility of language structure for translation in his letter proposing the idea of
automatic translation, and a 1957 paper laid out a general approach [Yngve, 2003]. is is unsur-
prising because the use of syntax is obvious: languages have structural diﬀerences that are often
used in their characterizations (e.g., SVO vs. SOV), and the syntactically motivated reorderings
between language pairs are clear to the student of a foreign language (e.g., pre- vs. post-positions
for noun adjective modiﬁers). Subsequent years, however, saw a story unfold that is familiar to
historians of machine translation: the facility that humans have with languages both in isolation
and in translation, and the readiness of ideas for how the mechanize the process of translation,
proved to belie the actual diﬃculties in formulating rules for doing so. Many attempts were made
at describing syntax-based systems (e.g., Kaplan et al. [2003], Kay [1984], Landsbergen [2003]),
but these were encumbered by the same problems hampering direct-transfer systems, namely, the
lack of any means of easily extending them from simple motivating examples to broad-coverage
scenarios.
Everything changed when the statistical approach to machine translation was invented in
the early 1990s. A fundamental idea contributed by this revolution—which provides an answer to
the problem that concluded the previous paragraph—might be stated as simplify your models and
learn them from data. To be clear, this is not to say that complicated methods should be eschewed
entirely, or that there are no insights or linguistically informed constraints that might be useful.
It is better thought of as a rough guide or core lesson that should be considered before moving
further on. Successful approaches to syntax in MT have taken this lesson to heart. And over the
years, as the research community’s understanding has grown, these simple models have in turn
grown in complexity, with the diﬀerence being that they are anchored to data and corpus-based
training methods.
28 1. MODELS
e ﬁrst use of syntax in statistical machine translation was in the Inversion Transduction
Grammar (ITG) formalism [Wu, 1995, 1997]. An ITG is a restricted form of SCFG that has
productions of two types: terminal productions, which pair a single source symbol with a single
target symbol (either of which may be a special “empty” symbol to allow for insertion and dele-
tion); and non-terminal productions, which contain sequences of paired non-terminals. ese
latter productions can be of any length, but the reordering between the source and target side
is required to be monotonic: either straight or inverted. Despite this constraint, the reordering
patterns permitted by ITG have been found to match well with actual reorderings occurring in
parallel texts, at least for some language pairs [Cherry and Lin, 2006, Wellington et al., 2006].
Importantly, this restriction permits ITG grammars to be binarized and thus parsed eﬃciently.
It was in ITG that techniques for intersecting a CFG with a ﬁnite-state machine were ﬁrst ap-
plied to machine translation, enabling integration of a target-side bigram language model with
the parsing-based search procedure [Wu, 1996].
A restricted form of ITG, called BracketingTransduction Grammar (BTG), permits only
a single non-terminal label (typically X). Although motivated with linguistically informed exam-
ples, ITG was typically used in the BTG form; under our classiﬁcation, it can therefore be con-
sidered to be a string-to-string model, with the hierarchical structure induced by the rules serving
mostly to structure the search space.
e ﬁrst successors of ITG sought to incorporate syntax in a more systematic way. Yamada
and Knight [2001] proposed a generative, string-to-tree noisy-channel model that transformed
parse trees in the target language to strings in the source language, using simple insertion, dele-
tion, translation, and reordering operations that worked on the children of a single node at a
time. eir model was ﬁrst proposed for alignment, and then quickly extended to decoding in a
noisy-channel framework that sought to restore the hidden parse tree that was imagined to have
produced (under the model operations) an observed source string [Yamada and Knight, 2002].
is was later extended to explicitly include probabilities from a bilexicalized CFG parsing model
[Charniak, 2001] employed as a syntax-based language model [Charniak et al., 2003].
Formally, these models were SCFGs. By then, the limitations of the formalism were start-
ing to show. e incorporation of syntax had revealed clashes between the target-side structure
and alignments that blocked the capture of many potentially useful rules. A push to work with
larger portions of trees therefore ensued. Poutsma [2000] introduced Data-Oriented Transla-
tion, a STSG-based tree-to-tree model, rooted in data-oriented parsing (DOP) and relying on
manually-aligned tree data. Later, Eisner [2003] presented a sketch of an EM-based algorithm
for learning STSG rules from unaligned data. e tree-to-tree trend has not caught on, for a
number of reasons: the need for parsers in two languages is often problematic, and the brittle-
ness and complexity of tree-pair extraction techniques are further impediments. However, the
approach remains scientiﬁcally interesting. More recent work has found some success in loos-
ening the tree constraints by allowing rules to apply despite mismatched labels, incurring (after
learning) penalties for particular mismatches [Chiang, 2010].
1.5. A BRIEF HISTORY OF SYNTAX-BASED SMT 29
Motivated by a thorough study of phrasal coherence between French and English [Fox,
2002], one of the ﬁrst successful STSG approaches limited itself to trees on the target side only.
GHKM [Galley et al., 2004, 2006] described a process taking an aligned parse tree and sentence
pair and learning STSG fragments, which permitted orderings across all levels of a parse tree.
GHKM has remained one of the most interesting formalisms to work with; the principled forma-
tion of target-language structure, for example, provides a solid foundation for syntax-based lan-
guage modeling. In the meantime, the original word-based approaches to translation had given
way to the intuitions and empirical success of phrase-based approaches [Koehn et al., 2003]. Chi-
ang [2005]’s Hierarchical Phrase-Based Model (Hiero) brought phrase-based approaches into
the sphere of syntax-based translation by extending the standard phrase-pair extraction method to
instead learn hierarchical phrase-based rules—phrase pairs with gaps. Chiang [2007] presented
a major update to the parsing-based decoding approach with the introduction of cube pruning,
which allowed fast, approximate integration of an n-gram language model.
Like BTG, Hiero does not use syntactic annotation. A variant called Syntax-Augmented
Machine Translation (SAMT) soon followed [Zollmann and Venugopal, 2006]. It used cate-
gories from a target-side parse tree to reﬁne the single non-terminal used in Hiero.
An alternative to parsing the input string into a target-side tree representation is the
Syntax-Directed Translation approach, which transforms a source-language tree into a string.
is input tree can take the form of a dependency tree (the Dependency Treelet approach [Quirk
et al., 2005]) or a constituent tree [Huang et al., 2006]. Once learned, rules can be matched against
the source-language tree in a bottom-up parsing-based approach (used by the treelet approach and
also by Liu et al. [2006]), or using a tree parsing framework [Huang et al., 2006].
Tree-to-string translation suﬀers from the fact that the source-language tree is produced
automatically, and is therefore likely to have mistakes. is is the case in many natural language
pipelines. One approach to dealing with this is the Forest-to-String model, which exposes the
parse forest (or a pruned sub-forest) to the decoder [Huang and Chiang, 2007]. is is the same
idea underlying lattice-based approaches to translating the output of automatically-segmented
text, or the output of a speech recognition system.
Finally, another appealing use of syntax is in explicitly modeling the structure of the target-
language sentence. Such eﬀorts had been successfully used in speech recognition [Chelba and
Jelinek, 1998] (although at great computational cost) and it stands to reason that their utility
would increase in light of MT’s reordering problem. e string-to-tree approaches do this im-
plicitly, but the system’s rules are learned from aligned parallel text, and thus the primary role
of the syntax is to learn reorderings between words and phrases in the two languages. e ﬁrst
approach to explicitly incorporate syntax-based language modeling was that of Charniak et al.
[2003], described above, which rescored a simple string-to-tree model with scores from a mono-
lingual bilexicalized parser. A manual analysis suggested the decoder was producing translations
with a decidedly diﬀerent feel, but that were not reﬂected in the BLEU scores. A subsequent
eﬀort examined the use of scores from probabilistic parsers (PCFGs) to rerank the 1,000-best
30 1. MODELS
output of a machine translation system, and found (rather infamously) that such scores could not
even be used to discriminate between MT output and the reference translation Och et al. [2004].
It is possible, however, to train the weights of a monolingual CFG to do much better at this
task [Cherry and Quirk, 2008].
A common complaint about reranking approaches is that n-bests list produced by an n-
gram based decoder represent only a small corner of the search space, and one unlikely to be
chosen by a syntax-based language model. However, attempts to incorporate parsing models into
the decoder search have not been successful. Post and Gildea [2008] added a vanilla PCFG model
in an ITG framework, to no avail, and Schwartz et al. [2011a], who used a syntactic language
model in a phrase-based translation system, similarly found no improvement [Schwartz et al.,
2011b]. In addition, incorporating these models explodes the search space and running time (the
latter reported a 6,000-fold increase), a common issue that is sometimes swept under the rug. In
contrast, syntax-based language models that simply score the syntactic structures produced by a
string-to-tree decoder are more eﬃcient. Collectively, these eﬀorts demonstrated the large gap
between the optimization goals of monolingual syntactic structure when used for parsing (where
the goal is to discover the best structure over a ﬁxed, grammatical string) and language modeling
(where the latent structure is used to try to discriminate among competing strings).
Attempts at syntax-based language modeling have not been wholly without success. Shen
et al. [2010] incorporated a trigram dependency model on top of a Hiero model, improving per-
formance in a large-data Chinese—English translation task. More recently, explicit linguistic
models of diﬃcult problems in German have improved performance in English—German trans-
lation, particularly when evaluated against human judgment [Williams and Koehn, 2014].
C H A P T E R 2
Learning from Parallel Text
e previous chapter provided formal foundations for syntax-based translation models, along with
a taxonomy of formalism types. But where do grammars come from? In this chapter, we describe
the concrete methods that are used to automatically extract grammar rules from aligned parallel
text. We focus our attention on three predominant instantiations of these formalisms: hierarchical
phrase-based grammars (Section 2.2), syntax-augmented grammars (Section 2.3), and GHKM,
which can be used for both string-to-tree and tree-to-string decoding (Section 2.4). As with
the learning of phrase-based models, the extraction of these grammars is laden with a host of
heuristics that have been shown to work in practice. As such, we present these three extraction
methods in light of speciﬁc proposals in the literature that have been shown to be successful over
the years.
2.1 PRELIMINARIES
Extracting grammars from parallel text works on large collections of word-aligned sentence pairs.
ese are pairs of sentence translations that have been automatically aligned with an alignment
tool such as GIZA++ [Och and Ney, 2003] or the Berkeley aligner [Liang et al., 2006]. Some of
the grammars additionally make use of a parse tree over the source or target side.
We begin with an example and some deﬁnitions, which will be used for describing the
extraction procedures in the rest of this chapter. Figure 2.1 contains a visualization of a source
language sentence s (here, German), a target language sentence t (here, and by convention, En-
.
.
.
.
.
.
.
.
.
.
Cologne
.
.
.
.
.
.
.
to
.
.
.
.
.
.
.
gone
.
.
.
.
.
.
.
has
.
.
.
.
.
.
.
Steiger
.
.
.
.
.
.
.
Mr
.
.
.
.
.
.
.
,
.
.
.
.
.
unfortunately
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
gefahren
.
.
.
.
.
.
.
Köln
.
.
.
.
.
.
.
nach
.
.
.
.
.
.
.
Steiger
.
.
.
.
.
.
.
Herr
.
.
.
.
.
.
.
ist
.
.
.
.
.
leider
.
.
.
.
.
.
.
Figure 2.1: A sentence pair with word alignment.
32 2. LEARNING FROM PARALLEL TEXT
glish), and an alignment A between the words in s and t. Both s and t are sequences of words
under a single vocabulary, V. From these, we can deﬁne the concepts of phrases and phrase pairs.
Deﬁnition 2.1 (Phrase). A phrase is a sequence of contiguous words in one language. When
working with a speciﬁc sentence, a phrase is denoted as sj
i for the source language sentence, s,
and tj 0
i0 for the target language, t, for a phrase starting at word i (i0
) and ending with word j (j0
).
For a generic phrase not rooted in a particular sentence, we use s and t.
Deﬁnition 2.2 (Phrase pair). A phrase pair is a pair of phrases, .s; t/.
For any sentence pair, there are O.jsj2
jtj2
/ possible phrase pairs. However, not all of these
will be good phrase pairs. e subset of all phrase pairs that will serve as the basis for building
syntactic rules are those that are consistent with the alignment. e word alignment, A, is a set of
alignment points, which are pairs of indices .k; k0
/, 1  k  jsj and 1  k0
 jtj, each connecting
a single word in s with a single word in t. ere are no restrictions on the alignment; each word
on either side can be aligned to zero or more on the other side. e word alignment is used to
ﬁlter the full set of phrase pairs down to a subset that are consistent with the alignment.
Deﬁnition 2.3 (Consistent alignment point). An alignment point .k; k0
/ is consistent with
respect to a phrase pair .sj
i ; tj 0
i0 / if and only if
i  k  j ^ i0
 k0
 j 0
or
.k < i _ k > j/ ^ .k0
< i0
_ k0
> j 0
/:
In other words, both ends of the alignment point must be either entirely inside or entirely out-
side of the phrase pair. A consistent alignment point that meets the ﬁrst condition is a positive
alignment point, and one that meets the second condition is a negative alignment point.
Deﬁnition 2.4 (Consistent phrase pair). A phrase pair .s; t/ is consistent with respect to an
alignment A if all the alignment points within A are consistent (under Deﬁnition 2.3).
Figure 2.2 contains a two-dimensional visualization of the sentence alignment in Figure 2.1.
It highlights both a consistent and an inconsistent phrase pair. For any aligned sentence pair, there
will be many consistent phrase pairs, and these will overlap.
2.2. HIERARCHICAL PHRASE-BASED GRAMMAR 33
unfortunately
,
Mr.
Steiger
has
gone
to
l
e
i
d
e
r
l
e
i
d
e
r
i
s
t
i
s
t
H
e
r
r
H
e
r
r
S
t
e
i
g
e
r
S
t
e
i
g
e
r
n
a
c
h
n
a
c
h
K
ö
l
n
K
ö
l
n
g
e
f
a
h
r
e
n
Cologne
Figure 2.2: A two-dimensional representation of the aligned sentence pair in Figure 2.1. e dashed
box (lower right) marks a consistent phrase pair. e dotted box (upper left) marks an inconsistent
phrase pair. Inconsistent phrase pairs can be easily identiﬁed by looking for alignment points above,
below, left, and right of the alignment box.
2.2 HIERARCHICAL PHRASE-BASED GRAMMAR
Chiang’s hierarchical phrase-based model (often referred to simply as “Hiero”) was not the ﬁrst
syntax-based statistical machine translation model; that credit goes to Wu [1997]’s Stochastic
Inversion Transduction Grammar (ITG) model. Nonetheless, Hiero can be considered the fore-
runner of most current models. It was the ﬁrst to bring together many necessary components for
state-of-the-art translation performance for SCFG, including a linear model and the combina-
tion of beam search and cube pruning for n-gram language model integration. Crucially, Hiero
also introduced a method for learning hierarchical grammars from parallel text that was based on
phrases as the fundamental unit of translation, instead of words. Hiero’s phrases are phrases that
have been generalized to include gaps, which allow for their recursive nesting.
2.2.1 RULE EXTRACTION
e phrase extraction process begins by identifying initial phrase pairs, which are a subset of all
possible phrase pairs meeting the following conditions:
1. the phrase pair is consistent;
34 2. LEARNING FROM PARALLEL TEXT
2. the phrase pair contains at least one positive alignment point; and
3. there are no unaligned words at the boundary of the phrase in either language.
e sentence pair in Figures 2.2 permits the extraction of fourteen initial phrase pairs (seven
phrase pairs with one word on each side, and seven with more than one word). Each of these
becomes a terminal production in an SCFG grammar, with a left-hand side symbol of X. Here
are those rules, excluding rules representing singleton pairs and the entire sentence pair:
X ! leider ist Herr Steiger , unfortunately , Mr Steiger has
X ! ist Herr Steiger , Mr Steiger has
X ! nach Köln , to Cologne
X ! nach Köln gefahren , gone to Cologne
X ! ist Herr Steiger nach Köln gefahren , Mr Steiger has gone to Cologne
X ! Herr Steiger , Mr Steiger
Once the set of initial phrase pairs has been identiﬁed, hierarchical phrases can be created through
a process that can be thought of as phrase subtraction, whereby shorter initial phrase pairs embed-
ded in longer ones are removed and replaced with non-terminals. ese removed phrases become
substitution points that are labeled with the same label, X. For example, we can build the rule
below from the following sequence:
X ! ist Herr Steiger nach Köln gefahren , Mr Steiger has gone to Cologne
X ! Herr Steiger , Mr Steiger
D X ! ist X₁ nach Köln gefahren , X₁ has gone to Cologne
For initial phrase pairs that are long enough, multiple non-terminals can be created by repeatedly
subtracting initial phrases. For instance, we can subtract out another initial phrase pair from the
rule we just produced:
X ! ist X₁ nach Köln gefahren , X₁ has gone to Cologne
X ! Köln , Cologne
D X ! ist X₁ nach X₂ gefahren , X₁ has gone to X₂
ese two rule subtractions are depicted visually in Figure 2.3.
Allowing all possible initial phrases and all possible substitutions would lead to an unus-
ably large grammar and many undesirable rules. Chiang [2007] provides the following heuristic
constraints:
1. the maximum length of the initial phrase pairs is set to ten;
2. there can be no more than ﬁve symbols (terminal or non-terminal) on the source side;
3. unaligned words are not permitted on initial phrase pair boundaries;
2.2. HIERARCHICAL PHRASE-BASED GRAMMAR 35
i
s
t
S
t
e
i
g
e
r
i
s
t
S
t
e
i
g
e
r
H
e
r
r
S
t
e
i
g
e
r
X1
X1
has
gone
to
S
t
e
i
g
e
r
n
a
c
h
n
a
c
h
K
ö
l
n
K
ö
l
n
g
e
f
a
h
r
e
n
Cologne
1
Mr.
Steiger
Cologne
X1
has
gone
to
i
s
t
n
a
c
h
g
e
f
a
h
r
e
n
X1
X2
X2
n
a
c
h
g
e
f
a
h
r
e
n
X2
K
ö
l
n
Figure 2.3: e Hiero rule extraction process. On the left, an initial phrase pair is extracted from a
larger one to form a rule with a single gap. On the right, that rule in turn has a second initial phrase
pair removed from it to form a rule with two gaps.
4. the number of substitution points per rule is limited to two; and
5. source-side non-terminals are not permitted to be adjacent.
e last two restrictions on non-terminals limit the complexity of parsing with the source-side
projection of these grammars. Altogether, 50 grammar rules can be extracted from our working
example sentence pair under these extraction heuristics (compared to 16 phrases without gaps
that are extracted under Moses’ default extraction heuristics).
e automatically extracted rules can then be used to translate new sentences at test time
with a decoding algorithm. However, it is often the case that new sentences cannot be parsed
with a single-rooted hierarchical analysis. is is due in part to the restriction on the maximum
length of initial phrase pairs,¹ but also due to eﬃciency considerations at decoding time. Hiero
imposes a maximum span constraint, which limits the span over which hierarchical rules can apply
(many decoders use a default value of 20). Sentences longer than this cannot be parsed entirely
hierarchically, but must instead join together adjacent derivations of the input sentence. To make
this possible, Hiero also manually deﬁnes a simple set of glue rules:
S ! hsi , hsi
S ! S₁ X₂ , S₁ X₂
S ! S₁ h/si , S₁ h/si:
¹It is not only new sentences that cannot be parsed in this way; the constraint also prevents many sentence pairs in the training
data from being covered with a structure.
36 2. LEARNING FROM PARALLEL TEXT
ese rules begin by parsing the start-of-sentence symbol, hsi. ey then allow the combination
(without reordering) of any number of subderivations, by allowing hierarchical substructures from
adjacent source-side spans to be “glued” together. e glue grammar makes use of the additional
non-terminal symbol S, which is the SCFG’s start symbol. Glue rules thus help control the com-
putational complexity of decoding, which is cubic in N, the size of the maximum span. ey also
provide a backoﬀ mechanism for situations where no derivation can be found using automatically
extracted rules alone.
Finally, during decoding, another set of rules is assumed to exist for handling unknown
words. Unknown words on the input are treated as if there was a singleton rule in the grammar
which has the unknown word on the source and target side; they are thus “pushed through” by
the decoder. Together, the automatically learned rules and the glue rules constitute a synchronous
context-free grammar of the form presented in the previous chapter.
2.2.2 FEATURES
e product of the extraction procedures in this chapter are not just rules, but also a set of dense
feature values associated with each rule. ese feature values are multipled against weights in
the decoder’s linear model in order to help steer the decoder’s search procedure toward good
derivations. It is possible to compute many diﬀerent kinds of features over these rules. Feature
sets are typically split into two groups: dense features, which have values for every rule, and sparse
features, which typically only apply to small subsets of the total set of rules. e exact set of features
extracted depends on the tool used. e following are common ones:
• P(s j t) and P(t j s), the conditional phrase probabilities [Och and Ney, 2002];
• Plex(s j t) and Plex(t j s), the lexical phrase probabilities [Koehn et al., 2003], which estimate
how well the words of each rule translate, in each direction; and
• exp.1/, which serves as a count of how many rules are used in a derivation, and which can
be used by the model to prefer longer or shorter derivations.
2.3 SYNTAX-AUGMENTED GRAMMAR
Syntax-augmented machine translation (SAMT) was proposed soon after Hiero [Zollmann and
Venugopal, 2006], and occupies an interesting place between Hiero and other tree-based models.
It is linguistically informed because it makes use of a parse tree over the target side of the training
data. e motivation for this was a step in the direction of syntax-based language modeling, where
the target-side tree structure ostensibly guides decoding. However, SAMT extends directly from
Hiero; its fundamental notion of phrase, therefore, does not derive from a linguistic notion of
constituents, but instead from the consistent phrase pair concept deﬁned in the previous section.
Many of the phrase pairs extracted are therefore not syntactic (in the linguistic sense); SAMT
2.3. SYNTAX-AUGMENTED GRAMMAR 37
retains such phrase pairs by using a set of extended categories formed by combining non-terminal
labels. For that reason, its use of parse tree labels is better viewed as a kind of symbol reﬁnement.
SAMT grammar diﬀers from Hiero in three important ways:
1. the use of constituent labels to assign category labels;
2. a change in the extraction heuristics; and
3. the use of many more features.
2.3.1 RULE EXTRACTION
Rule extraction proceeds in much the same way as it did for Hiero. We begin with the set of
initial phrase pairs, and repeatedly subtract an initial phrase pair from another. However, instead
of using the default non-terminal X, the gaps are assigned labels from the constituent parse tree
according to the following rules:
1. if the removed initial phrase pair is a constituent, assign the constituent label;
2. if the phrase pair exactly extends across multiple constituents A1 : : : AN that are not them-
selves a constituent, assign a concatenated label of the form “A1 C    C AN ;”
3. if the phrase pair is a complete constituent A except for a single missing constituent B to
the right or left, assign a partial label “A=B” or “B\A,” respectively;
4. ﬁnally, if none of the above situations applies, assign the default label, X.
Figure 2.4 contains a version of our working example augmented with a parse tree. Word-
word aligned pairs are trivially assigned the label of the preterminal above the target-language
word. SAMT also produces the following set of initial rules:
S/VP ! leider ist Herr Steiger , unfortunately , Mr Steiger has
NP+VBZ ! ist Herr Steiger , Mr Steiger has
ADV+COMMA ! leider , unfortunately ,
COMMA+NP ! Herr Steiger , , Mr Steiger
COMMA+NP+VBZ ! ist Herr Steiger , , Mr Steiger has
PP ! nach Köln , to Cologne
VP\VBN ! nach Köln , to Cologne
VP ! nach Köln gefahren , gone to Cologne
ADV\S ! ist Herr Steiger nach Köln , , Mr Steiger has gone to
gefahren Cologne
NP+VP ! ist Herr Steiger nach Köln , Mr Steiger has gone to
gefahren Cologne
NP ! Herr Steiger , Mr Steiger
38 2. LEARNING FROM PARALLEL TEXT
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
PP
.
.
.
.
.
.
.
NP
.
.
.
.
.
NNP
.
.
.
.
.
Cologne
.
.
.
.
.
TO
.
.
.
.
.
to
.
.
.
.
.
VBN
.
.
.
.
.
gone
.
.
.
.
.
VBZ
.
.
.
.
.
has
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NNP
.
.
.
.
.
Steiger
.
.
.
.
.
NNP
.
.
.
.
.
Mr
.
.
.
.
.
.
.
,.
.
.
.
.
,
.
.
.
.
.
ADV
.
.
.
.
.
unfortunately
.
.
.
.
.
.
.
.
.
gefahren
.
.
.
.
.
.
.
Köln
.
.
.
.
.
.
.
nach
.
.
.
.
.
.
.
Steiger
.
.
.
.
.
.
.
Herr
.
.
.
.
.
.
.
ist
.
.
.
.
.
leider
.
.
.
.
.
.
.
Figure 2.4: An aligned sentence pair with a target-side parse tree.
e above list is larger than the Hiero list. is is because, in addition to labeling, SAMT
changes the heuristics used to derive initial phrases. First, unaligned words are permitted at rule
boundaries, and since attachment is ambiguous, this leads to multiple extracted phrases (as with
the COMMA symbol above). SAMT also imposes some of its own constraints. For example,
in an eﬀort to reduce the already-large number of symbols, it is common to permit only one
label concatenation per rule. In that case, the “COMMA+NP+VBZ” rule above would become
X instead. Rules assigned the default label in this way are ineligible for further subtraction.
Deriving hierarchical rules then proceeds in the same way as before: all initial phrase pairs
are added to the grammar. Further rules are then created by subtracting initial phrases, and adding
the results to the grammar until there are no more ways to do so.
SAMT also makes use of glue rules to ensure that a derivation can be produced. e glue
grammar is much larger than for Hiero because it must include a rule for every non-terminal
symbol N in the grammar:
S
! hsi , hsi
8X 2 N S
! S
₁ X₂ , S
₁ X₂
S
! S
₁ h/si , S
₁ h/si:
2.3. SYNTAX-AUGMENTED GRAMMAR 39
If the symbol S occurs in the training data as a constituent label, then typically a diﬀerent, oth-
erwise unused, symbol (such as TOP, ROOT, or GOAL) is chosen as the start symbol S
.
2.3.2 EXTRACTION HEURISTICS
As we showed in the example above, SAMT signiﬁcantly alters Hiero’s extraction heuristics. e
diﬀerences outlined in the original article include the following. SAMT
• permits adjacent non-terminals on the source language side;
• increases the maximum length of initial phrase pairs from 10 to 12;
• allows unaligned words at phrase boundaries; and
• allows the extraction of abstract rules (ones with no terminals symbols on the source lan-
guage side).
ese changes result in a much larger number of extracted rules. Whereas Hiero extraction
produces 50 rules from this sentence, SAMT extraction produces 147. Furthermore, the extrac-
tion of rules with adjacent non-terminals means that the decoding algorithm has to search over
the split point when applying such rules. e larger number of rules and symbols, together with
the admission of adjacent non-terminals, have a deleterious eﬀect on parsing complexity. On the
other hand, they have often been found to produce improved BLEU scores over Hiero [Baker
et al., 2009, Weese et al., 2011], although it is diﬃcult to evaluate whether this is due to the label
projection or the revised extraction and decoding constraints.
2.3.3 FEATURES
Finally, SAMT in practice uses a much larger number of features for each rule than the standard
implementation of Hiero. ese include a number of binary-valued features. e following list
is included in the original formulation of SAMT and in rax [Weese et al., 2011], a grammar
extraction tool:
• Binary-valued features indicating
– whether the rule is purely lexical (contains only terminal symbols),
– whether there are terminals on the source but not the target (and vice versa),
– whether the rule is abstract (contains no terminal symbols),
– for binary rules, the permutation between the terminals (straight or inverted),
– whether the rule contains adjacent non-terminals, and
– whether the rule was assigned the default non-terminal;
• counters for
40 2. LEARNING FROM PARALLEL TEXT
– the number of unaligned words,
– the number of target-side words, and
• a constant phrase penalty.
2.4 GHKM
e Hiero model is syntactic in a generic sense of having structure: its rules apply hierarchi-
cally, and translation under the model (Chapter 5) involves parsing the source-language string.
However, it is not syntactic in the traditional sense of providing a linguistic explanation of the
constituent structure of either the source or target sentence. In fact, since Hiero is based on the
phrase-based model—which explicitly rejects such syntactic restraints [Koehn et al., 2003]—there
is no notion of linguistics at all. And we have already noted how SAMT is better viewed as symbol
reﬁnement of Hiero’s rules than as a linguistically-motivated alternative.
String-to-tree models are diﬀerent. ey provide a view of translation based on
linguistically-motivated transformations between strings and parse trees. e most well-known
instance of string-to-tree models is GHKM [Galley et al., 2004, 2006].² Formally, GHKM is
based on the STSG formalism, and deﬁnes a mapping between strings and tree fragments of arbi-
trary size. GHKM grammar comprises pairs of tree fragments and strings. Consider the following
rule:
.
.
.
.
S
.
.
.
.
.
.
.
.3
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
ADVP2
.
.
.
.
.
.
.
,.
.
.
.
.
,
.
.
.
.
.
VBD
.
.
.
.
.
signed
.
.
.
.
.
NP1
.
.
.
.
.
.
.
NP1 ont ADVP2 signé .3 !
.
is can also be written in a version of our SCFG format that has been extended to include tree
fragments:
S ! NP₁ ont ADVP₂ signé PUNC₃ , NP₁ (VP (VBD signed) (, ,) ADVP₂) PUNC₃.
But we will prefer an equivalent and more compact format, which emphasizes the transformation
from a source-language string to a target-language structure:
NP₁ ont ADVP₂ signé PUNC₃ ! (S NP₁ (VP (VBD signed) (, ,) ADVP₂) PUNC₃).
is rule takes a sequence of unparsed French words and already-parsed constituents to produce
a new constituent labeled with the S symbol. Both the string side of the rule and the leaves of the
tree fragment on the target side of the rule contain a mixture of terminals and non-terminals. e
²Named for its authors, Galley, Hopkins, Knight, and Marcu.
2.4. GHKM 41
leaf non-terminals are co-indexed and bijective, which permits them to be reordered (although
there is no reordering in this rule). Each side can have zero or more terminal symbols, which
may or may not be aligned. e use of fragments allows the leaf non-terminals to be reordered
across multiple levels of the tree, which is an important ﬂexibility over earlier models [Yamada
and Knight, 2001] that were limited to deletions, insertions, and reorderings of the children of a
single syntactic node.
We emphasize that at decoding time, the rules of GHKM grammar can be used in either
direction: as described here, transforming an input string into a target-language tree in a parsing-
based framework (Chapter 5), or in the other direction, transforming an input source-language
tree into a target-language string as a tree transduction task (Chapter 4).
Example rules of this sort are learned from the same kind of aligned tree pair presented
in the previous section. However, the manner in which rules are extracted is quite diﬀerent from
Hiero. GHKM outlines a multi-step procedure for learning translation grammars.
1. Determine the set of frontier nodes, which are the constituents in the parse tree whose leaves
are consistent phrase pairs.
2. From these, extract the minimal set of rules that explain the complete derivation.
3. Build a set of composed rules that provide for more general coverage and which reduce the
independence of rule application.
Figure 2.5 contains a new aligned tree pair. We will now step through a GHKM extraction
procedure using it as an example.
2.4.1 IDENTIFYING FRONTIER NODES
Informally, frontier nodes are internal nodes of the parse tree whose (target-side) yield form a
consistent phrase pair (according to Deﬁnition 2.4) with their aligned source words. ey are
important because each frontier node will serve as the root of a minimal rule, and also as substi-
tution points among the leaves of other rules. Figure 2.5 identiﬁes frontier nodes by placing boxes
around them.
Formally, GHKM extracts rules from tuples .; s; A/. s and A are the source language
sentence and alignment, as deﬁned before (Section 2.1), and  is a parse tree whose leaves are
the words of t. Together, we can consider the tuple as a directed, acyclic graph, G. Edges in the
parse tree point from parent to child, and edges in A point from words in t to words in s. Note
that the graph is not necessarily connected, since there may be words in s that are unaligned. For
each non-leaf node of the parse tree N 2 f tg, GHKM deﬁnes two properties:
• the span, span.N/, a range from the minimum to the maximum indices of words in s that
are reachable from N;³ and
³Galley et al. [2004] deﬁnes this as closure.span.N//.
42 2. LEARNING FROM PARALLEL TEXT
.
.
.
.
S
.
.
.
.
.
.
.
PUNC
.
.
.
.
.
.
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
ADVP
.
.
.
.
.
RB
.
.
.
.
.
too
.
.
.
.
.
.
.
,.
.
.
.
.
,
.
.
.
.
.
VBD
.
.
.
.
.
signed
.
.
.
.
.
NP
.
.
.
.
.
.
.
PP
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NNS
.
.
.
.
.
scientists
.
.
.
.
.
.
.
VBG
.
.
.
.
.
leading
.
.
.
.
.
NP
.
.
.
.
.
.
.
POS
.
.
.
.
.
’s
.
.
.
.
.
.
.
NN
.
.
.
.
.
world
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
IN
.
.
.
.
.
of
.
.
.
.
.
NP
.
.
.
.
.
JJ
.
.
.
.
.
many
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
signé
.
.
.
.
.
.
.
également
.
.
.
.
.
.
.
ont
.
.
.
.
.
.
.
reconnus
.
.
.
.
.
.
.
scientiﬁques
.
.
.
.
.
.
.
de
.
.
.
.
.
beaucoup
.
.
.
.
.
.
.
.
Figure2.5: An aligned sentence pair with a parse tree over the target side. Boxed nodes denote frontier
nodes; the dashed nodes mark frontier nodes that were removed to avoid unary cycles.
• the complement set,⁴ span.N/, which is the set of spans produced by taking the union of the
spans of all nodes N0
in  that are neither descendants nor ancestors of N.
e set of frontier nodes is then the set of nodes N such that
span.N/ \ span.N/ D ;:
Put another way, this deﬁnition of frontier node enforces that the range of words a node covers in
the target language is not interrupted by any node that is not its ancestor or descendant. Frontier
nodes thus project downward onto consistent phrase pairs. e set of phrase pairs covered by
frontier nodes, however, is a subset of all possible consistent phrase pairs, and therefore fewer
rules will be extracted than would be from Hiero.
Chung et al. [2011] recommends one further modiﬁcation to the deﬁnition of frontier
nodes: if the span of a frontier node is the exact same as its parent span, then the child node is no
longer considered to be a frontier node. is removes unary rules, which complicate decoding.
e dashed RB node in Figure 2.5 is one such node.
⁴Galley et al. [2006] calls this the complement span, but it is actually a set of one or more spans.
2.4. GHKM 43
2.4.2 EXTRACTING MINIMAL RULES
Each frontier node becomes the root of a rule that extends its reach along the graph, terminating
in (a) other frontier nodes, (b) unaligned words in t, or (c) words in s. Together, the set of rules
extracted from frontier nodes deﬁne a minimal, complete derivation of the alignment pair. ese
rules are STSG rules comprising a left-hand side, additional internal structure, and source and
target transformations. e complete set of minimal GHKM rules from Figure 2.5 are listed here:
NP₁ VP₂ PUNC₃ ! (S NP₁ VP₂ PUNC₃)
NP₁ PP₂ ! (NP NP₁ PP₂)
beaucoup ! (NP (JJ many))
IN₁ NP₂ ! (PP IN₁ NP₂)
de ! (IN of)
NNS₁ reconnus ! (NP (NP (DT the) (NN world) (POS ’s)) (VBG leading) NNS₁)
scientiﬁques ! (NNS scientists)
ont ADVP₁ signé ! (VP (VBD signed) (, ,) ADVP₁)
également ! (ADVP (RB too))
. ! (PUNC .)
ese rules are diﬀerent from the Hiero rules above in a number of ways. e main diﬀerence
is that the target-side is no longer ﬂat but contains tree structure, depicted here in Penn Tree-
bank [Marcus et al., 1993] format. Other diﬀerences are due to heuristic extraction decisions:
for example, Hiero as originally formulated did not permit adjacent non-terminals, for parsing
eﬃciency. GHKM rules do not have restrictions on non-terminal adjacency, or indeed on the
number of non-terminals in the right-hand side.⁵
2.4.3 UNALIGNED SOURCE WORDS
Unaligned target-side words are easily incorporated into the minimal rule rooted in the near-
est frontier node (and from there, merged into combined rules). Unaligned source-side words,
however, often present an ambiguity. ere are often many minimal rules they could be attached
to without violating the notion of consistent phrase pair. Consider the example fragment shown
in Figure 2.6. Where should the unaligned comma be attached? We could create any of the
following rules:
NP₁ , VP₂ ! (S NP₁ VP₂)
DT₁ poste , ! (NP DT₁ post oﬃce)
, c’ est ! (VBZ is)
A principled approach is to maintain counts for each possible attachment and let statistics over the
whole corpus make the determination. Galley et al. [2006] described a solution using a derivation
forest to represent the possibilities while ensuring that each complete derivation permits only a
⁵However, in practice, string-to-tree decoding ﬁlters out rules whose parsing complexity is too high (Section 5.3).
44 2. LEARNING FROM PARALLEL TEXT
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP
.
.
.
.
.
NN
.
.
.
.
.
democracy
.
.
.
.
.
VBZ
.
.
.
.
.
is
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
oﬃce
.
.
.
.
.
.
.
NN
.
.
.
.
.
post
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
.
.
.
.
démocratie
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
est
.
.
.
.
.
.
.
c’
.
.
.
.
.
.
.
,
.
.
.
.
.
.
.
poste
.
.
.
.
.
la
.
.
.
.
.
.
.
Figure 2.6: An example training fragment containing an unaligned source word.
single attachment point of the unaligned word. A much simpler approach is to heuristically attach
unaligned words. For example, the Moses GHKM extractor [Williams and Koehn, 2012] uses
the following heuristic: if there are aligned source words to both the left and the right of an
unaligned source word, then attach it to the lowest common ancestor of its nearest such left and
right neighbors; otherwise, attach it to the root of the parse tree.
2.4.4 COMPOSED RULES
e procedure above extracts the minimal set of rules that explain the derivation. In many cases,
these rules are fairly small, and fail to capitalize on the ability to capture larger tree-based patterns.
For this reason, it is eﬀective to combine minimal rules into composed rules. In theory, there is
nothing preventing the extraction of entire parse trees as rules. In practice, the formation of the set
of composed rules is constrained by a number of factors. e generalizability of composed rules
degrades as their size increases. Furthermore, the computational complexity of parsing with large
rules is exponential in the length of sequences of non-terminals uninterrupted by terminals. For
these reasons, composed rules are capped by a number of heuristically-determined thresholds.
e Moses GHKM extractor imposes the following constraints: the maximum distance from
fragment root to any of its leaves is limited to 4; the maximum number of nodes is 15; and the
number of nodes (excluding pre-terminal and leaf nodes) is limited to 3. ose constraints lead
to the extraction of the following composed rules:
NP₁ VP₂ . ! (S NP₁ VP₂ (PUNC .))
NP₁ PP₂ VP₃ . ! (S (NP₁ PP₂) VP₃ (PUNC .))
2.5. A COMPARISON 45
NP₁ ont ADVP₂ signé PUNC₃ ! (S NP₁ (VP (VBD signed) (, ,) ADVP₂) PUNC₃)
NP₁ ont ADVP₂ signé . ! (S NP₁ (VP (VBD signed) (, ,) ADVP₂) (PUNC .))
NP₁ de NP₂ ! (NP NP₁ (PP (IN of) NP₂))
beaucoup PP₁ ! (NP (JJ many) PP₁)
beaucoup IN₁ NP₂ ! (NP (NP (JJ many)) (PP IN₁ NP₂))
beaucoup de NP₁ ! (NP (JJ many) (PP (IN of) NP₁))
scientiﬁques reconnus ! (NP (NP (DT the) (NN world) (POS ’s))
(VBG leading) (NNS scientists))
de NP₁ ! (PP (IN of) NP₁)
IN₁ NNS₂ reconnus ! (PP IN₁ (NP (NP (DT the) (NN world) (POS ’s))
(VBG leading) NNS₂))
IN₁ scientiﬁques reconnus ! (PP IN₁ (NP (NP (DT the) (NN world) (POS ’s))
(VBG leading) (NNS scientists)))
de NNS₁ reconnus ! (PP (PP (IN of) (NP (NP (DT the) (NN world)
(POS ’s)) (VBG leading) NNS₁)))
de scientiﬁques reconnus ! (PP (PP (IN of) (NP (NP (DT the) (NN world)
(POS ’s)) (VBG leading) (NNS scientists)))
ont également signé ! (VP (VBD signed) (, ,) (ADVP (RB (too))))
2.4.5 FEATURES
As with the other grammars, any number of features can be computed over the extracted frag-
ments. e existence of parse tree fragments with internal structure practically begs for the use
of tree-conditioned weights. One common feature is PCFG.f /, deﬁned as the sum of the log
probabilities of the depth-one CFG rules constituting each fragment f . ese probabilities can
be estimated from the parsed training data using the maximum likelihood estimator. It is also
common to add the left-hand side label to the conditionining information for the phrase proba-
bilities; that is, instead of computing Hiero’s P(s j t) and P(t j s), we compute P(s j t, label(f ))
and P(t j s, label(f )).
2.5 A COMPARISON
We extracted Hiero, SAMT, and GHKM grammars from a number of diﬀerent corpora in order
to provide a comparison on grammar size. For reference, we also extract a Moses phrase table. e
Hiero and SAMT grammars were extracted with rax, and the GHKM grammar with Moses’
extract-ghkm tool. We used default extraction settings in all scenarios.⁶ e Berkeley parser
[Petrov et al., 2006] was used to parse the English corpora, and BitPar⁷ [Fraser et al., 2013] to
parse the German corpus. Statistics on the number of rules extracted are listed in Table 2.3.
⁶For phrase extraction, the maximum phrase length = 7. Defaults for the others are given in their respective sections.
⁷http://www.ims.uni-stuttgart.de/tcl/SOFTWARE/BitPar.html
46 2. LEARNING FROM PARALLEL TEXT
# rules (millions) sentence stats
Language pair description phrase Hiero SAMT GHKM # mean len.
Spanish–English speech 2 22 32 3 137k 12.7
Turkish–English newswire 7 15 80 9 200k 20.9
Chinese–English newswire 12 62 82 44 1,861k 18.4
English–German gov’t, web 186 786 1,756 494 4,434k 25.1
Table 2.3: Sizes of extracted grammars using the Hiero, SAMT, and GHKM algorithms.
For all languages, the hierarchical and syntactic models are much bigger than their phrase-
based counterparts. SAMT, in turn, always produces larger grammars compared to Hiero, because
of its loosened extraction heuristics. In contrast, the requirement that consistent phrase pairs be
constituents reduces the size of GHKM grammars compare to Hiero, even allowing for the fact
that larger rules can be formed from composed rules. However, this GHKM story is complicated
a bit by two extremes. For Spanish–English, relatively few GHKM rules are extracted, due to a
combination of very short sentences and possibly to repetitive speech data. For English–German,
a diﬀerent parser, together with longer sentence lengths combine with the combinatorics of both
span projection (SAMT) and composed rule formation (GHKM), yielding massive grammars.
Of course, grammar size is aﬀected by many factors and is hard to characterize precisely, and
these numbers can therefore vary widely when extraction parameters are altered. We present these
numbers here to give the reader a rough idea of model sizes under common heuristic defaults.
2.6 SUMMARY
We have reviewed the methods for extracting three diﬀerent grammars that have been proposed
in the literature. Of these three grammars, Hiero has been the most popular. e label projection
techniques stemming from SAMT have provided some gains over Hiero, but in general, the
approach is not widely used, and it is diﬃcult to know precisely how much of the score diﬀerences
are due to label projection and how much are due to the relaxed extraction constraints and much
deeper search. GHKM grammars are much smaller than Hiero and SAMT grammars; they are
still based on the notion of consistent phrase pairs, but enforce a constituency constraint from a
source- or target-language parse tree. In the past few years, rich syntactic approaches based on
GHKM have done very well in shared tasks; for example, English–German string-to-tree systems
were tied for best in the human evaluation of the WMT14 and WMT15 translation tasks.⁸ On
the downside, GHKM grammars exhibit higher computational complexity for parsing due to the
presence of multiple adjacent nonterminals, but this can be limited with rule ﬁltering and eﬃcient
decoding.
⁸http://statmt.org/wmt14/results.html and http://statmt.org/wmt15/results.html.
C H A P T E R 3
Decoding I: Preliminaries
Decoding in statistical machine translation is the computational process of searching for the most
probable translation (or k most probable translations) of a source sentence according to a model.
For the models described in the previous two chapters, the search space is so large that search is
only made possible through heavy use of dynamic programming techniques. Unfortunately, as we
will see, the use of non-local features means that these models are not ideally suited to dynamic
programming and require the use of approximate search algorithms.
Broadly speaking, decoding for syntax-based models falls into two categories: string decod-
ing and tree decoding. String decoding, which is used for hierarchical phrase-based and string-to-
tree models, takes a string as input and applies a heuristic search algorithm based on monolingual
parsing and beam search. Tree decoding, which is used for tree-to-string and tree-to-tree mod-
els, takes a parse tree as input and applies a heuristic search algorithm based on rule matching
(sometimes called tree parsing) and beam search.
e two approaches have a lot in common. Both are based on hypergraph algorithms, which
follows from the fact that the models deﬁne a hypergraph-shaped search space. While we assume
that most readers are familiar with graphs, we do not assume familiarity with hypergraphs and
so this chapter, the ﬁrst of three chapters on decoding, is dedicated to basic deﬁnitions and algo-
rithms. In the next chapter, we move onto tree decoding. We start with a simple algorithm for
decoding with local features and gradually build up to state-of-the-art decoding methods. Most
of the methods introduced in that chapter are common to both decoding approaches. e ﬁnal
decoding chapter covers string decoding. It builds on the material from the tree decoding chapter,
adding techniques for dealing with the eﬃciency issues that arise from the larger search space of
string decoding.
3.1 HYPERGRAPHS, FORESTS, AND DERIVATIONS
3.1.1 BASIC DEFINITIONS
Hypergraphs generalize the concept of the graph: whereas a graph has edges that connect pairs of
vertices, a hypergraph has hyperedges that connect sets of vertices. Just like graphs, hypergraphs
can be directed or undirected. While undirected hypergraphs have important applications in other
ﬁelds, all of the hypergraphs that we will encounter in this book will be directed. We therefore
48 3. DECODING I: PRELIMINARIES
provide only a brief deﬁnition of undirected hypergraphs and move straight on to directed hyper-
graphs.
Deﬁnition 3.1 (Undirected Hypergraph). An undirected hypergraph H is a pair hV; Ei where
V is a ﬁnite set of vertices and E is a ﬁnite set of undirected hyperedges. Each hyperedge e 2 E is
a subset of V .
Because their deﬁnition is so broadly encompassing, undirected hypergraphs are common-
place even if we do not recognize them as such. For example, consider the frontier graph frag-
ments generated by the GHKM algorithm (Section 2.4): each of these fragments is a subset of
the alignment graph’s vertex set, and so the alignment graph’s vertex set, together with its set of
frontier graph fragments, can be characterized as an undirected hypergraph.
Deﬁnition 3.2 (Directed Hypergraph). A directed hypergraph H is a pair hV; Ei, where V is a
ﬁnite set of vertices and E is a ﬁnite set of directed hyperedges. Each directed hyperedge e 2 E is
a pair hhead.e/; tail.e/i, where head.e/ and tail.e/ are subsets of V .
From here on, we will drop the qualiﬁer “directed” and just write hypergraph and hyperedge.
In ﬁgures we will depict hyperedges using multi-headed and multi-tailed arrows.
Example 3.3 Let H be a hypergraph with ﬁve vertices, V D v1; v2; v3; v4; v5, and ﬁve hy-
peredges, E D e1; e2; e3; e4; e5, such that:
head.e1/ D fv2; v3g , tail.e1/ D fv1g
head.e2/ D fv1g , tail.e2/ D fv2g
head.e3/ D fv2g , tail.e3/ D fv3g
head.e4/ D fv4; v5g , tail.e4/ D fv4; v5g
head.e5/ D ¿ , tail.e5/ D fv1; v3g.
We can depict H as follows:
v1
v2
v3
e1
e3
v4
e2
e4
v5
e5
3.1. HYPERGRAPHS, FORESTS, AND DERIVATIONS 49
In Example 3.3, we used arbitrary names (v1; v2; : : :) to refer individually to each vertex
and hyperedge. For some applications, it will be necessary to attach a label to every vertex or to
every hyperedge (or to both). In these cases, the label will be considered an integral part of the
hypergraph. When labeled hyperedges are used, we will allow multiple hyperedges to have the
same head and tail. (Technically, this makes the structure a hyperedge-labeled multi-hypergraph,
but we will not worry about making the distinction when the type is clear from the context.)
As well as potentially having a label, a hyperedge will often be assigned a weight. Like with
the weighted grammars of Section 1.2.4, we deﬁne weights abstractly as elements of a weight set
K. Hyperedge weights are given by a weight function.
Deﬁnition 3.4 (Hyperedge Weight Function). Given a set of hyperedges E and a weight set
K, a hyperedge weight function for E is a function WE W E ! K that assigns a weight to every
hyperedge e 2 E.
A hypergraph with weighted hyperedges is called a weighted hypergraph.
Deﬁnition 3.5 (Weighted Hypergraph). A weighted hypergraph is a pair hH; WE i, where H D
hV; Ei is a hypergraph and WE W E ! K is a weight function for H’s hyperedge set.
3.1.2 PARSE FORESTS
One application of hypergraphs that we will encounter numerous times in this book is the repre-
sentation of parse forests. A parse forest is a set of parse trees for a single sentence, produced from
a single grammar. A parse forest can be represented by a hypergraph in which the sink vertices
(those with no incoming hyperedges) represent the words of the sentence and the non-sink ver-
tices represent parsing states (a non-terminal and span). e hyperedges represent parsing steps,
where one or more words or parsing states are connected to a successor state. If the trees of the
parse forest are weighted, then the weighting can be preserved in the hypergraph (by assigning
each hyperedge the weight of the corresponding parsing step).
Example 3.6 A parse forest for the sentence “the giant drinks” might consist of the follow-
ing two CFG parse trees:
.
.
.
. .
.
.
.
S
.
.
.
.
.
.
.
Verb
.
.
.
.
.
drinks
.
.
.
.
.
NP
.
.
.
.
.
.
.
Noun
.
.
.
.
.
giant
.
.
.
.
.
Det
.
.
.
.
.
the
.
. .
.
.
.
.
.
NP
.
.
.
.
.
.
.
Noun
.
.
.
.
.
drinks
.
.
.
.
.
.
.
Adj
.
.
.
.
.
giant
.
.
.
.
.
Det
.
.
.
.
.
the
50 3. DECODING I: PRELIMINARIES
e hypergraph representation of this parse forest is:
NP1,2
S1,3
Verb
3,3
drinks
Det
1,1
the
Noun
2,2
giant
Adj
2,2
Noun
3,3
NP1,3
A key property of parse trees is that they are ordered (i.e., the children of a node have a ﬁxed
ordering). In general, hypergraphs are unordered, but notice that for a parse forest hypergraph an
implicit ordering is deﬁned by the spans encoded in the vertex labels. For a general hypergraph,
we can impose an explicit ordering by deﬁning an ordering relation for every hyperedge tail. A
hyperedge with an ordered tail is called an ordered hyperedge.
Deﬁnition 3.7 (Ordered Hyperedge). An ordered hyperedge is a pair he; ei, where e is a di-
rected hyperedge and e is a total order on tail.e/.
If he; ei is an ordered hyperedge, then we will use the notation pred.e; e; i/ to refer to
the ith
tail vertex of e. For example, in Example 3.6, if e is the incoming hyperedge of the vertex
with label (S,1,3), then pred.e; e; 2/ refers to the predecessor vertex with label (Verb,3,3).
Notice that we just used e without explicitly deﬁning it: if a hyperedge, e, belongs to a parse
forest, then we will use e to refer to the implicit ordering relation.
A hypergraph with ordered hyperedges is called an ordered hypergraph.
Deﬁnition 3.8 (Ordered Hypergraph). An ordered hypergraph is a pair hH; E i, where H D
hV; Ei is a directed hypergraph and E is a function that assigns a tail ordering relation e to
every e 2 E.
By the nature of their construction, the hypergraphs used to represent parse forests have
a quite speciﬁc form: they are directed, acyclic, vertex-labeled, and (implicitly) ordered; their
hyperedge tails are non-empty; and their hyperedge heads contain exactly one vertex each. A
3.1. HYPERGRAPHS, FORESTS, AND DERIVATIONS 51
directed hyperedge with this last property is given a special name: it is called backward hyperedge
(or B-hyperedge).
Deﬁnition 3.9 (Backward Hyperedge). A backward hyperedge, or B-hyperedge, is a directed
hyperedge e for which head.e/ is a singleton. A backward directed hypergraph, or B-hypergraph,
is a directed hypergraph for which all hyperedges are backward hyperedges.
Looking back to Example 3.3, hyperedges e2 and e3 are backward hyperedges, but e1, e4,
and e5 are not. e hypergraph is not a backward hypergraph due to the presence of non-backward
hyperedges.
3.1.3 TRANSLATION FORESTS
A translation forest is a set of linked tree pairs for a single source sentence, produced from a single
synchronous grammar. A translation forest can be represented as a hypergraph using a similar
construction to that of a parse forest. It diﬀers in two ways: ﬁrst, the vertex labels encode a pair of
source and target non-terminals (at least in the case of a distinct-category SCFG; for a shared-
category SCFG there is only one); second, the hyperedges are labeled with rule identiﬁers.
Example 3.10 Consider the following two pairs of linked trees. Each tree pair represents a
diﬀerent German translation of our ambiguous English example, “the giant drinks” (inciden-
tally, the resulting German translations do not share the ambiguity of the source sentence).
.
.
.
. .
.
.
.
S1
.
.
.
.
.
.
.
Verb5
.
.
.
.
.
drinks
.
.
.
.
.
NP2
.
.
.
.
.
.
.
Noun4
.
.
.
.
.
giant
.
.
.
.
.
Det3
.
.
.
.
.
the
.
.
, .
. .
.
.
.
S1
.
.
.
.
.
.
.
VVFIN5
.
.
.
.
.
trinkt
.
.
.
.
.
NP2
.
.
.
.
.
.
.
NN4
.
.
.
.
.
Gigant
.
.
.
.
.
ART3
.
.
.
.
.
der
.
.
.
.
.
.
NP1
.
.
.
.
.
.
.
Noun4
.
.
.
.
.
drinks
.
.
.
.
.
.
.
Adj3
.
.
.
.
.
giant
.
.
.
.
.
Det2
.
.
.
.
.
the
.
.
, .
.
.
.
.
.
NP1
.
.
.
.
.
.
.
NN4
.
.
.
.
.
Getränke
.
.
.
.
.
.
.
ADJA3
.
.
.
.
.
großen
.
.
.
.
.
ART2
.
.
.
.
.
die
e two tree pairs could be produced by a SCFG or STSG. In the SCFG case, a minimal
grammar that derives the two tree pairs contains the following rules:
52 3. DECODING I: PRELIMINARIES
r1: Det ! the , ART ! der
r2: Det ! the , ART ! die
r3: Noun ! giant , NN ! Gigant
r4: Adj ! giant , ADJA ! große
r5: Verb ! drinks , VVFIN ! trinkt
r6: Noun ! drinks , NN ! Getränke
r7: NP ! Det₁ Noun₂ , NP ! ART₁ NN₂
r8: NP ! Det₁ Adj₂ Noun₃ , NP ! ART₁ JJ₂ NN₃
r9: S ! NP₁ Verb₂ , S ! NP₁ VVFIN₂
e hypergraph representation is then:
NP,NP,1,2
S,S,1,3
Verb,VVFIN
3,3
drinks
Det,ART
1,1
the
Noun,NN
2,2
giant
Adj,ADJA
2,2
Noun,NN
3,3
NP,NP,1,3
r2
r1 r3 r4 r6
r5
r7
r9
r8
3.1.4 DERIVATIONS
If we were deﬁning graphs then at this point it would be natural to deﬁne the concept of a path.
For directed hypergraphs, there are several possible ways of deﬁning a path-like object, but there
is not a direct analog of the familiar digraph path. One path-like object that is particularly useful
for parse forest hypergraphs and related structures is the derivation.
Deﬁnition3.11 (Derivation). A derivation takes the form of a pair he; Si where e is a (directed)
hyperedge and S is itself a set of derivations (referred to as the subderivations). For a given vertex
v in a directed hypergraph H, the set of derivations of v, denoted D.v/, is deﬁned recursively as
follows:
3.1. HYPERGRAPHS, FORESTS, AND DERIVATIONS 53
he; Si is a derivation of v iﬀ e is an incoming hyperedge of v and S is a set comprising
exactly one derivation d 2 D.u/ for each non-sink predecessor u 2 tail.e/.
If tail.e/ is empty or if every u 2 tail.e/ is a sink node then he; ¿i is a derivation of v.
Example 3.12 e following hypergraph has four derivations.
v1
v3
v2
e2
e5
v5
e3
e9
v6
e1
e8
e6
v4
e4
e7
• e vertices, v4, v5, and v6, do not have any derivations. In all three cases, this is because
the vertex is part of a cycle.
• e vertices v2 and v3 have one derivation each. e derivation of v3 is he6; ¿i and this
is a subderivation of v2’s derivation, which is he5; fhe6; ¿igi.
• e vertex v1 has two derivations: he3; fhe6; ¿igi (which contains the derivation of v3
as a subderivation) and he2; fhe5; fhe6; ¿igi; he6; ¿igi (which contains the derivations
of both v2 and v3 as subderivations). e4 is not part of a third derivation because its tail
node, v4, is not a sink node and has no derivations.
Of course, it is no coincidence that the hypergraph derivation object shares its name with
the grammar formalism object. For a parse forest, the hypergraph deﬁnition of a derivation co-
incides with the CFG deﬁnition of a derivation (assuming a canonical form for the CFG deriva-
tions). In other words, every hypergraph derivation corresponds to a parse tree or subtree; con-
versely, every CFG parse tree and subtree is represented by a derivation in the hypergraph. For
a SCFG or STSG translation forest, a hypergraph derivation corresponds to a SCFG or STSG
derivation.
3.1.5 WEIGHTED DERIVATIONS
In many applications, derivations have an associated weight and we are interested in ﬁnding the
derivation (or derivations) with the highest weight. Often this weight is a probability. For instance,
54 3. DECODING I: PRELIMINARIES
a statistical parsing model assigns a probability to each derivation tree and the job of a statistical
parser is to ﬁnd the tree with the highest probability.
A derivation weight function can be any function that takes a derivation as input and pro-
duces a weight as output.
Deﬁnition 3.13 (Derivation Weight Function). Given a derivation set D and a weight set
K, a derivation weight function on D is a function WD W D ! K that assigns a weight to every
derivation d 2 D.
In practice, a derivation weight function may be subject to strict constraints in order that it can
be used in a particular algorithm or in order that its value can be computed eﬃciently. Two such
constraints are decomposability and monotonicity. Informally, a derivation weight function is de-
composable if the weight of a derivation can be calculated given only the weight of the incoming
hyperedge and the subderivation weights.
Deﬁnition 3.14 (Decomposability). Let WD be a weight function for the derivation set D
and let WE be a weight function for E, the set of hyperedges occurring in D. Let w.d/ be the
function that for any d D he; Si 2 D yields the set containing the weight of the hyperedge e and
the weights of the subderivations in S:
w.d/ D fWE .e/g [
"
[
d02S
WD.d0
/
#
:
e function WD is decomposable iﬀ it can be expressed as a composed function f ı w W D ! K
Informally, a decomposable weight function is monotonic if it has the additional property that, all
else being equal, increasing the weight of a derivation’s incoming hyperedge or of a subderivation
results in an increase in the overall weight of the derivation.
Deﬁnition 3.15 (Monotonicity). Let WD W D ! K be a decomposable derivation weight
function for the derivation set D, let WE W E ! K be a hyperedge weight function for E, the
set of hyperedges occurring in D, and let  be a total order on K. Now let d1 D he1; Si and
d2 D he2; Si be any pair of derivations in D that diﬀer only in the hyperedge and let d3 D he; S1i
and d4 D he; S2i be any pair of derivations in D that diﬀer only in the set of subderivations by a
single element (there is a single subderivation s1 that occurs in S1 but not S2 and a single sub-
derivation s2 that occurs in S2 but not S1). WD is monotonic iﬀ it satisﬁes both of the following
conditions:
1. WE .e1/  WE .e2/ ) WD.d1/  WD.d2/
2. WD.s1/  WD.s2/ ) WD.d3/  WD.d4/
3.2. ALGORITHMS ON HYPERGRAPHS 55
In probabilistic parsing models, the—often implicit—derivation weight function simply multiples
the incoming hyperedge weight with the weights of the subderivations. Clearly this function is
both decomposable and monotonic. is property also makes the parsing problem amenable to
the application of a variety of eﬃcient algorithms.
3.2 ALGORITHMS ON HYPERGRAPHS
In this section, we describe a small selection of hypergraph algorithms. e ﬁrst is the topological
sort algorithm for graphs adapted for use with hypergraphs. Many hypergraph algorithms perform
operations on vertices in topological order and so a topological sort of a hypergraph’s vertices
frequently appears as a substep in other algorithms.
We then give two algorithms for ﬁnding a hypergraph’s maximally weighted derivation. We
will refer to this type of algorithm as a max-derivation algorithm. e ﬁrst, a variant of the Viterbi
algorithm, is the more general of the two: it assumes a directed acyclic hypergraph as input, along
with an associated derivation weight function. While the Viterbi max-derivation algorithm is a
natural ﬁt for parsing problems, it makes no assumptions about the structure of the hypergraph,
other than that it is directed and acyclic. e second—the weighted CYK algorithm—deals with
the speciﬁc case of parse forest hypergraphs that are produced by parsing a string with a weighted
CFG.
Finally, we describe a pair of algorithms for generating the k-best derivations of a hyper-
graph (according to some monotonic derivation weight function).
3.2.1 THE TOPOLOGICAL SORT ALGORITHM
Topological sorting for hypergraphs is essentially the same as for graphs. e algorithm visits the
hypergraph’s vertices in any order it likes, but at each vertex v it checks if it has visited v’s prede-
cessors. If not, then the algorithm takes a detour to visit the unvisited predecessors (recursively
checking their predecessors) before proceeding. A Boolean ﬂag (initially false) is used to indicate
whether or not a vertex has been visited.
Pseudocode is shown in Algorithm 1. Our version of the algorithm does not produce a
stored representation of the sorted vertex sequence; instead, it takes a procedure, op, as an input
parameter and applies op once at every vertex (in topological order). Of course, an implementation
could choose to deﬁne op such that it adds its vertex argument to a list, resulting in a topologically
sorted list of vertices.
3.2.2 THE VITERBI MAX-DERIVATION ALGORITHM
e Viterbi max-derivation algorithm—shown in Algorithm 2—is the ﬁrst of the two max-
derivation algorithms. As input, it takes a directed acyclic hypergraph H together with a mono-
tonic derivation weight function WD, where D is the set of all derivations over H.
56 3. DECODING I: PRELIMINARIES
Algorithm 1 Topological Ordering
Input : Directed acyclic hypergraph H; procedure op taking a single vertex parameter
Output : None (but calls op once for each vertex of H (in topological order))
T-P.H; op/
1 for v in H’s vertex set
2 visitedŒv D 
3 for v in H’s vertex set
4 if visitedŒv D 
5 .v; op/
V.v; op/
1 visitedŒv D 
2 for u in v’s predecessor set
3 .u/
4 call op, v
Algorithm 2 Viterbi Max-derivation Algorithm
Input : Directed acyclic hypergraph H, monotonic derivation weight function WD
Output : None (but bestŒv contains max-derivation for every vertex v)
V-D.H; WD/
1 for v in H’s vertex set
2 bestŒv D 
3 T-P(H, F-B)
F-B.v/
1 for each incoming hyperedge e of v
2 S D ¿
3 for each vertex u in tail.e/
4 if u is not a sink vertex
5 S D S [ fbestŒug
6 candidate D he; Si
7 if bestŒv D  or WD.bestŒv/  WD.candidate/
8 best[v] = candidate
3.2. ALGORITHMS ON HYPERGRAPHS 57
e algorithm makes use of the Algorithm 1’s generic T-S, deﬁning op
to be a function that records for each vertex the current best-known derivation. Initially this
is set to  (lines 1 and 2 of V-D) to indicate that no derivations have yet
been considered. Each vertex is then visited in topological order (line 3 of V-D)
and the max-derivation is determined from the set of candidate derivations, one per incoming
hyperedge (lines 1 to 8 of F-B). At line 7, the relation  is a total order on K, the codomain
of the weight function WD.
e correctness of F-B relies on the weight function being monotonic: by visiting
the vertices in topological order, the algorithm already knows the optimal subderivations to use for
each candidate derivation and monotonicity guarantees that the optimal derivation will include
the optimal subderivations (in dynamic programming terms, this is referred to as the property of
optimal substructure).
3.2.3 THE CYK MAX-DERIVATION ALGORITHM
Whereas the Viterbi max-derivation assumes that the input hypergraph has already been con-
structed, the CYK max-derivation algorithm interweaves two processes: the construction of the
hypergraph and the max-derivation search itself. Furthermore, the algorithm (as presented here)
only constructs part of the hypergraph, disregarding portions that can be determined not to belong
to the optimal derivation.
Before presenting the full version of the algorithm, we will describe the simpler CYK recog-
nition algorithm. is algorithm takes as input a string, s, and a CFG, G, which must be in a
restricted form known as Chomsky normal form. e algorithm returns a Boolean value indicat-
ing whether or not the string belongs to the language deﬁned by G. e core of the two CYK
algorithms is identical, but the recognition version is shorter and more readable. Once the main
ideas of the recognition algorithm are understood, the max-derivation version can be seen as a
simple variant.
First let us examine the requirement that the input grammar is in Chomsky normal form
(CNF). A CNF grammar is a CFG grammar (Deﬁnition 1.1) in which all rules are of the form
A ! BC or A ! w, where A, B, and C are non-terminals and w is a terminal. To all intents
and purposes, any CFG grammar can be transformed into a weakly-equivalent CNF grammar.¹
Furthermore, there are standard² conversion algorithms that are reversible: if a CFG grammar G
is converted to a CNF grammar G0
, then G can be recovered from G0
, and a parse tree for G0
can
be converted to an equivalent parse tree for G. us, the CYK algorithm can be used with any
CFG provided the grammar is ﬁrst normalized and (for the max-derivation version) the resulting
hypergraph is de-normalized (or “ﬂattened”).
¹e only exceptions are CFGs that can generate the empty string. In that case, a grammar, G, has an almost weakly-equivalent
CNF grammar, G0, such that L.G0/ D L.G/ fg.
²Details of the conversion process are covered in most introductory textbooks on formal language theory.
58 3. DECODING I: PRELIMINARIES
S
SBAR
DT
PP V
s1 s2 s3 s4 s5 s6
Figure 3.1: A chart for the CYK recognition algorithm.
e CYK algorithm belongs to a family of parsing algorithms that use a chart data structure.
e chart contains a cell for each substring of the input string. So, for an input string s, it contains
jsj cells for the words of the string, jsj 1 cells for the two-word substrings, jsj 2 cells for the
three-word substrings, and so on. e contents of the cells depends on the exact variant of the
algorithm. For the recognition algorithm, each cell contains a vector of Boolean values, with one
element for each non-terminal in the input grammar. For a particular element, the value is true if
and only if A

) s0
, where A is the non-terminal and s0
is the substring of the input corresponding
to the chart cell.
Figure 3.1 depicts a chart for an input string of six words. e example cell entry indicates
that among all the possible derivations of the substring s2 : : : s6, there is at least one beginning
with the symbol PP, there are none beginning DT, there is at least one beginning SBAR, and so
on.
e CYK algorithm visits the cells of the chart in order of increasing subspan width. For
each cell in the bottom layer, the algorithm must determine if the corresponding input word si
is derivable from A, for each non-terminal A 2 N (where N is the grammar’s non-terminal set).
Since the input grammar is in Chomsky Normal Form, all this requires is a simple check for the
existence of a rule A ! si .
e other CNF rule form, A ! BC, is used for cells in the higher chart layers. For a cell
covering substring si : : : sj (j > i), the algorithm must determine for each non-terminal A 2 N
whether there is (i) a rule A ! BC in the grammar and (ii) a pair of adjacent subderivations
that derive B and C, where the ﬁrst derives the substring si : : : sx 1 and the second derives the
substring sx : : : sj , for some value i < x  j . e ﬁrst condition simply involves a check of the
grammar’s rule set. e second additionally involves consulting the lower, previously-computed,
cells of the chart for each possible value of x.
3.2. ALGORITHMS ON HYPERGRAPHS 59
Algorithm 3 e CYK Recognition Algorithm
Input : String s; CFG, G, in Chomsky normal form
Output : Boolean indicating whether s belongs to language deﬁned by G
CYK-R.G; s/
1 Initialize all entries in chart to 
2 for i D 1 to jsj
3 for each rule A ! si in G
4 chartŒi; i; A D 
5 for width D 2 to jsj
6 for i D 1 to jsj width C 1 /
/ i = span start
7 j D i C width 1 /
/ j = span end
8 for x D i C 1 to j /
/ x = split-point
9 for each rule A ! BC in G
10 if chartŒi; x 1; B and chartŒx; j; C
11 chartŒi; j; A D 
12 return chartŒ1; jsj; S

Pseudocode is shown in Algorithm 3. e chart is represented by a three-dimensional array
which is indexed by three values i, j , and A, where i and j are the ﬁrst and last index of the
substring and A is the non-terminal. For example, the SBAR ﬂag from the example in Figure 3.1
would be indexed as chartŒ2; 6; .
e two forms of CNF grammar rule are handled separately by the two top-level for loops:
the loop from lines 2–4 ﬁlls the cells for single-word substrings, using rules of the form A ! w;
the loop from lines 5–11 handles the remaining cells, using the rules of the form A ! BC.
At the end of the algorithm, the cell chartŒ1; jsj; S
 contains the value  if s is derivable
from S
, or  otherwise.
e worst-case time complexity of the algorithm is O.jNj3
 jsj3
/, where N is the number
of distinct non-terminal symbols in G and jsj is the length of the input string. To see this, consider
that for each of the chart’s O.jsj2
/ cells, the algorithm must try O.jsj/ split points, and for each
split point it must try O.jNj3
/ rules (since there are O.jN j/ possible non-terminal symbols for
each of A, B, and C). It is assumed that checking the existence of a rule in the grammar has
constant cost (which is easily achieved by storing the rules in a hash table³).
e max-derivation version of the algorithm is essentially the same as the recognition al-
gorithm except that instead of containing vectors of Boolean ﬂags, the cells of the chart contain
³Since the grammar is ﬁxed during parsing, perfect hashing can be used to guarantee constant-time lookup in the worst case.
60 3. DECODING I: PRELIMINARIES
Algorithm 4 e CYK Max-derivation Algorithm
Input : s a string
G a CFG in Chomsky normal form
WD a monotonic derivation weight function
Output : d
the max-derivation of s (or  if s is not in L.G/)
CYK-M-D.G; s/
1 Initialize all entries in chart to 
2 for i D 1 to jsj
3 for each rule A ! si in G
4 head D fvertexŒi; i; Ag
5 tail D fvertexŒi; i; si g
6 e D hhead; taili
7 S D ¿
8 chartŒi; i; A D he; Si
9 for width D 2 to jsj
10 for i D 1 to jsj width C 1 /
/ i = span start
11 j D i C width 1 /
/ j = span end
12 for x D i C 1 to j /
/ x = split-point
13 for each rule A ! BC in G
14 if chartŒi; x 1; B D  or chartŒx; j; C D 
15 continue
16 head D fvertexŒi; j; Ag
17 tail D fvertexŒi; x 1; B; vertexŒx; j; Cg
18 e D hhead; taili
19 S D fchartŒi; x 1; B; chartŒx; j; Cg
20 candidate D he; Si
21 chartŒi; j; A D maxWD
.chartŒi; j; A; candidate/
22 return chartŒ1; jsj; S

vectors of derivations (one for each non-terminal A 2 N ). e vector elements perform an equiv-
alent role to the bestŒv variables in Viterbi algorithm, recording the current best derivation for a
single non-terminal and span.
Pseudocode is presented in Algorithm 4. e algorithm constructs a portion of the parse
forest hypergraph that contains (at least) the best derivation. e forest’s vertices are assumed to
exist at the start of the algorithm (alternatively, they could be created on-demand). Vertices are
addressed using the three-dimensional array, vertex, which is indexed in the same way as chart.
3.2. ALGORITHMS ON HYPERGRAPHS 61
Hyperedges and derivations are created as the algorithm proceeds, but only the (locally) best
derivations are retained. At the end of the algorithm, the cell chartŒ1; jsj; S
 contains the best
sentential derivation of the input string s, or  if s is not derivable from S
.
3.2.4 THE EAGER AND LAZY k-BEST ALGORITHMS
e objective of a k-best algorithm is to ﬁnd the k-best derivations of some target vertex t in
a directed hypergraph H. By “best derivation” we mean the derivation that has the maximum
weight according to a derivation weight function WD. Finding the k-best derivations is a common
requirement in NLP, for instance when re-ranking the output of a statistical parser or in feature
weight optimization in SMT.
We will describe two closely related algorithms, both of which are due to Huang and Chi-
ang [2005]. In the original article, these are Algorithms 2 and 3. Both algorithms generate the
same output, but Algorithm 3 does so more eﬃciently. Algorithm 3 is described by the authors as
taking lazy computation “to an extreme” and so we will refer to this as the “lazy” k-best derivation
algorithm. Algorithm 2, although still admirably economical, is over-eager in comparison and so
we will refer to this as the “eager” k-best derivation algorithm. (See Huang and Chiang [2005] for
detailed comparisons of the algorithms’ theoretical time complexity and empirical performance.)
While the lazy algorithm would seem to make the eager version redundant, we choose to
describe both algorithms in detail since they turn out to be of interest beyond k-best extraction: the
eager algorithm is the basis of the cube pruning algorithm [Chiang, 2007], which is widely used
for language model integration and that we will describe later in this chapter; the lazy algorithm
is the basis of the cube growing algorithm, which aims to give a better trade-oﬀ in search accuracy
against time [Huang and Chiang, 2007].
e Eager k-best Derivation Algorithm
Before presenting the algorithm, let us spend a little time exploring the problem. Consider the
hypergraph fragment shown in Figure 3.2, which we will imagine to be part of a much larger
hypergraph (for instance, if the hypergraph were a CFG parse forest, then there could be hundreds
of thousands of hyperedges). Suppose that we wish to ﬁnd the k-best derivations of vertex v,
considering only a single incoming hyperedge e. Huang and Chiang’s algorithms require the
derivation weight function to be monotonic, so let us assume that to be the case.
ere are three vertices in e’s tail, each of which may have many derivations of their own.
Suppose that we have already found the k-best derivations of v1, v2, and v3 (in dynamic program-
ming terms, we have solved the subproblems). We can immediately generate the 1-best derivation
of v: it is he; Si where S is the set of 1-best derivations from v1, v2, and v3 (this is exactly what we
did in the Viterbi max-derivation algorithm). Now suppose that we want to generate the 2-best
derivation. is requires a little more work: there are three candidate derivations and the only
way to ﬁnd the best of the three is to generate them all and compute their weights. e three
candidates are he; S1i, he; S2i, and he; S3i, where
62 3. DECODING I: PRELIMINARIES
v1 v2 v3
v
e
Figure 3.2: A hypergraph fragment.
• S1 contains the 2-best derivation of v1 and the 1-best derivations of v2 and v3;
• S2 contains the 2-best derivation of v2 and the 1-best derivations of v1 and v3; and
• S3 contains the 2-best derivation of v3 and the 1-best derivations of v1 and v2.
(e fact that one of these derivations is the 2-best follows directly from the monotonicity of
the weight function.) Suppose that the 2-best derivation turns out to be S2. What is the 3-best
derivation? It could be one of the two existing candidates, he; S1i or he; S3i, or it could be one of
three new candidates, he; S4i, he; S5i, or he; S6i, where:
• S4 contains the 2-best derivation of v2, the 1-best derivations of v1, and the 2-best deriva-
tion v3;
• S5 contains the 2-best derivation of v2, the 2-best derivations of v1, and the 1-best deriva-
tion v3; and
• S6 contains the 3-best derivation of v2, the 1-best derivations of v1, and the 1-best deriva-
tion v3.
e only way to determine which of the ﬁve candidates is the 3-best derivation is by generating
he; S4i, he; S5i, and he; S6i then computing their weights.
is process can be repeated until we have found the k-best derivation (or run out of candi-
dates). At each step, three new candidate derivations are revealed (these are always the “neighbors”
of the last candidate that was chosen). e weights of the newly uncovered candidates are com-
puted and then one of the candidates is chosen (and removed from the pool of candidates). To
keep the candidates in order, we can store them in a max-priority queue, where the priority is the
weight of the derivation.
With only minor changes, this search procedure can be extended to accommodate vertices
with multiple incoming hyperedges. Suppose that vertex v has n incoming hyperedges, as depicted
in Figure 3.3. We can begin by priming our priority queue with the single-best derivation for each
of the n hyperedges, since we know that one of those must be the overall 1-best. e ﬁrst pop
3.2. ALGORITHMS ON HYPERGRAPHS 63
v
e1
e2
en
en-1
Figure 3.3: A vertex with multiple incoming hyperedges.
Algorithm 5 Partial sketch of the k-best search procedure.
Input : v, a vertex
k, an integer  1
Output : kbest, a list of the k-best derivations of v
E-K-B-V.v; k/
1 kbest D cand D ¿
2 for each incoming hyperedge e of v
3 d D the best derivation of v along e
4 P.cand; d/
5 while jkbestj < k and jcandj > 0
6 d D P-M.cand/
7 append d to kbest
8 P-N.cand; d/
9 return kbest
from the queue therefore yields the 1-best derivation of v. Now the 2-best derivation must either
be one of the candidates remaining in the queue or a neighbor of the 1-best derivation. e next
step is therefore to generate the neighbors of the freshly popped derivation and push them to
the queue. Popping from the queue will now yield the 2-best derivation. is process of popping
derivations and then pushing their neighbors can be repeated until we either have k derivations
or run out of candidates. e monotonicity of the derivation weight function guarantees that this
method will yield the derivations in the correct order.
Algorithm 5 is a pseudocode version of the search procedure as described so far.
E-K-B-V assumes that the k-best subderivations have already been found for all
predecessor vertices. Lines 2–4 prime the priority queue,⁴ cand, which is keyed on the derivation
⁴Huang and Chiang [2005] use a more eﬃcient queue priming method, which is based on the observation that when v has
more than k incoming hyperedges, we need only push the best k derivations to cand. For n hyperedges, we can select the
best k derivations in O.n/ time and the cost of priming the queue becomes O.n/ C O.k  log k/. is is an improvement
64 3. DECODING I: PRELIMINARIES
weight (in pseudocode, the calculation of the derivation weight will be implicit). Lines 5–8 re-
peatedly carry out the pop derivation/push neighbors method until either the list kbest has been
ﬁlled or there are no more candidate derivations.
To deﬁne the P-N function, we require an algorithmic means of generating
a derivation’s neighbors. We ﬁrst need to introduce an alternative representation for derivations.
Deﬁnition 3.16 (Derivation with Back-Pointers). A derivation with back pointers is a triple
he; e; xi, where he; ei is an ordered hyperedge and x is a coordinate vector in which each
element xi indicates the position of the ith
subderivation in its respective k-best list. Note that
the ordering relation e is only required to link subderivations with elements of x. If a hyperedge
is not already ordered then the relation can be deﬁned arbitrarily. As a notational convenience, we
will drop the ordering relation and write he; xi when the relation can be unambiguously inferred
from context.
In our example, he; S1i would be represented as he; e; .1; 1; 1/i and he; S5i would be represented
as he; e; .2; 2; 1/i, assuming that e puts tail.e/ in the order v1, v2, v3.
Algorithm 6 gives pseudocode for the full eager k-best derivation algorithm. e input
hypergraph is required to be acyclic. It is also required to be ordered, but only for the purposes of
pairing up subderivations with elements of the coordinate vectors. For an unordered hypergraph,
an arbitrary ordering can be imposed.
e top-level function, E-K-B, uses the V procedure from Algorithm 1. is
calls E-K-B-V on vertex t, the target vertex, but only after ﬁrst calling E-K-
B-V on every predecessor of t (in topological order).
e E-K-B-V procedure is identical to the version we have just described
except for the use of the back pointer representation of derivations and the use of a global, vertex-
speciﬁc k-best list. At line 3, the symbol 1 indicates a vector of length j tail.e/j in which every
element has the value 1. e derivation he; 1i is therefore the best derivation of v along that
particular hyperedge.
Finally, the E-P-N procedure loops over the predecessors of e, generat-
ing one new derivation for each. At each iteration, the current subderivation for the ith
predecessor
is replaced with its next best subderivation. is requires that we increment element xi in the co-
ordinate vector. In the pseudocode, this is achieved by summing x and bi, where the latter is
deﬁned to be a vector of length jxj, which contains the value 1 at position i and 0 everywhere
else. e resulting neighbors are added to the priority queue subject to two conditions, both given
in line 3. e ﬁrst condition is that there are suﬃcient subderivations for the new coordinates to
be valid—if a predecessor vertex only has 17 derivations then we cannot generate a neighbor with
coordinate 18. e second condition is that the neighbor has not already been added (since a
derivation can neighbor more than one other derivation).
over the O.n  log n/ cost of pushing all n derivations to the heap. For readability, we will use the simpler (but less eﬃcient)
method in pseudocode.
3.2. ALGORITHMS ON HYPERGRAPHS 65
Algorithm 6 Eager k-best Extraction
Input : hhV; Ei; E i, an ordered directed acyclic hypergraph
WD, a monotonic derivation weight function
t, a target vertex
k, an integer  1
Output : k-best derivations of t
E-K-B.hhV; Ei ; E i; WD; t; k/
1 for v 2 V
2 visitedŒv D 
3 V.t; E-K-B-V/ /
/ See Algorithm 1
4 return kbestŒt
E-K-B-V.v/
1 kbestŒv D cand D ¿
2 for each incoming hyperedge e of v
3 P.cand; he; 1i/
4 while jkbestŒvj < k and jcandj > 0
5 he; xi D P-M.cand/
6 append he; xi to kbestŒv
7 E-P-N.cand; he; xi/
E-P-N.cand; he; xi/
1 for i D 1 : : : jej
2 vi D pred.e; e; i/
3 if xi < jkbestŒvi j and he; x C bii 62 cand
4 P.cand; he; x C bii/
e Lazy k-best Derivation Algorithm
e lazy k-best derivation algorithm is based on the observation that the vast majority of
derivations generated and added to kbestŒv lists at the “bottom” of the forest will never become
subderivations of derivations at the “top” of the forest. e eager generation of those lower deriva-
tions is therefore wasted eﬀort. In fact, even for our tiny example the eﬀect is striking. Consider
again the hypergraph fragment in Figure 3.2. If k is 100 then, in the average case, v’s 100 best
derivations will only use 4 or 5 derivations each from v1, v2, and v3 (since 34
< 100 < 35
).
e lazy k-best derivation algorithm eliminates this wasted eﬀort by operating top-down
from the target vertex and only generating subderivations on demand. For instance, the 8-best
66 3. DECODING I: PRELIMINARIES
Algorithm 7 Lazy k-best Extraction
Input : hhV; Ei; E i, an ordered directed acyclic hypergraph
WD, a monotonic derivation weight function
t, a target vertex
k, an integer  1
Output : k-best derivations of t
L-K-B.hhV; Ei ; WD; t; k/
1 for v 2 V
2 visitedŒv D 
3 kbestŒv D candŒv D ¿
4 L-J-B-V(t, k)
5 return kbestŒt
L-J-B-V.v; j/
1 if visitedŒv D 
2 for each incoming hyperedge e of v
3 P.candŒv; he; 1i/
4 visitedŒv D 
5 while jkbestŒvj < j and jcandŒvj > 0
6 he; xi D P.candŒv/
7 append he; xi to kbestŒv
8 L-P-N.candŒv; he; xi/
L-P-N.cand; he; xi/
1 for i D 1 : : : jej
2 vi D pred.e; e; i/
3 L-J-B-V.vi ; xi C 1/
4 if xi < jkbestŒvi j and he; x C bii 62 cand
5 P.cand; he; x C bii/
derivation of v1 will only be generated if a candidate that uses the 7-best derivation is popped at
vertex v. Depending on how good the derivations of v2 and v3 are, this may never happen.
Pseudocode is given in Algorithm 7. Surprisingly, perhaps, very few changes are required to
turn the eager bottom-up iterative algorithm into a lazy top-down recursive algorithm. It is worth
comparing the pseudocode with that of Algorithm 6. e main diﬀerences are that instead of
always popping k derivations, L-J-B-V is given a parameter j  k which controls
3.3. HISTORICAL NOTES AND FURTHER READING 67
how many derivations are popped. And in L-P-N, a call is made to L-J-
B-V to create the jth
subderivation only when it is certain that it is required.
3.3 HISTORICAL NOTES AND FURTHER READING
Our deﬁnitions of hypergraphs and derivations are based on those of Huang and Chiang [2005]
(who build on the work of Klein and Manning [2001a] and Gallo et al. [1993]), but there are
minor diﬀerences in details (for instance, we do not assume an ordering among tail vertices).
e Viterbi algorithm presented here belongs to a family of algorithms that bear the name.
e common thread running through these algorithms is that they use dynamic programming to
ﬁnd the maximum probability solution to a problem. e algorithm family is named after Andrew
Viterbi, who described a version of the algorithm for calculating error probability bounds for
convolutional codes [Viterbi, 1967]. Variants were independently discovered by other researchers
shortly after.
Similarly, variations of the CYK algorithm were found by multiple researchers working
independently. e algorithm is named for three authors of early descriptions: the “C” is for John
Cocke [Cocke and Schwartz, 1970], the “Y” is for Daniel Younger [Younger, 1967], and the “K”
is for Tadao Kasami [Kasami, 1965]. It is also frequently referred to by the name CKY.
C H A P T E R 4
Decoding II: Tree Decoding
is chapter describes tree decoding, a two-stage approach to decoding in which the input sen-
tence is ﬁrst parsed and then the resulting parse tree is translated. We will focus on the second
stage, assuming that the input sentence has already been parsed. Tree decoding is the principal
decoding method for tree-to-string and tree-to-tree models.
To make the discussion more concrete, we will assume throughout this chapter that the
translation grammar is an STSG and that the probabilistic model is a log-linear model. Typically,
a shared-category STSG is used for tree-to-string models and a distinct-category STSG is used
for tree-to-tree models. Decoding with distinct-category STSGs is rare in practice and we will
therefore focus on the simpler, common case.
We will begin by describing decoding for models with rule-local features. ese models
are a perfect ﬁt for dynamic programming methods. Following common practice, we will make
the max-derivation approximation (described in Section 1.3.2). It turns out that the maximally
weighted derivation can be found using a combination of tree parsing and the Viterbi algorithm.
We will then describe approximate methods for decoding with non-local features, such as a n-
gram language model. In the ﬁnal section, we discuss alternative rule matching algorithms and
we sketch the extension to distinct-category STSGs (for use with tree-to-tree models).
roughout the chapter we will use the following input parse tree and STSG grammar as
part of a running example.
Example 4.1 Let  be the following German parse tree:¹
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP
.
.
.
.
.
verleumdet
.
.
.
.
.
NP
.
.
.
.
.
.
.
NE
.
.
.
.
.
K
.
.
.
.
.
NE
.
.
.
.
.
Josef
.
.
.
.
.
.
.
VMFIN
.
.
.
.
.
musste
.
.
.
.
.
NP
.
.
.
.
.
PIS
.
.
.
.
.
jemand
¹e sentence can be glossed “someone must Josef K. slandered have.”
70 4. DECODING II: TREE DECODING
and let G be the STSG with the following rules.
.
.
.
.
r1 .
.
= .
..
.
.
.
NP
.
.
.
.
.
.
.
NE
.
.
.
.
.
K.
.
.
.
.
.
NE
.
.
.
.
.
Josef
.
.
, .
..
.
.
.
NP
.
.
.
.
.
.
.
K.
.
.
.
.
.
Josef
.
. .
.
r2 .
.
= .
.
.
.
.
.
VVPP
.
.
.
.
.
verleumdet
.
.
, .
.
.
.
.
.
VVPP
.
.
.
.
.
slandered
.
.
r3 .
.
= .
.
.
.
.
.
VVPP
.
.
.
.
.
verleumdet
.
.
, .
.
.
.
.
.
VVPP
.
.
.
.
.
defamed
.
. .
.
r4 .
.
= .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP
.
.
.
.
.
NE1
.
.
, .
..
.
.
.
VP
.
.
.
.
.
.
.
NE1
.
.
.
.
.
VVPP2
.
.
r5 .
.
= .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP1
.
.
.
.
.
have
.
. .
.
r6 .
.
= .
. .
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP1
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
have
.
.
.
.
r7 .
.
= .
..
.
.
.
S
.
.
.
.
.
.
.
VP1
.
.
.
.
.
.
.
VMFIN
.
.
.
.
.
musste
.
.
.
.
.
NP
.
.
.
.
.
PIS
.
.
.
.
.
jemand
.
.
, .
. .
.
.
.
S
.
.
.
.
.
.
.
VP1
.
.
.
.
.
.
.
must
.
.
.
.
.
someone
.
.
r8 .
.
= .
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
.
.
.
.
.
.
.
VMFIN
.
.
.
.
.
musste
.
.
.
.
.
NP
.
.
.
.
.
PIS
.
.
.
.
.
jemand
.
.
, .
..
.
.
.
S
.
.
.
.
.
.
.
NP1
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
.
.
have
.
.
.
.
.
.
.
must
.
.
.
.
.
someone
.
.
r9 .
.
= .
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
.
.
.
.
.
.
.
VMFIN
.
.
.
.
.
musste
.
.
.
.
.
NP
.
.
.
.
.
PIS
.
.
.
.
.
jemand
.
.
, .
.
.
.
.
.
S
.
.
.
.
.
.
.
someone
.
.
.
.
.
.
.
by
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
.
.
been
.
.
.
.
.
.
.
have
.
.
.
.
.
.
.
must
.
.
.
.
.
NP1
4.1. DECODING WITH LOCAL FEATURES 71
4.1 DECODING WITH LOCAL FEATURES
Initially, we will assume that all of the log-linear model’s feature functions are rule-local. In other
words, for any feature function hi , the score of a derivation hi .d/ can be expressed as a sum of
scores of its component rules: hi .d/ D
P
r2d hi .r/. In practice, this is usually true for all feature
functions except the language model. Our ﬁrst version of the string decoding problem can thus
be expressed as follows.
Deﬁnition 4.2 (e Rule-Local String Decoding Problem). Given a source sentence s with
parse tree , a shared-category STSG G, and a conditional log-linear model p.djs/ with M
rule-local features, ﬁnd the most probable synchronous derivation, d
:
d
D arg max
d
X
r2d
M
X
mD1
mhm.r/;
where d 2 D.G; / is any derivation with source derived tree .
4.1.1 A BASIC DECODING ALGORITHM
Decoding with a rule-local model is straightforward in principle: all we need to do is construct
a hypergraph representation of the STSG translation forest and then apply the Viterbi max-
derivation algorithm. To put this into practice we need to answer two questions: (i) how do we
construct the hypergraph? and (ii) what is the derivation weight function for our rule-local STSG
model?
e hypergraph can be constructed by several methods of varying complexity and eﬃciency.
Perhaps the simplest method is to visit each node n of the tree and attempt to pattern match the
source-side of every rule against the subtree rooted at n. For every rule that matches, a hyperedge
is added to the hypergraph.
By pattern matching, we mean that the source-side fragment must be identical to the sub-
tree, except at the fragment’s non-terminals—a non-terminal indicates a gap in the form of an
omitted subtree. Algorithm 8 gives a simple recursive method. e algorithm essentially per-
forms a basic pre-order tree comparison, but with special handling of non-terminals. For each
non-terminal in the fragment, line 4 inserts the corresponding subtree node into the sites set. As
we will see shortly, this information will be used to construct the hyperedge corresponding to the
rule match.
For a rule-local log-linear model, deﬁning the derivation weight function is easy: the weight
of a hyperedge e labeled with rule r is simply the weighted-sum of the feature function values for
that rule, and the weight of a derivation d D he; Si is the sum of the derivation’s hyperedge and
subderivation weights:
WE .e/ D
M
X
mD1
mhm.r/ and WD.d/ D WE .e/ C
X
d02S
WD.d0
/: (4.1)
72 4. DECODING II: TREE DECODING
Algorithm 8 A Basic Rule Matching Algorithm
Input : m root node of rule fragment
n root node of subtree
sites output tree node set
Output : match Boolean ﬂag indicating match
M-R.m; n; sites/
1 if label.m/ ¤ label.n/
2 return 
3 if jchildren.m/j D 0 and jchildren.n/j > 0
4 (sites, m)
5 return 
6 if jchildren.m/j ¤ jchildren.n/j
7 return 
8 for i D 0 to jchildren.m/j 1
9 if not M-R.children.m/Œi; children.n/Œi; sites/
10 return 
11 return 
e derivation weight function is equivalent to summing the weights of all the hyperedges along
the derivation.
We now have everything we need. Algorithm 9 gives the pseudocode for a basic tree decod-
ing algorithm. e function C-H uses M-R to determine every
possible match of a grammar rule against the input parse tree. C-H begins
with an empty set of vertices, V , and hyperedges, E (lines 1 and 2). Lines 3–11 then perform a
post-order traversal of the tree, which for each node, n, generates the corresponding vertex, V Œn,
(line 4) and its incoming hyperedges (lines 5–11). While we have speciﬁed a post-order traversal,
any bottom-up order would work equally well.
Once the hypergraph has been constructed, we apply the Viterbi max-derivation algorithm
(line 2 of T-D-). Recall that V-D (Algorithm 2 from Chapter 3)
records the maximally weighted derivation of each vertex in a data structure called best. All that
remains to do is to look-up the best derivation for the vertex corresponding to the root node of
the input tree (line 3).
e algorithm can be modiﬁed to return the k-best derivations by substituting one of
the k-best algorithms (Algorithm 6 or Algorithm 7) for the Viterbi algorithm and returning
kbestŒV Œroot./ at line 3.
4.1. DECODING WITH LOCAL FEATURES 73
Algorithm 9 A Basic Tree Decoding Algorithm
Input :  a parse tree
G a shared-category STSG
WD a monotonic derivation weight function
Output : d
the maximally-weighted derivation of 
T-D-.; G; WD/
1 hV; Ei D C-H.; G/
2 V-D.hV; Ei; WD/
3 return bestŒV Œroot./
C-H.; G/
1 V D ¿
2 E D ¿
3 for each node n of , visited in post-order
4 add vertex V Œn
5 for each source-side tree fragment ˛ in projs.G/
6 sites D ¿
7 if M-R.root.˛/; n; sites/
8 head D fVŒng
9 tail D fVŒn0
 W n0
2 sitesg
10 for each rule r in proj0
s.˛/
11 add hyperedge hhead; tail; ri to E
12 return hV; Ei
Shortcomings of the Current Algorithm
As it stands, the tree decoding algorithm has a number of shortcomings (in addition to the limi-
tation that the model must be rule-local).
• Hypergraph construction, while linear in the number of input tree nodes,² potentially has a
very large constant factor: the algorithm tries to match every single source-side fragment in
G at every single node of . For now, we will ignore this source of ineﬃciency, but we note
that signiﬁcantly better algorithms are known. We will return to this issue in Section 4.4.
• e algorithm will fail to produce a derivation if there is no combination of source-side
fragments that fully covers the tree. In practice, this is a frequent occurrence: the input
to a decoder often contains words that were not observed during training (and so do not
²Assuming a constant limit on the size of a rule’s tree fragment.
74 4. DECODING II: TREE DECODING
occur in any grammar rule). A simple solution is to test, after the loop at lines 5–11 in
C-H, whether any hyperedges were produced and if not, to generate
a glue rule that is guaranteed to match. For a tree node n, the simplest possible glue rule
would have a 1-level source-side fragment with n’s label at its root node and one child non-
terminal for each of n’s children. e target-side would simply duplicate the source-side,
with a monotonic correspondence between source and target non-terminals. For instance,
in our example grammar (Example 4.1), there is no rule that matches the input tree at the
lower VP node. e monotonic 1-level glue rule for this node would be:
.
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
To allow the decoder to reorder the input, non-monotonic could also be generated. In prac-
tice, the use of non-monotonic glue rules has been shown to improve translation quality,
although the improvements are usually modest (see, for example, Chung et al. [2011] or Li
et al. [2013]). Additionally, a decoder may employ specialized routines to generate rules for
certain classes of unseen word. For instance, to convert dates into the correct target-speciﬁc
format or to transliterate named entities.
• For the max-derivation version of the algorithm, it is wasteful to construct the entire hyper-
graph: when a source-side fragment, ˛, has been matched, there is no need to add a hyper-
edge for every rule in proj0
s.˛/. Rule-locality means we need only add one (for the rule with
the highest weighted sum,
PM
mD1 mhm.r/). We could go further still and merge hyper-
graph construction with Viterbi search in an analogous fashion to the CYK max-derivation
algorithm (Algorithm 4). is would not aﬀect the time complexity of the algorithm (it still
has to generate a candidate derivation for every hyperedge) but it would reduce the space
complexity by eliminating the need to store most hyperedges.
• For the k-best version of the algorithm, we can also eliminate hyperedges, although not to
the same extent as for the max-derivation version. When a source-side fragment, ˛, has
been matched, we need to add a hyperedge for the k-best rules in proj0
s.˛/. By making a
minor modiﬁcation to the k-best algorithm, we can reduce this to a single hyperedge by
treating the k rules as a group: instead of labeling hyperedges with a single rule identiﬁer,
we label them with a list of up to k identiﬁers, sorted by rule weight. We will describe this
modiﬁcation in detail in the next section.
4.1.2 HYPEREDGE BUNDLING
Figure 4.1 shows the STSG hypergraph that results from pattern matching the grammar rules of
Example 4.1 against the parse tree. For clarity of presentation, where there are multiple hyper-
4.1. DECODING WITH LOCAL FEATURES 75
VP
3,6
S
1,6
haben
NP
3,4
K.
VVPP
5,5
verleumdet
Josef
jemand musste
r1
r8, r9
r2, r3
r7
r6
Figure 4.1: Hypergraph representation of the translation forest for the input parse tree and STSG
grammar of Example 4.1 (we have omitted vertices with no incoming hyperedges). Hyperedges labeled
with multiple rules represent multiple hyperedges (one per rule).
edges with the same head and tail, we have shown a single hyperedge and labeled it with a list of
rules. ese multi-labeled hyperedges should be understood to represent multiple distinct labeled
hyperedges, with one per STSG rule. By modifying the hypergraph representation used by our
algorithm, we can perform a similar merging of hyperedges. is is sometimes called hyperedge
bundling. Since there is an ordering on rule weights, we can use this representation to improve
the performance of the k-best algorithm for our particular application.
At this point, it may appear that we are indulging in an act of premature optimization:
why complicate a simple, general algorithm by optimizing it for a particular application when it
may already meet our needs? If we were to stop at a rule-local model then that would be a fair
criticism. For now, keep in mind that we are working toward a model with non-local features. In
that setting, the search hypergraphs become dramatically larger and any optimization of this kind
is more likely to pay oﬀ. Most decoders do in fact perform hyperedge bundling of this sort and
so it is worthwhile to introduce the method now in the simpliﬁed context of rule-local modeling.
Ultimately, this approach will be applied not in k-best derivation extraction, but in its non-local,
beam-search cousins.
To deﬁne the modiﬁed version of the k-best algorithm we ﬁrst need to update our repre-
sentations of hyperedges and hypergraph derivations. Instead of labeling a hyperedge with a rule
identiﬁer, r, we will label it with the identiﬁer, q, of the source-side projection, projs.r/. For each
source-side tree fragment, q, we will assume the existence of an array containing the rules of the
inverse projection, proj0
s.q/, sorted by weight. To distinguish the two representations, we will refer
to this form of hypergraph as a bundled hypergraph. e deﬁnition of a derivation with backpoint-
76 4. DECODING II: TREE DECODING
Algorithm 10 e Eager-Push-Neighbors Algorithm, Adapted for Bundled Hypergraphs
Input : m root node of rule fragment
n root node of subtree
sites output tree node set
Output : match Boolean ﬂag indicating match
E-P-N-B.cand; he; x; yi/
1 for i D 1 : : : jej
2 vi D pred.e; e; i/
3 if xi < jkbestŒvi j and he; x C bi; yi 62 cand
4 P.cand; he; x C bi; yi/
5 if y < jproj0
s.e/j and he; x; y C 1i 62 cand
6 P.cand; he; x; y C 1i/
ers (Deﬁnition 3.16 in Chapter 3) is extended for bundled hypergraphs to include an index into
one of these sorted rule arrays. e hyperedge e of a derivation with backpointers, is thus bound
to a distinct STSG rule through the combination of the source-side fragment identiﬁer and an
array index:
Deﬁnition 4.3 (Bundled Derivation with Back-Pointers). A bundled derivation with back
pointers is a tuple he; e; x; yi, where he; ei is an ordered hyperedge in a bundled hypergraph, x
is a coordinate vector in which each element xi indicates the position of the ith
subderivation in
its respective k-best list, and y is an index into an array that contains the synchronous rules from
proj0
s.q/, where q is the source-side rule projection corresponding to e.
e only substantive change required to the k-best algorithm is in P-N. Algo-
rithm 10 shows the eager version (the changes required for the lazy version are identical). Lines
1–4 are the same as in the original E-P-N (Algorithm 6) except for the pres-
ence of index y in the derivation tuples. Lines 5–6 create the neighbor that diﬀers from the input
derivation only in its translation. It is assumed that the array indexed by y is ordered best-ﬁrst by
rule weight.
4.2 STATE SPLITTING
In this section and the next, we discuss methods for decoding with non-local features. While
all of the methods presented here were originally developed for n-gram language models, they
can be (and have been) applied to other types of non-local feature. We begin by adding a bigram
language model feature to our log-linear model.
4.2. STATE SPLITTING 77
4.2.1 ADDING A BIGRAM LANGUAGE MODEL FEATURE
As a running example, we will suppose that our log-linear model includes a bigram language
model among its M feature functions. If the language model is the ith
feature function and all
other feature functions are rule-local then the objective function becomes
d
D arg max
d
0
@i log p.d/ C
X
r2d
X
m¤i
mhm.r/
1
A ; (4.2)
where d 2 D.G; / is any derivation with source-side derived tree  and p.d/ is the language
model probability of d’s target-side yield. e bigram language model probability for a sentence
s D w1; w2; : : : ; wjsj is
p.s/ D
jsjC1
Y
iD1
p.wi j wi 1/; (4.3)
where w0 D  and wjsjC1 D  are distinguished tokens that mark the beginning and end of
a sentence.
Of course, the objective function as currently deﬁned is not suitable for dynamic program-
ming: the search algorithm cannot wait until it has produced a derivation of a complete sentence
before scoring it with the language model feature. For search to be eﬃcient, the algorithm must
exploit the independence assumptions of the bigram language model. While our current algo-
rithm fully exploits the independence assumptions of the rule-local features, it is incompatible
with those of the language model. To demonstrate this, let us return to our example STSG hy-
pergraph (Figure 4.1). Consider the two derivations of the vertex labeled .VP; 3; 6/. Both deriva-
tions include the hyperedges labeled r1 and r6, but diﬀer in whether they also include r2 or r3.
e rules r1, r2, r3, and r6 have the following target-sides, respectively:
.
.
.
..
.
.
.
NP
.
.
.
.
.
.
.
K.
.
.
.
.
.
Josef
.
. .
.
.
.
.
.
VVPP
.
.
.
.
.
slandered
.
. .
.
.
.
.
.
VVPP
.
.
.
.
.
defamed
.
. .
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP1
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
have
e two derivations therefore have the target yields “have slandered Josef K.” and “have defamed
Josef K.” We will call these derivations d1 and d2, respectively. Now suppose that d1 and d2
become subderivations of some larger derivation, d3. To calculate the weight of d3, the derivation
weight function must include one of the terms p.slandered j have/ or p.defamed j have/ and one
of the terms p.Josef j slandered/ or p.Josef j defamed/. Without taking into account which of
these terms the language model favors, and by how much, it is impossible to determine which of
d1 or d2 will make the better subderivation. In general, it is not possible to deﬁne a decomposable
derivation weight function over the hypergraph when the score of a derivation depends on the
target yields of its subderivations.
78 4. DECODING II: TREE DECODING
4.2.2 THE STATE-SPLIT HYPERGRAPH
e impossibility of deﬁning a monotonic derivation weight function over the translation hy-
pergraph is the result of an incompatibility between the hypergraph structure and the indepen-
dence assumptions of the bigram language model. By restructuring the hypergraph it is possible
to remove this incompatibility. In fact, through a process of state splitting, it is always possible to
construct a hypergraph that encodes a STSG translation forest while also respecting the indepen-
dence assumptions of any non-local features (although, as we will see, the practicality of actually
doing so is another matter).
In the context of incorporating a n-gram language model feature, this process is often
referred to as intersection, since it is equivalent to Bar-Hillel et al. [1961]’s method for intersecting
a context-free grammar with a ﬁnite-state automaton (the translation forest is context-free and
the n-gram language model is a ﬁnite-state automaton³). For an n-gram language model of order
N, the method involves grouping the derivations of a vertex according to the N 1 left and right
boundary words of their target yield, since it is variation in the boundary words that compromise
the monotonicity of a potential derivation weight function. Each vertex is replaced by a set of
vertices with ﬁner-grained labels that encode the left and right boundary words.
For the bigram language model example, a minimally⁴ expanded hypergraph encodes the
leftmost and rightmost boundary words of a translation in each vertex label. Figure 4.2 shows part
of the STSG hypergraph from Figure 4.1 after being expanded to include the boundary words of
the translations. In order that the ﬁgure does not become unreadably cluttered, we have omitted
the two hyperedges arising from the application of rule r8 and we have omitted the vertex and
two hyperedges arising from the application of rule r9.
We can now deﬁne a monotonic derivation weight function, although doing so is a little
ﬁddly. We will begin by deﬁning a function to score a partial translation with the bigram language
model. Our ﬁrst version, h0
i .d/, takes a derivation d as input:
h0
i .d/ D log
jtj
Y
iD2
p.wi j wi 1/; (4.4)
where t D w1; w2; : : : ; wjtj is the target-side yield of d. In order that h0
i .d/ assigns the correct
language model score to a full sentence derivation, we require that the target yield is padded with
a  symbol to the left and a  symbol to the right. We will assume that the grammar contains
an additional rule,
³In practice, every target-side string of the translation grammar is usually also a valid string in the n-gram FSA. Although
the name “intersection” might suggest otherwise, the n-gram language model should be thought of as weighting derivations
rather than ﬁltering them.
⁴is is minimal if we treat the language model as a black box and assume that every sequence of N target words is potentially a
valid language model state. See Heaﬁeld [2011] for discussion of how the number of states can be minimized when knowledge
of the language model’s content is taken into account.
4.2. STATE SPLITTING 79
VP,3,6
have ... K.
haben
NP,3,4
Josef K.
K.
VVPP,5,5
slandered
verleumdet
Josef
r1 r2
r6
VVPP,5,5
defamed
r3
r6
S,1,6
Someone ... K.
musste
Jemand
r7
Figure 4.2: Partial STSG hypergraph for the input sentence and grammar of Example 4.1 after ex-
pansion to include boundary words. We have omitted the hyperedges for rules r8 and r9. (e dashed
hyperedge line is for visual clarity only: there is no diﬀerence in meaning.)
.
.
.
.
.
.
.
.
S
.
.
.
.
.
S
1
.
.
, .
.
.
.
.
.
S
.
.
.
.
.
.
.

.
.
.
.
.
.
.
S
1
.
.
.
.
.

e symbol S
replaces S
as the grammar’s start symbol. Now let us use h0
i to deﬁne a tentative
pair of weight functions:
WE .e/ D
X
m¤i
mhm.r/ (4.5)
and
WD.d/ D i h0
i .d/ C WE .e/ C
X
d02S
WD.d0
/ i h0
i .d0
/

: (4.6)
As currently deﬁned, WD.d/ does not decompose the calculation of the language model score.
Rather, it computes the score in full for each derivation and deducts the language model scores
of subderivations. In practice, querying the language model in order to calculate scores incurs a
signiﬁcant computational cost and, since a derivation usually shares most of its bigrams with its
subderivations, clearly the current method is wasteful. Instead, it is better to calculate for each
derivation the score for bigrams that appear only in that derivation, and not in any subderivation.
If we think of a derivation d D he; Si in terms of bottom-up rule application, the bigrams of
interest are those that are produced by the application of e’s rule. ese bigrams can be identi-
ﬁed by considering the target-side of e’s rule and the subderivations in S. ey are: (i) pairs of
80 4. DECODING II: TREE DECODING
Hypergraph Vertices Hyperedges
Original O.jj/ O.jGj  jj/
State-split O.jj  jTt j2.N 1/
/ O.jGj  jj  jTt j2.N 1/rank.G/
/
Table 4.1: Size of hypergraph for a translation forest (original) and the state-split version for a n-gram
language model (state-split). jGj is the number of grammar rules, jj is the number of input tree nodes,
Tt is the set of target-side terminals, N is the order of the n-gram language model, and rank.G/ is the
grammar’s rank.
consecutive terminals; (ii) the right and left border words of subderivations whose substitution
sites are neighboring pairs of non-terminals; (iii) the left border word of a subderivation whose
substitution site is a non-terminal that follows a terminal; and (iv) the right border word of a
subderivation whose substitution site is a non-terminal that precedes a terminal.
Let us give the name new-bigrams to the function that, given a derivation d D he; Si,
yields the set of freshly-produced bigrams, as enumerated above. Notice that new-bigrams does
not need to know anything about the subderivations other than their boundary words. Since the
boundary words are encoded in the vertices in e’s tail, new-bigrams is a function of e. Let us revise
our language model feature function:
hi .e/ D log
Y
.wj 1;wj /
p.wj j wj 1/; (4.7)
where .wj 1; wj / 2 new-bigrams.e/. We can now express the weight functions in a form that
matches the rule-local versions:
WE .e/ D i hi .e/ C
X
m¤i
mhm.r/ and WD.d/ D WE .e/ C
X
d02S
WD.d0
/: (4.8)
4.2.3 COMPLEXITY
We now have a dynamic programming method for ﬁnding the best derivation, or k-best deriva-
tions, of a model with non-local features—at least in theory. Algorithmically, it would be straight-
forward to fully expand the original translation forest hypergraph into a state-split hypergraph and
then apply standard max-derivation or k-best algorithms. Unfortunately, this approach is not fea-
sible in practice since the state-split hypergraph is too large: whereas a translation forest can be
represented as a hypergraph in O.jGj  jj/ space (where jGj is the number of grammar rules and
jj is the number of input tree nodes), with a n-gram language model, the state-split hypergraph
is much larger (see Table 4.1). With the full state-split forest out of reach, we must instead rely
on approximate methods that generate a manageable subhypergraph.
4.3. BEAM SEARCH 81
4.3 BEAM SEARCH
In this section, we present approximate methods for decoding with models that include non-local
features. All approximate tree decoding methods make use of beam search, a strategy that has been
widely-used elsewhere in natural language processing. In general terms, beam search is an approx-
imate form of best-ﬁrst search in which, at each step, partial solutions are ranked according to a
heuristic measure of how promising they appear to be, and all but the most promising solutions
are discarded. At a given step, the beam (or sometimes “stack”) is the set of promising partial
solutions that are deemed to be worth pursuing in a subsequent step.
In tree decoding, beams are used to conﬁne search to the most promising areas of the
state-split hypergraph. Decoding begins by constructing the basic translation forest hypergraph.
Each vertex of this hypergraph can be thought of as a window onto a set of ﬁner-grained vertices
belonging to the state-split hypergraph. As we have seen, the numbers of state-split vertices can
be unmanageably large and so a beam restricts the view to a manageable subset of state-split
vertices.
4.3.1 THE BEAM
Figure 4.3 depicts the search structure used for beam search. e base hypergraph is the translation
hypergraph from Figure 4.1, except that here we have labeled the hyperedges with source-side rule
identiﬁers. Every non-leaf vertex has an associated beam, which is essentially a list of k-best lists.
Additionally, every entry in the beam has an associated state value, which is equivalent to the
ﬁne-grained state in the state-split hypergraph.
ere are various ways to constrain the size of the beam. Usually, it is the number of entries
per beam (the number of rows in Figure 4.1) that is restricted and the maximum allowed size is
referred to as the beam size. Note that since each beam entry can hold multiple derivations, the
total number of derivations in the beam may be larger.
For a vertex v, we will write beamŒv to refer to that vertex’s beam; we will write beamŒvŒi
and stateŒvŒi to refer to the ith
k-best list and state object, respectively; and we will write
beamŒvŒiŒj to refer to an individual derivation in the k-best list.
A derivation over the beam search hypergraph has the form he; e; x; x0
; yi, closely re-
sembling a bundled derivation with backpointers (Deﬁnition 4.3). e diﬀerence here is that a
backpointer is a pair .xi ; x0
i /, where xi and x0
i are the ith
elements of x and x0
, respectively. For a
derivation he; e; x; x0
; yi, each back pointer .xi ; x0
i / refers to the subderivation beamŒui Œxi Œx0
i ,
where ui is the ith
vertex in tail.e/ (ordered by e). In other words, xi points to a k-best list and
x0
i points to a speciﬁc derivation within the list.
Notice that there is a direct correspondence between derivations in the beam search struc-
ture and derivations in the state-split hypergraph: every beamŒvŒiŒj corresponds to exactly one
state-split derivation. Of course, due to restrictions on the size of the beam, not every state-split
derivation can be represented in the beam search structure. Since all the information required to
calculate the state-split derivation weight function is also encoded in the beam-search structure,
82 4. DECODING II: TREE DECODING
VP
3,6
S
1,6
haben
NP
3,4
K.
VVPP
5,5
verleumdet
Josef
jemand musste
q1
q7
q2
q6
q5
d1, d2, d5, d7, ...
S,1,6
Someone ... K.
d3, d4, d10, d11, ...
d
d
S,1,6
Josef ... someone
Figure 4.3: e beam-based search structure for the input parse tree and STSG grammar of Exam-
ple 4.1. Each entry in the beam contains a representation of the search state (depicted as the equivalent
vertex in the state-split hypergraph) along with the group of derivations that share that state, ordered
by weight. We have depicted a larger number of derivations than are actually possible in the example
grammar. e “grayed-out” entries are the recombined derivations. ese are required for later k-best
derivation extraction, but redundant in max-derivation search.
the weight function carries over to the beam search structure with only superﬁcial diﬀerences. In
a slight abuse of notation, we will apply the existing function, WD, directly to derivations in the
beam search structure.
If we are using beam search as an approximate max-derivation algorithm, then it is un-
necessary for a beam entry to contain a k-best list. e beam need only record the single best
derivation for each state value. In this case, we can discard the other items without any risk of
increasing the search error.⁵ However, if we are using beam search as (part of) an approximate
k-best algorithm, then we should retain the items for later use. We will assume the latter case. To
refer to the Viterbi derivation formed from combining the 1-best subderivations, we will write
he; e; x; 1; yi.
4.3.2 REST COST ESTIMATION
One complication that arises from beam search is the requirement for a meaningful ordering be-
tween derivations with distinct search states. Recall that the current derivation weight function
⁵is is often called “recombination.”
4.3. BEAM SEARCH 83
ignores the ﬁrst N 1 words of a derivation and ignores the fact that the last N 1 words will
become the context of n-grams that extend to the right in subsequent derivations. ese simpli-
ﬁcations are ﬁne when comparing derivations that share the same N 1 boundary words (such
as those derivations inside a beam entry) since those derivations will all behave identically with
respect to the language model. e problem arises when comparing derivations belonging to dif-
ferent entries. ese comparisons are necessary because they are the basis for ordering the entries
within the beam.
In order to make weight comparisons meaningful, a heuristic method is used to adjust the
score. Here we will use the heuristic proposed by Heaﬁeld et al. [2012]⁶: if the language model
order is N , then the ﬁrst word is scored with a separate unigram model (estimated from the same
data); the second word is scored with a bigram model; and so on, up until the N 1th
, which is
scored with a language model of order N 1. e remaining words are scored using the same
method as previously (sub-N-grams are not scored for the N 1 right boundary words). e
adjusted bigram language model feature function is
h00
i .e/ D log
0
@p1
.w1/ 
Y
.wj 1;wj /
p2
.wj j wj 1/
1
A ; (4.9)
where w1 is the left-most word (yielded by any derivation d D he; Si), .wj 1; wj / 2
new-bigrams.e/ and 1 and 2 are the unigram and bigram language models, respectively.
e adjusted hyperedge and derivation weight functions are
W 0
E .e/ D i h00
i .e/ C
X
m¤i
mhm.r/ and W 0
D.d/ D W 0
E .e/ C
X
d02S
WD.d0
/: (4.10)
Note that the standard derivation weight function, WD, is used for scoring subderivations. It is
therefore necessary to calculate both the standard and adjusted derivation weights, WD.d/ and
W 0
D.d/, for every derivation d. e adjusted derivation weight is used only for ordering derivations
within the beam.
4.3.3 MONOTONICITY REDUX
We are now almost ready to describe the beam search algorithms. Before we do, let us recap the
(somewhat complicated) situation regarding search structures, derivations, and monotonicity.
1. For a model with non-local features, we cannot deﬁne a monotonic weight function over
derivations of the translation hypergraph.
⁶e estimation of the language model cost of the left-side boundary words has long been common practice [Li and Khudanpur,
2008]. However, these costs were usually obtained using probabilities from lower-order estimates of higher-order n-gram
models. Heaﬁeld et al. [2012] noted that these estimates were meant to be consulted in the context of a language model
backoﬀ routine, and showed that using separately trained lower order models resulted in better search.
84 4. DECODING II: TREE DECODING
2. rough state-splitting, it is possible (at least in theory) to construct a ﬁner-grained hyper-
graph over which we can deﬁne a monotonic derivation weight function (Section 4.2). For
this to be practical, the crucial question is the size of the hypergraph. If it is computationally
feasible to construct the hypergraph, then we can determine exactly the max-derivation or
k-best derivations using standard hypergraph algorithms from the previous chapter. is is
generally not possible when the model includes an n-gram language model.
3. For beam search, we take a diﬀerent tack: we perform search over the original hypergraph,
but we deﬁne a heuristic weight function that indicates how promising we believe a deriva-
tion to be for subsequent use as a subderivation. We hedge our bets by proposing multiple
promising derivations at each vertex (and storing them in a beam). Although the deriva-
tion weight function carries over essentially unchanged from the state-split hypergraph, the
change in hypergraph structure breaks the function’s monotonicity. In a nutshell, since a
vertex no longer distinguishes ﬁne-grained state values (with the distinction being made
within the beam instead), the maximally weighted derivation of a vertex is no longer guar-
anteed to be an optimal subderivation.
While non-monotonicty makes exact max-derivation and k-best derivation algorithms pro-
hibitively expensive, beam search allows us to approximate them.
• Beam search is an approximate max-derivation algorithm: after the algorithm has run, the
approximate best derivation is the highest scoring search item in the target vertex’s beam.
• Beam search combined with a standard k-best derivation algorithm gives an approximate
k-best algorithm: after beam search, the search structure can be converted into a state-split
hypergraph (which is a subhypergraph of the full state-split hypergraph) and standard k-
best extraction can be applied.
In the remainder of this section, we will describe four beam-ﬁlling algorithms. To help illustrate
how they work, we will use the following example.
Example 4.4 Consider the following hypergraph fragment:
e
v1 v2
v
-3.2
-3.3
-3.6
-4.2
-5.8
-6.5
-6.6
-6.7
4.3. BEAM SEARCH 85
e values shown in the two beams indicate the weight of the 1-best derivation for each
beam entry. Suppose that there are two derivation weight functions, one is monotonic and
the other not:
WD1
.d/ D WE .e/ C
X
d02S
WD1
.d0
/ and WD2
.d/ D WD1
.d/ C noise.d/:
e ﬁrst, WD1
.d/, simply sums the hyperedge weight and the subitem weights; the second,
WD2
.d/, calculates the same sum but adds a small amount of “noise”—a value between 0 and
-1.5 that can be determined given the derivation, but not from the subderivation weights.
Assume that WE .e/ returns the value -0.7.
We can depict the weights of all 16 possible subderivations in a 4 x 4 grid, where the
input weights for v1’s subderivations run along the vertical axis and the input weights for
v2’s subderivations run along the horizontal axis. e top-left corner represents the weight
of the derivation formed from the two best input subderivations and the bottom-right, the
derivation formed from the two worst input subderivations. e grids for the two weight
functions are:
-5.8 -6.5 -6.6 -6.7
-3.2 -9.7 -10.4 -10.5 -10.6
-3.3 -9.8 -10.5 -10.6 -10.7
-3.6 -10.1 -10.8 -10.9 -11.0
-4.2 -10.7 -11.4 -11.5 -11.6 and
-5.8 -6.5 -6.6 -6.7
-3.2 -10.6 -11.6 -11.6 -11.5
-3.3 -10.8 -10.5 -11.1 -11.5
-3.6 -10.7 -11.4 -11.2 -11.6
-4.2 -12.1 -11.9 -12.0 -11.7
Notice that in the ﬁrst grid, every weight is greater than its neighbor to the right or below. is
follows immediately from the deﬁnition of monotonicity. We can enumerate the derivations
in order by applying the priority-queue based search strategy used in the k-best algorithms
(Algorithms 6 and 7). is is not the case for the second, non-monotonic weight function.
4.3.4 EXHAUSTIVE BEAM FILLING
Exhaustive beam ﬁlling solves the problem of non-monotonicity by brute-force: at each vertex,
it enumerates every derivation (subject to the contents of the beams in predecessor vertices) then
sorts them by weight (including the rest cost estimate) and ﬁlls the beam with the most promising
k derivations. In Example 4.2, this method enumerates all 16 derivations. In practice, there will
usually be many more subderivations in each incoming beam, and there may be more than two
incoming beams. For instance, if k D 50 and some hyperedge has a tail with ﬁve vertices, then that
makes 505
D 312,500,000 derivations (just for that one hyperedge). Although still too expensive
86 4. DECODING II: TREE DECODING
Algorithm 11 A Beam Search-based Tree Decoding Algorithm
Input :  a parse tree
G a shared-category STSG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of 
T-D-.; G; WD; k/
1 hV; Ei D C-H.; G/ /
/ See Algorithm 9 (page 73)
2 for v 2 V
3 visitedŒv D 
4 t D V Œroot./
5 V.t; E-B-F/ /
/ See Algorithm 1 (page 56)
6 return .beamŒt/
E-B-F.v/
1 beamŒv D buffer D ¿
2 for each incoming hyperedge e of v
3 for x 2
˚
x1; : : : ; xjtail.e/j

W xi 2 range.jbeamŒpred.e; e; i/j
	
4 for y 2 range.jproj’s.e/j/
5 append he; x; 1; yi to buffer
6 R-S-P.buffer; beamŒv/
R-S-P.buffer; beam/
1 group buffer’s derivations according to state value
2 for each group g
3 sort g’s derivations by weight
4 beam D list of k best groups ordered by weight of group’s best derivation
to be practical, this is the simplest beam-ﬁlling method and will bring us within reach of a usable
search algorithm.
Algorithm 11 shows the pseudocode. e top-level function, T-D-, begins by
constructing the translation forest hypergraph. e V procedure calls E-B-
F on vertex t, the target vertex, but only after calling E-B-F on every pre-
decessor of t (in topological order).
At line 3 of E-B-F, every valid value of x is enumerated: this is the
Cartesian product of subderivation index sets (one index set for each predecessor vertex ui 2
4.3. BEAM SEARCH 87
tail.e/). e function pred.e; e; i/ returns the ith
predecessor vertex ui 2 tail.e/, subject to the
tail ordering, e, implicit in the translation forest. e function range.n/ returns the set of integers
in the range 1 : : : n. (Note in line 3 that the range calculations take account of the possibility that a
predecessor beam contains fewer than k entries.) At line 4, the STSG rule indices are enumerated
for the source-side rule speciﬁed by e. All derivations are collected in a list, called buﬀer. e
function R-S-P takes care of organizing the derivations into beam entries,
and then pruning the beam to k entries (the size of buﬀer is likely to far exceed the maximum
beam size, k).
If we are only interested in ﬁnding a single derivation (i.e., we are using beam search as
an approximate max-derivation algorithm), then the beam only needs to retain the maximally-
weighted subderivation for each distinct state and R-S-P can discard the sub-
optimal derivation. If we want to generate a k-best list from the (partial) state-split hypergraph
then the suboptimal derivations must be retained. In the pseudocode we assume the latter case.
4.3.5 CUBE PRUNING
Cube pruning is a heuristic beam-ﬁlling strategy. It is based on the assumption that the weight
function is approximately monotonic. In other words, it is based on the assumption that combin-
ing high-scoring subderivations makes a better strategy than combining low-scoring subderiva-
tions (while not guaranteeing the best result). In practice, this near monotonicity is a reasonable
assumption. e artiﬁcially generated grid in Example 4.2 exhibits the kind of near monotonicity
that is common in log-linear models with a n-gram language model. e noise factor (which re-
sembles the language model weight) is unpredictable, but it is only one component of the weight
function and has a limited ability to disorder the grid. On average, the items near the top-left will
be better than the items near the bottom-right.
e “cube” in the name “cube pruning” refers to the search grid that notionally contains the
derivations along a Hiero hyperedge. For the hyperedge corresponding to binary rule application,
the grid has three dimensions: the two incoming beams plus the translation dimension that results
from hyperedge bundling.
e cube pruning algorithm closely resembles the eager k-best algorithm (Algorithm 6).
It begins by generating the “corner” derivation and adding it to a priority queue. As each deriva-
tion is popped from the queue, its neighbors are constructed, weighted, and pushed. e crucial
diﬀerence is that when derivations are popped from the candidate queue, they do not arrive in
true best-ﬁrst order (because of the non-monotonicity). To compensate for this, the beam size is
typically increased, meaning that a larger number of derivations are popped for each vertex.
Figure 4.4 depicts the cube corresponding to the second, non-monotonic grid in Exam-
ple 4.2. As each derivation is popped from the candidate queue, its neighbors are pushed. e
popped derivations are gathered in a buﬀer, which will later be sorted before the derivations are
added to the beam.
88 4. DECODING II: TREE DECODING
Algorithm 12 e Cube Pruning Algorithm for STSG Derivations
Input :  a parse tree
G a shared-category STSG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of 
T-D-.; G; WD; k/
1 hV; Ei D C-H.; G/ /
/ See Algorithm 9 (page 73)
2 for v 2 V
3 visitedŒv D 
4 t D V Œroot./
5 V.t; CP-B-F/ /
/ See Algorithm 1 (page 56)
6 return .beamŒt/
CP-B-F.v/
1 beamŒv D cand D seen D buffer D ¿
2 for each incoming hyperedge e of v
3 P.cand; he; 1; 1; 1i/
4 while jbufferj < k and jcandj > 0
5 he; x; 1; yi D P-M.cand/
6 append he; x; 1; yi to buffer
7 CP-P-N.cand; seen; he; x; 1; yi/
8 R-S-P.buffer; beamŒv/ /
/ See Algorithm 11 (page 86)
CP-P-N.cand; seen; he; x; 1; yi/
1 for i D 1 : : : jej
2 vi D pred.e; e; i/
3 if xi < jkbestŒvi j
4 CP-P-I-N.cand; seen; he; x C bi; 1; yi/
5 if y < jproj0
s.e/j
6 CP-P-I-N.cand; seen; he; x; 1; y C 1i/
CP-P-I-N.cand; seen; d/
1 if d 62 seen
2 P.cand; d/
3 P.seen; d/
4.3. BEAM SEARCH 89
(a)
10:6 11:6 11:6 11:5
10:8 10:5 11:1 11:5
10:7 11:4 11:2 11:6
12:1 11:9 12:0 11:7 (b)
10:6 11:6 11:6 11:5
10:8 10:5 11:1 11:5
10:7 11:4 11:2 11:6
12:1 11:9 12:0 11:7
(c)
10:6 11:6 11:6 11:5
10:8 10:5 11:1 11:5
10:7 11:4 11:2 11:6
12:1 11:9 12:0 11:7 (d)
10:6 11:6 11:6 11:5
10:8 10:5 11:1 11:5
10:7 11:4 11:2 11:6
12:1 11:9 12:0 11:7
Figure 4.4: e cube after one (a), two (b), three (c), and four (d) derivations have been popped from
the priority queue and their neighbors pushed. e white boxes indicate the derivations that have
been popped, the light gray boxes indicate the contents of the candidate queue, and the dark gray
boxes indicate the derivations that have yet to be reached. Notice that the order in which derivations
are popped is approximately, but not exactly, monotonic.
Pseudocode for the cube pruning algorithm is given in Algorithm 12. CP-B-F is
the counterpart of E-K-B-V from Algorithm 6. As derivations are popped from
the priority queue they are collected in a buﬀer and then sorted after the pop derivation/push
neighbors loop (lines 4–7) has completed.
e function CP-P-N is identical to E-P-N-B
(from Section 4.1.2) except that the check for duplicate derivations is extended to allow for the
possibility that a derivation that was previously added to cand is popped before it is generated a
second time (an impossibility in the monotonic case). is is achieved through the use of a set
(called seen in the pseudocode) that records all of the derivations that have been generated for a
vertex. e function CP-P-I-S tests for the existence of a derivation in this set before
pushing it to the candidate queue.
4.3.6 CUBE GROWING
Cube growing [Huang and Chiang, 2007] is a beam-ﬁlling version of the lazy k-best algorithm
(Algorithm 7). Like its k-best counterpart, cube growing generates derivations on-demand—if
and when they are required as a subderivation of a larger derivation—instead of eagerly ﬁlling
every beam with k derivations.
Non-monotonicity makes lazily ﬁlling beams a more challenging problem than lazily ﬁlling
k-best lists. e diﬃculty in generating the jth
derivation of a vertex on-demand is of course that
cube-based generation does not yield derivations in true best-ﬁrst order. Like in cube pruning, the
algorithm must over-generate derivations and store them in a buﬀer. e challenge is restricting
90 4. DECODING II: TREE DECODING
10:6 11:6 11:6 11:5
10:8 10:5 11:1 11:5
10:7 11:4 11:2 11:6
12:1 11:9 12:0 11:7
9:7 0:9 10:4 1:2 10:5 ‹ 10:6 ‹
9:8 1:0 10:5 ‹ 10:6 ‹ 10:7 ‹
10:1 ‹ 10:8 ‹ 10:9 ‹ 11:0 ‹
10:7 ‹ 11:4 ‹ 11:5 ‹ 11:6 ‹
Figure 4.5: Left: the cube after the ﬁrst candidate has been popped from the priority queue. e cell
colors have the same meaning as in Figure 4.4. Right: the same cube with scores broken down into
their two components: the additive part and the noise part (if known). e white cells are those in
which the additive weight exceeds -10.6. ese are the cells that potentially hold a better derivation
than the ﬁrst candidate.
the growth of this buﬀer: if the buﬀer size reaches k, then the algorithm ceases to be any more
eﬃcient than cube pruning.
Huang and Chiang [2007] present cube growing in the context of language model integra-
tion. ey assume that the derivation weight function has two components: a monotonic, rule-
local part and a non-monotonic language model part. e language model always decreases⁷ the
weight. is resembles the behavior of our artiﬁcial noise function from Example 4.4. Returning
to that example, suppose that we want to lazily generate the 1-best derivation. If we pop the ﬁrst
derivation from the priority queue, then we arrive at the cube shown on the left in Figure 4.5. At
this point, we have no way of knowing if we have popped the true 1-best derivation. However, we
do have some knowledge about the weights of the unreached derivations: we know the additive
component of their weights and we know that this is an upper bound on the true weight (since
adding the noise component can only reduce the weight or leave it unchanged). From this we can
infer the region of the cube where better derivations potentially lie—and conversely we can safely
eliminate the remaining derivations from consideration. is information is summarized in the
right-hand cube of Figure 4.5.
Whereas our noise function has a minimum value of zero, potentially leaving the additive
weight unchanged, cube growing assumes that the language model will always reduce the weight
by some minimum value, ı. Huang and Chiang used a heuristic method to estimate a separate
value of ı for every hyperedge. ey do this as follows. First, they decode the input without the
language model to generate the k-best distinct derivations (k D 100 in their experiments). ey
then use the language model to compute the full hyperedge weights for all hyperedges occurring in
the k derivations. For each distinct hyperedge, they take the minimum observed weight diﬀerence
(with and without the language model) as the ı value. If a hyperedge is not represented in the k-
best derivations, they calculate the weight—with and without the language model—of the 1-best
derivation along that hyperedge and use that single weight diﬀerence as the ı value.⁸
⁷Huang and Chiang [2007] actually used derivation costs, where lower costs are better. We assume here that higher weights
are better.
⁸For further discussion of these heuristics, see also Vilar and Ney [2009] and Xu and Koehn [2012], who implemented cube
growing for hierarchical phrase-based models.
4.3. BEAM SEARCH 91
Algorithm 13 e Cube Growing Algorithm for STSG Derivations
Input :  a parse tree
G a shared-category STSG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of 
T-D-.; G; WD; k/
1 hV; Ei D C-H.; G/ /
/ See Algorithm 9 (page 73)
2 for v 2 V
3 visitedŒv D 
4 beamŒv D candŒv D seenŒv D bufferŒv D ¿
5 CG-B-F.V Œroot./; k/
6 return .beamŒV Œroot.//
CG-B-F.v; j/
1 if visitedŒv D 
2 for each incoming hyperedge e of v
3 CG-P-I-N.candŒv; seenŒv; he; 1; 1; 1i/
4 visitedŒv D 
5 while jbeamŒvj < j and jbeamŒvj C jbufferj < k and jcandj > 0
6 he; x; 1; yi D P-M.candŒv/
7 append he; x; 1; yi to bufferŒv
8 CG-P-N.candŒv; seenŒv; he; x; 1; yi/
9 bound D max fCG-E-U-B.d/ W d 2 candŒvg
10 CG-R.bufferŒv; beamŒv; bound/
11 CG-R.bufferŒv; beamŒv; 1/
CG-P-I-N.cand; seen; he; x; 1; yi/
1 return if he; x; 1; yi 2 seen or y > j proj0
s.e/j
2 for i D 1 : : : jej
3 vi D pred.e; e; i/
4 CG-B-F.vi ; xi /
5 return if xi > jbeamŒvi j
6 P.cand; he; x; 1; yi/
7 P.seen; he; x; 1; yi/
CG-R.buffer; beam; bound/
1 while jbufferj > 0 and M.buffer/  bound
2 add P-M.buffer/ to beam
92 4. DECODING II: TREE DECODING
Pseudocode for the cube growing algorithm is given in Algorithm 13. Broadly, the algo-
rithmic changes compared with cube pruning resemble the algorithmic changes from eager to
lazy k-best extraction: most notably in the single top-level call to CG-B-F at the root
vertex (line 5), which initiates a top-down recursive sequence of calls to CG-B-F with
varying values of the parameter j.
Let us examine the individual functions.
• CG-P-I-N pushes a (previously unseen) derivation onto a candidate priority queue,
but only after making calls to CG-B-F to ensure that the subderivations have been
created.
• CG-R shifts items from the buﬀer to the beam, but only if they have a weight
that matches or exceeds a threshold value. It is assumed that the derivation is added at the
correct position in the beam and is recombined if necessary.
• CG-E-U-B (not shown) computes an estimate of the maximum pos-
sible weight of a derivation. It does this by adding ı to the monotonic component of the
weight.
• CG-P-N (not shown) generates the neighbors of the given derivation and
pushes them to the priority queue. e function is identical to the cube pruning version
(CP-P-N in Algorithm 12) except that CG-P-I-N is used in place
of CP-P-I-N.
In experiments, Huang and Chiang demonstrate that cube growing can provide a better
speed-accuracy trade-oﬀ than cube pruning. While it provides an algorithmically interesting al-
ternative to cube pruning, cube growing is yet to see wide adoption.
4.3.7 STATE REFINEMENT
So far, all of the beam-ﬁlling algorithms have treated search state as atomic: two derivations either
have the same state value or they do not. Heaﬁeld et al. [2013] observed that states can have
varying degrees of similarity and that commonalities can be exploited to improve the accuracy of
beam search. ey focus on n-gram language model state, developing a bottom-up beam-ﬁlling
algorithm that takes into account the tendency for the language model to weight derivations more
similarly if they share boundary words. e algorithm begins with a coarse representation of state
in which all of the subderivations in a predecessor beam share the same state. As the algorithm
progresses, subderivation states are selectively reﬁned according to how promising a candidate
derivation appears to be.
As an example, imagine that we are applying rule r5 from the grammar in Example 4.1:
4.3. BEAM SEARCH 93
.
.
.
.
r5 .
.
= .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP1
.
.
.
.
.
have
e VP subderivation translates “Josef K. verleumdet,” which according to our grammar yields
“slandered Josef K.” or “defamed Josef K.” In a grammar learned from data, it is likely that the
translations include many alternatives:
“slandered K.,” “slandered him,” “defamed K.,” “defamed him,” “traduced Josef K.,”
“traduced K.,” …
Suppose that a 4-gram language model has been trained on data that contains many instance of
n-grams starting “have slandered” and “have defamed”
“have slandered Josef K.,” “have slandered K.,” “have defamed Josef K.,” …
but never containing the bigram “have traduced” (let alone “have traduced him” or “have traduced
Josef K.”). Conceivably, the model could have a strong enough preference for choosing one of
the translations beginning “slandered” or “defamed” that any translation beginning “traduced”
is barely worth considering. is kind of sub-state similarity can be used to prioritize groups of
subderivations during search.
Heaﬁeld et al.’s method groups subderivations at multiple levels of granularity. In eﬀect, it
transforms the structure of a beam from a ﬂat list into a tree. is tree is called a state tree, since
its nodes correspond to (language model) states and partial states, which are states with hidden
words. At the top of the tree, the root node corresponds to an empty state value (a partial state
where all words are hidden). Following a branch uncovers a single boundary word, with the leaves
corresponding to full state values. us, a state value becomes progressively reﬁned as one traces
a path from the tree’s root to a leaf. Figure 4.6 shows an example state tree. We have simpliﬁed it
by showing only right boundary words. A full state tree alternately adds left and right boundary
words (from outermost to innermost) as the tree grows from top to bottom.
e nodes of a state tree are given weights, which they inherit from their underlying deriva-
tions. Weights are assigned to nodes as follows.
• e weight of a leaf node is the weight of its best derivation. In Figure 4.6, the weight of
n10 is WD.d1/, the weight of n11 is WD.d3/, and so on.
• e weight of a non-leaf node is the weight of its best child. In Figure 4.6, the weight of
n5 is WD.d1/, the weight of n6 is WD.d6/, and so on.
Child nodes are ordered by weight, where the left child has the highest weight and the right child
has the lowest.
94 4. DECODING II: TREE DECODING
d8, d10, d18, ...
S,1,6
...have slandered him
d12, d22, d25, ...
S,1,6
...defamed by someone
d6, d9, d13, ...
S,1,6
... slandered by someone
d3, d4, d11, ...
S,1,6
... defamed Josef K.
d1, d2, d5, ...
S,1,6
... slandered Josef K.
d
d
d
d
d
d1, d2, d5 ...
n10
d3, d4, d11, ... d6, d9, d13, ... d12, d22, d25, ... d8, d10, d18, ...
n5
n2
n1
K.
n11
slandered defamed
Josef
n12
n6
n3
someone
n13
slandered defamed
by
n14
n7
n4
have
slandered
him
Figure 4.6: A beam (left) and the corresponding state tree (right). We assume the language model has
order four and so the state comprises (up to) three left and right boundary words. As a simpliﬁcation,
we show only right boundary words. We do not use state tree minimization.
e purpose of a state tree is to guide search. When a candidate derivation is being gener-
ated during bottom-up search, every predecessor beam has an associated state tree and these trees
determine the order in which subderivations are selected. To see how this works, we ﬁrst need to
deﬁne a couple of concepts. e ﬁrst is the breadcrumb.
Deﬁnition 4.5 (Breadcrumb). A breadcrumb is a pair hn; ci, where n is a state tree node and c
is a counter that tracks how many of n’s children have been visited. e weight of a breadcrumb
hn; ci is deﬁned as the weight of the state tree node n.
As we will see shortly, breadcrumbs are used to implement a form of backtracking.
Deﬁnition 4.6 (Partial Derivation). A partial derivation is a tuple he; e; B; yi, where he; ei
is an ordered hyperedge in a bundled⁹ hypergraph, B is a vector of breadcrumbs indicating the
positions of subderivation groups in their respective state trees, and y is an index into an array
that contains the synchronous rules from proj0
s.q/, where q is the source-side rule projection
corresponding to e. As for derivations with backpointers, we will drop the ordering relation and
write he; B; yi when the relation can be unambiguously inferred from context.
⁹In Heaﬁeld et al.’s original presentation, the hypergraph is not bundled. For consistency with the other beam-ﬁlling algorithms,
we will assume a bundled hypergraph, but will “unbundle” the partial derivations in the course of the search algorithm to avoid
changing the behavior of the original algorithm.
4.3. BEAM SEARCH 95
Notice the similarity of a partial derivation to a derivation with backpointers. In general, a partial
derivation does not represent a single derivation; rather, it represents a set of potential derivations,
each of which is reachable through state reﬁnement. Only at the maximum level of reﬁnement
(when all breadcrumbs point to state tree leaves) does a partial derivation correspond uniquely to
a derivation.
Like cube pruning and cube growing, Heaﬁeld et al.’s search algorithm uses a priority queue
to ﬁll beams. In this case, the queue holds partial derivations. Of course, to be used in a prior-
ity queue, a partial derivation must have a priority. is is calculated by summing the hyperedge
weight WE .e/ and the weights of the breadcrumbs,
P
i WD.Bi /. Recall from Section 4.2.2 that
the hyperedge weight function takes into account the cost of newly-produced n-grams. In our
bigram example, we deﬁned a function called new-bigrams that yielded this set. Here, the equiv-
alent function uses the breadcrumbs’ partial state to determine the set of newly produced n-grams.
e weight of a partial derivation therefore depends on how reﬁned the breadcrumbs are: the more
they are reﬁned, the better the estimated weights.
e queue is initialized with partial derivations in which all breadcrumbs pointing to root
state tree nodes and have a counter value of zero. As search progresses, the partial derivations are
reﬁned. When a fully-reﬁned derivation is popped it is added to a buﬀer for subsequent recom-
bination and insertion into the beam. When a partial derivation is popped from the queue, (up
to) two new partial derivations are pushed.
Algorithm 14 gives the pseudocode for the decoding algorithm. Let us examine the main
functions.
• SR-B-F has a similar structure to the cube pruning and cube growing beam-ﬁlling
functions. Line 1 initializes some vertex-speciﬁc variables. Lines 2–7 initialize the priority
queue by pushing the initial partial derivations. Note that we add one partial derivation for
each rule right-hand side (lines 6–7). is is for consistency with Heaﬁeld et al.’s presen-
tation, which does not use hyperedge bundling. Lines 8–14 pop partial derivations, either
adding them to the buﬀer if they are fully reﬁned (line 12) or partitioning them otherwise
(line 14). Line 15 moves items from the buﬀer to the beam. Finally, line 16 constructs the
state tree corresponding to the beam.
• SR-P attempts to expand the partial derivation he; B; yi, generating (up to) two
new partial derivations for each breadcrumb Bi in B. e ﬁrst is produced by reﬁning Bi
by following the leftmost branch in the state tree (lines 4 and 5). e second is produced by
advancing the breadcrumb counter, opening up a new region of the state tree for exploration
(lines 6–8).
• SR-C-N chooses the next state tree node to reﬁne from a vector of breadcrumbs.
Heaﬁeld et al. use the heuristic of picking the node with the fewest revealed words or in the
case of a tie, the left-most such node.
96 4. DECODING II: TREE DECODING
Algorithm 14 Heaﬁeld et al. [2013]’s State Reﬁnement Algorithm for STSGs
Input :  a parse tree
G a shared-category STSG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of 
T-D-.; G; WD; k/
1 hV; Ei D C-H.; G/ /
/ See Algorithm 9 (page 73)
2 for v 2 V
3 visitedŒv D 
4 t D V Œroot./
5 V.t; SR-B-F/ /
/ See Algorithm 1 (page 56)
6 return .beamŒt/
SR-B-F.v/
1 beamŒv D state-treeŒv D queue D buffer D ¿
2 for each incoming hyperedge e of v
3 B D ¿
4 for i D 1 : : : jej
5 append hroot.state-treeŒpred.e; e; i/; 0i to B
6 for y D 1 : : : jproj0
s.e/j
7 P.queue; he; B; yi/
8 while jbufferj < k and jqueuej > 0
9 he; B; yi D P-M.queue/
10 if every breadcrumb in B points to a leaf node
11 x D hx1; : : : ; xjBji, where xi is the index of the beam entry for Bi ’s node.
12 append he; x; 1; yi to buffer
13 else
14 SR-P.queue; he; B; yi/
15 R-S-P.buffer; beamŒv/ /
/ See Alg. 11 (page 86)
16 SR-B-S-T.beamŒv; state-treeŒv/
continued…
4.4. EFFICIENT TREE PARSING 97
Algorithm 15 Heaﬁeld et al. [2013]’s State Reﬁnement Algorithm for STSGs (continued)
SR-P.queue; he; B; yi/
1 i D SR-C-N.B/
2 hn; ci D Bi
3 B0
D B
4 B0
i D hL-C.n/; 0i
5 P.queue; he; B0
; yi/
6 if c < N-C.n/
7 B0
i D hn; c C 1i
8 P.queue; he; B0
; yi/
SR-C-N.B/
1 min D 1
2 imin D 0
3 for i D 1 : : : jBj
4 hn; ci D Bi
5 num D number of unrevealed words at node n
6 if num < min
7 min D num
8 imin D i
9 return imin
• SR-B-S-T (not shown) builds a state tree for a given beam. One possibility is
to construct the state tree in one go, as this function suggests. For eﬃciency, Heaﬁeld et al.
instead constructed the state tree lazily. e “tree” begins as a ﬂat list that is progressively
transformed into a tree as branch points are reached (i.e., at calls to L-C in SR-
P). We refer the reader to the original presentation for further details.
In experiments, the speed vs. accuracy trade-oﬀ is shown to be better than that of cube
pruning for Hiero and string-to-tree systems. For details, see the original paper.
4.4 EFFICIENT TREE PARSING
In Section 4.1.1, we described a simple algorithm for constructing a STSG forest hypergraph.
For an input tree  and grammar G, the algorithm has complexity O.jGj  /. For the grammars
used in state-of-the-art translation systems, the jGj term makes this algorithm prohibitively slow.
Fortunately, there are better algorithms.
98 4. DECODING II: TREE DECODING
Recall that in Section 1.2 we noted that, apart from superﬁcial diﬀerences, STSG rules
are equivalent to tree transducer rules of a speciﬁc, restricted form (namely, linear and non-
deleting¹⁰). is equivalence immediately opens up one method of hypergraph construction, since
we can compile the grammar into a tree transducer. ere are standard algorithms for transduc-
ing an input tree and the resulting structure is equivalent to the desired translation forest hyper-
graph. Matthews et al. [2014] present an eﬃcient version of this method that determinizes the
transducer, resulting in a compact deterministic ﬁnite-state automaton (DFSA). e input tree
is converted to a DFSA of the same form and transduction is performed using a standard DFSA
intersection algorithm.
An alternative hypergraph construction algorithm is given by Zhang et al. [2009]. eir
algorithm follows similar principles. In particular, it involves a similar compilation of the rule set
into a compact trie-style structure. However, Zhang et al. [2009] did not use standard DFSA
intersection. Instead, they design a specialized algorithm that accepts a parse forest hypergraph
as input. is makes the algorithm suitable for use in both tree-to-string and forest-to-string
models (we will introduce forest-to-string models in Chapter 6).
4.5 TREE-TO-TREE DECODING
So far, we have assumed that our model uses a shared-category STSG, which is the typical case for
a tree-to-string model. For a tree-to-tree model, the grammar would usually be a distinct-category
STSG (although, as we noted in Chapters 1 and 2, tree-to-tree models are rare in practice due to
the diﬃculty of learning grammars from data).
Decoding for tree-to-tree models diﬀers in the tree parsing step, but is otherwise essen-
tially the same as tree-to-string decoding. Recall from Section 3.1.3 that for a distinct-category
synchronous grammar, the translation hypergraph encodes both source and target non-terminal
symbols in the vertex labels. Tree parsing must therefore take account of the target-side tree struc-
ture. For a tree-to-tree model, a STSG rule h˛; ˇi can only be applied if:
1. the source-side tree fragment can be matched against the input parse tree (just as in shared-
category tree parsing); and
2. for each input parse tree node, v, where a leaf non-terminal of ˛ is matched,
A; B

) si : : : sj ; tm : : : tn,
where A D root.˛/; B D root.ˇ/, si : : : sj is the subspan of source words covered by v, and
tm : : : tn is any derivable sequence of target words.
¹⁰See Knight and Graehl [2005] for an excellent overview of tree transducers for natural language processing.
4.5. TREE-TO-TREE DECODING 99
Example 4.7 Consider the following STSG rule, which is a distinct-category variant of
rule r6 from the grammar in Example 4.1.
.
.
.
.
r0
6 .
.
= .
. .
.
.
.
VP
.
.
.
.
.
.
.
VAINF
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP2
.
.
.
.
.
NP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP1
.
.
.
.
.
VBN2
.
.
.
.
.
VB
.
.
.
.
.
have
Suppose that we are parsing the tree  (from that same example) and we want to apply rule
r0
6. e rule can only be applied if the source-side tree fragment can be matched against the
input parse tree and the following two conditions apply:
1. NP, NP

) Josef K, …
2. VVPP, VBN

) verleumdet, …
(e derived target strings can be any derivable word sequences.) In bottom-up tree parsing,
these two conditions can be tested by checking for the existence of vertices (NP,NP,3,4)
and (VVPP,VBN,5,5). For our example parse tree, applying rule r0
6 results in the generation
of the following hyperedge, but only if we assume the existence of rules that produce the
(NP,NP,3,4) and (VVPP,VBN,5,5) vertices:
VP,VP
3,6
haben
NP,NP
3,4
VVPP,VBN
5,5
r'6
It is straightforward to modify the simple shared-category tree parsing algorithm from
Section 4.1.1 to check for target-side non-terminal matches in the manner outlined above. More
eﬃciently, standard ﬁnite-state tree transducer methods could be used (similarly to tree-to-string
tree parsing—Section 4.4). Eisner [2003] also gives a simple algorithm that is eﬃcient provided
that there are tight limits on the number of nodes in a source-side tree fragment.
Once a distinct-category hypergraph has been constructed, beam-search can proceed as
before, giving a complete tree-to-tree decoding algorithm.
4.6 HISTORICAL NOTES AND FURTHER READING
Tree decoding, in various forms, was ﬁrst proposed for dependency-based translation models
[Ding and Palmer, 2005, Lin, 2004, Quirk et al., 2004, 2005] and shortly after for constituency-
based models [Huang et al., 2006, Liu et al., 2006]. e approach described here is closest to
Huang et al.’s, which was originally (and often still is) referred to as syntax-directed translation in
reference to its formal origins in compiler theory. Huang et al.’s approach was later extended by
Mi et al. [2008] to use forest decoding, which we will cover in Section 6.1.4.
e state-splitting approach to string decoding with a language model was used by Wu
[1996] to integrate a bigram language model into an ITG decoder.
e cube pruning algorithm was originally designed to solve the problem of integrating a n-
gram language model into the SCFG-based Hiero decoder [Chiang, 2007]. It was subsequently
generalized and applied to phrase-based and STSG-based decoding [Huang and Chiang, 2007],
as well as to reranking for statistical parsing [Huang, 2008]. Hopkins and Langmead [2009]
showed that the algorithm is equivalent to A* search using speciﬁc (inadmissible) heuristics.
C H A P T E R 5
Decoding III: String Decoding
String decoding has a lot in common with tree decoding. Almost all of the search machinery from
the previous chapter can be redeployed here (in fact, the methods were originally developed for
string decoding). So, how is string decoding diﬀerent? e answer depends on the grammar. For
some types of model—most notably Hiero—the only substantive diﬀerence is in the construction
of the translation hypergraph: whereas tree decoding matches rules against a tree, string decoding
employs monolingual string parsing. e hypergraph search techniques—beam search and the
beam ﬁlling algorithms—carry over unchanged.
For models with richer grammars, string decoding presents some additional challenges for
eﬃcient implementation. We will cover two prominent issues in this chapter, both of which can
be understood in terms of parse forest complexity. e ﬁrst challenge is decoding with large non-
terminal sets, such as those used in SAMT grammars. e second challenge is decoding with
non-binary grammars, such as GHKM grammars.
As in the previous chapter, we will assume a ﬁxed grammar and model form in order to keep
the discussion concrete. Speciﬁcally, we will assume that our grammar is a shared-category SCFG
and our statistical model is a log-linear model. Since we have already covered beam search, we
will assume from the outset that our model includes non-local features. Given this conﬁguration,
we will begin with the simplest possible decoding algorithm. We will then examine worst-case
parse forest complexity before discussing strategies for handling problematic cases. For most of
the chapter, we will treat the monolingual parser as a black box (if you keep in mind the CYK
parsing algorithm from Chapter 3 then this will not be too far wide of the mark). In Section 5.5,
we will open the box and study two parsing algorithms that are particularly well-suited for use in
decoding. Finally, in Section 5.6, we will discuss methods for decoding with STSGs and distinct-
category SCFGs.
As in the last chapter, we will use an input sentence and grammar in a running example.
Example 5.1 Let s be the following German sentence:
Jemand musste Josef K. verleumdet haben
Someone must Josef K. slandered have
and let G be the SCFG with the following rules:
102 5. DECODING III: STRING DECODING
r1 NP !Jemand , Someone
r2 NP !Josef K. , Josef K.
r3 VBN !verleumdet , slandered
r4 VBN !verleumdet , defamed
r5 VP !NP₁ VBN₂ haben , have VBN₂ NP₁
r6 S !NP₁ musste VP₂ , NP₁ must VP₂
r7 S !NP₁ musste NP₂ VBN₃ haben , NP₁ must have VBN₃ NP₂
r8 S !NP₁ musste NP₂ VBN₃ haben , NP₂ must have been VBN₃ by NP₁
e grammar rules in Example 5.1 are consistent with GHKM-style string-to-tree ex-
traction, where STSG rules are extracted and then converted to SCFG through the removal of
internal tree structure.
5.1 BASIC BEAM SEARCH
For an arbitrary input string s and a shared-category SCFG grammar G, the translation hyper-
graph hV; Ei can be constructed using little more than standard CFG parsing. e method is
analogous to the tree parsing-based method of the previous chapter: ﬁrst a monolingual parse
forest hypergraph is constructed by parsing s with the source-side CFG projection of G; then the
parse hypergraph is converted to a translation hypergraph by replacing hyperedges. Algorithm 16
shows the construction algorithm in pseudocode form.
Algorithm 16 Algorithm for constructing a shared-category SCFG hypergraph
Input : s a source string
G a shared-category SCFG
Output : hVs; Ei translation hypergraph
C-H.s; G/
1 hVs; Esi D P.s; projs.G//
2 E D ¿
3 for es 2 Es
4 for r 2 proj0
s.rule.es//
5 add labeled hyperedge hhead.es/; tail.es/; ri to E
6 return hVs; Ei
e P function (line 1) can be any CFG parsing algorithm that returns a hypergraph rep-
resentation of the parse forest. Lines 2–5 construct the corresponding translation hypergraph by
5.1. BASIC BEAM SEARCH 103
using the inverse projection to map source-side CFG rules to SCFG rules. In practice, the con-
structed hypergraph will usually be in the compact bundled form that we saw in Section 4.1.2.
Figure 5.1 shows the resulting SCFG hypergraph for the input sentence and grammar of Exam-
ple 5.1.
Algorithm 17 gives pseudocode for a basic string decoding algorithm that combines hy-
pergraph construction with bottom-up beam search. We have left the B-F function un-
speciﬁed. e most widely used beam-ﬁlling algorithm is cube pruning, but any of the bottom-
up beam-ﬁlling algorithms from the previous chapter could be used. Notice that the function
S-D- is almost identical to T-D- (Algorithm 11 on page 86).
As given, the algorithm does not handle parse failures. In practice, this is often unneces-
sary since a secondary glue grammar can specify rules to ensure that parsing will always succeed.
Alternatively, the algorithm could dynamically generate its own glue rules.
5.1.1 PARSE FOREST COMPLEXITY
In theory, S-D- works for any shared-category SCFG. However, the practicality of
the algorithm depends on the size of the parse forest hypergraph, which in turn depends on the
length of the input string, jsj, and on certain properties of the grammar, including the number of
rules, jGj.
e simplest grammar we are likely to encounter is a Hiero grammar. Recall from Chapter 2
that Hiero grammars are binary and have two distinct non-terminal symbols, X and S. Table 5.1
gives worst-case bounds for the numbers of vertices and hyperedges in a Hiero parse forest. How
did we arrive at these values? e vertex bound is simple: there are potentially two vertices for every
subspan of s, one for the X symbol and one for the S symbol, giving a maximum of 2  jsj2
D
O.jsj2
/ vertices.
For the hyperedge bound, consider the potential hyperedges arriving at a particular vertex v
and labeled with a particular rule identiﬁer r. By deﬁnition, there is one such hyperedge for every
possible application of rule r over the span corresponding to vertex v. Each distinct application
of a rule over a span of words wi : : : wj can be described by listing the subspans covered by its
subderivations. For example, consider the rule with the following source right-hand side:
Grammar Type Vertices Hyperedges
Hiero O.jsj2
/ O.jGj  jsj3
/
SAMT O.jNj  jsj2
/ O.jGj  jNj  jsj3
/
Arbitrary SCFG O.jNj  jsj2
/ O.jGj  jNj  jsjrank.G/C1
/
Table 5.1: Worst-case sizes of parse forest hypergraphs for Hiero, SAMT, and arbitrary SCFG gram-
mars. e input parameters are the source sentence length, jsj, the number of grammar rules, jGj, the
number of distinct non-terminal symbols, jNj, and the rank of the grammar, rank.G/.
104 5. DECODING III: STRING DECODING
VP,3,6
S,1,6
haben
NP,3,4
K.
VBN,5,5
verleumdet
Josef
Jemand musste
r2
r7, r8
r3, r4
r6
r5
NP,1,1
r1
Figure 5.1: SCFG hypergraph for the input sentence and grammar of Example 5.1. Hyperedges
labeled with multiple rules represent multiple hyperedges (one per rule).
Algorithm 17 A Beam Search-based String Decoding Algorithm
Input : s a source string
G a shared-category SCFG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of s
S-D-.s; G; WD; k/
1 hV; Ei D C-H.s; G/
2 for v 2 V
3 visitedŒv D 
4 t D V Œ1; jsj; S

5 V.t; B-F/ /
/ See Algorithm 1 (page 56)
6 return .beamŒt/
5.2. FASTER BEAM SEARCH 105
a b X c X.
Suppose that we want to enumerate rule applications over the span w5 : : : w13. One possible rule
application could have subderivations over w7 : : : w8 and w10 : : : w13, while another could have
subderivations over w7 : : : w10 and w12 : : : w13.¹ For a given span, we can summarize a binary rule
application with a single value: the number of words covered by the ﬁrst non-terminal (which has
O.jsj/ possible values). Once that value is known, the position of every other symbol is ﬁxed and
there is only one possible way of applying the rule—and one hyperedge. For example, if the width
of the ﬁrst subspan is three words, then that determines the position of w3, which must be 10.
e second non-terminal therefore covers words 11, 12, and 13.
To get the total number of hyperedges, we must multiply O.jsj/ by the number of rules
and by the number of vertices at which they can be applied. If we assume that every S rule applies
at every S vertex and every X rule applies at every X vertex, then we arrive at a total of jGj  jsj2
possible head vertex and rule pairs. Multiplying by O.jsj/ gives a total of O.jGj  jsj3
/ hyperedges.
For SAMT grammars, the analysis is identical except that we must take into account jNj,
the number of distinct non-terminal symbols. As we saw in Chapter 2, this number can run into
hundreds or even thousands.
For arbitrary SCFGs, the number of possible rule applications, and thus the number of
hyperedges, increases polynomially with rule rank: whereas for a ﬁxed binary rule and head vertex
there was one O.jsj/ choice (the width of the ﬁrst subspan) that uniquely determined a rule
application, there are two choices for a ternary rule, three for a quarternary rule, and so on. Unless
we take steps to address this issue, it eﬀectively rules out decoding with arbitrary SCFGs of rank
greater than two: a super-cubic number of hyperedges is likely to make decoding prohibitively
slow for all but the shortest sentences. We will return to this issue shortly.
5.2 FASTER BEAM SEARCH
Even for binary grammars with small non-terminal sets, string decoding is usually slow in com-
parison to tree decoding or phrase-based decoding. Most decoders therefore do not implement
the basic form of beam search outlined in the previous section, but make further approximations,
trading oﬀ search accuracy for performance. Two such measures are constrained width parsing
(typically used with Hiero and SAMT) and per-subspan beam search (typically used with SAMT
or GHKM models).
5.2.1 CONSTRAINED WIDTH PARSING
For Hiero and SAMT models it is standard to constrain parsing so that non-glue rules are only
applied over spans up to a constant length (typically 10 or 15 words). For longer spans, glue rules
¹Of course, we are ignoring the fact that most words in a typical source sentence are distinct. In a sentence with few (or no)
repeated words, the rule’s terminals are anchored to words, limiting the number of possible applications. We will discuss the
implications of this observation in Section 5.3.2.
106 5. DECODING III: STRING DECODING
Grammar Type Vertices Hyperedges
Hiero O.jsj/ O.jGj  jsj/
SAMT O.jNj  jsj/ O.jGj  jN j  jsj/
Arbitrary SCFG O.jNj  jsj/ O.jGj  jN j  jsjrank.G/ 1
/
Table 5.2: Worst-case sizes of parse forest hypergraphs for Hiero, SAMT, and arbitrary SCFG gram-
mars assuming a constant limit on span width for non-glue rule application. e input parameters are
as in Table 5.1. e rank of the grammar, rank.G/, is assumed to be greater than zero.
are used to combine suderivations from left-to-right (or sometimes allowing reordering). Part of
the motivation for this constraint is consistency with training: there is a constraint on the width
of initial phrases from which rules are extracted. If the source-side of a rule is extracted from an
initial phrase of length 10 then does it make sense to apply it over a span of length 30?
e second motivation is of course performance: for long sentences, constraining parsing
in this way dramatically reduces the size of the parse forest hypergraph. Table 5.2 gives worst-case
bounds if we assume a constant limit on subspan width for non-glue rule application.
While this restriction is well motivated (and borne out empirically) for string-to-string
models, it cannot be applied to string-to-tree models (recall that we deﬁned these as models that
are capable of producing trees of the same form as in the training data). Nor is it a panacea for
eﬃciency issues: if jN j or rank.G/ is too large, then decoding is still likely to be prohibitively
slow.
5.2.2 PER-SUBSPAN BEAM SEARCH
For grammars with large non-terminal sets, search complexity can be reduced by modifying the
beam ﬁlling algorithm so that instead of generating k derivations for every vertex, it generates
k derivations for every subspan. e modiﬁed beam-ﬁlling algorithm is called once per subspan
instead of once per vertex. e result is that instead of adding k derivations to a single beam,
the algorithm potentially distributes its k derivations to multiple beams, each ending up with less
than k derivations at the end.
is strategy is most eﬀective if running the per-subspan beam ﬁlling algorithm is faster
than running multiple instances of the per-vertex version (where there will be one instance for
every vertex over a particular subspan). is is easy to achieve for the non-exhaustive beam-ﬁlling
algorithms of the previous chapter. We will use cube pruning as an example.
To see how the beam-ﬁlling algorithm is extended to work over subspans, think back to
the discussion of the eager k-best algorithm (Section 3.2.4). e search procedure was ﬁrst pre-
sented at the hyperedge level, with one priority queue used to enumerate the derivations for a
single hyperedge. e procedure was then extended to share a priority queue across all incoming
hyperedges of a vertex. Here we extend the priority queue’s range further, initializing it with the
5.2. FASTER BEAM SEARCH 107
Algorithm 18 A String Decoding Algorithm with Per-span Cube Pruning
Input : s a source string
G a shared-category SCFG
WD a monotonic derivation weight function
k the maximum beam size (an integer  1)
Output : d
approximation to maximally-weighted derivation of s
S-D-.s; G; WD; k/
1 hV; Ei D C-H.s; G/
2 for w D 1 : : : jsj
3 for each subspan Œi; j of s with width w
4 Vi;j D
˚
v W v 2 V ^ span.v/ D Œi; j
	
5 M-CP-B-F.Vi;j/
6 t D V Œ1; jsj; S

7 return .beamŒt/
M-CP-B-F.Vi;j/
1 cand D seen D buffer D ¿
2 for v 2 Vi;j
3 for each incoming hyperedge e of v
4 P.cand; he; 1; 1; 1i/
5 beamŒv D ¿
6 while jbufferj < k and jcandj > 0
7 he; x; 1; yi D P-M.cand/
8 append he; x; 1; yi to buffer
9 CP-P-N.cand; seen; he; x; 1; yi/ /
/ See Alg. 12 (page 88)
10 for v 2 Vi;j
11 bufferv D fd W d D he; x; 1; yi 2 buffer ^ head.e/ D vg
12 R-S-P.bufferv; beamŒv; k/ /
/ See Alg. 11 (page 86)
he; 1; 1; 1i derivations from all incoming hyperedges of all vertices over a given subspan. A pop
from the queue can potentially yield a derivation for any of the input vertices.
Algorithm 18 gives the pseudocode. In S-D-, lines 2–5 iterate over every
subspan of s. Line 4 constructs the set of all vertices, Vi;j , over subspan i; j and line 5 passes
this vertex set to M-CP-B-F, the multiple-vertex version of the cube pruning beam
ﬁlling algorithm (Section 4.3.5).
108 5. DECODING III: STRING DECODING
M-CP-B-F diﬀers from the single-vertex version in two ways. First, the pri-
ority queue, cand, is initialized with he;  e; 1; 1i derivations from multiple vertices (lines 2–5).
Second, at lines 10–12, the buﬀer is separated into sub-buﬀers, with one for each vertex.
Using per-subspan beam ﬁlling can substantially reduce the cost of decoding. If constrained
width parsing is used, the beam ﬁlling algorithm is called O.jsj/ times instead of O.jNj  jsj/. If
parsing is unrestricted, the beam ﬁlling algorithm is called O.jsj2
/ times instead of O.jN j  jsj2
/.
Note that this adjustment to the beam ﬁlling algorithm does not come with any guarantees
about the distribution of derivations to beams. In fact, it is likely that many vertices’ beams will
not receive any derivations at all, since typical values of jNj can be similar to (or, in the case
of SAMT, much greater than) common values for k (a value somewhere between 30 and 1000
would not be surprising for either jN j or k). A minor adjustment to M-CP-B-F
could ensure that at least one derivation was popped for each incoming hyperedge, although this
may not be desirable, especially if jNj is very large. Treating vertices over a common subspan as
roughly equivalent and allowing derivations to compete within those spans, irrespective of the
vertex label, is an eﬀective way to control computational complexity and is often used in practice.
5.3 HANDLING NON-BINARY GRAMMARS
Neither of the techniques in the previous section addresses the polynomial multiplication of hy-
peredges that results from increasing rule rank. In GHKM grammars, rules with ﬁve or six non-
terminals are not uncommon and the problem must be addressed somehow. e most obvious
strategy is grammar binarization. As we will see, perfect binarization is generally not possible—
some SCFG (and STSG) grammar rules simply cannot be binarized—but this turns out not to
be a big problem in practice, since non-binarizable rules can be discarded with little or no impact
on translation quality. However, binarization has other, more serious drawbacks, which has led
researchers to consider alternative approaches. By developing a more nuanced analysis of parse
forest complexity, it has been demonstrated that not all non-binary rules are equally problematic.
For a high proportion of non-binary rules, it turns out that binarization is unnecessary. In the
simplest approach, scope pruning, problematic rules are simply discarded.
5.3.1 BINARIZATION
Before we discuss binarization for SCFGs, let us review how binarization works for CFGs.
CFG Binarization
CFG binarization replaces every super-binary rule with a set of rules of rank two or less, while
maintaining weak equivalence between the original grammar and the binarized grammar. e
exact details depend on the desired rule form for the output grammar, but generally speaking, each
individual rule A ! ˛ of rank three or more is replaced according to a recursive process. First, the
right-hand side, ˛, is split into two substrings, ˛1 and ˛2, where each substring contains at least
5.3. HANDLING NON-BINARY GRAMMARS 109
one non-terminal. e original rule is then replaced by one of three possible pairs of rules (the left
pair if ˛1 contains a single non-terminal, the middle pair if ˛2 contains a single non-terminal, or
the right pair otherwise):
A ! ˛1 Vx
Vx ! ˛2
or
A ! Vx ˛2
Vx ! ˛1
or
A ! Vx Vy
Vx ! ˛1
Vy ! ˛2
If a resulting Vx or Vy rule is super-binary then that rule is split and the process continues recur-
sively.
e non-terminals Vx and Vy are called virtual non-terminals. ese are symbols that
are introduced through the binarization process. e set of virtual non-terminals must be disjoint
from the original grammar’s non-terminal set. Although formally the choice of symbol is arbitrary,
by convention the symbol is usually a V followed by an integer (V1, V8721, etc.). A rule Vx ! ˛
is called a virtual rule.
In terms of grammatical equivalence, it makes no diﬀerence whether a terminal ends up on
the left or right of a split point. ere may, however, be reasons for taking terminals into account
when deciding where to split the rule (for instance, to maximize the sharing of virtual rules). Here
we will adopt a convention of always placing a split point immediately before a non-terminal.
Example 5.2 e grammar in Example 5.1 contains two ternary rules, r7 and r8. We will
use the source-side projection of r7 as an example:
S ! NP musste NP VBN haben
ere are two ways to split the right-hand side. e split point can occur either before or
after the second NP, resulting in one of the following two pairs of rules:
S ! NP musste V1
V1 ! NP VBN haben
and
S ! V2 VBN haben
V2 ! NP musste NP
All of the resulting rules are binary, so the process terminates.
Synchronous Binarization
Now consider the SCFG rule r7:
S ! NP₁ musste NP₂ VBN₃ haben , NP₁ must have VBN₃ NP₂
To binarize a SCFG rule, we must split both the source and the target right-hand side, while
respecting the non-terminal alignments. is is referred to as synchronous binarization. For rule
r7, we can do this by splitting the source side and the target side after their ﬁrst non-terminal:
110 5. DECODING III: STRING DECODING
S ! NP₁ musste V3₂ , NP₁ must have V3₂
V3 ! NP₁ VBN₂ haben , VBN₂ NP₁
But unlike the CFG case, we cannot split the source-side after its second non-terminal:
S ! V4₁ VBN₂ haben , ???
V4 ! NP₁ musste NP₂ , ???
e reason is that the two source-side NPs are aligned to the ﬁrst and third target-side non-
terminals and there is no way to split the target right-hand side without separating those two
non-terminals.²
It turns out that there is always at least one way to split a ternary SCFG rule. For rules of
rank four or higher, this is not the case—it depends on the rule’s non-terminal alignments. For
instance, there is no way to binarize the following rule:
S ! A₁ B₂ C₃ D₄ , B₂ D₄ A₁ C₃
Wherever you try to make a split point on the source-side, it is impossible to ﬁnd a split point on
the target side that correctly separates the corresponding non-terminals.
Clearly, the example above is artiﬁcial, but non-binarizable rules do occur in grammars
extracted from real data. Fortunately, they appear to be rare: Zhang et al. [2006] ﬁnd that only
0.3% of rules in a Chinese-English GHKM grammar are non-binarizable, and human analysis
suggests that most of those result from errors in the underlying machine-learned word alignments.
Using hand-aligned data, Huang et al. [2009] ﬁnd non-binarizable rules at rates of 0.3%, 0.1%,
and 0.6% for Chinese-English, French-English, German-English, respectively. Wu [1997] warns
that while non-binarizable reordering patterns are unusual for ﬁxed word-order language pairs,
this should not be expected to hold for pairs that involve free word-order languages.
Zhang et al. [2006] give an algorithm for synchronous binarization. For each input rule
in turn, the algorithm deterministically generates a binarization tree if the rule is binarizable or
returns a failure status otherwise. e binarization tree represents the recursive splitting process
that is necessary to produce an equivalent set of binary rules. e binarization tree is generated
solely from the rule’s non-terminal alignment pattern. e algorithm takes no account of the
rule’s terminals or its non-terminal labels. For instance, rule r7 is reduced to the sequence .1; 3; 2/,
which represents the target-side permutation of the non-terminals. e algorithm then generates
the tree
.
.
.
.
(1 3 2)
.
.
.
.
.
.
.
(3 2)
.
.
.
.
.
(1)
²Such a split becomes possible if we express our rules in a richer grammar formalism than SCFG. Speciﬁcally, Multitext
Grammar [Melamed, 2003] allows such discontinuities and there is a string decoding algorithm for it (albeit an exponential
one). See Zhang et al. [2006] for details.
5.3. HANDLING NON-BINARY GRAMMARS 111
e tree generation part of the algorithm is a shift-reduce algorithm and runs in time linear
with respect to the rule rank. Having generated a binarization tree, the binarization algorithm
then splits the rule, as prescribed by the tree, and adds the newly generated rules to the output
grammar.
Synchronous binarization reduces the parse forest complexity, making it cubic with respect
to the input sentence length (or linear if constrained width parsing is used). However, this comes
at a cost.
• e introduction of virtual non-terminals can greatly increase the jNj term (Section 5.1.1),
which multiplies the worst-case number of vertices and hyperedges.
• Since the target-side of a rule in the original grammar is distributed across multiple bina-
rized rules, beam search must make pruning decisions having seen only a fragment of the
full target side. is can lead to search errors (this problem can be ameliorated through the
use of terminal-aware synchronous binarization [Fang et al., 2011]).
Asynchronous Binarization
One alternative to synchronous binarization, proposed by DeNero et al. [2009b], is to perform
monolingual binarization twice. is results in a three-stage decoding process. For an input sen-
tence s and shared-category SCFG G we have the following.
1. e source sentence s is parsed using a binarized version of projs.G/. e target-side of the
grammar is ignored, so any CFG binarization technique can be used.
2. e resulting parse forest is converted back into n-ary form by “undoing” the binarization.
Of course, to maintain cubic complexity, the forest must be pruned. DeNero et al. [2009b]
use a method in which derivations are weighted according to a rule-local version of the
statistical model. Vertices are pruned based on max-marginal thresholds.
3. (Optionally) the n-ary forest is re-binarized, this time using a binarization strategy that
takes into account the target-side only, and is tailored to beam search.
5.3.2 ALTERNATIVES TO BINARIZATION
DeNero et al. [2009a] and Hopkins and Langmead [2010] both examine the relationship between
parsing complexity and the highly-lexicalized nature of translation grammars. ey point out that
the source-side terminals of a grammar rule eﬀectively anchor the rule to a sentence, reducing the
number of distinct ways in which the rule can be applied. For instance, consider again the source-
side projection of r7:
S ! NP musste NP VBN haben
For an arbitrary input sentence, an application of this rule is uniquely deﬁned by four values.
112 5. DECODING III: STRING DECODING
1. e start position of the ﬁrst NP.
2. e width of the subspan covered by the ﬁrst NP.
3. e width of the subspan covered by the second NP.
4. e width of the subspan covered by the VBN.
Now suppose that the input sentence contains one occurrence each of the words musste and haben,
at least two words apart. ese words ﬁx two out of the four values. Now all we need to know to
uniquely determine a rule application is:
1. the start position of the ﬁrst NP and
2. the width of the subspan covered by the second NP.
Despite having three non-terminals, the parsing complexity of this rule is only quadratic—under
the assumption that each of the rule’s terminals matches only one word in the input sentence.
In most instances, this assumption is realistic. When the input sentence contains repeated words
and this assumption does not hold, the analysis is a little more involved, but a lexicalized rule
will usually have better parsing complexity than a non-lexical rule with the same number of non-
terminals.
Lexical Normal Form
Based on this observation, DeNero et al. [2009a] propose Lexical Normal Form (LNF), a gram-
mar transform that allows rules to contain more than two non-terminals provided that there are
no adjacent non-terminals. Binarization is employed to handle non-terminal sequences. Com-
pared with binary transforms, such as as Chomsky Normal Form or Greibach Normal Form,
LNF generates far fewer virtual rules and virtual non-terminals.
Scope Pruning
A simpler approach to managing problematic rules is to remove them from the grammar alto-
gether. is is the approach proposed by Hopkins and Langmead [2010].
Hopkins and Langmead analyze parsing complexity in terms of scope. Intuitively, a rule’s
scope is the number of decisions that must be made to determine how to ﬁt the rule over the
input sentence under the assumption that every word of the input sentence is unique. For a non-
terminal, we must determine its start and end position. For a pair of non-terminals A B, we
must determine the start point of A, the end point of B, and the split point between them. e
important observation is that lexical items serve as anchors that remove the ambiguity of one or
more of these positions.
e scope of a SCFG rule r can be computed by examining the source-side CFG projection,
q D proj0
s.r/, and summing:
1. the number of non-terminals at the start of q’s right-hand side (0 or 1);
5.4. INTERIM SUMMARY 113
2. the number of non-terminals at the end of q’s right-hand side (0 or 1); and
3. the number of pairs of adjacent non-terminals in q’s right-hand side (0 or more).
e scope of a grammar G is the maximum scope of any rule in G.
Example 5.3 e following tables show a number of rule right-hand sides and their cor-
responding scope values (we have used the symbol ˘ to represent a generic non-terminal
symbol).
Rule RHS Scope
a b c d e 0
a ˘ c ˘ e 0
a ˘ ˘ d e 1
˘ b c d e 1
Rule RHS Scope
a ˘ ˘ ˘ e 2
˘ b c d ˘ 2
˘ ˘ c d ˘ 3
˘ ˘ ˘ ˘ ˘ 6
Hopkins and Langmead prove³ that a grammar with scope k can be used to parse a sentence
of length jsj in O.jsjk
/ chart updates without binarization (either explicit or implicit). In terms
of parse forest complexity, using a scope-k grammar guarantees that the hyperedge bound will be
O.jGj  jN j  jsjk
/. Scope thus eﬀectively replaces the concept of rank in determining the parsing
complexity of a grammar.
In experiments, Hopkins and Langmead demonstrate that reducing a GHKM grammar
to scope-3 by pruning does not harm translation quality (as measured by B) compared to
synchronous binarization.
5.4 INTERIM SUMMARY
is chapter began by describing the most basic possible approach to string decoding: hypergraph
construction through string parsing, combined with beam search. While this approach will work
reasonably well for some models, for others it will be prohibitively slow. For instance, the large
non-terminal vocabularies of SAMT grammars and the super-binary grammars of GHKM are
both problematic. We introduced a number of strategies to improve eﬃciency, but the eﬃcacy of
a strategy depends on the model. For Hiero, SAMT, and GHKM models, typical combinations
of strategies would be as follows:
Hiero Constrained width parsing.
SAMT Constrained width parsing; per-subspan beam search.
GHKM Binarization or scope pruning; per-subspan beam search.
³Under the assumption that there are constant limits on (i) the multiplicity of a symbol in the input sentence and (ii) the length
of a rule right-hand side.
114 5. DECODING III: STRING DECODING
Grammar Type Vertices Hyperedges
Hiero O.jsj/ O.jGj  jsj/
SAMT O.jNj  jsj/ O.jGj  jN j  jsj/
GHKM (binarization) O.jN0
j  jsj2
/ O.jG0
j  jN0
j  jsj3
/
GHKM (scope pruning) O.jNj  jsj2
/ O.jGj  jN j  jsjscope.G/
/
Table 5.3: Worst-case sizes of parse forest hypergraphs for Hiero, SAMT, and GHKM grammars
using recommended search strategies. e input parameters are as in Table 5.1. In the third row, G0
is the binarized grammar and N0
is its non-terminal set. Typically, these are much larger than the
original grammar and non-terminal set.
e computational cost of string decoding can be largely understood in terms of parse forest
complexity. Table 5.3 gives the worst-case bounds if we assume the combinations of strategies
just given. On top of the cost of parse forest construction, we must also account for the cost of
beam search. is cost depends on the beam-ﬁlling algorithm, but typically is proportional to the
beam width, k, times the number of calls to the beam-ﬁlling function (which is either one per
vertex or one per subspan, depending on whether per-subspan beam search is used).
In the following two sections, we return to hypergraph construction, examining two aspects
of string decoding that we have so far ignored: algorithms for string parsing and methods for
decoding with STSGs and distinct-category SCFGs.
5.5 PARSING ALGORITHMS
So far we have treated the parsing component of a decoder as a black box. One way to imple-
ment this component is to modify the CYK algorithm from Section 3.2.3 to construct a parse
forest hypergraph. CYK requires that the grammar is in Chomsky Normal form and so we must
perform conversion before and after parsing. Taking this approach, the P function used by
C-H (Algorithm 16) might resemble the version given in Algorithm 19.
Alternatively, there are parsing algorithms that can handle arbitrary CFGs, eliminating the need
for grammar conversion. A well-known example is the Earley parsing algorithm [Earley, 1970],
which performs grammar binarization internally. A more common choice for translation systems
is the CYK+ [Chappelier and Rajman, 1998] algorithm, which is closely related to both the CYK
and Earley algorithms.
All three parsing algorithms—CYK, Earley, and CYK+—have a O.jsj3
/ upper bound and,
at least in terms of worst-case complexity, there is no clear reason to choose one over the other.
In practice, diﬀerent parsing algorithms can have very diﬀerent performance characteristics. A
distinctive feature of translation grammars is that they tend to be extremely large: for the parallel
training corpora typically used in research, the rule extraction methods of Chapter 2 can easily
generate tens or hundreds of millions of distinct rules. Grammar size, and in particular the size of
5.5. PARSING ALGORITHMS 115
Algorithm 19 A P Function at Uses CYK
Input : s a source string
G a CFG
Output : hVs; Esi the parse forest hypergraph for s parsed according to G
P.s; G/
1 G0
D C-T-CNF.G/
2 hV 0
s ; E0
si D CYK.s; G0
/
3 hVs; Esi D F.hV 0
s ; E0
si/
4 return hVs; Esi
the non-terminal vocabulary, jNj, are well known to be signiﬁcant factors in parser performance.
For binarized or SAMT-style grammars, jNj can easily be in the thousands. ese characteristics
can inﬂuence the performance of parsing algorithms. Consequently, the choice of algorithm, and
its implementation, can matter quite a bit, and researchers have even developed specialized parsing
algorithms for use in translation systems.
In this section, we will ﬁrst describe the CYK+ algorithm. In order for CYK+ and related
algorithms to be practical for translation-scale grammars, we need an eﬃcient means of storing
and accessing grammar rules. We will describe trie-based grammar storage. We will then discuss
a variant of CYK+ that has been developed speciﬁcally with translation grammars in mind.
5.5.1 THE CYK+ ALGORITHM
CYK+⁴ is a bottom-up parsing algorithm. Unlike CYK, which requires the input grammar to
be in Chomsky Normal Form, CYK+ can handle arbitrary CFGs. In order to achieve O.jsj3
/
complexity, CYK+ binarizes grammar rules internally.
CYK+ uses a chart to record both completed and partial results. Each cell of the chart
contains two lists, referred to as the type-1 list and the type-2 list. For a CYK+ recognizer, the
type-1 list is similar to the Boolean vector of the CYK recognizer: for the cell corresponding to
sentence subspan Œi; j, the type-1 list contains items A such that A

) si : : : sj . e type-2 list
records the set of Earley-style dotted rules ˛ such that ˛

) si : : : sj and ˛ is a preﬁx of some
grammar rule (i.e., the grammar contains a rule A ! ˛ˇ, where ˛ and ˇ are symbol strings).
Pseudocode for a CYK+ recognizer is shown in Algorithm 20. e algorithm begins by
trying to match rules of the form A ! si  (lines 1–2). If a match is found where  is empty then
A is added to the type-1 list of chart cell Œi; i (line 3 of the M procedure). If a match is
⁴In Chappelier and Rajman [1998] original description, the algorithm requires that grammar rules are either purely lexical or
purely non-lexical, but the author’s note that this restriction was made to simplify the algorithm and is easily eliminated. Since
partially lexicalized rules are ubiquitous in translation grammars we describe a modiﬁed version here.
116 5. DECODING III: STRING DECODING
Algorithm 20 e CYK+ Recognition Algorithm
Input : String s; CFG, G, with no empty rules
Output : Boolean indicating whether s belongs to language deﬁned by G
CYK-P-R.G; s/
1 for i D 1 to jsj
2 M.G; i; i; ; si /
3 for width D 1 to jsj
4 for i D 1 to jsj width C 1 /
/ i = span start
5 j D i C width 1 /
/ j = span end
6 for x D i C 1 to j /
/ x = split-point
7 for ˛ in type-2 list of chartŒi; x 1
8 for A in type-1 list of chartŒx; j
9 M.G; i; j; ¸; A/
10 if width > 1
11 for ˛ in type-2 list of chartŒi; j 1
12 M.G; i; j; ¸; sj/
13 for A in type-1 list of chartŒi; j
14 M.G; i; j; ; A/
15 return  iﬀ S
is in type-1 list of chart[1, jsj]
M.G; i; j; ¸; ˛/
1 for each rule A ! ˛ˇ in G
2 if  is empty
3 append A to type-1-list of chartŒi; j
4 else
5 append ˛ˇ to type-2-list of chartŒi; j
found where  is non-empty then the dotted rule si  is added to the type-2 list (line 5 of the
M procedure).
Having completed this initialization stage, the algorithm proceeds to visit chart cells in
order of increasing width. At each cell, the algorithm ﬁrst attempts to extend existing dotted rules
to cover the current span by adding a non-terminal symbol (lines 7–9) or a terminal (lines 11–
12). It then checks for rules that begin with a non-terminal, which includes non-lexical unary
rules of the form A ! B (lines 13–14). Note that chains of unary rules are correctly handled
since the M procedure appends A to the type-1 list, which then becomes an argument to a
5.5. PARSING ALGORITHMS 117
NP ! Jemand
NP ! Josef K.
VBN ! verleumdet
VP ! NP VBN haben
S ! NP musste VP
S ! NP musste NP VBN haben
Josef
K.
NP
m
usste
VP
NP
VBN
haben
verleumdet
Jemand
VBN
h
a
b
e
n
Figure 5.2: e source-side projection of the grammar from Example 5.1 and the corresponding trie.
e black circles indicate the nodes for which there are rules.
subsequent M call later in the for loop. For a non-recognition version of the algorithm, cycles
(like A ) B ) C ) A) require special handling. Cycles are rarely useful in natural language
parsing, and a standard technique is to break cycles by checking for repeated symbols, or even to
remove unlexicalized unary rules from the grammar entirely.
Finally, the algorithm tests for the presence of the start symbol S
in the type-1 list of the
top-most cell.
Notice that the M procedure contains a loop over all grammar rules with a given preﬁx.
For a large grammar, a naive implementation that iterates over all rules would be prohibitively
slow. e following section describes a better approach.
5.5.2 TRIE-BASED GRAMMAR STORAGE
Decoders typically store the translation grammar in a trie-based data structure (sometimes called
a preﬁx tree). Each edge of the trie is labeled with a source-side terminal or non-terminal such
that the sequence of labels along a path (from the root) to a node n represent the preﬁx ˛ of some
source-side rule A ! ˛. A grammar rule hA ! ˛; B ! ˇi is stored at the node with preﬁx path
˛, meaning that some nodes in the trie store no rules while others store one or multiple rules. At
any given node, the rules are subgrouped by source and target left-hand side, hA; Bi.
Figure 5.2 gives an example of a grammar (only the source-side is shown) and its corre-
sponding trie.
A trie can save space by compactly representing rules with common preﬁxes. However, the
main advantage of using a trie is eﬃciency of access: the data structure is ideally suited for parsing
algorithms like CYK+ where a rule match over a subspan Œi; k is an extension (by one symbol)
of a rule match over a shorter subspan Œi; j. e M procedure from Algorithm 20 can be
118 5. DECODING III: STRING DECODING
9
8
10
7
6
5
3
2 4
1
s1 s2 s3 s4
6
9
10
3
5
8
2
4 1
7
s1 s2 s3 s4
Figure 5.3: Order of span visits for Algorithms 20 (CYK+) and Algorithm 21 (recursive CYK+). For
CYK+, the only inherent constraint on chart traversal is that it is bottom-up—the recursive CYK+
order would work equally well. e converse is not true: no other order is possible for recursive CYK+.
eﬃciently implemented if the grammar is stored in a trie: the trick is to store in the type-2 list, a
pointer to a trie node along with each dotted rule (naturally, for a dotted rule ˛, the pointer is to
the trie node ending at path ˛). e loop in lines 1 to 5 then becomes two loops: the ﬁrst (for the
case that ˇ is empty) is a loop over the rule left-hand sides stored at the trie node. e second is
a loop over the outbound edges.
5.5.3 THE RECURSIVE CYK+ ALGORITHM
e CYK+ algorithm performs binarization dynamically, storing its internal representation of
binarized rules—the dotted rules—in the chart’s type-2 lists. For the large and highly lexicalized
grammars used in translation, there can be a huge number of distinct binarized rules, making
dotted rule storage expensive in terms of memory.
Sennrich [2014] proposes a variant of CYK+ that minimizes dotted rule storage by recur-
sively expanding, and then discarding, every dotted rule as soon as it is constructed. To achieve
this, the chart’s cells must be visited in a speciﬁc order: both bottom-up and from right-to-left.
Figure 5.3 illustrates this ordering. By examining the input sentence and the chart, the algorithm
repeatedly proposes possible rule applications, which are tested by querying the grammar data
structure. However, unlike standard CYK+, when visiting subspan Œi; j, the algorithm does not
stop at proposing applications over Œi; j: immediately on ﬁnding a match, the algorithm tries to
extend the rule application by matching a further symbol (and then another, and another—i.e.,
recursively). us, when visiting span Œi; j, the algorithm can ﬁnd rule applications over any span
Œi; k, where k  j . For example, when visiting cell 8 in Figure 5.3, the algorithm can potentially
add items to the type-1 lists of cells 8, 9, and 10.
Pseudocode for a recursive CYK+ recognizer is given in Algorithm 21. e R-
CYK-P-R function visits each subspan in the prescribed order (lines 1–7). For each
subspan Œi; j, it does one of the following.
5.5. PARSING ALGORITHMS 119
Algorithm 21 e Recursive CYK+ Recognition Algorithm
Input : String s; CFG, G, with no empty rules and no non-lexical unary rules
Output : Boolean indicating whether s belongs to language deﬁned by G
R-CYK-P-R.G; s/
1 for i D jsj to 1 /
/ i = span start
2 for j D i to jsj /
/ j = span end
3 if i D j
4 M.G; s; i; i; ; si /
5 else
6 for A in type-1 list of chartŒi; j-1
7 M.G; s; i; j-1; ; A/
8 return  iﬀ S
is in type-1 list of chart[1, jsj]
M.G; s; i; j; ¸; ˛/
1 for each rule A ! ˛ˇ in G
2 add A to type-1-list of chartŒi; j
3 if j < jsj
4 M.G; s; i; j C 1; ¸˛; sjC1/
5 for k D j C 1 to jsj
6 for A in type-1 list of chartŒj C 1; k
7 M.G; s; i; k; ¸˛; A/
• If i D j , it proposes a rule application over span Œi; i with right-hand side si (line 4). e
M function tests this against the grammar, updating the type-1 list if a match is found,
and then recursively attempts to expand the rule application.
• If j > i, R-CYK-P-R tries rule applications that begin with a non-
terminal symbol over subspan Œi; j 1 (lines 6–7). e M function tests whether
a proposed rule application is valid (by consulting the grammar data structure) and then
recursively proposes extensions of the current rule application: either adding the next source
word (line 4) or adding a non-terminal from an adjacent position in the chart (lines 5–7).
As presented, the algorithm does not handle non-lexical unary rules, but could be extended
to do so. e recursive CYK+ algorithm yields the same result as the standard CYK+ algorithm,
and has the same time complexity; it essentially performs the same steps, just in a diﬀerent order.
Its advantage lies in minimizing the dotted rule storage, thereby reducing memory consumption,
120 5. DECODING III: STRING DECODING
and improving speed because storing and retrieving dotted rules typically involves RAM accesses
with high latency.
5.6 STSG AND DISTINCT-CATEGORY SCFG
e string decoding algorithms presented so far have assumed that the grammar is an SCFG
in shared-category form. ey can easily be adapted to work with STSGs and distinct-category
SCFGs.
5.6.1 STSG
Zhang et al. [2006] describe a simple, reversible procedure for converting a shared-category
STSG, G, into an equivalent shared-category SCFG, G0
. A one-to-one correspondence between
derivations in G0
and G means that an input sentence can be parsed with G0
to produce a SCFG
parse forest, which can then be converted into the equivalent STSG parse forest over G.
Zhang et al. [2006]’s method works as follows. For every STSG rule in the original gram-
mar, two SCFG rules are produced. e ﬁrst is eﬀectively a ﬂattened version of the original, with
internal tree structure removed. To allow for the subsequent reversal of the conversion, the rule is
given a left-hand side symbol that uniquely identiﬁes the original rule. For instance, if the original
STSG rule has identiﬁer r4427 then a symbol V4427 is introduced into the SCFG non-terminal
set (similarly to the introduction of virtual symbols in grammar binarization) and the SCFG rule
is given the left-hand side V4427. e second SCFG rule is a unary non-lexical rule that rewrites
the STSG’s original root symbol as the newly introduced symbol.
Example 5.4 Suppose that rule r4427 is the following string-to-tree shared-category STSG
rule:
.
.
.
.
r4427 .
.
= .
.
.
.
.
.
VP
.
.
.
.
.
.
.
haben
.
.
.
.
.
.
.
VBN2
.
.
.
.
.
NP1
.
.
, .
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP1
.
.
.
.
.
VBN2
.
.
.
.
.
VB
.
.
.
.
.
have
e two resulting SCFG rules are:
q1 VP ! V4427₁ , V4427₁
q2 V4427 ! NP₁ VBN₂ haben , have VBN₂ NP₁
After parsing, the SCFG derivations are converted to STSG derivations by reducing pairs of
hyperedges. For our example rules:
5.6. STSG AND DISTINCT-CATEGORY SCFG 121
V4427,3,6
haben
NP,3,4 VBN,5,5
q2
VP,3,6
haben
NP,3,4 VBN,5,5
r4427
VP,3,6
q1
Once the STSG parse forest has been constructed, decoding can proceed as normal.
5.6.2 DISTINCT-CATEGORY SCFG
For a shared-category SCFG, the translation hypergraph was constructed by parsing the input
sentence with the source-side CFG projection and then replacing the hyperedges of the resulting
parse forest according to the inverse grammar projection. e correctness of that algorithm relies
on two basic properties that hold for any shared-category SCFG G.
1. For any derivation d in G, the source tree of d is a CFG derivation in projs.G/.
2. For any CFG derivation ds D r1; r2; : : : ; rn in projs.G/, there is a non-empty set of SCFG
derivations in G that have ds as their source tree. is set is
˚
r0
1; r0
2; : : : ; r0
n W r0
i 2 proj0
s.ri /
	
.
If G were a distinct-category SCFG, then only the ﬁrst property would hold. e second fails
since a derivation in projs.G/ is not necessarily the source-side of any derivation in G.
Example 5.5 Let G be the distinct-category SCFG with the following rules:
r1: NP ! Josef K. , NP ! Josef K.
r2: VVPP ! verleumdet , VBN ! slandered
r3: VVPP ! verleumdet , VBN ! defamed
r4: VP ! NP₁ VVPP₂ , VP ! VBD₂ NP₁
r5: VP ! VP₁ haben , VP ! have VP₁
r6: VP ! NP₁ VVPP₂ haben , VP ! have VBN₂ NP₁
r7: S ! Jemand musste VP₁ , S ! Someone must VP₁
r8: S ! Jemand musste NP₁ , S ! Someone must have
VVPP₂ haben VBN₂ NP₁
122 5. DECODING III: STRING DECODING
r9: S ! Jemand musste NP₁ , S ! NP₁ must have been
VVPP₂ haben VBN₂ by someone
e following derivation in projs.G/ can be formed using the source projections of rules r1,
r2, r4, r5, and r7:
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
haben
.
.
.
.
.
VP
.
.
.
.
.
.
.
VVPP
.
.
.
.
.
verleumdet
.
.
.
.
.
NP
.
.
.
.
.
.
.
K.
.
.
.
.
.
Josef
.
.
.
.
.
.
.
musste
.
.
.
.
.
Jemand
but no derivation in G has this source derivation. We can see this by looking at rule r4, for
which the source-side projection is the CFG rule VP ! NP VVPP. If we check the grammar
rules in projs.G/, we ﬁnd that r4 is the only rule with this source-side projection. But r4
produces the target-side non-terminal VBD, which cannot be rewritten using any CFG rule
in the target-side projection.
Examples like this occur in distinct-category SCFGs because the source and target CFGs
mutually control the rewriting process: whether or not a synchronous rule can be applied depends
on the non-terminals of both the source and target sides (of the rule and of the symbol string pair
that is undergoing the rewrite). is property of mutually-controlled rewriting does not apply to
shared-category SCFGs since the paired source and target non-terminals always match.
Fortunately, as we saw in Chapter 1, it is easy to convert any distinct-category grammar
G to shared-category form G0
. e conversion simply conjoins source and target non-terminals.
For example, rule r4 of Example 5.5 becomes:
(VP, VP) ! (NP, NP)₁ (VVPP, VBD)₂ , (VVPP, VBD)₂ (NP, NP)₁.
Since there is a one-to-one mapping between the rules of G and G0
, it is trivial to convert deriva-
tions and hypergraphs from one form to the other. us, we can convert a distinct-category gram-
mar to shared-category form, construct the translation hypergraph, and convert back to distinct-
category form. Algorithm 22 gives the construction algorithm in pseudocode form. Once the
hypergraph has been constructed, beam search can proceed as for a shared-category SCFG. e
only diﬀerence is that vertex labels encode two non-terminal symbols (the conjoined source-target
pair) instead of one.
5.7. HISTORICAL NOTES AND FURTHER READING 123
Algorithm 22 Algorithm for constructing a distinct-category SCFG hypergraph
Input : s a source string
G a distinct-category SCFG
Output : hVs; Ei translation hypergraph
C-H.s; G/
1 G0
D C-N-T.G/
2 hVs; Esi D P.s; projs.G0
//
3 E D ¿
4 for es 2 Es
5 for r0
2 proj0
s.rule.es//
6 r D S-N-T.r0
/
7 add labeled hyperedge hhead.es/; tail.es/; ri to E
8 return hVs; Ei
5.7 HISTORICAL NOTES AND FURTHER READING
e presentation of string decoding given here, in line with most modern models and decoders, is
ﬁrmly based on the approach presented in Chiang [2007]. For an account of earlier parsing-based
approaches, see the brief history of syntax-based SMT in Section 1.5.
Translation, and particularly parsing-based approaches, are sometimes presented using the
framework of weighted deduction (notably, Chiang [2007] presents the Hiero model as a weighted
deductive proof system). is framework, which originated in the parsing literature, allows a par-
ticularly compact, modular, and uniform presentation and analysis of a wide range of translation
models. For a good introduction to the use of weighted deduction in the context of statistical
machine translation, see Lopez [2009].
e use of tries for storing grammar rules is not unique to machine translation: tries are
also used in monolingual parsing (see, for instance, Klein and Manning [2001b]). Nor is the idea
speciﬁc to syntax-based approaches (see, for instance, Germann et al. [2009]).
C H A P T E R 6
Selected Topics
e central focus of this book has been on the use of syntax to model the ways that phrases are
reordered between languages. is has taken six chapters, but even still, we are far from covering
all of the ways that syntax is used in machine translation. In this chapter, we cover a number of
considerations and extensions of the use of syntax that are important to know about, despite the
fact that they did not ﬁt within the core narrative presented in the previous chapters.
We begin with a central component of syntax-based translation that has so far been ignored:
the monolingual parsers that produce all the syntax we care about! Section 6.1 looks at a number
of ways that constituency structure and annotation are not optimal for machine translation, and
how they can be altered in felicitous ways. It also describes how the upstream errors of parsers can
be mitigated by extracting from and decoding from forests, instead of single-best output. is is
followed in Section 6.2 by a discussion of the use of dependency parses for both tree-to-string
and string-to-tree translation.
On its own, the presence of syntactic structure in translation rules is not enough to guar-
antee that translations are grammatically well formed, however it can provide a starting point for
addressing issues of grammaticality that have proven challenging for statistical translation mod-
els. In Section 6.3, we look at some ways that target-side syntactic structure has been used as
a foundation for developing linguistically-motivated extensions of the basic model. Finally, in
Section 6.4 we look at the use of syntax in evaluation metrics.
6.1 TRANSFORMATIONS ON TREES
Monolingual constituency parsers like the Berkeley parser have thus far played a background
role in this book. We have used their output directly to annotate one side of a parallel corpus in
preparation for learning rules (Chapter 2) or to prepare input for tree-based decoding (Chapter 4).
In so doing, we have ignored a number of issues. One problem is that parsers are not perfect; at
least 7–10% of the constituents are likely to be wrong even under the best circumstances. Even
if monolingual constituency parsing were perfect, it is not at all clear that the structure produced
by monolingual parsers is optimal for machine translation. In this section, we will look at ways
of altering the parser output in ways that better align with the needs of syntax-based translation.
ese methods include modifying the tree structure and the set and number of non-terminals
used, letting the rules be combined more loosely with penalty-driven non-terminal mismatches,
and, ﬁnally, mitigating the eﬀects of errors in the one-best parse tree by working with forests.
126 6. SELECTED TOPICS
.
.
.
.
.
.
.
.
.
.
télé
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
éteint
.
.
.
.
.
Il
.
.
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
TV
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
.
.
PRT
.
.
.
.
.
RP
.
.
.
.
.
oﬀ
.
.
.
.
.
VBZ
.
.
.
.
.
turns
.
.
.
.
.
NP
.
.
.
.
.
PRN
.
.
.
.
.
he
.
.
.
.
.
.
.
.
.
.
télécommande
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
avec
.
.
.
.
.
.
.
télé
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
allume
.
.
.
.
.
Il
.
.
.
.
.
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
PP
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
remote
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
IN
.
.
.
.
.
with
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
TV
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
.
.
PRT
.
.
.
.
.
RP
.
.
.
.
.
on
.
.
.
.
.
VBZ
.
.
.
.
.
turns
.
.
.
.
.
NP
.
.
.
.
.
PRN
.
.
.
.
.
he
Figure 6.1: English–French sentence pairs with English derivations, showing alignment of phrasal
verbs.
6.1.1 TREE RESTRUCTURING
One limitation of GHKM extraction is that only rules that span a syntactic constituent are ex-
tracted. For phrase-based SMT, it has long been known that translation pairs which are not syn-
tactic constituents are also helpful [Koehn et al., 2003]. GHKM is able to learn non-constituent
phrase pairs if they are embedded inside longer rules, but this often misses important generaliza-
tions.
As an example, consider the phrasal verbs turn on and turn oﬀ in Figure 6.1. Both phrasal
verbs are translations of a single French word, allumer and éteindre, respectively. It is desirable to
learn the translation units h allume , turns on i and h éteint , turns off i. Such phrase pairs
would allow the production of a sentence like il éteint la télé avec la télécommande, which combines
rules from both sentences. is pair would be learned by a phrasal translation system. However,
GHKM extraction on these alignments pairs produces the following rules:
r1 S ! NP₁ VP₂ , NP₁ VP₂
r2 VP ! éteint NP₁ , turns oﬀ NP₁
r3 NP ! allume NP₁ PP₂ , turns on NP₁ PP₂
r4 NP ! il , he
r5 NP ! la télé , the TV
r6 PP ! avec la télécommande , with the remote
e constituency constraint forces these phrase pairs to be part of much larger syntactic rules
with diﬀerent, conﬂicting predicate-argument structure. As such, GHKM is unable to produce
the desired generalization of il éteint la télé avec la télécommande.
6.1. TRANSFORMATIONS ON TREES 127
.
.
.
.
.
.
.
.
.
.
télé
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
éteint
.
.
.
.
.
Il
.
.
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
TV
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
VP
.
.
.
.
.
.
.
PRT
.
.
.
.
.
RP
.
.
.
.
.
oﬀ
.
.
.
.
.
VBZ
.
.
.
.
.
turns
.
.
.
.
.
NP
.
.
.
.
.
PRN
.
.
.
.
.
he
(a)
.
.
.
.
.
.
.
.
.
.
télé
.
.
.
.
.
.
.
la
.
.
.
.
.
.
.
éteint
.
.
.
.
.
Il
.
.
.
.
.
.
.
S
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
VP
.
.
.
.
.
.
.
NP
.
.
.
.
.
.
.
NN
.
.
.
.
.
TV
.
.
.
.
.
DT
.
.
.
.
.
the
.
.
.
.
.
PRT
.
.
.
.
.
RP
.
.
.
.
.
oﬀ
.
.
.
.
.
VBZ
.
.
.
.
.
turns
.
.
.
.
.
NP
.
.
.
.
.
PRN
.
.
.
.
.
he
(b)
Figure 6.2: Left binarization (a) and right binarization (b) of the ﬁrst derivation tree in Figure 6.1.
Various techniques have been proposed to restructure the training trees so that more use-
ful rules can be learned. We now look at two of them: binarization and transformation-based
learning.
Tree Binarization
Much of the problem is due to the ﬂat structure of the VPs in the examples above, which is
also somewhat arbitrary. One solution proposed by Wang et al. [2007] is called tree binarization.
It works by binarizing trees before grammar learning, so as to break up wide constituents into
many smaller ones with narrower spans, thereby increasing the number of extractable translation
units. Note that this should not be confused with synchronous grammar binarization discussed
in Section 5.3.1. Synchronous binarization is a post-processing operation that converts the n-
ary SCFG rules of an already-learned grammar into weakly equivalent binary ones. In contrast,
tree binarization yields an entirely diﬀerent grammar with a diﬀerent probability distribution. In
both cases, however, binarization is reversible by deleting virtual nodes bottom-up, assigning each
deleted virtual node’s children to its parent.
ere are many ways to binarize a parse tree. e two simplest are left-binarization and
right-binarization. Left binarization of a node n with r > 2 children is performed by recursively
replacing the leftmost two children of a node, N, with a new node, N, until N has only two
children. Right binarization is performed analogously by inserting new nodes to the right.
Binarization of the ﬁrst derivation tree in Figure 6.1 results in the trees shown in Figure 6.2.
By left-binarizing both sentences in Figure 6.1, GHKM extraction is able to extract the following
additional synchronous rules (among others):
128 6. SELECTED TOPICS
r7 VP ! éteint , turns oﬀ
r8 VP ! allume , turns on
r9 VP ! VP₁ NP₂ , VP₁ NP₂
r10 VP ! VP₁ PP₂ , VP₁ PP₂
Here, the translations of éteint and allume can be stored as small translation units in the
grammar, increasing the chance that they can be applied during translation. is allows us to
produce a derivation of the input il éteint la télé avec la télécommande, yielding the translation he
turns oﬀ the TV with the remote, by combining rules r7 and r10 with rules r1, r4, r5 and r6 (which
were binary in the original derivation tree).
In this example, left binarization had the desired eﬀect, but there are other examples
for which right binarization is more appropriate. One can also imagine diﬀerent binarization
schemes, such as identifying the head and binarizing outward, or trying to minimize the number
of new rules. Wang et al. [2007] propose a “parallel binarization” scheme that constructs a bina-
rization forest of possibilities for each tree in a treebank, and then uses Expectation-Maximization
to learn the most probable global restructuring. is results in trees with mixed left-binarized and
right-binarized nodes.
e increased generality of binarized treebanks comes at the cost of overgeneralizations
during translation. Williams et al. [2015] describe that subcategorization constraints are violated
more often by a synchronous grammar that is extracted from a binarized treebank than by one that
is extracted from the original treebank. For instance, the constraint that ﬁnite verbs have exactly
one subject in German (with the exception of coordinated structures and imperatives) is violated
frequently by the grammar based on the binarized treebank. Williams et al. [2015] unbinarize
dynamically during translation, employing a syntactic language model over the unbinarized trees
to enforce subcategorization constraints. is helps to mitigate the overgeneralization problem
caused by tree binarization.
Transformation-based Learning
Another tree restructuring technique has been proposed by Burkett and Klein [2012], who
use six types of tree transformations to maximize the number of extractable nodes. An ex-
ample is the articulate transformation type, shown in Figure 6.3. e transformation
articulatehVP,VBZ,PRTi converts the derivation trees in Figure 6.1 into ones where the
phrasal verb is extractable.
Burkett and Klein [2012] do not apply the transformations to all nodes, but use
transformation-based learning [Brill, 1992] to greedily search for a sequence of transformations
that maximally increases the number of extractable nodes on a small training set. An interesting
aspect of this approach is that it tries to answer a fundamentally diﬀerent question than previ-
ous work on tree binarization. Instead of searching for the “best” representation of a treebank
according to some monolingual metric, the objective function takes into account the other side
of the parallel corpus, and the alignments between the two texts. us, the algorithm will learn
6.1. TRANSFORMATIONS ON TREES 129
.
.
.
.
A
.
.
.
.
.
.
.
...
.
.
.
.
.
.
.
.
.
.
.
C
.
.
.
.
.
.
.
.
.
.
.
B
.
.
.
.
.
...
!
.
.
.
.
A
.
.
.
.
.
.
.
...
.
.
.
.
.
.
.
B+C
.
.
.
.
.
.
.
.
.
.
.
C
.
.
.
.
.
.
.
.
.
B
.
.
.
.
.
...
articulatehA,B,Ci
Figure 6.3: Articulate transformation schematic from Burkett and Klein [2012].
diﬀerent transformations for the same treebank if the alignment method or aligned language pair
are changed.
6.1.2 TREE RE-LABELING
e same observations that apply to structure also apply to the grammar non-terminals: they are
not necessarily optimal for learning translation rules. Consider the following simpliﬁed SCFG:
S ! NP₁ VP₂ , NP₁ VP₂
NP ! you , du
VP ! sleep , schlafen
VP ! sleep , schläfst
e coarseness of the non-terminal label set here allows the overgeneration of ungrammatical
sentences. When translating from English, both *du schlafen and du schläfst can be produced.
ese are permitted by the grammar because the non-terminals encode only the most general in-
formation about the part-of-speech, and exclude morphological features of the noun phrase, such
as grammatical person, gender, case, and number, which are relevant for morpho-syntactic phe-
nomena such as agreement. Enriching non-terminals with morphological features could reduce
the number of ill-formed sentences that the grammar generates:
S ! NP-2-SG-NOM₁ VP-2-SG₂ , NP-2-SG-NOM₁ VP-2-SG₂
NP-2-SG-NOM ! you , du
VP-3-PL ! sleep , schlafen
VP-2-SG ! sleep , schläfst
Of course, this statement about the coarseness of non-terminals is true of parsing, as well. A long
line of work there has established that automatically-reﬁned non-terminal sets produce much
higher accuracy parsers [Johnson, 1998, Matsuzaki et al., 2005, Petrov et al., 2006]. If this work is
successful in parsing, where the task is to discriminate among structures over a grammatical input,
it seems likely it should be helpful in language generation tasks like syntax-based MT, where the
dangers of leaky grammars would seem to be more problematic. And indeed, state-splitting has
been successfully applied to parse trees for GHKM extraction in string-to-tree systems [Chiang
et al., 2009, Wang et al., 2010].
130 6. SELECTED TOPICS
While ﬁner-grained non-terminal sets can prevent some overgeneralizations, they are also
problematic because they increase data sparsity and can prevent desirable derivations. For instance,
singular and plural noun phrases are interchangeable in object position, and a full morphological
annotation of all noun phrases symbols would needlessly constrain rule application. Having mul-
tiple variants of the same rule with diﬀerent left-hand side symbols also increases the size of the
grammar and the number of possible derivations that only diﬀer in their non-terminal symbols.
Hanneman and Lavie [2011] describe an algorithm for automatic label coarsening. eir objec-
tive is to maximize the probability of source symbols given the aligned target symbols, and vice-
versa. is improves the quality of tree-to-tree translation in experiments on Chinese-English
and French-English, with substantial reductions in the size of the joint non-terminal sets. A po-
tential drawback of this objective is that symbol distinctions in one language can be important
even if all symbols are mapped to a single symbol in the other language. For example, the function
of noun phrases is marked with case in German, and encoding the function of noun phrases in
the non-terminal symbol seems desirable to prevent overgeneralization, even if all noun phrase
symbols are aligned to the same symbol NP in English.
While it may seem contradictory that both increasing and decreasing the size of the non-
terminal set should improve translation performance, we expect various factors to play a role,
including the language pair, the size of the original non-terminal set and whether syntax is used
on the source, target, or both. Also, the drawbacks of non-terminal sets that are too coarse or
too ﬁne-grained can be compensated by other parts of the SMT system, for instance through
separate models for agreement (see Section 6.3.1) that reduce overgeneralizations from overly
coarse-grained symbols.
6.1.3 FUZZY SYNTAX
Another method of loosening the constraints imposed by syntax is to do away with the require-
ment that non-terminals match during rule application. is is often refered to as a “fuzzy” or
“soft” syntax. Recall that by deﬁnition, CFG (and SCFG) derivations proceed by replacing a
non-terminal in the derived string with any rule having that non-terminal on its left-hand side
(Section 1.2.1). (For distinct-category SCFG, the non-terminal might be a pair.) We can soften
this matching constraint by allowing a non-terminal A to be rewritten by any rule of the form
B ! ˛. Permitting mismatching in this way can be especially important when the grammars im-
pose extensive constraints, such as using very, very large non-terminal sets (like SAMT), or in
tree-to-tree decoding.
Chiang [2010] applied this fuzzy matching approach in a tree-to-tree setting. e pro-
cess was controlled with a set of sparse features designed to allow the model to learn how often
to match or mismatch rule applications. ese features included counters match and :match to
count the number of rewrites where A D B and A ¤ B, respectively, and a feature substA!B
that counts, for every pair of non-terminals, how often A is rewritten with a rule having B as
its left-hand side. Separate features were used for source and target labels. e resulting mod-
6.1. TRANSFORMATIONS ON TREES 131
els demonstrated improvement over a string-to-string baseline translating from both Arabic and
Chinese into English. Fuzzy matching was an important piece of the performance gains, helping
to loosen the tight constraints of tree-to-tree decoding.
In related work, Huck et al. [2014] collapsed rules that only diﬀered in their source-side
non-terminal symbols, but stored the set of non-terminal symbols with which the rule was seen
in training. ey described both dense and sparse features that penalized mismatches between
the syntactic annotation of the input sentence and the rules that were applied during decoding.
6.1.4 FOREST-BASED APPROACHES
In previous chapters, we have seen two main uses of parse trees. e ﬁrst is in grammar learning,
where each sentence pair is annotated with a source-side or target-side parse tree (or sometimes
both) prior to rule extraction. e second is in tree decoding, where each input sentence is parsed
prior to decoding.
In a typical setting, both of these approaches are vulnerable to parsing errors since they
use only the 1-best parse tree from a statistical parser. An obvious, and often eﬀective, way to
ameliorate this situation is to use the k-best parse trees. In the case of rule extraction, this involves
running rule extraction k-times (ﬁrst with the 1-best trees, then the 2-best trees, and so on)
and then merging the resulting grammars. In the case of tree decoding, this involves translating
the input k times and picking the output with the best model score. Clearly, this approach is
impractical for large values of k. It is also wasteful, since many of the k-best trees will share
subtrees and therefore much of the computational eﬀort is spent recomputing previous results.
Forest-based approaches provide more eﬃcient means to represent and process a set of parse trees,
or parse forest.
Forest Decoding
Forest decoding [Mi et al., 2008] is a natural extension of tree decoding. Instead of being given
a single parse tree as input, the decoder is given a parse forest (in the form of a hypergraph).
Typically, this parse forest is the pruned output of a statistical parser.
e main change to decoding is in hypergraph construction: the decoder must match gram-
mar rules against a forest instead of a tree. Mi et al. [2008] generalize a simple tree-based rule
matching algorithm for this purpose. Zhang et al. [2009] presents an alternative and more eﬃ-
cient algorithm (which we met previously in Section 4.4 when discussing eﬃcient tree parsing).
Once the hypergraph has been constructed, beam-search based decoding can proceed just as in
tree decoding.
e input parse forest will contain a mix of plausible and less plausible hyperedges. If the
parser produces a weighted forest then those forest weights can be used to inform translation.
Typically, this means adding a feature function to the log-linear model to encode the input hy-
peredge probability.
132 6. SELECTED TOPICS
Mi et al. [2008] demonstrate strong improvements for forest decoding over 1-best and 30-
best baselines on a Chinese-to-English tree-to-string translation task. Compared with the base-
line method of decoding k times with k-best parser outputs, forest decoding oﬀers a much better
speed-accuracy trade-oﬀ. Similar results have been reported for other language pairs: for instance,
Neubig and Duh [2014] achieve large gains over tree decoding for both English-to-Japanese and
Japanese-to-English translation. Zhang et al. [2011] report improvements on multiple language
pairs using forests produced by combining alternative binarizations of the 1-best parse.
To be strictly consistent with our classiﬁcation of syntax-based models (Section 1.4), forest
decoding is a method for decoding with a tree-to-string or tree-to-tree model. In the literature,
a model used with forest decoding is frequently referred to as a forest-to-string model.
Forest-based Rule Extraction
e other main use of parse trees is in grammar learning. Mi and Huang [2008] extend the
GHKM algorithm (Section 2.4) to extract rules from a generalized form of alignment graph in
which the parse tree is replaced with a parse forest. As in forest decoding, this forest is usually the
pruned output of a statistical parser.
Mi and Huang’s rule extraction algorithm involves three main changes compared to
GHKM.
1. e notion of a frontier node is generalized to forest nodes. e identiﬁcation of frontier
nodes remains essentially unchanged.
2. e process of minimal rule formation is generalized to take account of the forest structure.
e deﬁnition of a minimal rule remains essentially unchanged, but rule formation becomes
more complicated since, in general, non-frontier nodes can be expanded in multiple ways.
Mi and Huang describe a breadth-ﬁrst algorithm that explores all possible expansions. ey
use a queue to keep track of the non-frontier nodes that require expansion.
3. To penalize the use of subtrees from non 1-best parse trees, Mi and Huang assign fractional
counts to rules, so that rules extracted from the 1-best tree have a count of 1, but rules
including non 1-best subtrees are penalized commensurately. eir method is based on the
inside-outside algorithm from statistical parsing.
As for forest decoding, Mi et al. demonstrate strong improvements for forest-based rule
extraction over 1-best and 30-best baselines on a Chinese-to-English tree-to-string translation
task. eir results are further improved by combining forest-based rule extraction and forest de-
coding.
6.1.5 BEYOND CONTEXT-FREE MODELS
e context-free grammars that have been the focus of this book are a step up in the hierarchy of
formal languages from word-based and phrase-based models, which can be formalized as regular
6.1. TRANSFORMATIONS ON TREES 133
.
.
.
.
Wir .
.
müssen .
.
Systeme .
.
aufbauen .
.
, .
.
die .
.
sicher .
.
sind .
.
.
.
.
.
.
We .
.
have to .
.
build .
.
systems .
. .
.
that .
.
are .
.
safe .
.
.
.
root
.
root
.
subj
.
obja
.
aux
.
comma
.
subj
.
pred
.
rel
.
rel
Figure 6.4: Non-projective German dependency tree.
grammars. Context-free grammars can model structures in natural language that regular gram-
mars cannot, such as the hierarchical structure of subordinated clauses. However, there are struc-
tures in natural language that motivate the use of even more powerful, mildly context-sensitive
models. Explorations with such models have been limited, but there are a few.
Non-projective Structures
In Figure 6.4, we see a German sentence with a discontinuous accusative object. Speciﬁcally,
the relative clause that depends on the accusative object does not occur directly after the object,
but after the end of the main clause. is discontinuity is visible in the tree via a crossing de-
pendency edge, and trees with crossing edges are called non-projective [Ihm and Lecerf, 1963].
Non-projective structures cannot be derived by context-free grammars.
Just how prevalent such examples are depends highly on the languages under consideration.
Buchholz and Marsi [2006] compiled statistics on various treebanks used for the CoNLL-X
shared task on dependency parsing. In the treebanks used, between 0% (for Chinese) and 36% (for
Dutch) of sentences contain at least one non-projective arc, with the median of the 13 treebanks
being 12% (for Turkish).
Syntactic SMT models have similar options to parsers for dealing with non-projective
structures. One option that goes back to Chomsky’s early work [Chomsky, 1957] is to posit that
discontinuities are surface phenomena, and that there is a context-free deep-syntactic structure
which we can derive through a set of transformations such as syntactic movement. e Prague
Dependency Treebank [Hajič et al., 2000] follows such a multi-layered approach, and its tec-
togrammatical (deep) layer has been used in SMT [Bojar et al., 2008]. e set of movements
necessary can be complex [Hajičová et al., 2004].
A simpler, but linguistically less adequate option is to heuristically projectivize treebanks.
Nivre and Nilsson [2005] propose a simple algorithm to projectivize dependency trees by “lift-
ing” non-projective dependency edges. e non-projective tree in Figure 6.4 can be projectivized
by lifting just one edge. e pseudo-projective dependency edge is shown as a dotted line. e
downside of this projectivization is that it prevents the modeling of syntactic phenomena such
134 6. SELECTED TOPICS
as morphological agreement or subcategorization along the edges that have been lifted. is is
problematic for our example because the relativizer die is an indirect dependent of its antecedent
Systeme, and the morphological agreement between the two could be modeled only along the
original dependency chain. is relation is lost in the pseudo-projective structure.
Mildly Context-sensitive Grammars for SMT
A more principled approach to dealing with non-projectivity is to increase the expressive power of
the model, which also entails an increase in computational complexity. Mildly context-sensitive
grammars place constraints on the non-projectivity allowed in the grammar to allow for poly-
nomial parsing. A good introduction to theoretical and empirical aspects of constraints on non-
projectivity can be found in Kuhlmann and Nivre [2006], and Kuhlmann [2013].
Translation models based on mildly context-sensitive grammars are an active research in-
terest [e.g., Burbank et al., 2005, Carreras and Collins, 2009, Kaeshammer, 2015, Seemann
et al., 2015, Zhang et al., 2008]. Unsurprisingly, the increase in computational cost compared
to context-free models remains a major challenge. As such, such models often do not perform
well, although they remain theoretically and scientiﬁcally interesting.
6.2 DEPENDENCY STRUCTURE
Our discussion so far has focused on context-free constituency representations. In this section,
we broaden the scope to discuss models that have been developed for dependency grammars.
Projective dependency grammars describe context-free languages and are thus weakly
equivalent to the context-free constituency grammars we discussed previously. For each projective
dependency grammar, we can also induce a strongly equivalent context-free grammar [Gaifman,
1965]. is close relationship between dependency and constituency representations allows us to
apply the same SCFG models and algorithms that we have discussed in the previous chapters on
dependency structures.
All that is required is a simple mapping from dependency trees to constituency trees, and
we illustrate one such mapping in Figure 6.5. e mapping uses a preterminal node for each word,
labeled with its POS tag. Another constituent is added for each word, labeled with the incoming
dependency edge label of the corresponding word. Each constituent has all direct dependents of
the word, and the word’s preterminal node, as its children. A special root node is added to ensure
that there is a single root node per sentence. is mapping has been used for SMT by Hardmeier
et al. [2011], and Sennrich et al. [2015].
While we can in principle use the previously discussed SCFG algorithms for projective
dependency representations, some translation models have been speciﬁcally developed for de-
pendency representations. We describe some prominent approaches and discuss the similarities
and diﬀerences to the algorithms we presented in the previous chapters. e mapping from de-
pendency trees into constituency format shown in Figure 6.5 will help us shed light on these.
6.2. DEPENDENCY STRUCTURE 135
.
.
.
.
My .
.
dog .
.
likes .
.
playing .
.
catch .
.
.
.
root
.
root
.
poss
.
nsubj
.
xcomp
.
dobj
.
root
.
.
.
.
.
.
VROOT
.
.
.
.
.
.
.
ROOT
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ROOT
.
.
.
.
.
.
.
XCOMP
.
.
.
.
.
.
.
DOBJ
.
.
.
.
.
VB
.
.
.
.
.
catch
.
.
.
.
.
VBG
.
.
.
.
.
playing
.
.
.
.
.
.
.
VBZ
.
.
.
.
.
likes
.
.
.
.
.
NSUBJ
.
.
.
.
.
.
.
NN
.
.
.
.
.
dog
.
.
.
.
.
POSS
.
.
.
.
.
PRP$
.
.
.
.
.
My
Figure 6.5: Dependency tree and its mapping into a constituency representation.
.
.
.
.
tired .
.
men .
.
and .
.
dogs
.
.
.
.
hommes .
.
et .
.
chiens .
.
fatigués
.
.
.
.
Figure 6.6: English–French sentence pair with alignment and English dependency tree.
6.2.1 DEPENDENCY TREELET TRANSLATION
Quirk et al. [2004] present a translation model based on a dependency representation. For train-
ing, they perform dependency parsing on the source sentence, and project these parses onto the
target sentence via word alignments. ey use treelets as their primary translation unit. ey de-
ﬁne a treelet to be a “connected subgraph of the dependency tree” Quirk et al. [2004]. is is
more general than a subtree, which includes a node and all its descendants.
To illustrate what treelets are extracted from a small dependency tree, consider the depen-
dency tree in Figure 6.6. From this dependency tree, the following subtree pairs are extracted:
1. tired , fatigués
2. men , hommes
3. and , et
4. dogs , chiens
5. tired men and dogs , hommes et chiens fatigués
Additionally, Quirk et al. [2004] extract the following treelets that are not subtrees:
136 6. SELECTED TOPICS
6. men and dogs , hommes et chiens
7. men and , hommes et
8. and dogs , et chiens
9. men * dogs , hommes * chiens
e last treelet is a treelet with a missing root, indicated by the wildcard character. is is a
relaxation introduced by Quirk et al. [2004] to allow for more general rules, for instance for
negation in French (“ne * pas”).
To contrast these dependency extraction heuristics with those of the SCFG rule extrac-
tion algorithms that we saw in Chapter 2, we can assume a constituency representation of the
dependency tree as in Section 6.2. Hierarchical rule extraction would still extract rules 1–5, but
generalizes not by extracting treelets, but by replacing nodes in the subtree with non-terminal
symbols:
10. NP ! JJ₁ men and dogs , hommes et chiens JJ₁
11. NP ! JJ₁ men and NNS₂ , hommes et NNS₂ JJ₁
12. NP ! JJ₁ NNS₂ and dogs , NNS₂ et chiens JJ₁
13. NP ! JJ₁ men CC₂ dogs , hommes CC₂ chiens JJ₁
e SCFG rules 10–13 only represent a small selection of extractable rules. ere are two
major diﬀerences between the treelet pairs 6–9 and the SCFG rules 10–13 (the non-terminal
symbols are shown for readability). First, the SCFG rules only allow for replacing one constituent
with another, but not for their deletion or insertion. e treelet rules can be used to translate
the sentence “men and dogs,” whereas the SCFG rule set cannot. Tree binarization, introduced
in the previous section, or SAMT-style extraction, discussed in Section 2.3, are two ways of
increasing the generalization power of SCFG models. Second, the SCFG rules encode reordering
information, whereas the treelets do not. Quirk et al. [2004] search over all possible orderings, and
introduce a separate, discriminative reordering model. Since the number of possible reorderings
is factorial, the search space is pruned heavily. Menezes and Quirk [2007] propose to extract
dependency order templates at training time, which are unlexicalized treelet pairs, the source nodes
being represented by their POS. As the treelets themselves, the order templates are scored via
relative frequency estimates. During translation, treelets are combined with compatible order
templates, as illustrated in Example 6.1, to create translation rules.
Example 6.1 Treelets and order templates required to translate “a very old man”
.
.
.
.
old .
.
man
.
.
JJ .
.
NN
.
.
.
.
hombre .
.
viejo
.
. .
.
.
.
a
.
.
DT
.
.
.
.
un
. .
.
.
.
very
.
.
RB
.
.
.
.
muy
.
treelets
.
.
.
.
DT .
.
JJ .
.
NN
.
.
.
.
X .
.
X .
.
X
.
.
. .
.
.
.
RB .
.
JJ
.
.
.
.
X .
.
X
.
.
order templates
6.2. DEPENDENCY STRUCTURE 137
.
.
.
.
a .
.
very .
.
old .
.
man
.
.
DT .
.
RB .
.
JJ .
.
NN
.
.
.
.
un .
.
hombre .
.
muy .
.
viejo
.
.
.
.
e space of possible reorderings is limited to those licensed by an order template, which
reduces the search space over the original model. Note, however, that the model is still more
general than a SCFG which only allows substitution of subtrees. Because the order templates
enable search with less heavy pruning, the authors observe improvements in both translation
quality and speed.
Menezes and Quirk [2008] further increase the generalization power of the model by mod-
eling insertion and deletion via the order templates. Since the order templates are unlexicalized,
this is more general than modeling them via unaligned words in treelet pairs. Menezes and Quirk
[2008] motivate their insertion and deletion model with structurally divergent languages, such as
when translating from English to Japanese, where determiners and pronouns are often deleted,
but case markers inserted. Experimental results on translating from English to Japanese and Span-
ish indicate that their model is helpful for both translation directions, but that the type and fre-
quency of deletions and insertions are very dependent on the language pair.
Modeling insertion and deletion via order templates is similar to SCFG rules that are un-
lexicalized except for unaligned words, which are being deleted (if they are on the source side) or
inserted (if they are on the target side). As an example, consider the phrase “committee member,”
whose translation we could learn in a fully lexicalized rule (rule 14), or by combining an unlex-
icalized rule that models the insertion of “del” (rule 17) with the translation of the individual
words.
14. NP ! committee member , miembro del comité
15. NN ! member , miembro
16. NN ! committee , comité
17. NP ! NN₁ NN₂ , NN₂ del NN₁
Whether such (mostly) unlexicalized insertion and deletion rules are learned for SCFG
grammars depends on the extraction heuristics. For instance, Chiang [2007] require at least one
pair of aligned words in translation rules, which would make rule 17 invalid. GHKM extraction
places no such restriction, but unlexicalized rules may be eliminated by other heuristics, such as
scope pruning (discussed in Section 5.3.2).
A noteworthy component of the dependency treelet system is the agreement model, which
is a simple bigram dependency language model. e motivation for this model is to model agree-
138 6. SELECTED TOPICS
ment phenomena, for instance learning a preference for the modiﬁer-noun pair (la, table) over
(le, table), even if the two words are separated by intervening modiﬁers. Although the target
dependency trees are only obtained indirectly by projecting the source parse trees, Quirk et al.
[2004] note that this agreement model is useful for their system, although they do not report
empirical results on its impact.
6.2.2 STRING-TO-DEPENDENCY SMT
Dependency trees have also been shown to be useful as a target-side language model. e general
ability to have gaps in rules (a capability of SCFGs) introduces the possibility of modeling long-
distance dependencies between words, a capability that has thus far not been realized in this book.
Shen et al. [2010] describe an approach that ﬁlls this gap. Its main components are a translation
model that is similar to an unlabeled Hiero system, but restricted to be able to produce only
well-formed dependency structures. ese structures are composed from smaller structures, and
their combination models the attachment of arguments to head-words in a trigram dependency
model. is permits the modeling of relations between distant words, in constrast to n-gram
models, which can only model local ones.
ey divide well-formed dependency structures into two subcategories. Fixed structures
consist of the root of a subtree, plus any number of children of the root. e extracted struc-
ture must be consecutive, and each child must be a complete subtree. is represents a head word
that has taken zero or more arguments, and which may take more. Together, these constraints
enforce the idea of constructing the target-language syntax bottom-up. Examples of well-formed
and ill-formed structures are shown in Example 6.2.
Example 6.2 Consider the following English sentence along with its dependency structure:
.
.
.
.
the .
.
boy .
.
will .
.
ﬁnd .
.
it .
.
interesting
.
.
.
.
.
.
If we take this as the intended translated target-language sentence and structure, the follow-
ing examples are well-formed substructures:
.
.
.
.
the .
.
boy .
.
will .
.
ﬁnd
.
.
.
. .
.
.
.
will .
.
ﬁnd .
.
it
.
.
. .
.
.
.
boy
.
Whereas the following two are ill-formed:
6.3. IMPROVING GRAMMATICALITY 139
.
.
.
.
boy .
.
will .
.
ﬁnd
.
.
. .
.
.
.
will .
.
ﬁnd .
.
interesting
.
.
.
e ﬁrst is ill-formed because boy has not yet had its arguments attached, and therefore is
not yet completely built. e second is ill-formed because it is not a continuous segment of
the original English sentence.
As a second type of well-formed structures, the authors deﬁned ﬂoating structures in which
the root of the subtree is unspeciﬁed. Relating back to the previous subsection’s discussion of de-
pendency treelets, this corresponds to the optional replacement of the treelet root with a wildcard.
e deﬁnition of well-formed structures as unit of extraction is more general than only extracting
full constituents, but less general than treelets.
Shen et al. deﬁne four operations to combine partial dependency structures: left adjoining,
right adjoining, left concatenation, and right concatenation. e adjoining operations add a new
child node to a ﬁxed dependency structure, whereas the concatenation operations combine two
siblings. e authors note that these operations are in principle suﬃcient to combine partial de-
pendency structures into a full parse, but result in a large search space. Hence, they also use the
substitution operation, using a single non-terminal label for ﬂoating structures, and the POS tag
of the root for ﬁxed structures. Mismatches between the label of the substitution site and the sub-
stituted structure are allowed, but incur a penalty. e adjoining and concatenation operations are
implemented as special rules which are comparable to glue rules in hierarchical MT.
At decoding time, scores from a trigram dependency language model can be computed and
scored as an additional weight in the log-linear model. is allows the decoder to model relations
between words that are too distant to be modeled with n-gram models. In their evaluation, Shen
et al. ﬁnd that the restriction to well-formed dependency structures has a mixed eﬀect when it is
merely used to ﬁlter the rules of a hierarchical model. However, their main motivation for the use
of dependency structure on the target side was to include the dependency language model. For
that task, they obtain consistent improvements over a hierarchical baseline in their experiments
on Chinese-to-English and Arabic-to-English.
6.3 IMPROVING GRAMMATICALITY
e previous chapters have mainly been devoted to describing algorithms for syntax-based SMT.
In this section, we focus again on the question of why syntax-based algorithms are appropriate
for SMT, and how a syntactic framework allows for solutions to problems that are hard to solve
with phrase-based, or unlabeled hierarchical models.
140 6. SELECTED TOPICS
As an introductory example in Chapter 1, we discussed reorderings between the source and
the target language as a motivating example for syntax-based SMT. Failing to correctly reorder
the words in the source text produces a “word salad” in the target language that may be hard to
comprehend. Word order is one aspect of syntax, but for most languages, putting words in the
right order is not suﬃcient to generate grammatically well-formed text. In this section, we will
discuss other aspects of grammaticality, and how these aspects can be modeled in the framework
of syntax-based SMT.
6.3.1 AGREEMENT
Inﬂectional languages typically mark features like gender and grammatical case on determiners,
adjectives, nouns, and pronouns, and features like tense and number on verbs. In some syntactic
relationships, feature agreement is required by the syntax of the language. In most Germanic and
Romance languages, determiners and adjectives need to agree in case, number, and gender with
their head noun, and ﬁnite verbs need to agree with their subject in number and person.
From a translation perspective, producing the right features in the translation is hard be-
cause feature types and values can diﬀer substantially between languages. Consider the following
example:
1. a bold decision
its translation into French,
2. une décision courageuse
and three alternative translations into German:
3. eine mutige Entscheidung
4. ein mutiger Entschluss
5. ein mutiges Urteil
Both German and French have grammatical gender, and mark the gender of the noun
phrases through the inﬂection of determiners and attributes, whereas modern English has no
grammatical gender. When translating into German or French, the translation of the determiners
and attributes cannot be predicted from the source text, but is dependent on the head noun in the
translation. is is highlighted by the fact that there are (at least) three possible translations of
“decision” into German, the feminine noun “Entscheidung,” the masculine “Entschluss,” and the
neuter “Urteil.” Even when translating between two languages that both have the same feature
types, feature values may diﬀer, as between Example 2, the French noun “décision” being feminine,
and Examples 4 or 5.
Agreement over short string distances can be modeled via n-gram language models, but
agreement constraints can involve words that are distant in the surface string, which poses a
6.3. IMPROVING GRAMMATICALITY 141
problem for traditional n-gram models. is limitation of word-level n-gram language models
has sparked considerable eﬀorts on alternative models. Koehn and Hoang [2007] report im-
provements in grammatical coherence through language models on the level of morphological
features, which are less sparse than surface word models. Ammar et al. [2013] use similar class-
based language models, but trained on unsupervised clusters instead of linguistic features, and
conclude that these models are eﬀective at modeling morphological agreement. Neural Networks
can better encode long n-grams than back-oﬀ models, and n-gram contexts of up to 32 have been
used in the hope of better modeling long range dependencies [Ter-Sarkisov et al., 2014]. Recur-
rent Neural Networks are not limited to a ﬁxed n-gram window, and can learn dependencies over
longer distances [Elman, 1991]. Agreement has also been modeled outside of the main transla-
tion step, for instance through a rule-based error correction module [Rosa et al., 2012], or by ﬁrst
translating a morphologically underspeciﬁed representation of the text, and then generating the
morphological features [Fraser et al., 2012, Toutanova et al., 2008].
Syntactic models facilitate the design of agreement models because words that need to agree
are in clearly deﬁned syntactic relationships. While the inﬂection prediction system by Toutanova
et al. [2008] is implemented as n-best reranking, they use a treelet translation system as their base
system, and the discriminative model for inﬂection prediction uses features based on source and
target-side dependency structure, among others. Discriminative lexicon models that make use
of syntactic context have also been integrated into the main translation pass of a tree-to-string
translation system [Jeong et al., 2010].
Another class of models that can model agreement are dependency language models [Popel
and Mareček, 2010, Quirk et al., 2004, Sennrich, 2015, Shen et al., 2010], which we will discuss in
more detail in Section 6.3.4. Dependency language models are especially suitable to model agree-
ment between words that are directly linked by a dependency arc, for which even a bigram model is
suﬃcient. is applies to the aforementioned agreement requirements within (non-coordinated)
noun phrases, and between subjects and verbs.
For other agreement phenomena, e.g., for the agreement between predicative adjectives and
subjects in French, both the string distance and the syntactic distance can be high, which is prob-
lematic for both n-gram and syntactic language models with strong independence assumptions.
Consider the subject and predicative adjective (highlighted) in the dependency tree in Figure
6.7. ere is number agreement between the subject “elles” (‘they’) and the ﬁnite verb “peuvent”
(“can”), but also between the subject and the predicative adjective “trouvées” (“found”). While the
subject and the verb are directly linked in the dependency tree, the subject and the predicative
adjective are only indirectly linked, and are predicted independently by dependency language
models.
A more ﬂexible method to enforce agreement constraints that is based on linguistic re-
sources and manually designed constraints has been proposed by Williams and Koehn [2011],
who model agreement in a uniﬁcation grammar. ey use a ﬁnite-state morphology to extract
a lexicon of feature structures from the target side of the training corpus. e lexicon associates
142 6. SELECTED TOPICS
.
.
.
.
elles .
.
ne .
.
peuvent .
.
pas .
.
être .
.
trouvées
.
.
they .
. .
.
can .
.
not .
.
be .
.
found
.
.
suj
.
mod
.
mod
.
obj
.
ats
Figure 6.7: French dependency tree, annotated according to the guidelines by Candito et al. [2010].
each target surface form with a set of feature structures. e rules in the synchronous grammar
are augmented with constraints that identify which words or substitution sites need compati-
ble feature structures, based on the syntactic relations. Example feature structures are shown in
Example 6.3; an SCFG rule in Example 6.4.
Example 6.3 Two German feature structures¹ for the indeﬁnite article “eine” and the noun
“Welt” (“world”):
“eine”!
2
6
6
6
6
6
6
6
6
4
cat ART
inﬂ
2
6
6
6
6
4
case nom
declension mixed
agr
"
gender f
num sg
#
3
7
7
7
7
5
3
7
7
7
7
7
7
7
7
5
“Welt”!
2
6
6
6
6
6
4
cat NN
inﬂ
2
6
6
4
case nom
agr
"
gender f
num sg
#
3
7
7
5
3
7
7
7
7
7
5
Example 6.4 SCFG rule augmented with uniﬁcation constraints:
SUBJ ! ART ADJA NN
hSUBJ  i D hART  i
hSUBJ  i D hADJA  i
hSUBJ  i D hNN  i
hSUBJ   i D nom
e ﬁrst three constraints ensure agreement between the article, adjective, and noun by uni-
fying the respective feature structures. e resulting feature structure is made into the feature
6.3. IMPROVING GRAMMATICALITY 143
structure of the rule’s left-hand side, which can be re-used in later rules, e.g., to enforce agree-
ment with the verb. e ﬁnal constraint is based on the rule’s left-hand side label SUBJ, and
encodes that German subjects require nominative case.
During translation, feature structures are uniﬁed where the constraints require compatibil-
ity, and translation hypotheses with incompatible feature structures are penalized or discarded.
Discarding hypotheses with incompatible feature structures could theoretically guarantee perfect
agreement. In practice, the approach relies on a high-quality lexicon, and missing or wrong entries
can both result in agreement errors. Also, if data is sparse, the model may be unable to produce a
correct translation, and may have to decide between allowing a wrongly inﬂected word form, or
producing a grammatically correct, but semantically less plausible translation.
Despite the plethora of syntactic and non-syntactic models that aim to improve grammati-
cal agreement, some problems are still unsolved. Agreement constraints become harder to model
in coordinated structures, where the coordinated structure may have other feature values than its
constituents. For example, the coordination of a masculine singular noun and a feminine sin-
gular noun in French is considered masculine plural for the purpose of agreement. In elliptical
constructions, syntactic constituents that are relevant for agreement may be missing, and must
be inferred from the context. Consider the three German translations of Example 6, where the
noun in the second noun phrase is elided. e grammatical features of the elliptical noun phrase
depend on the antecedent, which is underlined.
6. this is a bold decision, and hopefully the right one.
7. das ist eine mutige Entscheidung, und hoﬀentlich die richtige.
8. das ist ein mutiger Entschluss, und hoﬀentlich der richtige.
9. das ist ein mutiges Urteil, und hoﬀentlich das richtige.
Agreement phenomena may also cross sentence boundaries. Many languages, including
most Romance and Germanic languages, require personal pronouns to agree in gender and num-
ber with their antecedent. For anaphora that cross sentence boundaries, the currently dominant
approach of translating sentences independently of each other is unable to model these agree-
ment constraints.Models speciﬁc to the problem of translating pronominal anaphora have been
developed [Hardmeier and Federico, 2010], but pronoun translation remains an open challenge.
6.3.2 SUBCATEGORIZATION
Subcategorization [Chomsky, 1965], or the closely related concept of valency [Tesnière, 1959],
is concerned with the number and types of syntactic arguments of words. Especially for verbs,
there is a wide range of possible types of syntactic arguments, but speciﬁc verbs typically only
144 6. SELECTED TOPICS
allow for (or require) a small subset thereof. A well-known distinction is that between intransitive
verbs that only take a subject as argument, such as “sleep”², and transitive ones that also take a
direct object, such as “remember.” ere are various types of arguments, such as direct or indirect
objects, prepositional objects (Example 2), or clausal objects (Example 3), and their interaction is
complex. For instance, the verb “remind” may have both a direct and a clausal object (Example 4),
where the direct objects denotes the receiver of the information, and the clausal object denotes
the information. e verb “remember” takes either a direct or a clausal object (Examples 1 and 3,
respectively), but not both, since both subcategory types are mapped to the same semantic role,
the content of the memory.
1. he remembers his medical appointment.
2. I remind him of his medical appointment.
3. he remembers that he has a medical appointment.
4. I remind him that he has a medical appointment.
5. she was killed by the river.
Subcategorization is closely intertwined to the semantics of a word, for instance the number
of participants in an action.³ However, the mapping between syntax and semantics is complex.
Multiple syntactic realizations can correspond to the same semantic frame, as do Examples 1
and 3, and a single syntactic structure can have multiple semantic interpretations. In Example 5,
the prepositional phrase “by the river” could be interpreted as either the agent that killed her, or
as an adjunct expressing the location of the murder.
For translation, subcategorization is challenging because subcategorization constraints
need to be respected in the translation, which requires the modeling of additional inter-
dependencies. Where the surface distance between a word and its arguments is long, n-gram
models are insuﬃcient to model these inter-dependencies. Also, the subcategorizations in the
source sentence are imperfect evidence for those in the target sentence because subcategorization
frames often diﬀer between languages. For instance, the German verb “erinnern” takes a reﬂexive
object in the meaning of “remember,” and the content of the memory is realized as a preposi-
tional object with the preposition “an.” e German translation of Example 1 that respects these
constraints is shown in Example 6. A literal translation in either direction, shown in Examples 7
and 8, is ungrammatical.
6. er erinnert sich an den Arzttermin.
7. *er erinnert den Arzttermin.
8. *he remembers himself to the medical appointment.
²While “sleep” is intransitive in its basic meaning, there are transitive meanings, as in “he sleeps a restless sleep.”
³See Fisher et al. [1991] for an in-depth discussion.
6.3. IMPROVING GRAMMATICALITY 145
Research that explicitly addresses the problem of subcategorization for SMT includes
[Shilon et al., 2012, Weller et al., 2013].
Like for agreement, syntactic representations are an appropriate level of representation for
subcategorization. While the surface distance between a word and its arguments varies, they are
directly linked in the derivation tree, and various syntactic language models are in principle able
to model subcategorization. Modeling subcategorization was a motivation for “head-driven,” or
lexicalized, probabilistic context-free grammars [Charniak, 2001, Collins, 2003]. By condition-
ing the probability of production rules on the head of the structure, lexicalized PCFGs can learn
word-speciﬁc probability distributions, and thus model diﬀerences in subcategorization. Lexical-
ized PCFGs have been applied as language models in SMT [Charniak et al., 2003, Och et al.,
2004, Post and Gildea, 2008], but early results were discouraging. Better success has been re-
ported for dependency language models, which model subcategorization implicitly in the prob-
ability distribution of dependency bigrams [Quirk et al., 2004], or by explicitly predicting the
subcategory type of each argument [Sennrich, 2015].
Subcategorization is not purely a ﬂuency problem. Diﬀerent meanings of a polysemous
word often have diﬀerent subcategorizations. For instance, “apply” has diﬀerent meanings, and
is translated diﬀerently, in the meaning of submitting oneself as a candidate for a job (German:
“bewerben”), in the meaning of being relevant (German: “gelten”), and in the meaning of using
something (German: “anwenden”). Examples 9–11 demonstrate the diﬀerent meanings.
9. she applies for a job.
10. this rule applies to everyone.
11. he applies the wrong test.
In principle, composed GHKM rules can encode diﬀerent translations for diﬀerent subcatego-
rizations of the same word, encoding both the correct translation and subcategorization in the
target language:
S ! SUBJ₁ applies for PN₂ . , SUBJ₁ bewirbt sich auf PN₂ .
S ! SUBJ₁ applies to PN₂ . , SUBJ₁ gilt für PN₂ .
S ! SUBJ₁ applies OBJA₂ . , SUBJ₁ wendet OBJA₂ an .
However, data will often be sparse, especially for languages that are highly inﬂected and/or have
free word order. If more minimal rules are used, coverage improves, but the ability to disambiguate
on the basis of subcategorization decreases.
A failure to account for cross-lingual diﬀerences in subcategorization not only hurts ﬂu-
ency, but can also cause semantic translation errors. Consider the English, German, and French
analyses of “I like her” in Figure 6.8, where the English subject of “like” is expressed as object in
German and French, and the English object is expressed as subject.
146 6. SELECTED TOPICS
.
.
.
.
I .
.
like .
.
her
.
.
subj
.
obj
.
.
.
.
sie .
.
gefällt .
.
mir
.
.
she .
.
is pleasing .
.
to me
.
.
subj
.
obj
.
.
.
.
elle .
.
me .
.
plaît
.
.
she .
.
to me .
.
is pleasing
.
.
subj
.
obj
Figure6.8: English, German and French dependency tree of I like her and corresponding translations.
Composed GHKM rules are in principle able to encode cases where grammatical roles are
translated unusually, for instance in rule 7 in the following toy grammar, which can be combined
with rules 2 and 3 to produce the correct translation.
1. SUBJ ! I , ich
2. SUBJ ! I , mir
3. OBJ ! her , sie
4. OBJ ! her , ihr
5. V ! like , gefalle
6. SENT ! SUBJ₁ V₂ OBJ₃ , SUBJ₁ V₂ OBJ₃ .
7. SENT ! SUBJ₁ like OBJ₂ , SUBJ₂ gefällt OBJ₁ .
However, rules 1, 4, 5, and 6, which are all minimal translation rules that express frequent trans-
lation pairs, are likely to score better, and result in the mistranslation “ich gefalle ihr,” which
expresses a diﬀerent meaning, namely “she likes me.”
Preventing semantic errors like this is still an open challenge in Machine Translation, and
some models speciﬁcally model the semantic structure of sentences. One avenue of research is to
incorporate semantic roles into syntax-based translation systems [Bazrafshan and Gildea, 2013,
Liu and Gildea, 2008]. is line of work is based on the semantic role annotation from projects
such as PropBank [Palmer et al., 2005] and FrameNet [Baker et al., 1998].
Another potential solution is to use an intermediate representation that abstracts away from
shallow syntax and maps diﬀerent realizations of the same meaning to the same expression. is
reﬂects going one step higher in the famous Vauquois pyramid of translation. Research in this
direction spans over more than half a century, and is beyond the scope of this book. However,
we note that such semantic-based or interlingua-based approaches do not replace, but build on
top of syntactic models, and syntactic knowledge is still required to analyze the source text, and
generate syntactically well-formed output.
6.3.3 MORPHOLOGICAL STRUCTURE IN SYNCHRONOUS GRAMMARS
To some degree, Machine Translation is possible without an explicit model of morphology, and
the models we discussed so far, as many phrase-based and hierarchical models, only require a
segmentation of the text into words, or not even that for character-based translation [e.g., Vi-
6.3. IMPROVING GRAMMATICALITY 147
.
.
.
.
die .
.
neue .
.
Handgepäckgebühr
.
.
ART .
.
ADJA .
.
NN
.
.
.
.
les .
.
nouveaux .
.
frais de bagages à main
.
subj
.
attr
.
det
.
.
.
.
die .
.
neue .
.
Hand .
.
 .
.
gepäck .
.
 .
.
gebühr
.
.
ART .
.
ADJA .
.
SEG .
.
LN .
.
SEG .
.
LN .
.
SEG
.
.
.
.
les .
.
nouveaux .
.
frais .
.
de .
.
bagages .
.
à .
.
main
.
subj
.
mod
.
link
.
mod
.
link
.
attr
.
det
Figure 6.9: German noun phrase with dependency tree, and with/without hierarchical representation
of compound.
lar et al., 2007]. However, for morphologically rich languages, treating words as atomic units
is suboptimal. Morphological processes such as agglutination and compounding can produce
complex morphological structures whose meaning is expressed as a syntactic structure in other
languages. For instance, consider the German term “Handgepäckgebühr” (“carry-on bag fee”),⁴
which is translated into French “frais de bagages à main.” Without having seen the German term
in the training data, its translation exceeds the capability of models that only operate on the level
of words or larger units.
Compound splitting is an established technique for translation out of compounding lan-
guages [Koehn and Knight, 2003]. Applying compound splitting on the target side to produce
new compounds is more complicated because compounds typically need to be merged, which
requires their identiﬁcation in the translation output [Stymne and Cancedda, 2011].
Context-free grammars cannot only represent syntactic structures, but also morphological
structures [e.g., Heemskerk, 1993, Johnson et al., 2007]. Sennrich and Haddow [2015] represent
the morphological structure of German compounds as part of the syntactic annotation. Figure 6.9
shows a representation of “Handgepäckgebühr” as a dependency tree, before and after splitting.
A dependency representation of morphological structure not only allows the extraction and
use of morpheme-level translation rules, but also has the advantage over phrase-based and un-
labeled hierarchical models that the output structure makes it easy to identify and merge split
morphemes after decoding. Also, SCFG decoding constrains the reordering of morphemes, en-
suring that compounds are not broken up by spurious reordering operations. e morpheme hier-
archy within the compound also encodes morphosyntactic interactions, for instance the fact that
in German, the last segment is the head of the compound and needs to agree with determiners
and articles.
⁴While the English term is also a compound, it is written with spaces between the units. is minor spelling diﬀerence makes
English compounds much easier to translate than German ones.
148 6. SELECTED TOPICS
6.3.4 SYNTACTIC LANGUAGE MODELS
is chapter has highlighted various linguistic phenomena, syntactic, semantic, and morphologi-
cal, which are relevant for producing accurate and grammatically well-formed translations. e ac-
curacy of a translation, commonly referred to as adequacy [White et al., 1994], inherently depends
on the source text, whereas the ﬂuency, i.e., the degree to which the translation is grammatical
and idiomatic, is independent from the source text. Traditionally, n-gram language models have
been used to improve the ﬂuency of translations, but they are limited to local ﬂuency phenomena,
and do not model the grammaticality of a sentence globally. us, errors such as agreement or
subcategorization violations involving distant words, or even missing verbs, are common in SMT
systems with n-gram language models [Williams and Koehn, 2014]. While this book is mainly
devoted to learning and decoding with syntactic translation models, syntax can also be used in
language modeling.
We brieﬂy touched on the history of syntactic language models in Section 1.5. In this
section, we will illustrate how syntactic language models can improve grammaticality with the
example of dependency language models.
Like n-gram language models, dependency language models decompose the task of pre-
dicting the probability of a sentence w0; :::; wn into predicting the probability of each word wi ,
given some context. Whereas n-gram models use a history of previous words as context, depen-
dency models deﬁne the context through the dependency structure. Quirk et al. [2004] deﬁne
a simple bigram model that uses the direct ancestor in the dependency tree as context. e se-
quences resulting from following the dependency arcs have also been referred to as syntactic n-
grams [Sidorov et al., 2013]. Example 6.5 shows a German dependency tree and the syntactic
n-grams of size 2.
Example 6.5 Syntactic n-grams of size 2
.
.
.
.
die .
.
Passagiere .
.
auf .
.
dem .
.
Zug .
.
begrüßen .
.
den .
.
mutigen .
.
Entschluss
.
.
the .
.
passengers .
.
on .
.
the .
.
train .
.
welcome .
.
the .
.
bold .
.
decision
.
root
.
subj
.
det
.
pp
.
pn
.
det
.
obja
.
attr
.
det
<s> begrüßen begrüßen Passagiere Passagiere die
Passagiere auf auf Zug Zug dem
begrüßen Entschluss Entschluss den Entschluss mutigen
A bigram dependency model is suﬃcient to enforce some grammaticality constraints.
For instance, p.Passagierepl j begrüßenpl/ will be substantially higher than p.Passagiersg j
begrüßenpl/, since the latter violates gender agreement and is unlikely to occur in the training
6.4. EVALUATION METRICS 149
data. However, more sophisticated contexts may be required. In German prepositional phrases,
gender and number is determined by the head noun, but case is governed by the preposition. To
predict the correct determiner in the prepositional phrase, we need to condition the decision on
the grandparent node, i.e., the preposition “auf,” which governs the dative (or accusative) case:
p.demdat j Zug; auf/  p.dernom j Zug; auf/.
Note that in German, like in English, there is number and person agreement between a verb
and its subject, but not between a verb and its objects. In other words, we do not want to penalize
the probability p.Entschlusssg j begrüßenpl/, despite the mismatch in grammatical number. In-
cluding dependency labels in the context allows for a targeted modeling of subject—verb agree-
ment, with p.Entschlusssg j subj; begrüßenpl/  p.Entschlusssg j obja; begrüßenpl/. e labels
also help to learn selectional preferences, for instance that the subject of “welcome” is likely to be
an animate entity.
e context of dependency language models can also been ﬁlled with syntactic siblings
[Shen et al., 2010], which is useful to model the order between siblings, or mutual exclusion of
siblings. Sennrich [2015] deﬁnes a dependency language model that uses ancestor and sibling
nodes as context, and both the word and the dependency label of each node:
p.word jlabel; parent; label-of-parent; grandparent; label-of-grandparent; (6.1)
closest-sibling; label-of-closest-sibling/:
Depending on the number of ancestor and sibling nodes, the context can become quite
large, and it is unclear how to formulate a back-oﬀ strategy. us, Sennrich [2015] implements
the model as a feed-forward neural network, utilizing the network’s power to generalize to unseen
contexts.
Syntactic language models are especially attractive for syntax-based SMT because both can
be trained on the same syntactic structures, and if the decoder produces syntactic derivations, it
can provide the information required by a syntactic language model at little cost.
6.4 EVALUATION METRICS
e dominant method of automatically measuring the quality of a machine translation system
is to compare an automatic translation to a human reference translation of the same text. e
ﬁrst such metric that was proposed, B [Papineni et al., 2002], is still widely used today. A
plethora of alternative metrics have been introduced since—just at the metrics shared task of the
2014 Workshop on Statistical Machine Translation, 29 metrics were compared, with the majority
thereof new at the time [Machacek and Bojar, 2014].
e discussion of metrics is relevant for this book because popular translation quality met-
rics such as B suﬀer from the same limitations as phrase-based translation models. n-gram
quality metrics are most sensitive to local ﬂuency, whereas inter-dependencies between distant
words, such as agreement or subcategorization phenomena, are captured indirectly at best. ere
150 6. SELECTED TOPICS
is thus a risk that these metrics vastly underestimate the quality of systems that are good at pro-
ducing grammatically well-formed sentences, and not just a string of locally ﬂuent fragments.
Callison-Burch et al. [2006] discuss the fact that a rule-based system was ranked ﬁrst in the hu-
man evaluation of the 2005 NIST MT evaluation, despite relatively poor B scores, and con-
clude that it is inappropriate to use B to “compar[e] systems which employ radically diﬀerent
strategies (especially comparing phrase-based statistical machine translation systems against sys-
tems that do not employ similar n-gram-based approaches),” or to try “to detect improvements
for aspects of translation that are not modeled well by B.”
Translation quality metrics that operate on syntactic structures may be more suitable for the
development and evaluation of syntax-based MT systems. During development, automatic met-
rics are regularly used to determine whether new model components are useful, and are typically
employed as objective function for the optimization of the scaling parameters of the global log-
linear model. A metric that is unable to detect that a new model improves an aspect of translation
quality could lead to the new model being discarded as ineﬀective.
Liu and Gildea [2005] introduce two syntactic translation quality metrics. Both metrics
assume the availability of parse trees of both the automatic translation and the human reference
translation.
e subtree metric (STM) counts the number of subtrees of depth n in the translation
output that also occur in the reference tree. Like B, the metric is precision-based and clips the
count of repeated subtrees to their count in the reference. Unlike B, the units being counted
are subtrees instead of n-grams, and subtree precision for diﬀerent depths is averaged with the
arithmetic mean, rather than the geometric mean. Also, the score does not include a brevity
penalty, which is used in B to penalize hypotheses that are too short.
STM D
1
D
D
X
nD1
P
t2subtreesn.hyp/ min.counthyp.t/; countref .t//
P
t2subtreesn.hyp/ counthyp.t/
: (6.2)
A second metric introduced by Liu and Gildea [2005] is the head-word-chain metric
(HWCM). e metric is analogous to the subtree metric, but the unit is redeﬁned to be chains
of words linked by dependency relations:
HWCM D
1
D
D
X
nD1
P
g2chainn.hyp/ min.counthyp.g/; countref .g//
P
g2chainn.hyp/ counthyp.g/
: (6.3)
Example 6.6 shows a dependency tree and head-word chains that can extracted from it.
Example 6.6 Head-word chains of size 2
6.4. EVALUATION METRICS 151
.
.
.
.
der .
.
Mann .
.
, .
.
der .
.
den .
.
letzten .
.
Marathon .
.
gewonnen .
.
hat
.
.
.
.
the .
.
man .
. .
.
who .
.
won .
.
the .
.
last .
.
marathon
.
root
.
det
.
rel
.
comma
.
aux
.
obja
.
attr
.
det
.
subj
Mann der Mann hat hat , hat der
hat gewonnen gewonnen Marathon Marathon den Marathon letzten
Note that the dependency chains correspond to the syntactic n-grams discussed in the context
of dependency language models (Example 6.5). Appropriately, words that are in an agreement
relationship, speciﬁcally determiners/attributes and their head noun, or subjects and their verb,
are bigrams in this representation, which makes the metric sensitive to the quality of agreement.
Word order in the surface string is ignored, which makes the metric insensitive to reordering
errors, but could also be appropriate for languages with a free word order. Liu and Gildea [2005]
ﬁnd that both HWCM and STM correlate better with human judgments than B. Owczarzak
et al. [2007] deﬁne a metric which extract (labeled) dependency structures from LFG parse trees,
and computes the f-score between the structures of the reference and the translation hypothesis.
A drawback of syntactic metrics is that they rely on a syntactic parse of the reference trans-
lation and the automatic system output. is requires language-speciﬁc resources for syntactic
parsing, and it is hard to predict how well parsers will perform at annotating the texts produced
by MT systems, which are typically far more noisy than the texts that parsers are trained on
and developed for. A method of avoiding to parse the translation output was proposed by Yu
et al. [2014], who only parse the reference translation, extract its head-word chains, and then use
string matching to ﬁnd correspondences of the head-word chains in the translation hypotheses.
For models with target syntax, we can also directly use the internal tree structure of the automatic
translation for the evaluation.
While human judgements are the ﬁnal authority for evaluation, automatic evaluation met-
rics are used during system development, for instance for tuning the parameters of the log-linear
model. B is commonly used as objective function, but may not be the best choice. Sennrich
[2015] optimized the log-linear parameters of a syntax-based SMT system on an interpolation
of B and (a variant of) HWCM. e inclusion of a syntactic metric as an objective function
led to a higher weighting of a dependency language model, compared to optimizing on B
alone, and the resulting system produced better translations with fewer agreement errors than the
system optimized on B.
C H A P T E R 7
Closing Remarks
7.1 WHICH APPROACH IS BEST?
Phrase-based and syntax-based translation are the two dominant paradigms in statistical machine
translation. In many ways, syntax-based translation can be viewed as the natural successor to the
phrase-based approach, just as phrase-based models succeeded word-based ones. While the pur-
pose of this book was not to argue this case, consider many of the following supporting points that
have been discussed herein. e rules that constitute syntax-based systems generalize phrases in
a compelling way, by permitting gaps. Decoding with syntax is theoretically less computationally
complex (polynomial in the input string, vs. NP-hard [Knight, 1999]). In practice, the reliance
of syntax-based systems on parsers and their larger model sizes make them slower, but they ex-
pose easily-turned knobs that often permit near-parity in speed to phrase-based approaches. e
structure provided by syntax-based translation allows reorderings over much longer distances to
be explored in a systematic, empirically justiﬁed way.
At the same time, the paradigms are rightly aligned, not as old vs. new, but as competi-
tive peers. Chronologically, their development has been largely parallel and intertwined. Syntax-
based decoding, with its roots in 1960s-era compiler theory, precedes the stack-based decoding
approach underlying phrase-based translation. But in terms of functioning systems, syntax-based
approaches have lagged behind word- and phrase-based ones by a few years, from the original
respective proposals in the late 1990s to the advent of each system’s dominant instantiation in the
mid 2000s. In applications where raw speed is of primary consideration, phrase-based decoding is
the hands-down winner. Finally, many people also ﬁnd phrase-based decoding to be conceptually
simpler, whereas syntactic methods require a deeper understanding of natural language processing
data structures and algorithms.
In light of these comparisons, it is natural to ask: which is the best approach?
It is impossible to answer such a question outside a particular use case and a set of objec-
tives. For example, a key and growing use of machine translation is in computer-aided translation
(CAT), whose goal is to assist translators and increase their eﬃciency. Distinct from post-editing,
where translators correct the output of an MT system, CAT works with the translators during
the translation process, making suggestions as they work through a sentence in a left-to-right
manner. Although left-to-right syntax-based decoding algorithms have been developed, most
syntax-based methods are non-directional and are hard to apply in this setting.
A more compatible use case for syntax-based output is in the consumption of automatically
translated documents by human users, say, to enable one to understand a foreign news site. But
154 7. CLOSING REMARKS
even within that task, myriad questions must be answered. e issue is complicated by the fact
that “syntax-based MT” encompasses a lot of diﬀerent approaches (string-to-string, string-to-
tree, and so on). e following guidelines provide a useful summary.
• How diﬀerent are the two languages? Syntax-based approaches have worked best in settings
with typologically distinct language pairs where reordering is more of an issue. is includes
language pairs like Chinese—English, Arabic—English, and English—German.
• What tools are available? e use of syntax obviously requires the availability of linguis-
tic parsing tools, whether dependency or constituency. With the exception of tree-to-tree
models, a parser is required only on one side. is is fortunate because, in practice, English
is often one of these language pairs, and means that many tools are likely available.
• How important is quality? Syntax-based approaches can achieve higher quality than phrase-
based models for some language pairs, and are good candidates for settings where higher-
quality translations produce a tangible beneﬁt, such as reduced post-editing eﬀort. If only
the gist of a document is required, any hypothetical quality that could be gained from a
syntax-based approach is unlikely to matter.
• How much time is available to translate the page? Computers are constantly getting faster, as
are syntax-based decoding algorithms (Section 5.5.3). But given a ﬁxed amount of comput-
ing infrastructure, it is likely that a phrase-based approach will yield a higher translation
throughput. is is again related to the objectives being pursued: if speed and cost are of the
utmost importance, phrase-based or string-to-string translation may be best. If the quality
of the output is the highest concern, a deeper look into potential beneﬁts of syntax, along
with the potential tradeoﬀs, is warranted.
• How well formed is the input? Tree-to-string translation is dependent on a source-side parse.
We have already noted that this parse is almost certainly an automatic one, and therefore
will contain errors that have downstream eﬀects on the MT system. ese errors can grow
quickly, however, when applied to ungrammatical text or text from a diﬀerent domain used
to train the parser. us, even for language pairs where such a tree-to-string system had
been shown to be best, cascading errors could seriously aﬀect performance.
• How well formed does the output need to be? As we saw in the previous chapter, an advan-
tage of string-to-tree systems is the ability to model language-speciﬁc details related to text
grammaticality. e ability to condition on rich target-side structure has only just begun to
receive attention, and it is likely that future string-to-tree models will further capitalize on
this advantage.
• How much person-time is available for developing the system? Syntactic approaches, partic-
ularly ones employing language-speciﬁc grammars and linguistic tools, require specialized
7.2. WHAT’S NEXT? 155
attention. Lessons learned for a single language may not generalize. Independent work de-
veloping universal dependency grammar formalisms may help mitigate this.
Ultimately, the question is an empirical one. e wide range of available, well-documented
and supported, open-source translation systems—many of which include both phrase-based and
syntax-based decoders—means that the choice of which paradigm to use is one that can be an-
swered by the tried-and-true method of experimentation.
7.2 WHAT’S NEXT?
As the saying goes, it’s dangerous to make predictions, especially about the future. In many ways,
the question of what lies ahead for statistical machine translation is a question of what lies ahead
for machine translation in general. ere are, however, a few areas of early and trending work that
we think have particular bearing on the syntax question.
e balance between universality and speciﬁcity Most recent work on syntax-based approaches
has been focused on constituency-grammar. is presents a problem: relatively few languages
have well-developed constituency grammars, and even fewer have treebanks that can be used to
train such grammars. As noted above, pairing with English allows the use of syntax in learning
SCFG rules, which can then be applied in either direction. But in order to really utilize the parse
tree, we are eﬀectively limited to tree-to-string systems out of English and string-to-tree ones
into it. is raises an important question in MT research: whether to focus our eﬀorts on what’s
common to most translation pairs, or to focus on language-speciﬁc issues. e former approach is
reﬂected in eﬀorts such as the Universal Dependencies Project [McDonald et al., 2013], and there
is certainly much to be learned from it. On the other hand, it is clear that there is also much to
gain from language-speciﬁc eﬀorts, as well. is can also be viewed as a classic trade-oﬀ between
engineering and science. Exactly where the inﬂection point shifts in favor of speciﬁcity for a
particular language pair, and when and whether it’s worthwhile to chase this long tail, remains an
important issue.
Morphology Syntax-based MT constructs richer representations of the source and target sen-
tences than does phrase-based MT, but at the very bottom, they are the same: word tokens are
represented as unrelated integers. ere is no modeling of the complex relationships between
words reﬂected in inﬂectional and derivational morphology. It is quite clear that this misses im-
portant generalizations, and that this can impact the entire translation pipeline, beginning with
word segmentation and alignment. But morphology is another area where the desire for language
universals is pitted against the need for speciﬁc human-informed models of particular languages.
Most languages do not have morphological analyzers, and many of the tools that do exist are
hand-built symbolic (not statistical) ones. Although there is hope that morphology can eventu-
ally be approached in a language-neutral way, we expect that in the interim there will be lots of
language-speciﬁc work required.
156 7. CLOSING REMARKS
Semantics e fundamental goal of translation is to preserve the meaning of the source text.
Syntactic models do not explicitly model semantic equivalence, and there are many situations
where semantic representations generalize over syntactic variations, such as the active-passive
shift. A compelling extension to syntactic models is therefore semantic models that more explicitly
capture the meaning of translation units, and ensure that meaning is preserved during translations.
ere have been promising pilot studies in the semantic annotation of parallel corpora [e.g., Bos,
2014, Sulem et al., 2015, Xue et al., 2014], but research is still at an early stage. (Similar, in many
ways, to the status of work on syntax in the early 2000s). Given the dependence of semantics on
syntax, we expect that syntax will remain an important component of these eﬀorts.
Neural networks Finally, over the past few years, neural networks have proven to be a disruptive
technology in statistical machine translation. An important open question is whether these ap-
proaches will establish themselves as a third translation paradigm, on par with phrase-based and
syntax-based ones. at is a distinct possibility. However, it is also likely that neural networks
will continue to complement syntax-based machine translation. e relatively recent advance in
neural approaches described in Devlin et al. [2014], for example, was in fact integrated into a
syntax-based (Hiero) translation system. In both cases, it is safe to say that neural networks will
signiﬁcantly shape the future of machine translation.
Open-Source Tools
ere are many open-source packages for hierarchical and syntax-based model extraction and
translation. We here provide a list of some popular tools.
Last
Name Description License Updated
cdec Hiero extractor and SCFG decoder written in C++. cdec-
decoder.org
Apache 2.0 2016
GHKM e original GHKM extractor, written by Michel Galley.
Java-based. github.com/joshua-decoder/galley-ghkm
GPL 2.0 2012
Jane Hiero extraction, hierarchical and phrase-based translation.
C++. www-i6.informatik.rwth-aachen.de/jane
Non-
commercial
2014
Joshua Hierarchical and phrase-based decoder. Java. joshua-
decoder.org
Apache 2.0 2016
Moses Phrase-based and syntax-based decoder. Includes tools for
extracting Hiero, SAMT, and GHKM grammars. C++.
statmt.org/moses
LGPL 2016
NiuTrans Phrase-based, hierarchical, and syntax-based decoder. C++.
www.nlplab.com/NiuPlan/NiuTrans.html
GPL 2.0 2014
SAMT e original SAMT extractor. Hadoop-based.
www.cs.cmu.edu/~zollmann/samt
GPL 2010
rax Hadoop-based Java-tool for Hiero and SAMT grammar
extraction. Included with Joshua, available separately at
github.com/joshua-decoder/thrax
BSD 2-clause 2015
Travatar Tree-to-string decoder written in C++.
www.phontron.com/travatar
LGPL 2016
UCAM-
SMT
Hierarchical phrase-based statistical machine translation sys-
tem based on OpenFST. ucam-smt.github.io
Apache 2.0 2015
Bibliography
Waleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin
Matthews, Kenton Murray, Nicola Segall, Alon Lavie, and Chris Dyer. e CMU ma-
chine translation systems at WMT 2013: syntax, synthetic translation options, and pseudo-
references. In Proc. of the 8th Workshop on Statistical Machine Translation, pages 70–77, Soﬁa,
Bulgaria, August 2013. Association for Computational Linguistics. 141
Abhishek Arun, Chris Dyer, Barry Haddow, Phil Blunsom, Adam Lopez, and Philipp Koehn.
Monte Carlo inference and maximization for phrase-based translation. In Proc. of the 13th Con-
ference on Computational Natural Language Learning (CoNLL-2009), pages 102–110, Boulder,
CO, June 2009. Association for Computational Linguistics. DOI: 10.3115/1596374.1596394.
22
Collin F. Baker, Charles J. Fillmore, and John B. Lowe. e Berkeley FrameNet project. In Proc.
of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International
Conference on Computational Linguistics, vol. 1, pages 86–90, Montreal, Quebec, Canada, 1998.
Association for Computational Linguistics. DOI: 10.3115/980845.980860. 146
Kathy Baker, Steven Bethard, Michael Bloodgood, Ralf Brown, Chris Callison-Burch, Glen
Coppersmith, Bonnie Dorr, Wes Filardo, Kendall Giles, Ann Irvine, Mike Kayser, Lori Levin,
Jusin Martineau, Jim Mayﬁeld, Scott Miller, Aaron Philips, Andrew Philpot, Christine Piatko,
Lane Schwartz, and David Zajic. Semantically informed machine translation (SIMT). Tech-
nical report, Johns Hopkins University Human Language Technology Center of Excellence,
2009. 39
Yehoshua Bar-Hillel, Micha A. Perles, and Eli Shamir. On formal properties of simple phrase
structure grammars. Zeitschrift für Phonetik, Sprachwissenschaft und Kommunikationsforschung,
(14), pages 143–172, 1961. DOI: 10.1524/stuf.1961.14.14.143. 78
Marzieh Bazrafshan and Daniel Gildea. Semantic roles for string to tree machine translation.
In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics, vol. 2, Short
Papers, pages 419–423, Soﬁa, Bulgaria, 2013. 146
Phil Blunsom and Miles Osborne. Probabilistic inference for machine translation. In Proc.
of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 215–
223, Honolulu, HI, October 2008. Association for Computational Linguistics. DOI:
10.3115/1613715.1613746. 22
160 BIBLIOGRAPHY
Ondřej Bojar, Silvie Cinková, and Jan Ptácek. Towards English-to-Czech MT via tectogram-
matical layer. Prague Bull. Math. Linguistics, 90, pages 57–68, 2008. DOI: 10.2478/v10108-
009-0007-5. 133
Ondřej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes
Leveling, Christof Monz, Pavel Pecina, Matt Post, Hervé Saint-Amand, Radu Soricut, Lucia
Specia, and Aleš Tamchyna. Findings of the 2014 Workshop on Statistical Machine Trans-
lation. In Proc. of the 9th Workshop on Statistical Machine Translation, pages 12–58, Baltimore,
MD, June 2014. Association for Computational Linguistics. DOI: 10.3115/v1/w14-3302. xiv
Ondřej Bojar, Rajen Chatterjee, Christian Federmann, Barry Haddow, Matthias Huck, Chris
Hokamp, Philipp Koehn, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Car-
olina Scarton, Lucia Specia, and Marco Turchi. Findings of the 2015 Workshop on Statis-
tical Machine Translation. In Proc. of the 10th Workshop on Statistical Machine Translation,
pages 1–46, Lisbon, Portugal, September 2015. Association for Computational Linguistics.
DOI: 10.18653/v1/w15-3001. xiv
Johan Bos. Semantic annotation issues in parallel meaning banking. In Proc. of the Tenth Joint
ACL-ISO Workshop of Interoperable Semantic Annotations (ISA-10), pages 17–20, Reykjavik,
Iceland, 2014. 156
Eric Brill. A simple rule-based part of speech tagger. In Proc. of the 3rd Conference on Applied
Natural Language Processing, ANLC ’92, pages 152–155, Trento, Italy, 1992. Association for
Computational Linguistics. DOI: 10.3115/974499.974526. 128
Peter F. Brown, Vincent J. Della Pietra, Stephen A. Della Pietra, and Robert L. Mercer. e
mathematics of statistical machine translation: Parameter estimation. Computational Linguis-
tics, 19 (2), pages 263–311, June 1993. 19, 20
Sabine Buchholz and Erwin Marsi. CoNLL-X shared task on multilingual dependency pars-
ing. In Proc. of the 10th Conference on Computational Natural Language Learning (CoNLL-
X), pages 149–164, New York City, 2006. Association for Computational Linguistics. DOI:
10.3115/1596276.1596305. 133
Andrea Burbank, Marine Carpuat, Stephen Clark, Marcus Dreyer, Pamela Fox, Declan Groves,
Keith Hall, Mary Hearne, I. Dan Melamed, Yihai Shen, Andy Way, Benjamin Wellington,
and Dekai Wu. Final report of the 2005 language engineering workshop on statistical machine
translation by parsing. Technical Report, Johns Hopkins University Center for Speech and
Language Processing. 134
David Burkett and Dan Klein. Transforming trees to improve syntactic convergence. In Proc. of
the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computa-
tional Natural Language Learning, pages 863–872, Jeju Island, Korea, July 2012. Association
for Computational Linguistics. 128, 129
BIBLIOGRAPHY 161
Chris Callison-Burch, Miles Osborne, and Philipp Koehn. Re-evaluating the role of BLEU
in machine translation research. In Proc. the 11th Conference of the European Chapter of the
Association for Computational Linguistics, pages 249–256, Trento, Italy, 2006. 150
Marie Candito, Benoît Crabbé, and Pascal Denis. Statistical French dependency parsing: tree-
bank conversion and ﬁrst results. In Proc. of the 7th International Conference on Language Re-
sources and Evaluation (LREC’10), Valletta, Malta, May 2010. European Language Resources
Association (ELRA). 142
Xavier Carreras and Michael Collins. Non-projective parsing for statistical machine translation.
In Proc. of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 200–
209. Association for Computational Linguistics, 2009. DOI: 10.3115/1699510.1699537. 134
Jean-Cédric Chappelier and Martin Rajman. A generalized CYK algorithm for parsing stochastic
CFG. In Proc. of the 1st Workshop on Tabulation in Parsing and Deduction, pages 133–137, 1998.
114, 115
Eugene Charniak. Immediate-head parsing for language models. In Proc. of the 39th Annual
Meeting on Association for Computational Linguistics, ACL ’01, pages 124–131, 2001. DOI:
10.3115/1073012.1073029. 28, 145
Eugene Charniak, Kevin Knight, and Kenji Yamada. Syntax-based language models for statistical
machine translation. In MT Summit IX, New Orleans, 2003. 28, 29, 145
Ciprian Chelba and Frederick Jelinek. Exploiting syntactic structure for language modeling.
In Proc. of the 36th Annual Meeting on Association for Computational Linguistics, 1998. DOI:
10.3115/980451.980882. 29
Colin Cherry and Dekang Lin. A comparison of syntactically motivated word alignment spaces.
In Proc. the 11th Conference of the European Chapter of the Association for Computational Lin-
guistics, Trento, Italy, 2006. 28
Colin Cherry and Chris Quirk. Discriminative, syntactic language modeling through latent
SVMs. In Proc. of the 8th Conference of the Association for Machine Translation in the Ameri-
cas, 2008., Waikiki, HI, October 2008. 30
David Chiang. A hierarchical phrase-based model for statistical machine translation. In Proc.
of the 43rd Annual Meeting on Association for Computational Linguistics, pages 263–270, Ann
Arbor, MI, 2005. DOI: 10.3115/1219840.1219873. 29
David Chiang. Hierarchical phrase-based translation. Computational Linguistics, 33(2),
pages 201–228, 2007. ISSN 0891-2017. DOI: 10.1162/coli.2007.33.2.201. 29, 34, 61, 100,
123, 137
162 BIBLIOGRAPHY
David Chiang. Learning to translate with source and target syntax. In Proc. of the 48th Annual
Meeting of the Association for Computational Linguistics, pages 1443–1452, Uppsala, Sweden,
2010. 28, 130
David Chiang, Kevin Knight, and Wei Wang. 11,001 new features for statistical machine transla-
tion. In Proc. of Human Language Technologies: e 2009 Annual Conference of the North Ameri-
can Chapter of the Association for Computational Linguistics, pages 218–226, Boulder, CO, 2009.
DOI: 10.3115/1620754.1620786. 25, 129
Noam Chomsky. Syntactic Structures. Mouton, e Hague, Netherlands, 1957. DOI:
10.1515/9783110218329. 133
Noam Chomsky. Aspects of the eory of Syntax. MIT Press, Cambridge, MA, 1965. 143
Tagyoung Chung, Licheng Fang, and Daniel Gildea. Issues concerning decoding with syn-
chronous context-free grammar. In Proc. of the 49th Annual Meeting of the Association for Com-
putational Linguistics: Human Language Technologies, pages 413–417, Portland, OR, June 2011.
42, 74
Jonathan Clark, Chris Dyer, and Alon Lavie. Locally non-linear learning for statistical machine
translation via discretization and structured regularization. Transactions of the Association for
Computational Linguistics, 2, pages 393–404, 2014. 23
John Cocke and Jacob T. Schwartz. Programming languages and their compilers: Preliminary
notes. Technical report, Courant Institute of Mathematical Sciences, New York University,
NY, 1970. 67
Michael Collins. Head-driven statistical models for natural language parsing. Computational
Linguistics, 29, pages 589–637, 2003. DOI: 10.1162/089120103322753356. 145
John DeNero, Mohit Bansal, Adam Pauls, and Dan Klein. Eﬃcient parsing for transducer
grammars. In Proc. of Human Language Technologies: e 2009 Annual Conference of the North
American Chapter of the Association for Computational Linguistics, pages 227–235, Boulder, CO,
2009a. DOI: 10.3115/1620754.1620788. 111, 112
John DeNero, Adam Pauls, and Dan Klein. Asynchronous binarization for synchronous gram-
mars. In Proc. of the ACL-IJCNLP 2009 Conference Short Papers, pages 141–144, Singapore,
2009b. Association for Computational Linguistics. DOI: 10.3115/1667583.1667627. 111
Jacob Devlin, Rabih Zbib, Zhongqiang Huang, omas Lamar, Richard Schwartz, and John
Makhoul. Fast and robust neural network joint models for statistical machine translation. In
Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 1, Long
Papers, pages 1370–1380, Baltimore, MD, 2014. DOI: 10.3115/v1/p14-1129. 156
BIBLIOGRAPHY 163
Yuan Ding and Martha Palmer. Machine translation using probabilistic synchronous de-
pendency insertion grammars. In Proc. of the 43rd Annual Meeting of the Association for
Computational Linguistics (ACL’05), pages 541–548, Ann Arbor, MI, June 2005. DOI:
10.3115/1219840.1219907. 100
Bonnie Dorr. Machine translation divergences: A formal description and proposed solution.
Computational Linguistics, 20 (4), pages 597–633, 1994.
Jay Earley. An eﬃcient context-free parsing algorithm. Communications of the ACM, 13(2),
pages 94–102, February 1970. DOI: 10.1145/357980.358005. 114
Jason Eisner. Learning non-isomorphic tree mappings for machine translation. In
Proc. of the 41st Annual Meeting on Association for Computational Linguistics, 2003. DOI:
10.3115/1075178.1075217. 28, 99
Jeﬀrey L. Elman. Distributed representations, simple recurrent networks, and grammatical struc-
ture. In Machine Learning, pages 195–225, 1991. DOI: 10.1007/bf00114844. 141
Licheng Fang, Tagyoung Chung, and Daniel Gildea. Terminal-Aware Synchronous Binariza-
tion. In Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human
Language Technologies, pages 401–406, Portland, OR, June 2011. 111
Cynthia Fisher, Henry Gleitman, and Lila R. Gleitman. On the semantic content of subcatego-
rization frames. Cognitive Psychology, 23(3), pages 331–392, July 1991. DOI: 10.1016/0010-
0285(91)90013-e. 144
Heidi J. Fox. Phrasal cohesion and statistical machine translation. In Proc. of the ACL-02 Confer-
ence on Empirical Methods in Natural Language Processing, Philadelphia, PA, July 2002. DOI:
10.3115/1118693.1118732. 29
Alexander Fraser, Marion Weller, Aoife Cahill, and Fabienne Cap. Modeling inﬂection and
word-formation in SMT. In Proc. of the 13th Conference of the European Chapter of the Association
for Computational Linguistics, pages 664–674, Avignon, France, 2012. 141
Alexander Fraser, Helmut Schmid, Richárd Farkas, Renjing Wang, and Hinrich Schütze. Knowl-
edge sources for constituent parsing of German, a morphologically rich and less-conﬁgurational
language. Computational Linguistics, 2013. DOI: 10.1162/coli_a_00135. 45
Haim Gaifman. Dependency systems and phrase-structure systems. Information and Control,
8(3), pages 304–337, 1965. DOI: 10.1016/s0019-9958(65)90232-9. 134
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. What’s in a translation rule? In
Proc. of the Main Conference on Human Language Technology Conference of the North American
Chapter of the Association of Computational Linguistics, Boston, MA, May 2004. 29, 40, 41
164 BIBLIOGRAPHY
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and
Ignacio ayer. Scalable inference and training of context-rich syntactic translation models.
In Proc. of the 21st International Conference on Computational Linguistics and the 44th Annual
Meeting of the Association for Computational Linguistics, Sydney, Australia, July 2006. DOI:
10.3115/1220175.1220296. 18, 19, 20, 23, 26, 29, 40, 42, 43
Giorgio Gallo, Giustino Longo, Stefano Pallottino, and Sang Nguyen. Directed hyper-
graphs and applications. Discrete Applied Mathematics, 42(2–3), pages 177–201, 1993. DOI:
10.1016/0166-218x(93)90045-p. 67
Ulrich Germann, Eric Joanis, and Samuel Larkin. Tightly packed tries: How to ﬁt large mod-
els into memory, and make them load fast, too. In Proc. of the Workshop on Software Engi-
neering, Testing, and Quality Assurance for Natural Language Processing (SETQA-NLP 2009),
pages 31–39, Boulder, CO, June 2009. Association for Computational Linguistics. DOI:
10.3115/1621947.1621952. 123
Spence Green, Daniel Cer, and Christopher D. Manning. An empirical comparison of features
and tuning for phrase-based machine translation. In Proc. of the 9th Workshop on Statistical Ma-
chine Translation, pages 466–476, Baltimore, MD, June 2014. Association for Computational
Linguistics. DOI: 10.3115/v1/w14-3360. 25
Jan Hajič, Alena Böhmová, Eva Hajičová, and Barbora Vidová-Hladká. e Prague Dependency
Treebank: A ree-Level Annotation Scenario. In A. Abeillé, Ed., Treebanks: Building and
Using Parsed Corpora, pages 103–127. Amsterdam, Kluwer, 2000. DOI: 10.1007/978-94-010-
0201-1. 133
Eva Hajičová, Jiří Havelka, Petr Sgall, Kateřina Veselá, and Daniel Zeman. Issues of projectivity
in the Prague Dependency Treebank. Prague Bulletin of Mathematical Linguistics, (81), pages 5–
22, 2004. 133
Greg Hanneman and Alon Lavie. Automatic category label coarsening for syntax-based machine
translation. In Proc. of the 5th Workshop on Syntax, Semantics and Structure in Statistical Transla-
tion, SSST-5, pages 98–106, Portland, OR, 2011. Association for Computational Linguistics.
ISBN 978-1-932432-99-2. 130
Christian Hardmeier and Marcello Federico. Modelling Pronominal Anaphora in Statistical
Machine Translation. In Marcello Federico, Ian Lane, Michael Paul, and François Yvon, Eds.,
Proc. of the 7th International Workshop on Spoken Language Translation (IWSLT), pages 283–
289, 2010. 143
Christian Hardmeier, Jörg Tiedemann, Markus Saers, Marcello Federico, and Prashant Mathur.
e Uppsala-FBK systems at WMT 2011. In Proc. of the 6th Workshop on Statistical Machine
Translation, pages 372–378, 2011. 134
BIBLIOGRAPHY 165
Kenneth Heaﬁeld. KenLM: faster and smaller language model queries. In Proc. of the 6th Workshop
on Statistical Machine Translation, pages 187–197, Edinburgh, Scotland, July 2011. Association
for Computational Linguistics. 78
Kenneth Heaﬁeld, Philipp Koehn, and Alon Lavie. Language model rest costs and space-eﬃcient
storage. In Proc. of the 2012 Joint Conference on Empirical Methods in Natural Language Process-
ing and Computational Natural Language Learning, pages 1169–1178, Jeju Island, Korea, July
2012. Association for Computational Linguistics. 83
Kenneth Heaﬁeld, Philipp Koehn, and Alon Lavie. Grouping language model boundary words to
speed K-best extraction from hypergraphs. In Proc. of the 2013 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta,
GA, June 2013. 92, 93, 94, 95, 96, 97
Josée S. Heemskerk. A probabilistic context-free grammar for disambiguation in morpholog-
ical parsing. In Proc. of the 6th Conference on European Chapter of the Association for Com-
putational Linguistics, EACL ’93, pages 183–192, Utrecht, e Netherlands, 1993. DOI:
10.3115/976744.976767. 147
Mark Hopkins and Greg Langmead. Cube pruning as heuristic search. In Proc. of the 2009
Conference on Empirical Methods in Natural Language Processing, pages 62–71, Singapore, 2009.
Association for Computational Linguistics. DOI: 10.3115/1699510.1699519. 100
Mark Hopkins and Greg Langmead. SCFG decoding without binarization. In Proc. of the 2010
Conference on Empirical Methods in Natural Language Processing, pages 646–655, Cambridge,
MA, October 2010. Association for Computational Linguistics. 111, 112, 113
Liang Huang. Forest reranking: discriminative parsing with non-local features. In Proc. of the
45th Annual Meeting of the Association for Computational Linguistics, pages 586–594, Columbus,
OH, June 2008. Association for Computational Linguistics. 100
Liang Huang and David Chiang. Better k-best parsing. In Parsing ’05: Proc. of the 9th Interna-
tional Workshop on Parsing Technology, pages 53–64, Vancouver, Canada, 2005. Association for
Computational Linguistics. DOI: 10.3115/1654494.1654500. 61, 63, 67
Liang Huang and David Chiang. Forest rescoring: faster decoding with integrated language
models. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,
pages 144–151, Prague, Czech Republic, June 2007. 29, 61, 89, 90, 92, 100
Liang Huang, Kevin Knight, and Aravind Joshi. Statistical syntax-directed translation with ex-
tended domain of locality. In Proc. of the 7th Conference of the Association for Machine Translation
in the Americas, pages 66–73, 2006. 29, 100
166 BIBLIOGRAPHY
Liang Huang, Hao Zhang, Daniel Gildea, and Kevin Knight. Binarization of synchronous
context-free grammars. Computational Linguistics, 35(4), pages 559–595, 2009. DOI:
10.1162/coli.2009.35.4.35406. 110
Matthias Huck, Hieu Hoang, and Philipp Koehn. Preference grammars and soft syntactic con-
straints for GHKM syntax-based statistical machine translation. In Proc. of SSST-8, 8th Work-
shop on Syntax, Semantics and Structure in Statistical Translation, pages 148–156, Doha, Qatar,
2014. Association for Computational Linguistics. DOI: 10.3115/v1/w14-4018. 131
Peter Ihm and Yves Lecerf. Éléments pour une grammaire générale des langues projec-
tives. Centre commun de recherche nucléaire, Etablissement d’Ispra, Centre de traitement
de l’information scientiﬁque. CETIS, 1963. 133
Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, and Chris Quirk. A discriminative lexicon
model for complex morphology. In e 9th Conference of the Association for Machine Translation
in the Americas. Association for Computational Linguistics, 2010. 141
Mark Johnson. PCFG models of linguistic tree representations. Computational Linguistics, 24(4),
pages 613–632, 1998. 129
Mark Johnson, omas L. Griﬃths, and Sharon Goldwater. Bayesian inference for PCFGs via
Markov chain Monte Carlo. In Proc. of Human Language Technologies 2007: e Conference of
the North American Chapter of the Association for Computational Linguistics, 2007. 147
Miriam Kaeshammer. Hierarchical machine translation with discontinuous phrases. In Proc. of the
10th Workshop on Statistical Machine Translation, pages 228–238, Lisbon, Portugal, September
2015. Association for Computational Linguistics. DOI: 10.18653/v1/w15-3028. 134
Ronald M Kaplan, Klaus Netter, Jürgen Wedekind, and Annie Zaenen. Translation by Struc-
tural Correspondences. In Sergei Nirenburg, Harold L. Somers, and Yorick A. Wilks, Eds.,
Readings in Machine Translation. MIT Press, 2003. 27
Tadao Kasami. An eﬃcient recognition and syntax-analysis algorithm for context-free languages.
Technical report, Air Force Cambridge Research Lab, Bedford, MA, 1965. 67
Martin Kay. Functional uniﬁcation grammar: A formalism for machine translation. In Proc.
of the 10th International Conference on Computational Linguistics and 22nd Annual Meet-
ing on Association for Computational Linguistics, pages 75–78, Stroudsburg, PA, 1984. DOI:
10.3115/980491.980509. 27
Dan Klein and Christopher D. Manning. Parsing and hypergraphs. In 7th International Workshop
on Parsing Technologies, pages 123–134, Bejing, China, 2001a. DOI: 10.1007/1-4020-2295-
6_18. 67
BIBLIOGRAPHY 167
Dan Klein and Christopher D. Manning. Parsing with treebank grammars: Empirical bounds,
theoretical models, and the structure of the penn treebank. In Proc. of 39th Annual Meeting
of the Association for Computational Linguistics, pages 338–345, Toulouse, France, July 2001b.
DOI: 10.3115/1073012.1073056. 123
Kevin Knight. Decoding complexity in word-replacement translation models. Computational
Linguistics, 25(4), pages 607–615, 1999. 153
Kevin Knight and Jonathan Graehl. An Overview of Probabilistic Tree Transducers for Natural
Language Processing. In Alexander F. Gelbukh and Alexander F. Gelbukh, Eds., CICLing,
vol. 3406 of Lecture Notes in Computer Science, pages 1–24. Springer, 2005. 6, 98
Philipp Koehn. Statistical Machine Translation. Cambridge University Press, New York, NY,
2010. DOI: 10.1017/cbo9780511815829. xv
Philipp Koehn and Hieu Hoang. Factored Translation Models. In Proc. of the 2007 Joint Confer-
ence on Empirical Methods in Natural Language Processing and Computational Natural Language
Learning (EMNLP-CoNLL), pages 868–876, Prague, Czech Republic, 2007. Association for
Computational Linguistics. 141
Philipp Koehn and Kevin Knight. Empirical Methods for Compound Splitting. In Proc. of the
10th Conference on European Chapter of the Association for Computational Linguistics, pages 187–
193, Budapest, Hungary, 2003. DOI: 10.3115/1067807.1067833. 147
Philipp Koehn, Franz Josef Och, and Daniel Marcu. Statistical phrase-based translation. In Proc.
of the 2003 Conference of the North American Chapter of the Association for Computational Lin-
guistics on Human Language Technology, pages 48–54, Edmonton, Canada, 2003. Association
for Computational Linguistics. DOI: 10.3115/1073445.1073462. 19, 20, 29, 36, 40, 126
Philipp Koehn, Abhishek Arun, and Hieu Hoang. Towards better machine translation quality for
the German-English language pairs. In Proc. of the 3rd Workshop on Statistical Machine Transla-
tion, pages 139–142, Columbus, OH, June 2008. Association for Computational Linguistics.
DOI: 10.3115/1626394.1626412. 2
Marco Kuhlmann. Mildly Non-Projective Dependency Grammar. Computational Linguistics,
39(2), pages 355–387, 2013. DOI: 10.1162/coli_a_00125. 134
Marco Kuhlmann and Joakim Nivre. Mildly non-projective dependency structures. In 21st
International Conference on Computational Linguistics and 44th Annual Meeting of the Association
for Computational Linguistics, Sydney, Australia, 2006. DOI: 10.3115/1273073.1273139. 134
Jan Landsbergen. Montague Grammar and Machine Translation. In Sergei Nirenburg, Harold L.
Somers, and Yorick A. Wilks, Eds., Readings in Machine Translation. MIT Press, 2003. 27
168 BIBLIOGRAPHY
Junhui Li, Philip Resnik, and Hal Daumé III. Modeling syntactic and semantic structures in hier-
archical phrase-based translation. In Proc. of the 2013 Conference of the North American Chapter
of the Association for Computational Linguistics: Human Language Technologies, pages 540–549,
Atlanta, GA, June 2013. 74
Zhifei Li and Sanjeev Khudanpur. A scalable decoder for parsing-based machine trans-
lation with equivalent language model state maintenance. In Proc. of the NAACL-
HLT/AMTA Workshop on Syntax and Structure in Statistical Translation, page 10, 2008. DOI:
10.3115/1626269.1626271. 83
Percy Liang, Ben Taskar, and Dan Klein. Alignment by agreement. In Proc. of the Main Conference
on Human Language Technology Conference of the North American Chapter of the Association of
Computational Linguistics, pages 104–111, 2006. DOI: 10.3115/1220835.1220849. 31
Dekang Lin. A path-based transfer model for machine translation. In Proc. of the 18th Interna-
tional Conference on Computational Linguistics (Coling 2004), pages 625–630, Geneva, Switzer-
land, Aug. 23–27 2004. DOI: 10.3115/1220355.1220445. 100
Ding Liu and Daniel Gildea. Syntactic features for evaluation of machine translation. In Proc. of
the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or
Summarization, pages 25–32, Ann Arbor, MI, 2005. 150, 151
Ding Liu and Daniel Gildea. Improved tree-to-string transducer for machine translation. In
Proc. of the 3rd Workshop on Statistical Machine Translation, pages 62–69, Columbus, OH, 2008.
Association for Computational Linguistics. DOI: 10.3115/1626394.1626402. 146
Yang Liu, Qun Liu, and Shouxun Lin. Tree-to-string alignment template for statistical ma-
chine translation. In Proc. of the 21st International Conference on Computational Linguistics and
the 44th Annual Meeting of the Association for Computational Linguistics, pages 609–616, 2006.
DOI: 10.3115/1220175.1220252. 29, 100
Adam Lopez. Statistical machine translation. ACM Comput. Surv., 40(3), pages 1–49, 2008.
DOI: 10.1145/1380584.1380586. xv
Adam Lopez. Translation as weighted deduction. In Proc. of the 12th Conference of the
European Chapter of the Association of Computational Linguistics (EACL 2009), pages 532–
540, Athens, Greece, March 2009. Association for Computational Linguistics. DOI:
10.3115/1609067.1609126. 123
Matous Machacek and Ondřej Bojar. Results of the WMT14 metrics shared task. In Proc. of the
9th Workshop on Statistical Machine Translation, pages 293–301, Baltimore, MD, June 2014.
Association for Computational Linguistics. DOI: 10.3115/v1/w14-3336. 149
BIBLIOGRAPHY 169
Daniel Marcu and William Wong. A phrase-based, joint probability model for statistical ma-
chine translation. In Proc. of the ACL-02 Conference on Empirical Methods in Natural Language
Processing, vol. 10, pages 133–139, Stroudsburg, PA, 2002. Association for Computational
Linguistics. DOI: 10.3115/1118693.1118711. 19
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated
corpus of English: e Penn Treebank. Computational Linguistics, 19(2), page 330, 1993. 43
Takuya Matsuzaki, Yusuke Miyao, and Jun’ichi Tsujii. Probabilistic CFG with latent annotations.
In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor,
MI, June 2005. DOI: 10.3115/1219840.1219850. 129
Austin Matthews, Paul Baltescu, Phil Blunsom, Alon Lavie, and Chris Dyer. Tree transduction
tools for cdec. e Prague Bulletin of Mathematical Linguistics, (102), pages 27–36, 2014. DOI:
10.2478/pralin-2014-0011. 98
Ryan McDonald, Joakim Nivre, Yvonne Quirmbach-Brundage, Yoav Goldberg, Dipanjan Das,
Kuzman Ganchev, Keith Hall, Slav Petrov, Hao Zhang, Oscar Täckström, Claudia Bedini,
Núria Bertomeu Castelló, and Jungmee Lee. Universal dependency annotation for multilingual
parsing. In Proc. of the 51st Annual Meeting of the Association for Computational Linguistics,
vol. 2, Short Papers, pages 92–97, Soﬁa, Bulgaria, August 2013. 155
I. Dan Melamed. Multitext grammars and synchronous parsers. In Proc. of the 2003 Confer-
ence of the North American Chapter of the Association for Computational Linguistics on Human
Language Technology, pages 79–86, Edmonton, Canada, 2003. Association for Computational
Linguistics. DOI: 10.3115/1073445.1073466. 110
Arul Menezes and Chris Quirk. Using dependency order templates to improve generality in
translation. In Proc. of the Second Workshop on Statistical Machine Translation at ACL 2007.
Association for Computational Linguistics, July 2007. DOI: 10.3115/1626355.1626356. 136
Arul Menezes and Chris Quirk. Syntactic models for structural word insertion and deletion
during translation. In Proc. of the 2008 Conference on Empirical Methods in Natural Language
Processing, pages 735–744, Honolulu, HI, October 2008. Association for Computational Lin-
guistics. DOI: 10.3115/1613715.1613808. 137
Haitao Mi and Liang Huang. Forest-based translation rule extraction. In Proc. of the 2008
Conference on Empirical Methods in Natural Language Processing, pages 206–214, Honolulu, HI,
October 2008. Association for Computational Linguistics. DOI: 10.3115/1613715.1613745.
132
Haitao Mi, Liang Huang, and Qun Liu. Forest-based translation. In Proc. of the 45th Annual
Meeting of the Association for Computational Linguistics, pages 192–199, Columbus, OH, June
2008. Association for Computational Linguistics. 100, 131, 132
170 BIBLIOGRAPHY
Moses Contributors. Moses Syntax Tutorial. http://www.statmt.org/moses/?n=Moses.Sy
ntaxTutorial. Accessed November 28, 2015. 25
Graham Neubig and Kevin Duh. On the elements of an accurate tree-to-string machine transla-
tion system. In Proc. of the 52nd Annual Meeting of the Association for Computational Linguistics,
vol. 2, Short Papers, pages 143–149, Baltimore, MD, June 2014. DOI: 10.3115/v1/p14-2024.
132
Joakim Nivre and Jens Nilsson. Pseudo-projective dependency parsing. In Proc. of the 43rd Annual
Meeting of the Association for Computational Linguistics, pages 99–106, Ann Arbor, MI, 2005.
DOI: 10.3115/1219840.1219853. 133
Franz Josef Och and Hermann Ney. Discriminative training and maximum entropy models for
statistical machine translation. In Proc. of the 40th Annual Meeting on Association for Computa-
tional Linguistics, pages 295–302, Pennsylvania, PA, 2002. DOI: 10.3115/1073083.1073133.
18, 21, 36
Franz Josef Och and Hermann Ney. A systematic comparison of various statisti-
cal alignment models. Computational Linguistics, 29(1), pages 19–51, 2003. DOI:
10.1162/089120103321337421. 31
Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur, Anoop Sarkar, Kenji Yamada, Alexander
Fraser, Shankar Kumar, Libin Shen, David Smith, Katherine Eng, Viren Jain, Zhen Jin, and
Dragomir Radev. A smorgasbord of features for statistical machine translation. In Proc. of the
Main Conference on Human Language Technology Conference of the North American Chapter of
the Association of Computational Linguistics, pages 161–168, Boston, MA, 2004. 30, 145
Karolina Owczarzak, Josef van Genabith, and Andy Way. Labelled dependencies in ma-
chine translation evaluation. In Proc. of the 2nd Workshop on Statistical Machine Translation,
pages 104–111, Prague, Czech Republic, 2007. Association for Computational Linguistics.
DOI: 10.3115/1626355.1626369. 151
Martha Palmer, Daniel Gildea, and Paul Kingsbury. e Proposition Bank: a corpus annotated
with semantic roles. Computational Linguistics Journal, 31(1), 2005. 146
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: A method for
automatic evaluation of machine translation. In Proc. of the 40th Annual Meeting on
Association for Computational Linguistics, pages 311–318, Philadelphia, PA, 2002. DOI:
10.3115/1073083.1073135. 149
Slav Petrov, Leon Barrett, Romain ibaux, and Dan Klein. Learning accurate, compact, and
interpretable tree annotation. In Proc. of the 21st International Conference on Computational Lin-
guistics and the 44th Annual Meeting of the Association for Computational Linguistics, pages 433–
440, Sydney, Australia, 2006. DOI: 10.3115/1220175.1220230. 17, 45, 129
BIBLIOGRAPHY 171
Martin Popel and David Mareček. Perplexity of n-Gram and Dependency Language Models.
In Petr Sojka, Ales Horák, Ivan Kopecek, and Karel Pala, Eds., Text, Speech and Dialogue,
vol. 6231 of Lecture Notes in Computer Science, pages 173–180. Springer, 2010. ISBN 978-3-
642-15759-2. 141
Matt Post and Daniel Gildea. Parsers as language models for statistical machine translation. In
Proc. of the 8th Conference of the Association for Machine Translation in the Americas, 2008. 30,
145
Arjen Poutsma. Data-oriented translation. In Proc. of the 14th International Con-
ference on Computational Linguistics (Coling 2000), Saarbrücken, Germany, 2000. DOI:
10.3115/992730.992738. 28
Chris Quirk, Arul Menezes, and Colin Cherry. Dependency Tree Translation: Syntactically
Informed Phrasal SMT. Technical Report MSR-TR-2004-113, Microsoft Research, 2004.
100, 135, 136, 138, 141, 145, 148
Chris Quirk, Arul Menezes, and Colin Cherry. Dependency treelet translation: Syntactically
informed phrasal SMT. In Proc. of the 43rd Annual Meeting of the Association for Computational
Linguistics, Ann Arbor, MI, June 2005. DOI: 10.3115/1219840.1219874. 29, 100
Rudolf Rosa, David Mareček, and Ondřej Dušek. DEPFIX: A system for automatic correction of
Czech MT outputs. In Proc. of the 7th Workshop on Statistical Machine Translation, pages 362–
368, Montreal, Canada, 2012. Association for Computational Linguistics. 141
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. Incremental syntactic
language models for phrase-based translation. In Proc. of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Language Technologies, pages 620–631, Portland,
OR, June 2011a. 30
Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu. Erratum to in-
cremental syntactic language models for phrase-based translation. In Proc. of the 49th An-
nual Meeting of the Association for Computational Linguistics: Human Language Technologies,
pages 620–631, Portland, OR, June 2011b. Association for Computational Linguistics. http:
//dowobeha.github.io/papers/acl11.erratum.pdf 30
Nina Seemann, Fabienne Braune, and Andreas Maletti. String-to-tree multi bottom-up tree
transducers. In Proc. of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, vol. 1, Long Papers,
pages 815–824, Beijing, China, July 2015. DOI: 10.3115/v1/p15-1079. 134
Rico Sennrich. A CYK+ variant for SCFG decoding without a dot chart. In Proc. of SSST-8, 8th
Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 94–102, Doha,
172 BIBLIOGRAPHY
Qatar, October 2014. Association for Computational Linguistics. DOI: 10.3115/v1/w14-
4011. 118
Rico Sennrich. Modelling and optimizing on syntactic n-grams for statistical machine transla-
tion. Transactions of the Association for Computational Linguistics, 3, pages 169–182, 2015. 141,
145, 149, 151
Rico Sennrich and Barry Haddow. A joint dependency model of morphological and syntactic
structure for statistical machine translation. In Proc. of the 2015 Conference on Empirical Meth-
ods in Natural Language Processing, pages 2081–2087, Lisbon, Portugal, 2015. Association for
Computational Linguistics. DOI: 10.18653/v1/d15-1248. 147
Rico Sennrich, Philip Williams, and Matthias Huck. A tree does not make a well-formed
sentence: Improving syntactic string-to-tree statistical machine translation with more lin-
guistic knowledge. Computer Speech and Language, 32(1), pages 27–45, 2015. DOI:
10.1016/j.csl.2014.09.002. 134
Libin Shen, Jinxi Xu, and Ralph Weischedel. String-to-dependency statistical machine trans-
lation. Computational Linguistics, 36(4), pages 649–671, 2010. DOI: 10.1162/coli_a_00015.
30, 138, 139, 141, 149
Reshef Shilon, Hanna Fadida, and Shuly Wintner. Incorporating linguistic knowledge in sta-
tistical machine translation: translating prepositions. In Proc. of the Workshop on Innovative
Hybrid Approaches to the Processing of Textual Data, pages 106–114, Avignon, France, April
2012. Association for Computational Linguistics. 145
Grigori Sidorov, Francisco Velasquez, Efstathios Stamatatos, Alexander Gelbukh, and Liliana
Chanona-Hernández. Syntactic dependency-based n-grams as classiﬁcation features. In Proc.
of the 11th Mexican International Conference on Advances in Computational Intelligence, vol. Part
II, pages 1–11, San Luis Potosí, Mexico, 2013. Springer-Verlag. DOI: 10.1007/978-3-642-
37798-3_1. 148
Sara Stymne and Nicola Cancedda. Productive generation of compound words in statistical
machine translation. In Proc. of the 6th Workshop on Statistical Machine Translation, pages 250–
260, Edinburgh, Scotland, 2011. Association for Computational Linguistics. 147
Elior Sulem, Omri Abend, and Ari Rappoport. Conceptual annotations preserve structure across
translations: a French-English case study. In Proc. of the 1st Workshop on Semantics-Driven
Statistical Machine Translation (S2MT 2015), pages 11–22, Beijing, China, 2015. Association
for Computational Linguistics. DOI: 10.18653/v1/w15-3502. 156
Aram Ter-Sarkisov, Holger Schwenk, Fethi Bougares, and Loïc Barrault. Incremental Adapta-
tion Strategies for Neural Network Language Models. In Proc. of the 3rd Workshop on Contin-
uous Vector Space Models and their Compositionality, Bejing, China, 2015. 141
BIBLIOGRAPHY 173
Lucien Tesnière. Elements de Syntaxe Structurale. Editions Klincksieck, Paris, 1959. 143
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp. Applying morphology generation mod-
els to machine translation. In Proc. of the 45th Annual Meeting of the Association for Computa-
tional Linguistics, 2008. 141
David Vilar and Hermann Ney. On LM heuristics for the cube growing algorithm. In An-
nual Conference of the European Association for Machine Translation, pages 242–249, Barcelona,
Spain, May 2009. 90
David Vilar, Jan-orsten Peter, and Hermann Ney. Can we translate letters? In Proc. of the Second
Workshop on Statistical Machine Translation, pages 33–39, Prague, Czech Republic, June 2007.
Association for Computational Linguistics. DOI: 10.3115/1626355.1626360. 146
Andrew Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. IEEE Transactions on Information eory, 13(2), pages 260–269, September 1967.
DOI: 10.1109/tit.1967.1054010. 67
Wei Wang, Kevin Knight, and Daniel Marcu. Binarizing syntax trees to improve syntax-based
machine translation accuracy. In Proc. of the 2007 Joint Conference on Empirical Methods in Nat-
ural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),
2007. 127, 128
Wei Wang, Jonathan May, Kevin Knight, and Daniel Marcu. Re-structuring, re-labeling, and
re-aligning for syntax-based machine translation. Computational Linguistics, 36(2), pages 247–
277, June 2010. DOI: 10.1162/coli.2010.36.2.09054. 129
Warren Weaver. Translation. In William N. Locke and A. Donald Booth, Eds., Machine Trans-
lation of Languages: Fourteen Essays, pages 15–23. John Wiley & Sons, 1955. 27
Jonathan Weese, Juri Ganitkevitch, Chris Callison-Burch, Matt Post, and Adam Lopez. Joshua
3.0: syntax-based machine translation with the rax grammar extractor. In Proc. of the 6th
Workshop on Statistical Machine Translation, 2011. 39
Marion Weller, Alexander Fraser, and Sabine Schulte im Walde. Using subcategorization knowl-
edge to improve case prediction for translation to German. In Proc. of the 51st Annual Meeting
of the Association for Computational Linguistics, vol. 1, Long Papers, pages 593–603, Soﬁa, Bul-
garia, August 2013. 145
Benjamin Wellington, Sonjia Waxmonsky, and I. Dan Melamed. Empirical lower bounds on the
complexity of translational equivalence. In Proc. of the 21st International Conference on Compu-
tational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics,
Sydney, Australia, July 2006. DOI: 10.3115/1220175.1220298. 28
174 BIBLIOGRAPHY
John S. White, eresa O’Connell, and Francis O’Mara. e ARPA MT evaluation methodolo-
gies: evolution, lessons, and future approaches. In Proc. of the 1st Conference of the Association
for Machine Translation in the Americas, pages 193–205, Columbia, MD, 1994. 148
Philip Williams and Philipp Koehn. Agreement constraints for statistical machine translation
into German. In Proc. of the 6th Workshop on Statistical Machine Translation, pages 217–226,
Edinburgh, UK, 2011. Association for Computational Linguistics. 141
Philip Williams and Philipp Koehn. GHKM rule extraction and scope-3 parsing in Moses. In
Proc. of the 7th Workshop on Statistical Machine Translation, pages 388–394, Montréal, Canada,
June 2012. Association for Computational Linguistics. 44
Philip Williams and Philipp Koehn. Using feature structures to improve verb translation in
English-to-German statistical MT. In Proc. of the 3rd Workshop on Hybrid Approaches to Ma-
chine Translation (HyTra), pages 21–29, Gothenburg, Sweden, April 2014. Association for
Computational Linguistics. DOI: 10.3115/v1/w14-1005. 30, 148
Philip Williams, Rico Sennrich, Maria Nadejde, Matthias Huck, and Philipp Koehn. Edin-
burgh’s syntax-based systems at WMT 2015. In Proc. of the 10th Workshop on Statistical Machine
Translation, pages 199–209, Lisbon, Portugal, 2015. Association for Computational Linguis-
tics. DOI: 10.18653/v1/w15-3024. 128
Dekai Wu. Trainable coarse bilingual grammars for parallel text bracketing. In Proc. of the 3rd
Workshop on Very Large Corpora (VLC), 1995. DOI: 10.1007/978-94-017-2390-9_15. 28
Dekai Wu. A polynomial-time algorithm for statistical machine translation. In Proc. of the 34th
Annual Meeting on Association for Computational Linguistics, Santa Cruz, CA, June 1996. DOI:
10.3115/981863.981884. 28, 100
Dekai Wu. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3), pages 377–403, 1997. 28, 33, 110
Wenduan Xu and Philipp Koehn. Extending hiero decoding in Moses with cube growing. e
Prague Bulletin of Mathematical Linguistics, (98), pages 133–142, 2012. DOI: 10.2478/v10108-
012-0015-8. 90
Nianwen Xue, Ondřej Bojar, Jan Hajič, Martha Palmer, Zdenka Urešová, and Xiuhong Zhang.
Not an interlingua, but close: comparison of English AMRs to Chinese and Czech. In Proc. of
the 9th International Conference on Language Resources and Evaluation (LREC’14), Reykjavik,
Iceland, 2014. European Language Resources Association (ELRA). 156
Kenji Yamada and Kevin Knight. A Syntax-based statistical translation model. In Proc. of the
39th Annual Meeting on Association for Computational Linguistics, Toulouse, France, July 2001.
DOI: 10.3115/1073012.1073079. 28, 41
BIBLIOGRAPHY 175
Kenji Yamada and Kevin Knight. A decoder for syntax-based statistical MT. In Proc. of the 40th
Annual Meeting on Association for Computational Linguistics, Philadelphia, PA, July 2002. DOI:
10.3115/1073083.1073134. 28
Victor H. Yngve. A Framework for Syntactic Translation. In Sergei Nirenburg, Harold L.
Somers, and Yorick A. Wilks, Eds., Readings in Machine Translation. MIT Press, 2003. 27
Daniel H. Younger. Recognition and parsing of context-free languages in time n3. Information
and Control, 10(2), pages 189–208, 1967. DOI: 10.1016/s0019-9958(67)80007-x. 67
Hui Yu, Xiaofeng Wu, Jun Xie, Wenbin Jiang, Qun Liu, and Shouxun Lin. RED: A refer-
ence dependency based MT evaluation metric. In COLING 2014, 25th International Confer-
ence on Computational Linguistics, Proc. of the Conference: Technical Papers, August 23–29, 2014,
Dublin, Ireland, pages 2042–2051, 2014. 151
Hao Zhang, Liang Huang, Daniel Gildea, and Kevin Knight. Synchronous binarization
for machine translation. In Proc. of the Main Conference on Human Language Technol-
ogy Conference of the North American Chapter of the Association of Computational Linguistics,
pages 256–263, New York City, 2006. Association for Computational Linguistics. DOI:
10.3115/1220835.1220868. 110, 120
Hao Zhang, Licheng Fang, Peng Xu, and Xiaoyun Wu. Binarized forest to string translation. In
Proc. of the 49th Annual Meeting of the Association for Computational Linguistics: Human Lan-
guage Technologies, pages 835–845, Portland, OR, June 2011. Association for Computational
Linguistics. 132
Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan. Fast translation rule matching for
syntax-based statistical machine translation. In Proc. of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1037–1045, Singapore, August 2009. Association for
Computational Linguistics. DOI: 10.3115/1699571.1699647. 98, 131
Min Zhang, Hongfei Jiang, Haizhou Li, Aiti Aw, and Sheng Li. Grammar comparison study
for translational equivalence modeling and statistical machine translation. In Proc. of the 22nd
International Conference on Computational Linguistics (Coling 2008), pages 1097–1104, Manch-
ester, UK, August 2008. DOI: 10.3115/1599081.1599219. 134
Andreas Zollmann and Ashish Venugopal. Syntax augmented machine translation via chart
parsing. In Proc. of the Workshop on Statistical Machine Translation, pages 138–141, New York
City, 2006. Association for Computational Linguistics. DOI: 10.3115/1654650.1654671. 29,
36
Index
agglutination, 147
agreement, 129, 130, 134, 137–138, 140–143
alignment point, 32
consistent, 32
negative, 32
positive, 32, 34
asynchronous binarization, 111
automatic speech recognition, 19
backward hyperedge, 51
backward hypergraph, 51
Bayes’ rule, 19–20
beam search
in string decoding, 101–108
in tree decoding, 80–97
binarization
alternatives to, 111–113
and subcategorization, 128
asynchronous, 111
of context-free grammars, 108–109
of synchronous grammars, 108–113
of trees, 127–128, 136
bracketing transduction grammar, 28
bundled hyperedge, 74–76
bundled hypergraph, 74–76
canonical derivation, 8–9, 12
chart, 57–61
Chomsky normal form, 57, 112, 114
complexity
and rule form, 108–114
of CYK algorithm, 59
of exact tree decoding, 80
of parse forest, 103–106, 114
composed rules (in GHKM), 44–45
compounding, 147
consistent alignment point
deﬁnition of, 32
consistent phrase pair
deﬁnition of, 32
context-free grammar, 6–9
binarization, 108–109
deﬁnition of, 6
derivation, 7
language of, 9
rank, 9
cube growing, 89–92, 95
cube pruning, 87–90, 92, 95, 97, 100
CYK
complexity, 59
max-derivation algorithm, 57–61
recognition algorithm, 57–60
CYK+
parsing algorithm, 114–120
recursive variant of, 118–120
data-oriented translation, 28
decoding, 47–123
for GHKM, 101–114
for hierarchical phrase-based models,
101–106, 113–114
186 INDEX
for syntax-augmented machine
translation, 101–108, 113–114
forest decoding, 131–132
k-best, 72, 74–76, 80, 82, 84–85, 92
string decoding, 47, 101–123
tree decoding, 47, 69–100
with n-gram language models, 76–97
with non-local features, 76–97
decomposability
of derivation weight function, 54
dense features, 23–25, 36
dependency language models, 137, 139, 141,
145, 148–149, 151
dependency structure, 100, 133–139
non-projective, 133–134
dependency treelet translation, 135–138
derivation (grammatical)
canonical, 8–9, 12
examples, 7–8, 11
in context-free grammar, 7
in synchronous tree-substitution
grammar, 15–16
relationship to hypergraph derivation, 53
weight function, 17–18
derivation (hypergraph), 52–55
deﬁnition, 52–53
relationship to grammatical derivation,
53
weight function, 54
weighted, 53–55
derivation tree, 8–9, 11–12, 15–16, 20
derived tree, 15–16, 20
directed hyperedge, 48
directed hypergraph, 48
discriminative models, 21–25
distinct category form
of synchronous context-free grammar,
12–13, 121–122
of synchronous tree-substitution
grammar, 16
eager k-best algorithm, 61–67
relationship to cube pruning, 87–89
equivalence (of two grammars), 9, 12, 16
in binarization, 127
strong equivalence, 9
weak equivalence, 9
evaluation metrics, 149–151
head-word-chain metric, 150–151
subtree metric, 150–151
feature functions (of log-linear models),
21–25
dense features, 23–25, 36
in GHKM models, 45
in hierarchical phrase-based models, 36
in syntax-augmented machine
translation, 39–40
local features, 71–73
sparse features, 24–25, 36
features (grammatical), 129
forest decoding, 131–132
forest-based rule extraction, 132
forest-to-string models, 29, 132
frontier nodes (in GHKM), 41–43
fuzzy syntax, 27, 130–131
generative models, 18–21
GHKM, 29, 40–46
and subcategorization, 145–146
and tree binarization, 127
composed rules, 44–45
decoding for, 101–114
feature functions for, 45
frontier nodes, 41–43
limitations of, 126
minimal rules, 43–44
unaligned words, 41, 43–44
INDEX 187
glue rules, 35–36, 74, 103
grammar
probabilistic, 16–17
storage, 117–118
weighted, 16–17
head
of directed hyperedge, 48
head-word-chain metric, 150–151
hierarchical phrase-based models, 29, 47, 101
decoding for, 101–106, 113–114
feature functions for, 36
rule extraction for, 33–36
hyperedge
backward, 51
bundled, 74–76
directed, 48
label, 49
ordered, 50
undirected, 48
weight function, 49
hypergraph
backward, 51
bundled, 74–76
directed, 48
ordered, 50
state-split, 77–81
undirected, 48
weighted, 49
inﬂection, 140–143
initial phrase pair, 33–39
inverse projection
of SCFG, 10–11
of STSG, 15
inversion transduction grammar, 27–28, 33,
100
k-best algorithm
eager, 61–67, 87–89
in decoding, 72, 74–76, 80, 82, 84–85,
92
lazy, 64–67, 89
label
in training data, 129–130
of hyperedge, 49
of vertex, 49
language
of a context-free grammar, 9
of a synchronous context-free grammar,
12
of a synchronous tree-substitution
grammar, 16
language models
decoding with n-gram language model,
76–97
dependency, 137, 139, 141, 145,
148–149, 151
syntactic, 5, 29–30, 128, 148–149
lazy k-best algorithm, 64–67
relationship to cube growing, 89
left binarization (of parse trees), 127–128
lexical normal form, 112
local features
decoding with, 71–73
log-linear models, 21–25
max-derivation objective, 22
max-translation objective, 22
mildly context-sensitive grammars, 133–134
minimal rules (in GHKM), 43–44
models
forest-to-string, 29, 132
hierarchical phrase-based, 29, 47, 101
phrase-based, 1, 19–20, 29, 31, 40, 46,
126, 132, 139, 149, 150, 153–155
string-to-dependency, 138–139
188 INDEX
string-to-string, 25–26
string-to-tree, 26, 29, 47
tree-to-string, 26, 29, 47
tree-to-tree, 26–28, 47
word-based, 19–20, 29, 132
monotonicity
in decoding, 83–85
of derivation weight function, 54–55
morphology, 155
agglutination, 147
agreement, 129, 130, 134, 137–138,
140–143
compounding, 147
inﬂection, 140–143
n-gram language models
decoding with, 76–97
in discriminative translation models, 22
in generative translation models, 19
limitations of, 140–141
negative alignment point, 32
neural networks, 149, 156
noisy channel model, 19, 28
non-local features
decoding with, 76–97
non-projective dependencies, 133–134
ordered hyperedge, 50
ordered hypergraph, 50
parse errors, 125
parse forest, 49–51
complexity, 103–106, 114
parsing, 7
phrase, 32–46
deﬁnition of, 32
phrase pair, 1–5, 32–46
consistent, 32
deﬁnition of, 32
initial, 33–39
phrase-based models, 1, 19–20, 29, 31, 40,
46, 126, 132, 139, 149, 150,
153–155
positive alignment point, 32, 34
preﬁx tree
for grammar storage, 117–118
probabilistic context-free grammar, 16–17
probabilistic grammar, 16–17
as translation model, 19
production rule, 6–17
projection
of SCFG, 10–11
of STSG, 15
rank
of a context-free grammar, 9
of a synchronous context-free grammar,
12
of a synchronous tree-substitution
grammar, 16
recursive CYK+, 118–120
rest cost estimation, 82–83
right binarization (of parse trees), 127–128
rule extraction
for hierarchical phrase-based models,
33–36
for syntax-augmented machine
translation, 37–39
forest-based, 132
GHKM, 40–45
rule matching (in tree decoding), 71–72
rule weight function, 16–18
scope, 112–113
scope pruning, 112–113
semantics
modeling in SMT, 146
relationship to syntax, 144, 156
INDEX 189
sentence (formal language), 7
sentential form (deﬁnition), 8
shared category form
of synchronous context-free grammar,
12–13
of synchronous tree-substitution
grammar, 16
sink vertex, 49
soft syntax, 27, 130–131
source projection
of SCFG, 10–11
of STSG, 15
sparse features, 24–25, 36
speech recognition, 19
state reﬁnement, 92–97
state splitting, 76–81, 100
state-split hypergraph, 77–81
string decoding, 47, 101–123
string-to-dependency models, 138–139
string-to-string models, 25–26
string-to-tree models, 26, 29, 47
strong equivalence (of two grammars), 9
subcategorization, 143–146
and binarization, 128
subtree metric, 150–151
synchronous binarization, 108–113
synchronous context-free grammar, 6, 10–13
deﬁnition of, 10
distinct category form, 12–13, 121–122
language of, 12
rank, 12
shared category form, 12–13
source projection, 10–11
target projection, 10–11
synchronous tree-substitution grammar, 6,
13–16
deﬁnition of, 13
derivation, 15–16
distinct category form, 16
in string decoding, 120–121
in tree decoding, 69–100
language of, 16
rank, 16
shared category form, 16
source projection, 15
target projection, 15
syntactic language models, 5, 29–30, 128,
148–149
syntax-augmented machine translation, 29,
36–40, 101
decoding for, 101–108, 113–114
feature functions for, 39–40
rule extraction for, 37–39
unaligned words, 38–40
syntax-directed machine translation, 29, 100
target projection
of SCFG, 10–11
of STSG, 15
topological sort algorithm, 55
transformation-based learning, 128–129
translation forest, 51–52
tree binarization, 127–128, 136
diﬀerence from grammar binarization,
127
for forest decoding, 132
tree decoding, 47, 69–100
complexity, 80
rule matching, 71–72
tree parsing, 26, 97–99
tree relabeling, 129–130
tree restructuring, 126–129
tree transducers, 6
tree-to-string models, 26, 29, 47
decoding for, 47, 69–100
forest decoding, 132
tree-to-tree models, 26–28, 47
decoding for, 98–100
190 INDEX
forest decoding, 132
problems with, 26–28
trie
for grammar storage, 117–118
unaligned words
in GHKM, 41, 43–44
in hierarchical phrase-based models, 34
in syntax-augmented machine
translation, 38–40
undirected hyperedge, 48
undirected hypergraph, 48
vertex
label, 49
sink, 49
Viterbi
algorithm family, 67
max-derivation algorithm, 55–57
weak equivalence (of two grammars), 9
weight function
for derivations (grammatical), 17–18
for derivations (hypergraph), 54
for hyperedges, 49
for rules, 16–18
weighted derivation (hypergraph), 53–55
weighted grammar, 16–17
weighted hypergraph, 49
word-based models, 19–20, 29, 132
