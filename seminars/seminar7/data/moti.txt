On Motifs and Verb Valency | 21
For the Hungarian data we obtain the results presented in Tab. 6 and Fig. 6.
In this data we have a simple decreasing phenomenon but the oscillation is
evident. Hence we try to capture it applying a simple power function
� = ���
(�)
and obtain the results presented in Tab. 6 (See Appendix).
Fig. 6: Graph of the relationship between the frequency and the average length of FSV-motifs
from Tab. 6
In accordance with the hypothesis, there is a tendency to emphasize the central
FVS-motif length and shorten less frequent and more frequent FSV-motifs,
22 | Radek Čech – Veronika Vincze – Gabriel Altmann
hence, the curve should rather bell-shaped. But the Hungarian data which are
very extensive show that the dependence need not be bell-shaped. Perhaps the
parameters of the functions are associated with the kind of linguistic entity.
However, this is, so to say, not the last word. Analyses of many texts will,
perhaps, show that the link is very complex.
Here we considered frequency as the independent variable but the other
way round is possible, too.
6 Conclusion
The study reveals that verb valency motifs can be considered to be the linguistic
unit which share the same characteristics as the majority of well-established
traditional units. Specifically, both the rank frequency and spectrum abide by
the Zipf-Mandelbrot distribution; further, there is the relationship between the
frequency and the length of the motif.
Valency is a property of the verb stated on the basis of its associated
environment. Verbs themselves express a number of various activities which
need not be restricted to Man. Capturing them, e.g. according to Ballmer (1982),
cf. also Köhler, Altmann (2014: 66 f.) in form of B-motifs, one can study the
relation of the text to the reality. This aspect has nothing to do with grammar
but rather with the semantics of verbs. A further possibility is to give names to
the members of the valency, e.g. N, V, Adv, Prep, Pron, …. or to the phrases in
which they occur, and then study the form of the valencies, i.e. consider the
frequencies and similarities of valency frames.
References
Allerton, D. J. (2005). Valency Grammar. In K. Brown (Ed.), The Encyclopedia of Language and
Linguistics (pp. 4878-4886). Elsevier Science Ltd.
Ballmer, Th.T. (1982). Biological foundations of linguistic communication.
Amsterdam/Philadelphia: Benjamins.
Beliankou, A., Köhler, R., & Naumann, S. (2013). Quantitative properties of argu mentation
motifs. In I. Obradović, E. Kelih, & R. Köhler (Eds.), Methods and Applications of
Quantitative Linguistics (pp. 33-43). Selected papers of the VIIIth
Inter national Conference
on Quantitative Linguistics (QUALICO) in Belgrade, Serbia.
Boroda, M.G. (1973). K voprosu o metroritmičeski elementarnoj edinice v muzyke. Bulletin of
the Academy of Sciences of the Georgian SSR, 71(3), 745–748.
On Motifs and Verb Valency | 23
Boroda, M.G. (1982). Die melodische Elementareinheit. In J.K. Orlov, M.G. Boroda, & I.Š.
Nadarejšvili (Eds.), Sprache, Text, Kunst. Quantitative Analysen (205–222). Bochum:
Brockmeyer.
Boroda, M.G. (1988). Towards a problem of basic structural units of musical texts.
Musikometrika 1 (pp. 11-69), Bochum: Brockmeyer.
Čech, R., Pajas, P., & Mačutek, J. (2010). Full valency. Verb valency without distinguishing
complements and adjuncts. Journal of Quantitative Linguistics, 17, 291–302.
Köhler, R. (2005). Synergetic linguistics. In R. Köhler, G. Altmann, & R.G. Piotrowski (Eds.),
Quantitative Linguistics: An International Handbook (pp. 760–774). Berlin/New York: de
Gruyter.
Köhler, R. (2006). The frequency distribution of the length of length sequences. In J. Genzor, M.
Bucková (Eds.), Favete linguis. Studies in honour of Viktor Krupa (pp. 145-152). Bratislava:
Slovak Academy Press.
Köhler, R. (2008a). Word length in text. A study in the syntagmatic dimension. In S.
Mislovičová (Ed.), Jazyk a jazykoveda v pohybe (pp. 421–426). Bratislava: Veda.
Köhler, R. (2008b). Sequences of linguistic quantities. Report on a new unit of in vestigation.
Glottotheory 1(1), 115–119.
Köhler, R. (2015). Linguistic motifs. In G.K. Mikros, & J. Mačutek (Eds.), Sequences in Language
and Text (pp. 89-108). Berlin/Boston: de Gryuter.
Köhler, R., & Altmann, G. (2014). Problems in Quantitative Linguistics Vol. 4. Lüdenscheid:
RAM-Verlag
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In C.
Preisach, H. Burkhardt, L. Schmidt-Thieme, R. Becker, R. (Eds.), Data Analysis. Machine
Learning and Applications (pp. 635-646). Berlin/ Heidelberg: Springer.
Köhler, R., & Naumann, S. (2009). A contribution to quantitative studies on the sentence level.
In R. Köhler (Ed.), Issues in Quantitative Linguistics (pp. 34-57). Lüdenscheid: RAM-Verlag.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text clas sification.
Statistical properties of F- and L-motifs as text characteristics. In P. Grzybek, E. Kelih, J.
Mačutek (Eds.), Text and Language. Structures, functions, interrelations, quantitative
perspectives (pp. 81-89). Wien: Praesens.
Mačutek, J. (2009). Motif richness. In R. Köhler (Ed.), Issues in Quantitative Linguistics (pp. 51-
60). Lüdenscheid: RAM-Verlag.
Mačutek, J., & Mikros, G. (2015). Menzerath-Altmann law for word length motifs. In G.K. Mikros,
& J. Mačutek (Eds.), Sequences in Language and Text (pp. 125-131). Berlin/Boston: de
Gryuter.
Meľčuk, I. (1988). Dependency Syntax: Theory and Practice. Albany: State Univers ity of New
York Press.
Milička, J. (2015). Is the distribution of L-motifs inherited from the word length distribution In
G.K. Mikros, & J. Mačutek (Eds.), Sequences in Language and Text (pp. 133-145).
Berlin/Boston: de Gryuter.
Popescu, I.-I., Altmann, G., Grzybek, Jayaram, B.D., Köhler, R., Krupa, V., P., Mačutek, J., Pustet,
R., Uhlířová, L., & Vidya, M.N., (2009). Word frequency studies. Berlin/ New York: Mouton
de Gruyter.
Popescu, I. I., Čech, R., & Altmann, G. (2011). Vocabulary richness in Slovak poetry.
Glottometrics, 22, 62–72.
Popescu, I.I., Lupea, M., Tatar, D., & Altmann, G. (2015). Quantitative Analysis of Poetic Texts.
Berlin: de Gruyter.
24 | Radek Čech – Veronika Vincze – Gabriel Altmann
Sanada, H. (2010). Distribution of motifs in Japanese texts. In P. Grzybek, E. Kelih, & J. Maču-
tek, (Eds.), Text and Language. Structures, functions, interrelations, quantitative
perspectives (pp. 183-194) Wien: Praesens.
Vincze, V. (2014). Valency frames in a Hungarian corpus. Journal of Quantitative Linguistics
21(2), 153–176.
Wimmer, G., Altmann, G., Hřebíček, L., Ondrejovič, S., & Wimmerová, S. (2003) Úvod do
analýzy textov [Introduction to text analysis]. Bratislava: VEDA.
Wimmer, G., & Altmann, G. (2005). Unified theory of some linguistics laws. In Köhler, R.,
Altmann, G., Piotrowski, R., (Eds.) Quantitative Linguistics. An International Handbook (pp.
791-807). Berlin, New York: Walter de Gruyter.
Appendix
Tab. 1: Rank-frequency distribution of the FSV motifs in the short story Šlépěj (Footprint)
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
1 1-3 11 11.68 25 2-3-3 1 1.30
2 2 10 8.73 26 3-3-3 1 1.26
3 1-3-3 7 6.97 27 1-2-4 1 1.21
4 2-3 7 5.81 28 2-2-4 1 1.17
5 1-4 5 4.98 29 3-3-4 1 1.14
6 3 4 4.35 30 1-4-4 1 1.10
7 2-2-2 4 3.87 31 2-4-4 1 1.07
8 2-2 4 3.49 32 2-3-5 1 1.04
9 1-1-1-2-2-2-2 3 3.17 33 2-2-6 1 1.01
10 1-1-2-2-3 3 2.91 34 4-4 1 0.98
11 1-2-2-2-2 3 2.69 35 0-1 1 0.95
12 1-2-2 2 2.50 36 0-2-2 1 0.93
13 1-2 2 2.33 37 0-2-2-2-3 1 0.91
14 0-2 2 2.19 38 0-3 1 0.88
15 0-2-2-2-2 2 2.06 39 0-3-3-3 1 0.86
16 1-2-2-2-4 2 1.95 40 0-3-4 1 0.84
17 2-2-2-2 2 1.84 41 1-1-2-2-5 1 0.82
18 2-2-2-4 2 1.75 42 1-2-2-2 1 0.80
19 1 1 1.67 43 1-2-2-2-3-3-3 1 0.79
On Motifs and Verb Valency | 25
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
20 4 1 1.59 44 1-2-2-4 1 0.77
21 5 1 1.53 45 1-2-3-3 1 0.75
22 1-1-2 1 1.46 46 1-3-3-3-4 1 0.74
23 1-1-3 1 1.40 47 2-2-2-2-2-2 1 0.72
24 1-2-3 1 1.35 48 2-2-3-3-4 1 0.71
a = 0.9859, b = 1.9063, n = 48, DF = 36, X2
= 2.7839, P = 1.00
Tab.2: Rank-frequency distribution of the FSV motifs in the Hungarian translation of Orwel’s
1984
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
1 2 422 514.92 244 1-1-2-2-2-4 2 2.22
2 3 384 391.78 245 0-0-3-3 2 2.21
3 2-3 278 312.09 246 1-1-1-1-2-3 2 2.20
4 1-3 273 258.87 247 1-1-1-2-2-2 2 2.19
5 1-4 226 216.65 248 0-1-2-5 2 2.18
6 2-4 203 186.24 249 2-2-2-2-5 2 2.16
7 1-2 153 162.54 250 2-2-3-6 2 2.15
8 2-2 129 143.63 251 0-3-3-3-4 1 2.14
9 0-3 107 128.24 252 2-4-4-6 1 2.13
10 2-5 103 115.53 253 0-0-3-4 1 2.12
11 4 103 104.81 254 2-3-3-3-3-3-3-4 1 2.11
12 3-3 90 95.73 255 1-4-7 1 2.10
13 3-4 88 87.93 256 4-4-6 1 2.08
14 0-4 82 81.18 257 2-2-3-3-3-4 1 2.07
15 2-3-3 75 75.27 258 1-2-2-5-6 1 2.06
16 1-5 74 70.08 259 0-1-1-2-6 1 2.05
17 1-3-3 73 65.47 260 2-3-8 1 2.04
26 | Radek Čech – Veronika Vincze – Gabriel Altmann
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
18 1-2-3 71 61.37 261 4-5-6 1 2.03
19 2-2-3 69 57.70 262 0-1-2-3-3-4 1 2.02
20 1-2-4 68 54.39 263 3-4-7 1 2.01
21 2-2-4 66 51.39 264 0-3-5-9 1 2.00
22 1 64 48.67 265 3-3-3-3-4-4 1 1.99
23 0-2 59 46.20 266 2-2-3-3-3-3-3-3-
3-3-4
1 1.98
24 1-3-4 57 43.93 267 0-0-1-2-3 1 1.98
25 2-3-4 54 41.85 268 2-4-4-4-4 1 1.96
26 1-1-3 48 39.93 269 0-1-4-4-4 1 1.95
27 1-2-2 47 38.17 270 1-1-1-1-1-4 1 1.94
28 2-2-2 40 36.53 271 2-2-2-4-5 1 1.93
29 3-5 39 35.01 272 1-1-1-1-3-3-4 1 1.92
30 0-2-3 36 33.60 273 0-1-3-3-6 1 1.91
31 2-2-5 35 32.28 274 2-3-3-3-5-5 1 1.90
32 0-5 35 31.05 275 1-1-1-3-7 1 1.89
33 1-1-4 35 29.90 276 5-5 1 1.88
34 1-1-2 34 28.83 277 0-1-2-2-2-3-3-4-
4
1 1.88
35 1-6 33 27.81 278 1-1-2-3-4 1 1.87
36 0-3-3 31 26.86 279 0-3-4-4-4 1 1.86
37 2-2-2-3 30 25.97 280 1-3-3-3-5-5-6 1 1.85
38 2-4-4 28 25.12 281 2-2-2-2-4-4 1 1.84
39 2-2-3-3 28 24.32 282 1-4-6 1 1.83
40 2-3-5 27 23.56 283 0-0-2-4 1 1.82
41 1-2-2-3 26 22.85 284 1-2-7 1 1.81
42 1-4-4 25 22.17 285 0-1-3-3-3 1 1.81
On Motifs and Verb Valency | 27
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
43 0-1-3 25 21.52 286 3-3-4-4-4 1 1.80
44 1-2-5 25 20.91 287 0-3-3-6 1 1.79
45 4-4 24 20.33 288 0-4-6 1 1.78
46 3-3-4 23 19.77 289 2-3-4-5 1 1.77
47 1-1-5 23 19.24 290 0-2-2-3-3-4 1 1.76
48 1-2-3-3 23 18.74 291 1-3-6-6 1 1.76
49 0-2-4 22 18.25 292 2-2-2-2-2-3-4 1 1.75
50 2-2-2-4 22 17.79 293 1-1-2-5 1 1.74
51 1-2-2-4 21 17.35 294 2-2-2-3-3-5 1 1.73
52 2-6 21 16.93 295 2-2-2-3-3-4 1 1.72
53 3-3-3 20 16.52 296 0-1-2-4-6 1 1.72
54 1-3-3-3 19 16.13 297 1-2-2-2-2-3-4 1 1.71
55 0-3-4 19 15.76 298 1-2-2-2-2-2-3-3 1 1.70
56 2-3-3-3 18 15.40 299 0-0-2-3-6-6 1 1.69
57 0-2-2 18 15.10 300 1-2-5-5 1 1.69
58 1-3-3-4 17 14.72 301 4-4-4-6 1 1.68
59 2-2-3-4 17 14.41 302 1-3-5-5 1 1.67
60 1-1-3-3 15 14.10 303 1-1-3-3-3-5 1 1.66
61 3-4-4 15 13.80 304 2-3-3-3-5 1 1.66
62 1-1-2-3 15 13.52 305 1-1-2-2-6 1 1.65
63 0-3-3-3 15 13.24 306 0-2-3-3-3-3 1 1.64
64 1-1-2-2 15 12.97 307 1-1-1-2-4-5 1 1.63
65 2-2-2-2-3 15 12.72 308 1-1-3-3-3-3 1 1.63
66 1-1-2-4 15 12.47 309 2-2-2-2-3-3-3-5 1 1.62
67 2-3-3-4 14 12.23 310 1-1-3-3-3-4-4 1 1.61
68 1-4-5 13 12.00 311 2-3-3-3-3-4 1 1.61
28 | Radek Čech – Veronika Vincze – Gabriel Altmann
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
69 1-2-2-2 13 11.78 312 4-8 1 1.60
70 0-2-5 12 11.56 313 0-8 1 1.60
71 0-6 12 11.35 314 4-4-4 1 1.59
72 3-3-5 12 11.15 315 0-2-2-2-5 1 1.58
73 1-3-5 12 10.95 316 1-2-2-2-2-3-3-3 1 1.57
74 0-2-2-3 12 10.76 317 0-0-1-4 1 1.57
75 3-6 11 10.57 318 1-3-3-6 1 1.56
76 2-3-3-5 11 10.39 319 0-1-3-5 1 1.55
77 1-1-3-4 11 10.22 320 2-2-2-2-2-2-2 1 1.55
78 2-2-2-2 11 10.05 321 1-1-1-2-3-3-3 1 1.54
79 2-2-4-5 11 9.88 322 2-2-2-2-2-5 1 1.53
80 1-7 11 9.72 323 0-4-4-6 1 1.53
81 0-1-4 11 9.56 324 1-1-1-2-2-3-3 1 1.52
82 5 10 9.41 325 2-2-3-3-4-5 1 1.51
83 1-2-3-4 10 9.27 326 0-2-2-4 1 1.51
84 2-3-4-4 10 9.12 327 1-2-2-2-3-3-4-4 1 1.50
85 1-1-1-4 10 8.98 328 1-1-3-3-3-3-3 1 1.50
86 1-2-3-5 9 8.85 329 1-3-4-8 1 1.49
87 2-2-2-3-3 9 8.71 330 2-2-5-6 1 1.48
88 2-3-6 9 8.58 331 1-8 1 1.48
89 0-3-3-5 9 8.46 332 0-2-2-6 1 1.47
90 1-2-3-3-4 9 8.34 333 0-5-6 1 1.47
91 1-2-2-2-3 9 8.22 334 0-1-1-2-2-5 1 1.46
92 0-1-2-2 9 8.10 335 0-1-1-1-3 1 1.45
93 0-1-2 8 7.99 336 0-1-1-1-2 1 1.45
94 3-3-3-4 8 7.88 337 0-4-5-6 1 1.44
95 2-2-6 8 7.77 338 1-1-1-2-2-5 1 1.44
96 0-2-2-2 8 7.66 339 0-0-2-3-3-3-6 1 1.43
On Motifs and Verb Valency | 29
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
97 2-2-3-5 8 7.56 340 0-1-1-1-3-3 1 1.42
98 1-1 8 7.46 341 1-2-2-3-3-3-4 1 1.42
99 2-2-2-2-4 8 7.36 342 0-3-6 1 1.41
100 2-7 8 7.26 343 0-1-1-3-3-3-3-4 1 1.41
101 2-4-5 7 7.17 344 2-2-2-8 1 1.40
102 1-2-2-3-3 7 7.08 345 0-2-2-2-2-4-4-4 1 1.40
103 2-2-2-3-4 7 6.99 346 2-2-3-5-5 1 1.39
104 3-3-6 7 6.90 347 2-2-8 1 1.39
105 1-2-6 7 6.81 348 0-1-2-2-2-4 1 1.38
106 1-1-1-3 7 6.73 349 0-2-3-6 1 1.38
107 2-2-4-4 7 6.64 350 1-3-3-3-3-3 1 1.37
108 1-2-2-2-4 7 6.56 351 0-1-1-4-5 1 1.37
109 1-3-3-3-3 7 6.48 352 1-2-2-3-3-5 1 1.36
110 1-3-3-5 7 6.41 353 1-2-2-2-5-5 1 1.35
111 1-2-4-4 7 6.33 354 1-3-4-6-6 1 1.35
112 2-2-3-3-4 7 6.26 355 1-1-2-2-2-2-3-3-
3
1 1.34
113 3-4-5 6 6.18 356 1-2-2-2-2-2-2-4 1 1.34
114 2-3-3-3-4 6 6.11 357 2-3-3-4-4-5 1 1.33
115 0-4-4 6 6.04 358 2-2-2-2-3-4-4 1 1.33
116 0-3-5 6 5.97 359 0-1-2-4-5 1 1.32
117 2-4-6 6 5.91 360 1-1-2-2-2-2-2 1 1.32
118 1-3-4-4 6 5.84 361 1-1-1-1-2-4 1 1.31
119 1-3-6 6 5.78 362 2-2-3-3-3-4-4 1 1.31
120 0-2-6 6 5.71 363 0-2-4-8 1 1.30
121 2-2-3-3-3 6 5.65 364 1-2-2-3-3-4-4-4 1 1.30
122 2-3-3-3-3 6 5.59 365 0-1-1-2-5 1 1.30
123 1-2-2-2-2-3 6 5.53 366 0-0-6 1 1.29
124 1-2-2-5 6 5.47 367 0-1-1-3-4 1 1.29
125 4-5 5 5.41 368 1-2-2-3-3-3 1 1.28
126 3-3-3-3 5 5.36 369 0-0-3-3-3-3 1 1.28
30 | Radek Čech – Veronika Vincze – Gabriel Altmann
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
127 2-8 5 5.30 370 1-1-3-4-4 1 1.27
128 0-3-3-4 5 5.25 371 1-2-3-3-4-5 1 1.27
129 0-1-6 5 5.19 372 1-1-1-2-2 1 1.26
130 2-5-5 5 5.14 373 0-2-2-2-4-4 1 1.26
131 0-4-5 5 5.09 374 0-3-5-6 1 1.25
132 1-3-3-3-4 5 5.04 375 1-2-2-2-2-2-3-3-
3-3-3
1 1.25
133 1-1-2-2-4 5 4.99 376 2-2-2-3-6 1 1.24
134 1-5-5 5 4.94 377 2-2-2-3-4-4 1 1.24
135 1-2-2-3-4 5 4.89 378 1-2-2-2-3-4-4 1 1.24
136 1-1-2-2-3 5 4.84 379 2-2-2-2-6 1 1.23
137 2-2-2-2-2 5 4.80 380 1-2-2-2-4-5 1 1.23
138 1-1-1-1-3 5 4.75 381 0-1-1-1-3-5 1 1.22
139 1-2-3-3-3 5 4.70 382 0-0-3-5 1 1.22
140 3-3-4-4 5 4.66 383 1-1-3-3-4 1 1.21
141 1-1-1-2-3 5 4.62 384 1-1-1-4-4 1 1.21
142 2-3-3-4-4 5 4.57 385 1-2-2-2-2-2-2 1 1.21
143 2-2-7 4 4.53 386 1-1-1-1-2-2-2-2-
2-3
1 1.20
144 2-4-4-4 4 4.49 387 1-1-2-2-2-3-4 1 1.20
145 0-1-2-4 4 4.45 388 2-2-2-2-2-2-2-3 1 1.19
146 2-3-3-6 4 4.41 389 1-1-1-1-2-2-3 1 1.19
147 1-1-4-5 4 4.37 390 0-1-3-4-5 1 1.18
148 2-2-2-3-5 4 4.33 391 3-5-6 1 1.18
149 1-4-4-4 4 4.29 392 2-2-3-4-4-4 1 1.18
150 0-1-3-3 4 4.25 393 1-2-3-3-3-4 1 1.17
151 0-1-5 4 4.22 394 4-4-5 1 1.17
152 0-2-3-3 4 4.18 395 3-3-3-3-4-6 1 1.16
153 1-1-3-5 4 4.14 396 1-3-3-4-5 1 1.16
154 1-1-1-3-4 4 4.11 397 0-2-2-2-7 1 1.16
155 2-2-2-2-3-3 4 4.07 398 1-1-3-6 1 1.15
156 2-2-2-4-4 4 4.04 399 1-3-3-3-3-3-4 1 1.15
On Motifs and Verb Valency | 31
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
157 0-0-4 4 4.00 400 3-3-3-5-6-6 1 1.15
158 2-4-4-5 4 3.97 401 0-3-3-4-4 1 1.14
159 0-1-1-2 4 3.94 402 4-4-4-5 1 1.14
160 1-1-2-2-2 4 3.90 403 1-3-3-7 1 1.13
161 0-1 4 3.87 404 0-1-2-2-5 1 1.13
162 1-3-7 4 3.84 405 2-2-2-5-6 1 1.13
163 1-5-6 4 3.81 406 2-5-6 1 1.12
164 1-1-2-3-3 4 3.78 407 1-1-1-2-4-4 1 1.12
165 2-2-2-5 4 3.75 408 1-4-5-6 1 1.11
166 1-1-1-2 4 3.72 409 -3-10 1 1.11
167 0-7 3 3.69 410 1-3-4-6 1 1.11
168 3-3-3-3-3 3 3.66 411 1-5-5-6 1 1.10
169 1-1-4-4 3 3.63 412 0-2-3-4-4 1 1.10
170 1-1-1-2-5 3 3.60 413 1-1-3-3-4-5 1 1.10
171 1-2-2-4-4 3 3.57 414 2-2-2-3-3-6 1 1.09
172 1-1-6 3 3.55 415 1-2-3-4-4-4-4 1 1.09
173 0-1-2-3 3 3.52 416 2-4-4-4-5 1 1.09
174 0-0-3 3 3.49 417 3-3-3-4-4 1 1.08
175 1-1-1-1-3-3 3 3.46 418 1-3-3-5-5 1 1.08
176 0-1-2-2-4 3 3.44 419 3-4-6 1 1.08
177 2-2-2-2-2-2 3 3.41 420 1-2-2-3-6 1 1.07
178 3-7 3 3.39 421 1-2-4-4-4-4 1 1.07
179 2-2-2-3-3-3 3 3.36 422 1-1-2-3-3-4-4 1 1.07
180 1-1-2-2-2-2-
3
3 3.34 423 1-1-1-1 1 1.06
181 0-1-1-3 3 3.31 424 0-4-4-5 1 1.06
182 0-2-2-2-3 3 3.29 425 2-2-2-2-3-5 1 1.06
183 1-2-2-2-2-4 3 3.26 426 1-2-2-2-2-2-2-3 1 1.05
184 1-2-2-6 3 3.24 427 2-2-3-3-3-3 1 1.05
185 1-3-4-5 3 3.22 428 0-1-1-1-4 1 1.05
186 1-3-3-4-4 3 3.19 429 0-2-3-3-3-3-3-3-
3-3
1 1.04
32 | Radek Čech – Veronika Vincze – Gabriel Altmann
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
187 1-1-2-3-5 3 3.17 430 0-1-1-1-2-3 1 1.04
188 1-1-1-5 3 3.15 431 0-1-2-3-3-3 1 1.04
189 1-1-2-2-2-2 3 3.13 432 0-2-2-2-3-3 1 1.03
190 2-6-6 2 3.11 433 0-0-5 1 1.03
191 2-4-5-5 2 3.08 434 2-2-2-2-3-4 1 1.03
192 0-3-5-5 2 3.06 435 1-2-3-3-3-3-3 1 1.02
193 1-2-3-4-4 2 3.04 436 2-2-3-4-7 1 1.02
194 3-3-3-5 2 3.02 437 2-2-2-2-3-3-4 1 1.02
195 0-1-1-3-3 2 3.00 438 1-1-2-2-3-5 1 1.01
196 2-2-2-2-2-3 2 2.98 439 1-1-1-1-4 1 1.01
197 4-4-4-4 2 2.96 440 0-2-3-3-3-3-3-4 1 1.01
198 4-6 2 2.94 441 1-1-3-5-5 1 1.00
199 1-2-2-2-3-4 2 2.92 442 1-1-1-2-2-2-3 1 1.00
200 1-2-4-6 2 2.90 443 1-2-4-4-4 1 1.00
201 1-4-4-5 2 2.88 444 1-1-3-3-3 1 0.99
202 2-3-4-4-4 2 2.86 445 2-2-2-2-2-2-3 1 0.99
203 0-2-3-5 2 2.84 446 0-1-1-2-3 1 0.99
204 0-2-5-5 2 2.82 447 1-1-1-1-1-2 1 0.99
205 0-0-2 2 2.81 448 0-1-7 1 0.98
206 2-2-2-2-2-3-
3
2 2.79 449 0-1-1-4 1 0.98
207 2-3-4-6 2 2.77 450 2-2-2-6 1 0.98
208 2-2-4-6 2 2.75 451 0-1-2-3-3 1 0.97
209 0-1-1-5 2 2.73 452 0-1-3-6 1 0.97
210 1-1-1-1-2-2-2 2 2.72 453 1-1-1-2-2-3-3-3-
4
1 0.97
211 0-1-3-4 2 2.70 454 1-1-1-1-1-1-5 1 0.97
212 2-2-3-3-5 2 2.68 455 1-1-1-2-4 1 0.96
213 0-2-3-3-5 2 2.67 456 1-1-1-3-3-4 1 0.96
214 0-0-2-2-3 2 2.65 457 1-2-2-3-4-4 1 0.96
215 0-2-2-2-6 2 2.63 458 1-1-2-2-2-3 1 0-95
216 1-2-2-2-5 2 2.62 459 1-1-2-2-2-2-2-2- 1 0.95
On Motifs and Verb Valency | 33
Rank Motif Fr. ZI-MA Rank Motif Fr. ZI-MA
2-2-2
217 3-4-4-5 2 2.60 460 0-2-2-2-3-3-4 1 0.95
218 0-2-3-4 2 2.58 461 0-1-2-2-2-2-3 1 0.95
219 1-2-3-3-5 2 2.57 462 2-2-2-7 1 0.94
220 0-2-2-3-4 2 2.55 463 2-2-2-2-3-3-3 1 0.94
221 1-2-2-2-2-3-
3
2 2.54 464 0-1-2-2-2 1 0.94
222 1-2-2-2-2 2 2.52 465 1-2-5-6 1 0.93
223 1-2-3-6 2 2.51 466 0-2-2-2-2 1 0.93
224 0-5-5 2 2.49 467 0-2-3-3-4 1 0.93
225 0-4-5-5 2 2.48 468 0-1-2-4-4-5 1 0.93
226 1-1-3-7 2 2.46 469 0-0-1-1-1-1 1 0.92
227 1-1-1-3-3 2 2.45 470 1-2-2-2-3-3-4 1 0.92
228 0-2-2-3-3 2 2.43 471 1-1-2-2-3-3 1 0.92
229 1-2-2-3-3-4 2 2.42 472 0-0-2-3 1 0.92
230 1-2-2-2-3-3 2 2.41 473 2-3-3-3-3-3-3 1 0.91
231 0-1-4-4 2 2.39 474 0-0-1-3-3 1 0.91
232 1-1-2-3-3-3 2 2.38 475 1-2-2-2-2-2 1 0.91
233 0-2-2-5 2 2.36 476 0-2-2-2-4 1 0.91
234 0-1-2-4-4 2 2.35 477 0-3-4-4 1 0.90
235 0-4-4-4 2 2.34 478 1-1-2-7 1 0.90
236 1-2-2-4-5 2 2.32 479 1-4-4-4-4 1 0.90
237 1-1-2-2-3-4 2 2.31 480 0-3-3-3-3 1 0.90
238 1-1-2-2-2-5 2 2.30 481 1-1-4-4-5 1 0.89
239 0-1-1-2-2 2 2.29 482 0-2-3-3-7 1 0.89
240 1-1-1 2 2.27 483 1-2-3-4-4-4-5 1 0.89
241 1-2-4-5 2 2.26 484 3-3-3-7 1 0.89
242 1-1-1-1-2 2 2.25 485 0-2-2-2-3-6 1 0.88
243 1-2-3-4-5 2 2.24 486 1-2-3-3-4-4 1 0.88
34 | Radek Čech – Veronika Vincze – Gabriel Altmann
Tab. 3: The frequency spectrum of the FSV-motifs in the short story Šlépěj (Footprint)
Frequency No. of FVS-motifs Theoretical (5)
1 30 29.90
2 7 7.87
3 3 3.41
4 3 1.84
5 1 1.13
7 2 0.53
10 1 0.24
11 1 0.19
a = 1.4071, R2
= 0.9917
Tab. 4: The frequency spectrum of the FSV-motifs in the Hungarian translation of Orwel’s 1984
Frequency No. of FVS-motifs Theoretical (5) Frequency No. of FVS- motifs Theoretical (5)
1 237 235.33 34 1 0.17
2 60 67.26 35 3 0.16
3 23 30.61 36 1 0.15
4 24 17.15 39 1 0.12
5 18 10.83 40 1 0.12
6 12 7.39 47 1 0.08
7 12 5.34 48 1 0.08
8 8 4.01 54 1 0.06
9 7 3.12 57 1 0.05
10 4 2.48 59 1 0.05
11 7 2.02 64 1 0.04
12 5 1.67 66 1 0.04
13 2 1.40 68 1 0.04
14 1 1.19 69 1 0.03
15 7 1.03 71 1 0.03
17 2 0.78 73 1 0.03
18 2 0.69 74 1 0.03
19 2 0.61 75 1 0.03
20 1 0.54 82 1 0.02
21 2 0.49 88 1 0.02
On Motifs and Verb Valency | 35
Frequency No. of FVS-motifs Theoretical (5) Frequency No. of FVS- motifs Theoretical (5)
22 2 0.44 90 1 0.02
23 3 0.40 103 2 0.01
24 1 0.36 107 1 0.01
25 3 0.33 129 1 0.01
26 1 0.30 153 1 0.01
27 1 0.28 203 1 0.003
28 2 0.26 226 1 0.002
30 1 0.22 273 1 0.002
31 1 0.21 278 1 0.001
33 1 0.18 384 1 0.0007
422 1 0.0006
a = 1.2657, c = 402.8904, R2
= 0.9922
Tab. 5: The frequency and average length of the FSV-motifs in the short story Šlépěj (Footprint)
by K. Čapek
Frequency Average
length
Lorentzian
1 3.36 2.29
2 3.57 3.71
3 5.66 5.63
4 2 2.47
5 2 2.12
7 2.5 1.98
10 1 1.95
11 2 1.94
a = 1.9279, b = 6.2626, c = 2.6563,
d = 0.4132, R2
= 0.82
36 | Radek Čech – Veronika Vincze – Gabriel Altmann
Tab. 6: The frequency and average length of the FSV-motifs in the Hungarian translation of
Orwel’s 1984
Fr L Power Fr. L Power Fr. L Power
1 5.43 5.70 22 3.50 3.11 64 1.00 2.52
2 4.70 4.97 23 3.33 3.08 66 3.00 2.51
3 4.61 4.59 24 2.00 3.06 68 3.00 2.49
4 4.00 4.34 25 3.00 3.03 69 3.00 2.49
5 4.06 4.16 26 4.00 3.01 71 3.00 2.47
6 3.92 4.01 27 3.00 2.99 73 3.00 2.46
7 4.12 3.89 28 3.50 2.97 74 2.00 2.45
8 3.38 3.79 30 4.00 2.93 75 3.00 2.45
9 4.29 3.71 31 3.00 2.91 82 2.00 2.40
10 3.25 3.63 33 2.00 2.87 88 2.00 2.37
11 3.29 3.56 34 3.00 2.86 90 2.00 2.36
12 3.00 3.50 35 2.67 2.84 103 1.50 2.30
13 3.50 3.45 36 3.00 2.82 107 2.00 2.28
14 4.00 3.40 39 2.00 2.78 129 2.00 2.20
15 4.00 3.35 40 3.00 2.77 153 2.00 2.13
17 4.00 3.27 47 3.00 2.68 203 2.00 2.01
18 3.50 3.24 48 3.00 2.67 226 2.00 1.97
19 3.50 3.20 54 3.00 2.61 273 2.00 1.90
20 3.00 3.17 57 3.00 2.58 278 2.00 1.89
21 3.00 3.14 59 2.00 2.56 384 1.00 1.78
422 1.00 1.74
a = 5.6962, b = -0.1957, R2
= 0.7082
Heng Chen, Junying Liang
Chinese Word Length Motif and Its
Evolution
Abstract: In this paper, we compare the word length motif in modern spoken
Chinese and written Chinese, and attempt to make clear how it evolves in
written Chinese. The synchronic studies show that the rank-frequency
distributions can be fitted with power law function y=axb
, and the length
distributions can be fitted with the Hyper-Pascal function. As for the diachronic
studies, evolutionary regularities are found. On the one hand, we found an
increasing trend of parameter a and a decreasing trend of parameter b in the
rank-frequency distribution function y=axb
. On the other hand, although the
motif length distributions of ancient Chinese cannot be fitted with the Hyper-
Pascal function, the entropy analyses show that there is a tendency that the
distributions are more concentrated on some certain motif patterns.
Keywords: word length; motif; evolution; spoken Chinese; written Chinese
1 Introduction
Virtually all prior studies of word length are devoted to the problem of the
frequency distribution of word length in texts, regardless of the syntagmatic
dimension (Köhler, 2006). In recent years, word length sequences gradually
come into vision, e.g., word length correlations (Kalimeri et al., 2012), word
length repetitions (Altmann et al., 2009), word length entropies (Kalimeri et al,
2015; Papadimitriou, 2010; Ebeling & Pöschel, 1994), and the latest word length
motifs (Köhler, 2006, 2015). Word length motif is a new syntagmatic approach to
automatic text classification (Köhler & Naumann, 2008, 2010).
Basically, word length sequences are inherited from word sequence. Word
sequence (or word order) has been used as an indicator of language typologies
(Jiang & Liu, 2015; Levinson et al., 2011; Liu, 2010). Köhler (2006) stated that
word attributes may reflect the properties of its basic language units, i.e., words.
||
Heng Chen: Center for Linguistics & Applied Linguistics, Guangdong University of Foreign
Studies, Guangzhou, China, chenheng1003@163.com
Junying Liang: Department of Linguistics, Zhejiang University, Hangzhou, China,
jyleung@zju.edu.cn
38 | Heng Chen – Junying Liang
Indeed, word length is one attribute that transmits a certain amount of word
information. For example, Piantadosi et al. (2011) point out that word length is
closely related to the information that words transmit; Garcia et al.（2012）state
that those longer words are more likely to be used to express abstract things.
These studies predicate that word length sequences may also predict the
regularities in languages.
Despite that motif is rather new in word length sequence studies, an
increasing number of studies show that it is a very promising attempt to
discover the syntagmatic relations of the word lengths in texts (Milička, 2015;
Köhler, 2006, 2008, 2015). In this study, we intend to explore if word length
motifs co-evolve with word length from a diachronic and dynamic point of view.
Our hypothesis is that word length motifs co-evolve with word length (in
written Chinese). In a recent study (Chen et al., 2015), we investigate how word
length evolves based on the analysis of written texts from ancient Chinese
within a time span of over 2000 years. In the present study, we will investigate
the correlations between the evolution of word length and word length motifs.
Besides, since motif frequency can be modelled by the rank-frequency
distribution model y=axb
, we will also investigate if it co-evolves with word
length.
The rest of this paper is organized as follows. Part 2 describes the materials
and methods used in this study; Part 3 gives the results of the synchronic and
diachronic investigations, as well as some discussions. Part 4 is a conclusion.
To anticipate, this study may give us a much more in-depth understanding of
word length motifs.
2 Materials and Methods
This study includes both synchronic and diachronic investigations. On the one
hand, in order to explore whether the word length motifs are sensitive to
different language styles, we compare the rank-frequency distributions and
length distributions of word length motifs between modern spoken and written
Chinese. On the other hand, we investigate the evolution of word length motifs
of written Chinese in the last 2000 years, with the purpose of finding out
whether Chinese word length motifs co-evolve with word length.
Chinese Word Length Motif and Its Evolution | 39
2.1 Materials
For the comparison between spoken and written Chinese, we built a dialogue
text collection (spoken language) and a prose text collection (written language)
with 20 texts respectively, and the number of words in each text ranges from 726
to 3792. The spoken language texts come from a TV talk show (three people)
named Qiang Qiang San Ren Xing (Behind the Headlines) on Phoenix TV, from
2013.06 to 2013.09, 5 texts each month and 20 texts in total, in the form of daily
conversation. This TV program mainly discusses the current social hot issues.
The written language texts come from a well-known Chinese prose journal
Selective Prose (http://swsk.qikan.com), from 2013.06 to 2013.09, 5 texts each
month and 20 texts in total.
Since there are no natural boundaries between words, word segmentation is
needed before measuring word length. It is generally held that word
segmentation involves the definition of the word, and it is a particularly difficult
problem in Chinese. However it is not the issue we will discuss here, in the
present investigation we segment words with a unified standard. First, we used
the ICTCLAS (http://ictclas.nlpir.org/), one of the best Chinese word
segmentation software, to segment words automatically. Then we checked and
corrected the errors manually. Tab. 1 and 2 show the number of characters and
words in each text.
Tab.1: Number of characters and words in modern spoken Chinese texts
Text Character tokens Word tokens Text Character tokens Word tokens
S1 2168 1589 S11 5441 3792
S2 1561 1068 S12 5419 3783
S3 2520 1763 S13 5216 3592
S4 2245 1526 S14 5021 3444
S5 1373 941 S15 4959 3498
S6 1002 726 S16 5251 3609
S7 2287 1567 S17 5093 3571
S8 1306 883 S18 5127 3437
S9 2047 1445 S19 4848 3329
S10 1822 1278 S20 4668 3197
40 | Heng Chen – Junying Liang
Tab.2: Number of characters and words in modern written Chinese texts
Text Character tokens Word tokens Text Character tokens Word tokens
W1 1920 1366 W11 1928 1368
W2 1309 952 W12 2655 1861
W3 2055 1490 W13 1423 948
W4 2394 1657 W14 2318 1779
W5 2014 1502 W15 1471 962
W6 1550 1119 W16 4128 2876
W7 1786 1269 W17 5143 3654
W8 1466 993 W18 5012 3512
W9 1830 1366 W19 4423 3057
W10 2693 1928 W20 4403 2953
As for the diachronic investigation, we use a collection of reliable written
Chinese texts ranging from around 300 BC to 2100 AD, which is divided into 6
time periods: BC 3th–BC 2th, AD 4th–AD 5th, AD 12th–AD 13th, AD 16th–AD
17th, Pre-AD 20th, AD 21th. The original scale of the whole texts collection in
each time period ranges from about ten thousand to two million characters. The
details of the texts are shown in Tab. 21 in Appendix. To have a reliable
measurement of word length, we select a sample of ten-thousand-character text
from the text collections in each time period randomly. Moreover, to guarantee
the impartiality of the results, the segmentation work was done by an expert in
old Chinese.
2.2 Methods
As defined by Köhler (2015), a length-motif is a continuous series of equal or
increasing length values (e.g. of morphs, words or sentences).
An example of Chinese word length motif segmentation is listed as follows.
The Chinese sentence
Chinese: 汉语 词长 动链 是 如何 演化 的？
Pinyin: Hànyǔ Cícháng Dònɡliàn Shì Rúhé Yǎnhuà De?
English (word-for-word translation): (Chinese) (word length) (motif) (is) (how) (evolve)
(particle ‘de’)?
English: How does Chinese word length motif evolve?
Chinese Word Length Motif and Its Evolution | 41
is according to the above-given definition, represented by a sequence of 3 word
length motifs: (2-2-2) (1-2-2) (1). It should be noticed that the Chinese word
length is measured in syllables in this study.
3 Results and Discussions
In the following sections, the results and discussions of our synchronic and
diachronic investigations are given respectively.
3.1 Word Length Motifs in Spoken and Written Chinese
In this section we investigate both rank-frequency and length distributions of
word length motifs in spoken and written Chinese.
3.1.1 The rank-frequency distributions of word length motifs
It should be noticed that since the longest word in Chinese does not exceed 9，
we did not put the “-” in word length motifs. The rank-frequency distributions
of word length motifs in Text S1 are shown in Tab. 22 in Appendix.
As we can see in Tab. 22, motifs “12” and “112” are the only two most
frequent ones whose percentage exceeds 10%, and this is corroborated by the
other 19 texts. The motifs data about all the 20 written texts are shown in Tab.
3.What is more, we found that “12”, “112”,”122” and “1112” are the most frequent
motifs in the 20 spoken texts.
Tab. 3: The Motifs data about the 20 spoken texts
Text Word length
motif types
Word length
motif tokens
Text Word length
motif types
Word length
motif tokens
S1 96 893 S11 79 834
S2 70 939 S12 91 787
S3 90 924 S13 61 418
S4 67 343 S14 57 406
S5 52 275 S15 47 242
S6 85 869 S16 41 170
42 | Heng Chen – Junying Liang
Text Word length
motif types
Word length
motif tokens
Text Word length
motif types
Word length
motif tokens
S7 72 889 S17 51 407
S8 84 860 S18 54 219
S9 89 879 S19 67 346
S10 87 903 S20 54 293
Next we fit the power law function y=axb
to the rank-frequency distribution data
of word length motifs in the 20 spoken texts. Fig. 1 is the fitting result of Text S1.
Fig. 1: Fitting the power law function y=axb
to the rank-frequency distribution data in Text S1
The goodness of fit R2
in Fig. 1 is 0.9859, which means that the relation can be
captured by power law. The whole 20 fitting results in spoken texts can be seen
in Tab. 4.
Chinese Word Length Motif and Its Evolution | 43
Tab. 4: The rank-frequency fitting results of word length motifs in 20 spoken texts
Text a b R2
S1 189.7287 -0.99279 0.98585
S2 221.63959 -0.99334 0.97604
S3 211.93943 -1.00448 0.97473
S4 66.3312 -0.91236 0.97498
S5 54.64877 -0.89137 0.96225
S6 185.60126 -0.97553 0.98363
S7 215.36072 -1.00972 0.97722
S8 181.78242 -0.96029 0.9713
S9 201.7334 -1.02898 0.98719
S10 207.03127 -1.01568 0.9882
S11 199.41935 -1.03769 0.98813
S12 174.24107 -1.01198 0.98529
S13 85.49793 -0.9069 0.9503
S14 84.08575 -0.91851 0.96472
S15 47.13713 -0.85896 0.93009
S16 40.99748 -0.99462 0.9887
S17 86.94427 -0.91944 0.95011
S18 48.2563 -0.99044 0.98131
S19 76.23932 -0.96282 0.95829
S20 59.18672 -0.9018 0.96449
It can be seen from Tab. 4 that all the fittings are successful.
In the following, we turn our attention to the 20 written texts. The rank-
frequency distribution data in Text W1 can be seen in Tab. 23 in Appendix.
As we can see in Tab. 23, the most frequent motifs in Text W1 are “12”, “112”,
“122”, “1112”, and so are the rest 19 written texts. The motifs data about the 20
written texts are shown in Tab. 5.
44 | Heng Chen – Junying Liang
Tab. 5: The Motifs data about the 20 written texts
Text Motif types Motif tokens Text Motif types Motif tokens
W1 110 894 W11 86 854
W2 79 901 W12 59 299
W3 55 321 W13 71 458
W4 52 213 W14 94 746
W5 61 326 W15 60 340
W6 54 441 W16 67 442
W7 58 329 W17 48 256
W8 53 260 W18 55 308
W9 53 296 W19 82 762
W10 53 253 W20 48 255
Next we fit the power law function y=axb
to the rank-frequency distribution data
of word length motifs in the 20 written texts. Fig. 2 shows the fitting in Text W1.
Fig. 2: Fitting the power law function y=axb
to the rank-frequency distribution data in Text W1
Chinese Word Length Motif and Its Evolution | 45
The fitting in Fig. 2 succeeded, and in the same way we fit the power law func-
tion to the rest 19 written texts. Tab. 6 shows the fitting results.
Tab. 6: The rank-frequency fitting results of word length motifs in 20 written texts
Text a b R2
W1 188.49475 -0.99192 0.98043
W2 217.48211 -1.03134 0.98509
W3 67.88936 -0.92708 0.96132
W4 36.90815 -0.82067 0.90456
W5 58.89495 -0.8638 0.96935
W6 118.78263 -1.09064 0.99293
W7 69.4951 -0.91219 0.93201
W8 56.51712 -0.95244 0.98106
W9 59.48396 -0.89846 0.96267
W10 62.1218 -1.02801 0.96833
W11 186.93144 -0.98059 0.97655
W12 70.68574 -1.02447 0.99064
W13 122.53303 -1.11158 0.9902
W14 164.1028 -0.99796 0.98057
W15 85.436 -1.05757 0.9849
W16 88.3765 -0.91278 0.96074
W17 54.30844 -0.91184 0.95147
W18 63.85772 -0.91261 0.95573
W19 162.48936 -0.95915 0.97757
W20 61.96259 -1.00867 0.98549
As can be seen in Tab. 6, the goodness fit of all the 20 written texts are larger
than 0.9, which means that all the fittings are successful. This indicates that the
word length motifs are self-organizing systems in texts.
To compare the rank-frequency distributions of word length motifs in both
spoken and written Chinese texts, we take a student t-test. The student t-test
results show that there is no significant difference in both the values of
parameters a and b (with p=0.117 and p =0.799 respectively).
Since there is no significant difference in the rank-frequency distributions
for word length motifs of spoken and written texts, we continue examining the
length distributions of word length motifs in both the two language styles.
46 | Heng Chen – Junying Liang
3.1.2 Length distributions of word length motifs
Since the preceding section shows that word length motif is a self-regulating
system, we hypothesize that the length distributions of word length motifs in
spoken and written Chinese texts should also accord with laws.
Tab. 22 in Appendix shows the word length motifs distribution data in Text
S1. It should be noticed that the calculation of frequency of word length motifs
in Tab. 7 is based on tokens.
Tab. 7: Word length motif distribution data in Text S1
Length Frequency Length Frequency
1 19 10 9
2 222 11 8
3 180 12 5
4 150 13 3
5 116 14 4
6 71 15 1
7 52 17 2
8 31 18 1
9 19 - -
Köhler & Naumann’s (2008) study shows that Hyper-Pascal model is adequate
to describe word length distributions. Through theoretical deduction, Köhler
(2006) states that the Hyper-Pascal model can also be used to describe word
length motif distributions. Here, we fit this model to Text S1, as well as all the
other texts.
First, we take a close look into the Hyper-Pascal function:
(1)
Chinese Word Length Motif and Its Evolution | 47
There are three parameters in the function, i.e., k, m, q. We used Altmann-Fitter
to fit the function to all the texts. The fitting results of all the 20 spoken texts are
shown in Tab. 8.
Tab. 8: The results of fitting the Hyper-Pascal function to the length distributions of word
length motifs in spoken texts
Text k m q X2
P(x)2
df c R2
S1 0.2428 0.0137 0.6570 11.1712 0.5965 13 0.0125 0.9903
S2 0.7401 0.0206 0.5502 13.5090 0.2614 11 0.0144 0.9930
S3 0.6881 0.0489 0.5530 10.7625 0.4634 11 0.0116 0.9921
S4 0.3882 0.0474 0.6651 10.9825 0.5304 12 0.0320 0.9808
S5 0.9136 0.0754 0.5226 8.9775 0.3442 8 0.0326 0.9921
S6 0.7008 0.0625 0.5656 14.0225 0.2318 11 0.0161 0.9916
S7 0.7826 0.0309 0.5275 8.9850 0.5335 10 0.0101 0.9903
S8 0.8527 0.0556 0.5638 12.5464 0.3240 11 0.0146 0.9886
S9 1.5327 0.3824 0.9681 24.8329 0.0156 12 0.0283 0.9875
S10 0.7205 0.0495 0.5339 13.0836 0.2190 10 0.0145 0.9917
S11 0.4199 0.0197 0.6009 15.3403 0.1674 11 0.0184 0.9929
S12 0.2748 0.0188 0.6394 13.4042 0.4171 13 0.0170 0.9948
S14 1.9039 0.1674 0.4235 8.8882 0.3518 8 0.0219 0.9797
S15 2.6097 0.1486 0.3807 9.7396 0.2038 7 0.0402 0.9589
S16 0.5152 0.0493 0.6316 6.3306 0.7064 9 0.0372 0.9789
S17 0.6749 0.0254 0.5415 7.8998 0.4433 8 0.0194 0.9769
S18 0.2959 0.0222 0.6328 4.2475 0.8944 9 0.0194 0.9941
S19 0.9328 0.0533 0.5526 14.8051 0.0964 9 0.0428 0.9573
We can see from Tab. 8 that only the fitting of Text S13 and Text S20 failed. As
the fittings can be affected by factors such as text sizes, authorship, writing time,
etc., the failures are a normal phenomenon. Since we cannot expect to use one
function to fit all the texts, exceptions are acceptable. Based on this, we con-
clude that the Hyper-Pascal function is also fit for the length distributions of
word length motifs in spoken Chinese texts, which is consistent with Köhler’s
(2006) hypothesis.
Next we investigate the fittings in written Chinese texts. Tab. 9 shows the
data of Text W1; the frequency is also based on tokens.
48 | Heng Chen – Junying Liang
Tab. 9: Word length motif distribution data in Text W1
Length Frequency Length Frequency
1 29 10 19
2 228 11 7
3 193 12 12
4 147 13 5
5 96 14 2
6 61 16 1
7 43 17 3
8 29 18 1
9 17 19 1
Again we fit the Hyper-Pascal function to the 20 written texts, and the results
are shown in Tab. 10.
Tab. 10: The results of fitting the Hyper-Pascal function to the length distributions of word
length motifs in written texts
Text k m q X2
P(x)2
df c R2
W1 0.2826 0.0241 0.6489 14.6117 0.3322 13 0.0163 0.9970
W2 0.3324 0.0194 0.6231 11.1503 0.5161 12 0.0124 0.9945
W3 1.1316 0.0370 0.5224 3.3577 0.9484 9 0.0105 0.9859
W5 0.4702 0.0360 0.6572 13.0606 0.3647 12 0.0401 0.9713
W6 0.6266 0.0281 0.5277 13.9460 0.0832 8 0.0316 0.9890
W8 0.2527 0.0182 0.6571 6.7868 0.8161 11 0.0262 0.9801
W9 0.7193 0.0261 0.5788 6.7639 0.6617 9 0.0229 0.9764
W10 0.2416 0.0128 0.6286 10.2833 0.3280 9 0.0406 0.9760
W11 0.6904 0.0345 0.5724 11.2706 0.4209 11 0.0132 0.9953
W12 0.0315 0.0014 0.7189 9.3846 0.6698 12 0.0315 0.9866
W13 0.2206 0.0045 0.6440 21.3565 0.0299 11 0.0466 0.9776
W14 0.4266 0.0277 0.6118 8.3785 0.7549 12 0.0112 0.9912
W15 0.7190 0.0426 0.5644 17.9030 0.0363 9 0.0527 0.9500
W16 0.8888 0.0928 0.5753 10.7904 0.3741 10 0.0244 0.9812
W17 0.8318 0.0570 0.5175 11.0858 0.1969 8 0.0433 0.9741
W18 0.4518 0.0251 0.6775 10.1155 0.6845 13 0.0328 0.9876
Chinese Word Length Motif and Its Evolution | 49
Text k m q X2
P(x)2
df c R2
W19 0.6406 0.0481 0.5616 10.9952 0.3579 10 0.0144 0.9935
W20 0.3463 0.0082 0.5796 5.6708 0.6841 8 0.0222 0.9871
As can be seen in Tab. 10, only 2 fittings failed, i.e., Text W4 and Text W7. Simi-
larly, we conclude that the Hyper-Pascal function is also fit for written Chinese
texts.
Up to now, the length distributions of word length motifs in both spoken
and written Chinese texts have been investigated. To compare if the parameters
of Hyper-Pascal function are different in the two different language styles, we
used statistical tests. Firstly, the KS tests show that both the data fit normal
distributions. Then take three independent samples T tests to groups of values
of parameters k, m, q. The T tests show that there is a significant difference in
parameter (P＝0.047), but not in parameters m (P＝0.060) and q (P =0.534).
Köhler（2006）analyzed the significance of the parameters in the Hyper-
Pascal function, and hypothesizes that they are correlated with long words,
short words, as well as mean word length. Chen et al. (2015) found that the in-
crease of word length is an essential regularity in Chinese word evolution,
which cause a series of changes in the self-organizing lexical system, especially
in word length distribution. Since word length motif is correlated with word
length distributions, we are going to investigate how word length motif evolves
in the following section.
50 | Heng Chen – Junying Liang
Fig. 3: Fitting the power law function y=axb
to the rank-frequency distribution of word length
motifs in time period 1
3.2 The Evolution of Chinese Word Length Motifs
Since Chinese word length increases with time (Chen et al., 2015), we hypothe-
size that the word length motifs may also evolve with time.
3.2.1 Evolution of rank-frequency distributions
Firstly, we obtained the rank-frequency distribution data in the six time periods.
Then we fitted the power law function y＝axb
to all the data. Fig. 3 shows the
fitting in time period 1.
The fitting results in period 1 as well as in other time periods can be seen in
Tab. 11.
Chinese Word Length Motif and Its Evolution | 51
Tab. 11: The results of fitting y=axb
to rank-frequency distributions of word length motifs in 6
historical time periods
Time Period a b R2
1 203.7642 -0.8373 0.8661
2 253.9157 -0.9119 0.9610
3 278.6423 -0.8991 0.9084
4 298.9862 -0.9567 0.9792
5 377.545 -1.0059 0.9725
6 375.6220 -1.0035 0.9843
As can be seen in Tab. 9, the values of goodness of fit R2
are all larger than 0.8,
which means that the fittings succeeded. As for the parameters, we can see that
there is an increasing trend in parameter a, and a decreasing trend in parameter
b. In order to see if this regulation is correlated with the case in word length, we
also fit the power law function y=axb
to the rank-frequency distributions of word
length in 6 historical time periods, the data of which can be seen in Tab. 12.
Tab. 12: The results of fitting y=axb
to rank-frequency distributions of word length in 6 histori-
cal time periods
Time Period a b R2
1 0.7176 -2.6975 0.99186
2 0.5704 -1.9977 0.94791
3 0.5013 -1.7255 0.89313
4 0.4828 -1.6451 0.84742
5 0.5053 -1.7302 0.88671
6 0.4555 -1.5749 0.85701
As can be seen from Tab. 12, contrary to the case in Tab. 11, there is a decreasing
trend in parameter a, and an increasing trend in parameter b, which corrobo-
rates that word length motifs and word length are highly interrelated (the Per-
son Correlation Coefficient is -0.828, P =0.042).
Besides, we also fit the power law function y=axb
to the rank-frequency dis-
tributions of word frequencies in the 6 historical time periods, the results of
which can be seen in Tab. 13.
52 | Heng Chen – Junying Liang
Tab. 13: The results of fitting y=axb
to rank-frequency distributions of word frequencies in 6
historical time periods
Time Period a b R2
1 544.9705 -0.7547 0.9597
2 342.2418 -0.7334 0.9819
3 250.6101 -0.6427 0.8867
4 245.7847 -0.6578 0.9189
5 434.8907 -0.7965 0.9911
6 403.5935 -0.7946 0.9904
In Tab. 13, we cannot see significant increasing or decreasing trends in the
changes of parameters a and b, which means that there are no significant corre-
lations between word length motifs and word frequencies in this sense.
3.2.2 Evolution of length distributions of word length motifs
Word length motif distributions, i.e., the length distributions of word length
motifs, show the frequencies of motifs (here based on tokens) in different length
(ranging from the shortest to the longest motifs). Tab. 14 shows the data in peri-
od 1.
Tab. 14: Length distributions of word length motifs in period 1
Length Frequency Length Frequency
1 3 21 6
2 167 22 7
3 185 23 5
4 166 24 6
5 124 25 3
6 103 26 3
7 72 27 1
8 59 28 1
9 46 30 2
10 53 31 3
Chinese Word Length Motif and Its Evolution | 53
Length Frequency Length Frequency
11 25 33 1
12 24 34 6
13 17 38 1
14 21 39 1
15 13 40 1
16 12 42 1
17 14 45 2
18 11 55 1
19 14 56 1
20 5 77 1
As can be seen in Tab. 14, the most frequent motif length is 3, and there are 40
different lengths of word length motifs. We fitted the Hyper-Pascal function to
the data in Tab. 14, but failed. Then we used Altmann-Fitter and also failed to
find an appropriate model.
Tab. 15: Length distributions of word length motifs in period 2
Length Frequency Length Frequency
1 18 16 6
2 185 17 12
3 306 18 1
4 193 19 3
5 163 20 6
6 138 21 2
7 97 22 2
8 70 24 1
9 63 25 1
10 40 26 1
11 27 27 1
12 15 29 2
13 11 30 1
14 12 31 1
15 13 38 1
54 | Heng Chen – Junying Liang
It can be seen from Tab. 15 that the most frequent length is 3, which is con-
sistent with the case in period 1. However, there are less different lengths (there
are 30 in period 2) motifs. Once again, we tried to find a model with the help of
Altmann-Fitter but also failed.
Tab. 16: Length distributions of word length motifs in period 3
Length Frequency Length Frequency
1 25 11 24
2 253 12 17
3 323 13 20
4 235 14 8
5 208 15 6
6 142 16 5
7 107 19 1
8 65 20 1
9 41 21 3
10 33 - -
In period 3, the most frequent motif length is also 3, which is the same as case in
the former two periods. What is more, the types of different motif lengths are
also decreasing (19 different lengths). There are also no fit models in this period.
Let us see the case in period 4, the data of which can be seen in Tab. 17.
Tab. 17: Length distributions of word length motifs in period 4
Length Frequency Length Frequency
1 27 11 21
2 192 12 13
3 381 13 19
4 245 14 7
5 221 15 2
6 149 16 3
7 88 17 5
Chinese Word Length Motif and Its Evolution | 55
Length Frequency Length Frequency
8 64 20 1
9 52 21 1
10 19 - -
In Tab. 17, we can see that length 3 has the most motifs tokens, which is the
same as the case in the former 3 periods. Moreover, period 4 also sees less
lengths (only 19). The fittings also failed in this period.
The above four periods are all in ancient Chinese. In the following, we in-
vestigate the cases in modern Chinese. Tab. 18 shows the data in period 5, i.e.,
in the early 20th
century.
Tab. 18: Length distributions of word length motifs in period 5
Length Frequency Length Frequency
1 20 12 13
2 381 13 13
3 367 14 4
4 292 15 8
5 172 16 2
6 129 17 1
7 99 19 1
8 62 20 2
9 35 21 1
10 33 24 1
11 17 28 1
As can be seen from Tab. 18, the most frequent motif length is 2, which is differ-
ent from the former 4 time periods. As for the motif length types, although there
is a slight increase, the types are also relatively few (only 22).
Next we used Altmann-Fitter to fit the data and found that Hyper-Pascal
function is the only fit model, with k=0.4558, m=0.0159，q＝0.6422；
x2
=25.9730，p(x)2
=0.0544，DF＝16，C＝0.0157，R2
=0.9935. The fitting can be
seen in Fig. 4.
56 | Heng Chen – Junying Liang
Fig. 4: Fitting the Hyper-Pascal function to the word length motif distribution data in time
period 5
Finally it comes to time period 6, i.e., the 21th century. The word length motif
distribution data are shown in Tab. 19.
Tab. 19: Length distributions of word length motifs in period 6
Length Frequency Length Frequency
1 42 10 21
2 441 11 14
3 397 12 6
4 288 13 6
5 216 14 7
6 135 15 3
7 72 16 1
8 51 19 1
9 32
It can be seen from Tab. 19 that the most frequent motif length is 2, which is the
same as the case in time period 5 and different from ancient Chinese. In time
period 6, there are only 17 different motif lengths.
Chinese Word Length Motif and Its Evolution | 57
Then we fit the Hyper-Pascal function to the data in Tab. 19, which can be seen
in Fig. 5.
Fig. 5: Fitting the Hyper-Pascal function to the word length motif distribution data in time peri-
od 6
We found that Hyper-Pascal function is the only fit model, with k=0.4558,
m=0.0159， q ＝ 0.6422； x2
=25.9730 ， p(x2
)=0.0544 ， DF ＝ 16， C ＝ 0.0157 ，
R2
=0.9935.
Up to now, all the word length motif distribution data in the 6 historical
time periods have been investigated. We find that the Hyper-Pascal model is
only fit for modern Chinese, but not ancient Chinese. In view of this, we turn to
some more general statistical methods to seek rules. Here we use entropy to
describe the evolution of word length motif distributions.
For entropy, generally speaking, large entropy means large information. As
for entropy in language property distributions, Popescu et al. (2009) find that
the changes of entropy (H) in language entities are closely related with entity
types (V), which can be fitted by power law function H = aVb
. Therefore, this
means that larger entropies indicate richer lexicon.
From a mathematical point of view, the values of entropies range from 0 to
	
loge
V
. When the entropy approaches 0, the distributions concentrate to
some certain entity types, and the information content is low; when the entropy
approaches 	
loge
V
, the distributions are uniform, and the information con-
tent is high.
58 | Heng Chen – Junying Liang
Based on the distribution data of word length motifs in the 6 historical time
periods, we calculate the entropies, as can be seen in Tab. 20.
Tab. 20: Entropies of word length motif distributions in 6 time periods
Time period Entropy
1 2.7073
2 2.4089
3 2.2449
4 2.1873
5 2.1197
6 2.0158
It can be seen from Tab. 20 that there is a decreasing trend in the values of en-
tropies. Therefore, although the Hyper-Pascal model is not fit for ancient Chi-
nese, the decrease of entropy values can be seen as a rule in word length motifs
evolution. As stated above, when the entropy approaches 0 (i.e., when it de-
creases, just as in this case), the distributions concentrate to some certain entity
types, and the information content is low. The decrease of entropy in Tab. 20
indicates that as time went on, Chinese word length series are more likely to be
developed into certain patterns of word length motifs.
4 Conclusion
In this paper, we investigate word length motifs in spoken and written Chinese,
and try to make clear how it evolves in written Chinese.
Firstly, we investigate the synchronic investigations of word length motifs
in contemporary Chinese. On the one hand, the rank-frequency distributions of
both spoken and written Chinese word length motifs can be modeled by the
power law function y=axb
, which indicates that Chinese word length motifs are
also self-organizing systems. However, as for the fitting parameters, the student
t-test results show that there is no significant difference in the rank-frequency
distributions between spoken and written Chinese with respect to word length
motifs.
Secondly, both modern spoken and written Chinese motif length distribu-
tions can be modeled by the Hyper-Pascal function deduced by Köhler (2006).
Chinese Word Length Motif and Its Evolution | 59
Moreover, the t-test results show that there is a significant difference in parame-
ter k, but not in parameters m and q.
Thirdly, as for word length motif of written Chinese in the last 2000 years,
the rank-frequency distributions data in all the 6 time periods can be fitted with
the power law function y=axb
. As for the parameters, we can see that there is an
increasing trend of parameter a, and a decreasing trend in of parameter b. The
results show that word length and word length motif truly co-evolve.
Fourthly, different from the rank-frequency distribution, the evolution of
motif length distributions is rather complicated since only modern Chinese
(time periods 5 and 6) word length motif distributions can be fitted with the
Hyper-Pascal function. A deeper entropy analysis shows that the decrease of
entropy is a tendency in motif length distribution that they are more and more
concentrated on some certain motif patterns.
Last but not least, further explorations are still needed: the reason why
length distributions of ancient Chinese word length motifs cannot be fitted with
the Hyper-Pascal function; and whether this problem is correlated with the
decrease of word length motif types?
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188.
References
Altmann, E. G., Pierrehumbert, J. B., & Motter, A. E. (2009). Beyond word frequency: Bursts,
lulls, and scaling in the temporal distributions of words. PLoS One, 4(11), E7678.
doi:10.1371/journal.pone.0007678
Chen, H., Liang, J., & Liu, H. (2015). How does word length evolve in written Chinese? PLoS
One, 10(9), E0138567. doi:10.1371/journal.pone.0138567
Ebeling, W., & Pöschel, T. (1994). Entropy and long-range correlations in literary English. EPL
(Europhysics Letters), 26(4), 241–246.
Garcia, D., Garas, A., & Schweitzer, F. (2012). Positive words carry less information than nega-
tive words. Epj Data Science, 1(1), 1–12.
Jiang, J. & Liu, H. 2015. The Effects of Sentence Length on Dependency Distance, Dependency
Direction and the Implications - Based on a Parallel English-Chinese Dependency Tree-
bank. Language Sciences, 50, 93–104.
60 | Heng Chen – Junying Liang
Kalimeri, M., Constantoudis, V., Papadimitriou, C., Karamanos, K., Diakonos, F. K., & Pa-
pageorgiou, H. (2012). Entropy analysis of word-length series of natural language texts:
Effects of text language and genre. International Journal of Bifurcation and Chaos, 22(9)
doi:10.1142/S0218127412502239
Kalimeri, M., Constantoudis, V., Papadimitriou, C., Karamanos, K., Diakonos, F. K., & Pa-
pageorgiou, H. (2015). Word-length entropies and correlations of natural language written
texts. Journal of Quantitative Linguistics, 22(2), 101–118.
doi:10.1080/09296174.2014.1001636
Köhler, R. (2006) The frequency distribution of the lengths of length sequences. In J. Genzor &
M. Bucková (Eds.), Favete linguis. Studies in honour of Viktor Krupa (pp. 145–152). Brati-
slava: Slovak Academic Press.
Köhler, R. (2008) Word length in text. A study in the syntagmatic dimension. In S. Mislovičová
(Ed.), Jazyk a jazykoveda v pohybe (pp. 416–421). Bratislava: Veda.
Köhler, R. (2015) Linguistic Motifs. In G. K. Mikros & J. Mačutek (Eds.) Sequences in Language
and Text (pp. 89-108). Berlin/Boston: De Gruyter Mouton.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In: B.
Preisach, & D. Schidt-Thieme (Eds.), Data Analysis, Machine Learning and Applications.
Proceedings of the Jahrestagung der Deutschen Gesellschaft für Klassifikation 2007 in
Freiburg (pp. 637-646). Berlin-Heidelberg: Springer.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F- and L-motifs as text characteristics. In: P. Grzybek, E. Kelih & J.
Mačutek (Eds.). Text and Language. Structures – Functions – Interrelations – Quantitative
Perspectives (pp. 81–89). Wien: Praesens.
Levinson, S. C., Greenhill, S. J., Dunn, M., & Gray, R. D. (2011). Evolved structure of language
shows lineage-specific trends in word-order universals. Nature, 473(7345), 79–82.
doi:10.1038/nature09923
Liu, H. (2010). Dependency direction as a means of word-order typology: a method based on
dependency treebanks. Lingua, 120(6): 1567-1578.
Mačutek, J. & Mikros, G.K. (2015) Menzerath-Altmann Law for Word Length Motifs. In G. K.
Mikros & J. Mačutek (Eds.) Sequences in Language and Text. Berlin/Boston: De Gruyter
Mouton.
Milička, J. (2015) Is the distribution of L-Motifs inherited from the word length distribution? In G.
K. Mikros & J. Mačutek (Eds.) Sequences in Language and Text (pp. 133-146). Ber-
lin/Boston: De Gruyter Mouton, 2015.
Papadimitriou, C., Karamanos, K., Diakonos, F. K., Constantoudis, V., & Papageorgiou, H.
(2010). Entropy analysis of natural language written texts. Physica A: Statistical Mechan-
ics and its Applications, 389 (16), 3260-3266. doi:10.1016/j.physa.2010.03.038
Piantadosi, S., Tily, H., & Gibson E. (2011) Word lengths are optimized for efficient communica-
tion. PNAS, 108(9), 3526-3529.
Popescu, I.-I., Altmann, G., Grzybek, Jayaram, B.D., Köhler, R., Krupa, V., P., Mačutek, J., Pustet,
R., Uhlířová, L., & Vidya, M.N., (2009). Word frequency studies. Berlin/ New York: Mouton
de Gruyter.
Chinese Word Length Motif and Its Evolution | 61
Appendix
Tab. 21: Diachronic corpus details
Time Period 1 2 3 4 5 6
Texts Work Work Work Work Work Work
MèngZǐ
(Mencius)
Shìshuōxīn
yǔ
(A New
Account of
the Tales of
the World)
Niǎn
Yùguānyīn
(Grinding Jade
Goddess of
Mercy)
Shíèrlóu
(Twelve
Floors)
Nàhǎn
(Yelling)
Xīndàofózhī
(The Bud-
dha Knows
Your Mind)
Lǚshìchūnqī
u
(Mister Lv’s
Spring and
Autumn
Annals)
Yánshì
Jiāxùn
Shū
(Mister
Yan’s Fami-
ly Motto)
Cuòzhǎncuīn-
íng
(Wrongfully
Accused of
Ying Ning)
Jiǎntiēhéshan
g
(A letter from a
monk)
Wúshēngx
ì
(A Silence
Play)
Pánghuán
g
(Hesitat-
ing)
Huíménlǐ
(A Wedding
President)
Scale
(characters)
141864 94729 11220 233430 91705 12980
Time
span
B.C. 3th–
B.C. 2th
A.D.
4th –
A.D.
5th
A.D.
12th–
A.D.
13th
A.D.
16th–
A.D. 17th
Pre-A.D.
20th
A.D. 21th
Tab. 22: The rank-frequency distributions of word length motifs in Text S1
Rank Word length motif Frequency Rank Word length motif Frequency
1 12 188 49 11111111111122 2
2 112 86 50 111111111122 2
3 122 72 51 111114 2
4 1112 58 52 111222222 2
5 11112 49 53 1224 1
6 1222 37 54 12222222 1
7 1122 30 55 11111222 1
62 | Heng Chen – Junying Liang
Rank Word length motif Frequency Rank Word length motif Frequency
8 11122 20 56 11111123 1
9 11222 20 57 111111111111122 1
10 111122 20 58 111111111111111112 1
11 13 20 59 11222222 1
12 111112 20 60 24 1
13 1111112 18 61 114 1
14 2 18 62 1111123 1
15 12222 14 63 1222223 1
16 22 12 64 1111113 1
17 11111122 11 65 133 1
18 11111112 11 66 111112223 1
19 1111122 11 67 11111111112222222 1
20 1113 10 68 11111111111111222 1
21 112222 7 69 1133 1
22 1111222 7 70 1111111111122 1
23 113 7 71 223 1
24 111222 7 72 111112222 1
25 123 6 73 1234 1
26 122222 6 74 111122222222 1
27 1123 5 75 11111111122222 1
28 11113 5 76 1111111112222 1
29 222 5 77 222222 1
30 11123 5 78 11112223 1
31 111111112 5 79 2222 1
32 111111122 4 80 124 1
33 1111111112 4 81 14 1
34 1223 4 82 112222222 1
35 111113 4 83 112223 1
36 111111222 4 84 1122223 1
37 1122222 4 85 11111114 1
38 1111223 3 86 1111111111112 1
39 11112222 3 87 11114 1
40 1112222 3 88 111111111112 1
41 11111111122 3 89 3 1
42 11111111112 3 90 111111124 1
Chinese Word Length Motif and Its Evolution | 63
Rank Word length motif Frequency Rank Word length motif Frequency
43 1124 2 91 1111111122 1
44 1222222 2 92 22222222222 1
45 122223 2 93 111123 1
46 1111111113 2 94 11111111111112 1
47 1111112222 2 95 11111111222 1
48 11223 2 96 111111111222 1
Tab. 23: The rank-frequency distributions of word length motifs in Text W1
Rank Word length motifs Frequency Rank Word length motifs Frequency
1 12 181 56 1111111113 2
2 112 99 57 1111111122 2
3 122 75 58 11111113 2
4 1112 70 59 1112222 2
5 11112 34 60 124 2
6 1122 30 61 1223 2
7 1222 27 62 11111111112 2
8 2 25 63 111111112222 2
9 111112 23 64 111111114 2
10 1111112 19 65 1133 2
11 13 18 66 111111122 2
12 14 18 67 11111111111112 1
13 11122 18 68 11112223 1
14 111122 14 69 1234 1
15 12222 12 70 12224 1
16 11222 11 71 11111111114 1
17 11111112 9 72 23 1
18 1111122 8 73 111112223 1
19 22 8 74 11111111111111222 1
20 111222 7 75 111122222222 1
21 112222 7 76 11124 1
22 1113 6 77 1222222222 1
23 113 5 78 1114 1
24 1111111112 5 79 11111123 1
64 | Heng Chen – Junying Liang
Rank Word length motifs Frequency Rank Word length motifs Frequency
25 1111112222 5 80 1111113 1
26 11223 5 81 1111111111113 1
27 11111122 5 82 11111111222 1
28 11114 5 83 1222223 1
29 111111222 5 84 1124 1
30 11113 5 85 122223 1
31 11112222 4 86 11111223 1
32 3 4 87 111122222 1
33 114 4 88 1111111111222 1
34 111111111112 4 89 11111244 1
35 122222 4 90 112224 1
36 111111111122 3 91 111113 1
37 11111222 3 92 11122222 1
38 111111112 3 93 222 1
39 1111223 3 94 1222222 1
40 111112222 3 95 111111111222 1
41 1123 3 96 11111111111123 1
42 1111111222 3 97 1122222222 1
43 11111111122 3 98 11144 1
44 1111222 3 99 1111111111111122 1
45 133 2 100 111111122222 1
46 12223 2 101 111111111111122222 1
47 1122222 2 102 1111114 1
48 24 2 103 11222222 1
49 1111123 2 104 1134 1
50 2222 2 105 1111111112222 1
51 1111111111122 2 106 1224 1
52 223 2 107 1111111111111111112 1
53 111114 2 108 111123 1
54 123 2 109 11123 1
55 11111111112222222 2 110 134 1
Ruina Chen
Quantitative Text Classification Based on
POS-motifs
Abstract: In the present study, we testify that POS (Part of Speech)-sequence in
the syntagmatic dimension can also be used to distinguish certain text types. By
employing vocabulary-independent properties, POS-motifs, an unrepeated
sequence of POS tags within the sentence boundaries, we tend to classify five
different text types in Chinese and English, with text samples from the Lancaster
Corpus of Mandarin Chinese (LCMC) and the Freiburg-Brown corpus of American
English (Frown). Datasets are evaluated by six quantitative indices indicating
variation or richness of POS sequences, namely, TTR, Hapax percentage, R1,
Entropy, RR and Gini’s coefficient. The results of discriminant analysis, decision
trees, and random forests support the conclusion that the richness of POS-
motifs may function as an acceptable indicator in classifying some text types in
both Chinese and English, especially for distinguishing texts into the narrative
vs. expository dichotomy.
Keywords: POS-motifs, text classification, decision tree, random forests
1 Introduction
Most approaches to text classification are based on paradigmatic information,
that is, they apply a “bag-of-words” model or “language in the mass” of text
(Herdan 1966) to obtain text features for classification. In the field of corpus-
linguistics, Biber (1988, 1995) and his follower-ups (Conrad and Biber 2001;
Grieve et al 2010; Xiao 2009) use the Multi-Feature/Multi-Dimension (MF/MD)
method to obtain a classification of texts in various registers; in the shared field
of computational linguistics and natural language processing (Jurafsky and
Martin, 2009; Manning and Schütze, 1999), documents are represented and
then classified or clustered by term weight vectors based on word frequency
information.
Researchers from quantitative linguistics have tried to contribute to text
||
Ruina Chen: College of Foreign languages, Guizhou University, Guiyang, China,
chenruina@sina.com
66 | Ruina Chen
classification on the basis of, e.g. word length and sentence length distributions
looking for statistical properties of these quantities that could be typical of text
genres or text sorts in general (Köhler & Naumann 2008; Popescu et al. 2013).
Similar investigations are also emerging in stylometrics (Hoover 2002, 2003a,
2003b, 2007) and, in recent years, in forensic linguistics (Finegan 2010; Kredens
& Coulthard 2012), where, in the first place, word frequency distributions play a
crucial role.
POS can be used as a distinctive feature of texts, especially for determining
the Chinese quantitative stylistic features, which mainly adopts a “bag-of-POS”
method, like the study of Hou and Jiang (2014). In the current study, we adhere
to the syntagmatic properties of texts for classification. A unit for sequential
analyses, the motif, is introduced into linguistics. Motif is based on sequences
of monotonously increasing values of selected linguistic properties, thus is in
contrast to the commonly applied bag-of-word model.
A POS-motif is defined as a POS tag segment which, beginning with the first
POS of a word in the given text without extending the sentence boundaries,
consists of POS sequences which are not the same as the one in the left neigh-
bor. As soon as a POS tag is identical to the previous one, the end of the current
POS-motif is reached. Thus, the POS tags fragment (1) will be segmented as
shown by the POS-tags sequence (2):
(1) “或许_d , 严同_nr 已_d 在_p 这_rzv 忙碌_a 中_f ，开始_v 算清_v 了_ule
为_p 那_rzv 九十万_m 斤_q 粮_n 票_n 他_rr 应_v 付出_v 的_ude1 巨大
_a 代价_n; ”
(2) {d+nr},{d+p+rzv+a+f+v},{v+ule+p+rzv+m+q+n},{n+rr+v},{v+u+a+n}.
(3) {II+JJ+NP1+NN},{NN},{NN+VH+VV+TO+VV+AT},{NN+IO+AT1},{NN},{N
N},{NN},{NN+VV+NN+NP+AT+MD},{NN},{NN+IO+APPGE},{NN+RR+MC
+NNT+II+AT},{NN}.
The first POS-motif consists of two elements because in the following POS-motif
there is a repetition of the first; the second one ends also where one of its ele-
ments occurs again.
For the formation of POS-motif in English, lemmatization of word forms is
conducted first. This is because we tend to make better comparisons in both
Quantitative Text Classification Based on POS-motifs | 67
Chinese and English the force of POS-motifs in classifying equivalent text types1
.
Thus, the POS-motif of an English segment (3) will be like that in (4):
(4) “Despite_II intense_JJ White_NP1 House_NN1 lobbying_NN1, Con-
gress_NN1 has_VHZ voted_VVN to_TO override_VVI the_AT veto_NN1
of_IO a_AT1 cable_NN1 television_NN1 regulation_NN1 bill_NN1, deal-
ing_VVG President_NNB Bush_NP1 the_AT first_MD veto_NN1 de-
feat_NN1 of_IO his_APPGE presidency_NN1 just_RR four_MC
weeks_NNT2 before_II the_AT election_NN1.”
(5) {II+JJ+NP1+NN},{NN},{NN+VH+VV+TO+VV+AT},{NN+IO+AT1},{NN},{N
N},{NN},{NN+VV+NN+NP+AT+MD},{NN},{NN+IO+APPGE},{NN+RR+MC
+NNT+II+AT},{NN}.
These motifs can be measured by the type(s) of POS tags embedded within, that
is, POS-motif richness. Thus, a frequency distribution of x-POS-motifs in each
sample text can be obtained, based on which quantitative indices can be de-
rived for further analysis.
The computations of motifs have been applied to various versions of lin-
guistic entities, such as word lengths and word frequencies (Köhler 2008a,
2008b, Köhler & Naumann 2010, Sanada 2010), the lengths of the motifs per se
(Köhler 2006), and the quantitative properties of motifs of RST (Rhetorical
Structure Theory) relations (Beliankou et al. 2013).
The goal of the current research is to examine: firstly, to what degree a clas-
sification of text types can be achieved on the basis of POS-motifs, or, to put it
differently, to what degree POS-motifs may contribute to a classification of
texts? Secondly, can POS-motifs contribute to distinguish text types in both
Chinese and English?
In Section 2, the language materials, the basic statistical data of POS-motifs
in the research corpus, the instruments adopted to process the language mate-
rials and the six quantitative indices employed are elaborated; in Section 3, our
empirical results are presented, concerning the classification results from dis-
||
1 The typological differences between Chinese and English has made the comparison of de-
rived POS-motifs in different text types in both languages not on the equal standard, that is,
the existence of tense, aspect and the singular and plural form of nouns and verbs in Chinese,
but not in English. Thus, lemmatization of English words and the abstraction of their POS as
well, are deemed necessary. After that, all words and their POS are manually checked in order
to ensure accuracy.
68 | Ruina Chen
criminant analysis, decision trees, and random forests in LCMC and Frown.
Finally, the results are summarized.
2 The Corpora, Methods and Quantitative Indices
2.1 The Research Corpora
The Lancaster Corpus of Mandarin Chinese (further LCMC2
) and the Freiburg-
Brown corpus of American English (further Frown) are purposely chosen to
“conduct contrastive research between Chinese and English” (McEnery & Xiao
2004). Both are one-million-word balanced corpora, comparable in size, and
represent the language of the early 1990s. Both contain 500 texts of around
2000 words each; they comprise 15 text categories, falling into 4 macro-domains:
Press, Non-fiction, Academic and Fiction.
The compositions of two corpora are similar except for two differences. One
is in text types of “adventure fiction”, which is martial art fiction in LCMC (since
Chinese has no western fiction) but western and adventure fiction in Frown; the
other is that word segmentation and part-of-speech (POS) annotations are per-
formed in LCMC, and only POS in Frown. Text types of both corpora are pre-
sented in Table 1.
Tab. 1: Text types in LCMC and Frown
Domain Text Type Samples
news press reportage (A) 44
press editorial (B) 27
press reviews (C) 17
non- fiction religious writing (D) 17
instructional writing (E) 38
popular lore (F) 44
biographies/essays (G) 77
reports/official documents (H) 30
||
2 This corpus is available at http://www.lancaster.ac.uk/fass/projects/corpus/LCMC/
Quantitative Text Classification Based on POS-motifs | 69
Domain Text Type Samples
academic academic prose (J) 80
fiction general fiction (K) 29
mystery/detective fiction (L) 24
Science fiction (M) 6
Adventure fiction (N) 29
Romantic fiction (P) 29
Humor (R) 9
Total 500
Our pilot study has found that the classification for sub-domains of news and
fiction with POS-motifs were quite unsatisfactory, thus we will not choose all 15
text types in both corpora for the present study. We finally pinned down to 5
text types for more depth comparison and discussion: press reportage (further
news), biographies/essays (further essays), reports/official document (further
official), academic prose (further academic), and general fiction (further fic-
tion). Admittedly, they do not cover all text types in either Chinese or English,
but they are representative and considered adequate for the purpose of demon-
strating the current research methodology.
In the current study, the formation of POS-motifs in all sample texts is done
by a Python program; the rank-frequency information of POS-motifs, as well as
the calculation of six indices is conducted with the software QUITA (Kubát,
Matlach and Čech 2014). Our research concerning discriminant analysis, the
decision tree and random forest are conducted by R programs.
The statistical data of POS-motifs in five text types is presented in Table 2.
Altogether there are 134,228 and 154,400 POS-motif tokens in Chinese and Eng-
lish respectively. On the average, a sentence contains 5.86 POS-motifs in Chi-
nese while 5.67 POS-motifs in English. Though the types and tokens of POS-
motifs in Chinese are less than those in English, the TTR of POS-motifs in Chi-
nese is still a bit higher than those in English. POS-motifs per sentence in Chi-
nese in different text types are higher than in English, except for the text type of
essays.
70 | Ruina Chen
Tab. 2: The basic statistical data of POS-motifs in the research corpus
Text
Type
Types Tokens TTR POS-motifs
Per Sentence
CN EN CN EN CN EN CN EN
news(A) 14,343 16,279 21,891 27,027 0.66 0.60 5.50 5.30
essays(G) 25,622 27,443 35,833 42,509 0.72 0.65 4.83 6.08
official(H) 8,062 9,614 19,168 19,939 0.42 0.48 10.18 6.70
academic(J) 24,882 26,481 44,044 48,368 0.57 0.55 6.97 6.39
fiction(K) 9,637 11,148 13,292 16,557 0.73 0.67 4.05 3.58
Total 82,546 90,965 134,228 154,400
00
Average 0.61 0.59 5.86 5.67
2.2 Quantitative Indices Measuring POS-motifs Richness
In quantitative linguistics, TTR (type-token ratio) (V/N), Hapax percentage, R1,
Entropy, RR and Gini’s coefficient are all measures of lexical richness, and the
first four indices have a positive correlation with lexical richness, while the
latter two are negative. These indices are used in the current study for measur-
ing richness of POS-motifs. As TTR and Hapax percentage are the most common
statistics, we will elaborate on the latter four indices.
R1 is a quite different approach to lexical richness which considers the h-
point. Words with ranks smaller than h are mostly auxiliaries and synsemantics
which occur quite frequently but do not contribute to the richness. Richness is
produced rather by autosemantics that occur more seldom. Popescu et al.
(2009:29) take into account the fixed point h and consider all words whose fre-
quency is smaller than h as contributors to richness. To obtain a comparable
indicator, we first define the cumulative probabilities up to h as ([h ])
F , which
is the sum of relative frequencies of words whose ranks are smaller or equal to h,
illustrated as
[ ]
1
([h]) ( )
1 h
r
r
F F r h f
N 
   
. (1)
Quantitative Text Classification Based on POS-motifs | 71
Then a slight correction to ([h ])
F is conducted as
 
2
([h]) / 2
F h N

: (2)
the subtraction of the quantity
2
/ 2
h N (the half of the square of the h-point)
from ([h ])
F . Based on these conditions, R1 is defined as
 
2
1 1 ([h]) /2
R F h N
   . (3)
Higher R1 indicates greater POS-motif richness.
Entropy in our analysis is adopted as that proposed by C. Shannon and applied
in linguistics to show the diversity (uncertainty of the information) and the
concentration of the distribution. It is defined as:
2
1
log
V
r r
r
H p p

  , (4)
where r
p is the relative frequency of one word in a sample (that is, the propor-
tion of words frequency fr ), V is the total number of types of words. The entropy
value varies in the interval
2
0
H < , >
log V
  ; (5)
if the entropy is zero, all frequencies are concentrated on one entity
1
2
1
1log 1 0
r
H

  

, (6)
and the predictability is quite simple; if entropy attains its maximum, then all
entities have the same number of frequencies (for example 1/V,
72 | Ruina Chen
2
1
2
log log
1 1
V
r
H V
V V

   
 ), (7)
there is a perfect uniformity and nothing can be predicted. Thus, higher entropy
of POS-motifs indicates more richness of POS sequence variation of a text and
vice versa.
RR is the repeat rate which is asymptotically the same as Entropy, but is inter-
preted in a reverse sense. RR is defined as:
1
2 2
2
1
1
V
r
r
V
r
r
RR p f
N
 
 
 
. (8)
If all frequencies fall upon one POS-motif, then the text is maximally concen-
trated. If all POS-motifs have the same frequency, then the smallest concentra-
tion is given. Thus, higher value of RR indicates lower POS-motif richness and
vise visa.
Gini’s coefficient has been introduced by Popescu and Altmann (2006), here
it is used as an indicator of measuring POS-motif richness. In quantitative lin-
guistics, fortunately, it is not necessary to revert and cumulate the distribution
and compute the sum of trapezoids to obtain the area above the Lorenz curve.
Instead, one can directly compute
1
1 2
1
V
r
r
G V rf
V N 
 
  
 
 

(9)
As Gini’s coefficient represents the area between the diagonal and the Lorenz
curve, the greater the area, the smaller the POS-motif richness.
3 Results and Discussion
3.1 The Discriminant Analysis
The discriminant analysis is a supervised classification method, which is used
more commonly for dimensionality reduction before later classification. The
Quantitative Text Classification Based on POS-motifs | 73
result shows that in LCMC 93.83% of the between-group variance were on the
first discriminant axis, which indicates that nearly all the differences between
groups can be “explained” using the first discriminant, whereas a further 4.28%
was explained on the second discriminant axis. In Frown, 75.13% of the be-
tween-group variance was on the first discriminant axis whereas a further
15.56% was explained on the second.
The coefficients for each discriminant function are shown in Table 3 below.
In LCMC, the most influential variables in the first discriminant are RR and Ha-
pax Percentage, the second discriminant has added Gini, R1 and TTR; while in
Frown, TTR and Gini are the most influential variables in the first discriminant,
TTR and RR are among the second discriminant.
Tab. 3: Coefficients of linear discriminants
First discriminant Second discriminant
CN EN CN EN
TTR -6.26 -71.08 44.70 361.67
Entropy -0.64 8.00 4.56 -8.44
R1 5.43 -31.95 -25.98 -29.92
RR 23.04 39.98 5.32 -123.75
Gini -6.77 -61.85 90.49 237.10
Hapax 17.19 18.14 35.58 -95.32
Plotting the scores of each observation on the first-second discriminant plane,
we get the result in Fig. 1 (LCMC) and Fig. 2 (Frown) respectively. The graphs are
very useful for identifying clusters.
74 | Ruina Chen
Fig. 1: Linear discrimination between different text types in LCMC “A” for news, “G” for es-
says, “H” for officials, “J” for academics, and “K” for fiction
In LCMC (Figure 1), we note that most scores are concentrated on the first dis-
criminant plane. Recall that it was evident that 93.83% of the between-group
variance was on the first discriminant axis. In addition, official documents (H)
and academic proses (J) are closely clustered on the left-hand side of the plot,
while essays (G), news (A) and fiction (K) are on the right-hand side. This seems
to signal that further discrimination between the “expository vs. narrative”
dichotomy in Chinese may be feasible.
In Frown (Fig. 2) over the two discriminant planes, we can still find that ex-
pository (H and J) and narrative (A, G and K) texts are on opposite side of the
first discriminant axis, just like those in LCMC. But there are more misclassifica-
tions in news (A) and fiction (K), which may be also manifested by the classifi-
cation results listed in Tab. 3, compared with those in LCMC.
Quantitative Text Classification Based on POS-motifs | 75
Fig. 2: Linear discrimination between different text types in Frown “A” for news, “G” for essays,
“H” for officials, “J” for academics, and “K” for fiction
The results of classification according to the discriminants are shown in Tab. 4.
In order to reveal where our discrimination succeeds and where it fails, we then
form a misclassification table, presented in Tab. 5. Combing the two tables, it
can be seen that in LCMC, news (A) is the most poorly attributed text type, either
classified as essays (G) or academics (J). Fiction (K) is also badly classified,
mostly mixed with essays (G). While essays (G) are mostly correctly classified,
followed by officials and academics. While in Frown, all text types are poorly
attributed.
Tab. 4: The result of classification according to discriminants
Corpus A G H J K
LCMC 6.82 90.91 80.00 75.00 34.48
Frown 34.09 70.67 53.33 67.50 51.72
76 | Ruina Chen
Tab. 5: Linear discriminant misclassification table
LCMC
A G H J K Total
A 3 21 0 19 1 44
G 1 70 0 5 1 77
H 0 0 24 6 0 30
J 3 11 6 60 0 80
K 1 16 0 2 10 29
Frown
A G H J K Total
A 15 16 0 11 2 44
G 5 53 0 12 5 75
H 1 2 16 11 0 30
J 4 13 9 54 0 80
K 1 11 0 2 15 29
3.2 Decision Trees
Decision trees (DTs) are an increasingly popular method used for classifying
data. In a DT, each sub-region is represented by a node in the tree. The node can
be either terminal or non-terminal. Non-terminal nodes are impure and can be
split further using a series of tests based on the feature variables, a process
called splitting. The split which maximizes the reduction in impurity is chosen,
the data set split and the process repeated. Splitting continues until the terminal
nodes are too small or too few to be split. To obtain a fully grown tree, this pro-
cess is recursively applied to each non-terminal node until terminal nodes are
reached. The terminal nodes correspond to homogeneous or near homogeneous
sub-regions in the feature space. Each terminal node is assigned the class label
that minimizes the misclassification cost at the node.
In LCMC the tree has six end nodes and only Hapax Percentage and RR are
employed for the classification. This has reinforced RR and Hapax Percentage as
two important variables in the discriminant analysis. The first rule involves the
Hapax Percentage, with texts exhibiting low Hapax percentage being official
texts (H). The second type of texts classified is academics (J) with a Hapax per-
centage ranging above 0.362 but below 0.519. While those texts varying between
the range above 0.519 but below 0.581 can either be news (A) or academics (J),
which may explain the large number of news which were misclassified as aca-
demics (J) in LCMC (Tab. 5). On the other hand, if texts have a higher Hapax
percentage (> 0.581) but a low RR (≤ 0.008) as well, they are classified as fiction
(K). Those texts with higher values on Hapax Percentage (> 0.581) and RR (>
0.008) are mostly essays (G).
In Frown, seven end nodes are derived, and three variables are employed
for the classification --- R1, Hapax Percentage and Entropy. Text types with ex-
Quantitative Text Classification Based on POS-motifs | 77
pository attributes like official (H) and academic (J) are the first group to be
distinguished from narratives texts of news (A), which is with R1≤0.693 and
hapax percentage ≤0.431. This has indicated the lack of variation of POS se-
quence of expository texts. Text types with narrative attributes like news (A) are
singled out at endnote 9 with entropy > 7.228 and R1≤ 0.747, essays (G) at end-
note 12 with R1>0.693 and entropy > 7.312, and fiction (K) at endnote 13 with
R1>0.693 and entropy > 7.726.
Comparing the trees of Frown with LCMC, it can be seen that the rules for
distinguishing expository texts are relatively simple and straightforward, plus,
the classification results are much better than those of the narrative texts; while
narrative ones generally require more rules and variables to distinguish, even
this, the misclassification into other subtypes still exist. This is shared in both
corpora, which also echoes the results in discriminant analysis.
78 | Ruina Chen
Fig. 3: Classification tree for distinguishing five Types in LCMC
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
Node2(n=28)
Node5(n=61)
Node6(n=45)
Node8(n=10)
Node10(n=71)
Node11(n=45)
RR
p=0.003
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
9
RR
p＜0.001
7
Hapax
p=0.003
4
Hapax
p＜0.001
1
Hapax
p＜0.001
3
≤0.362
＞0.362
≤0.581
＞0.581
＞0.008
≤0.008
＞0.014
≤0.014
≤0.519
＞0.519
Quantitative Text Classification Based on POS-motifs | 79
Fig. 4: Classification tree for distinguishing five Types in Frown
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
Node3(n=45)
Node4(n=10)
Node7(n=50)
Node9(n=26)
Node11(n=7)
Node12(n=83)
Entropy
p=0.011
10
Entropy
p＜0.001
5
Entropy
p＜0.001
6
R1
p＜0.001
1
Hapax
Percentage
p=0.018
2
＞0.693
≤0.693
0
0.2
0.4
0.6
0.8
1.0
A
G
H
J
K
Node13(n=37)
R1
p=0.009
8
≤0.431
＞0.431
≤7.726
＞7.726
＞7.228
≤7.228
＞0.747
≤0.747
＞7.312
≤7.312
80 | Ruina Chen
Misclassified observations for each text type are listed in Tab. 6. The overall
error rate for LCMC and Frown is 35.38% (92/260 = 0.3538) and 44.96%
(116/258=0.4496) respectively. It can be seen that in LCMC and Frown, text types
are more likely to attribute to similar groups rather than otherwise, that is, nar-
rative text types of news (A), essay (G) and fiction (K) are more possible to be
misclassified into each other, the same as expository texts of official (H) and
academic (J), but they are less likely to enter the opposite dichotomy.
Tab. 6: Misclassification Table of decision trees
A G
LCMC
H J K Total A G
Frown
H J K Total
A 0 22 0 22 0 44 A 20 12 0 10 2 44
G 0 68 0 7 2 77 G 9 46 2 6 12 75
H 0 0 24 6 0 30 H 1 3 25 1 0 30
J 0 8 4 68 0 80 J 6 13 25 32 4 80
K 0 18 0 3 8 29 K 0 9 0 1 19 29
3.3 Random Forest
Random forests are a combination of tree predictors such that each tree depends
on the values of a random vector sampled independently and with the same
distribution for all trees in the forest. The generalization error for forests con-
verges to a limit as the number of trees in the forest becomes large. The general-
ization error of a forest of tree classifiers depends on the strength of the individ-
ual trees in the forest and the correlation between them (Breiman 2001).
Random forest can be used for classification. We select random forest to
measure the contribution of each quantitative indicator to distinguish different
text types. We adopt random forest to build classification models on training
data (the ratio of the training set vs. the testing set is 7:3). The classification
results of LCMC and Frown are shown in Tab. 7, it can be seen that the overall
classification result in LCMC(OOB estimate of error rate 42.41%) is a bit better
than that in Frown (OOB estimate of error rate 52.33%).
Quantitative Text Classification Based on POS-motifs | 81
Tab. 7: The confusion matrix by random forest in LCMC (left) & Frown (right)
LCMC Frown
A G H J K class.error
A 5 11 0 14 1 0.8387
G 6 46 0 5 3 0.2333
H 0 0 17 6 0 0.2609
J 6 6 3 41 1 0.2807
K 0 16 0 3 1 0.9500
A G H J K class.error
A 7 8 0 11 1 0.7407
G 5 36 0 7 6 0.3333
H 0 3 6 11 0 0.7000
J 3 9 9 36 0 0.3684
K 0 9 1 2 11 0.5217
In LCMC the class error in essays (G), officials (H) and academics (J) is much
lower than that in news and fiction, indicating that POS-motifs are acceptable in
classifying these text types. In Frown, except for the text type of essays (G) and
academics (J), the rest text types are all poorly classified.
Fig. 5: Importance of quantitative indicator according to random forest in LCMC(Left) and Frown
(Right) (Expressed in MeanDecreaseGini)
In Figure 5, MeanDecreaseGini computes the impurity-level influence of varia-
bles to compare the importance of the six indices towards the contribution to
82 | Ruina Chen
classification. A higher MeanDecreaseGini of an indicator indicates its greater
importance. The importance of each quantitative indicator in LCMC and Frown
is a bit different.
We also adopt random forests for classify all text samples into only narra-
tive (A, G and K) and expository (H and J) categories, the results are presented in
Tab. 8.
Tab. 8: The confusion matrix of narrative vs. expository distinction by random forest in LCMC
(Left) & Frown (Frown)
LCMC Frown
Ex Na class.error
Ex 86 24 0.2182
Na 25 125 0.1667
Ex Na class.error
Ex 62 24 0.2791
Na 14 96 0.1273
Note: “Na” stand for “Narrative”, while “Ex” for “Expository”.
It can be seen that the overall classification results of dichotomous categories
are much better than if attributing all texts to individual types, with the overall
error rate of 18.85% in LCMC, and 23.64% in Frown. In addition, adopting POS-
motifs for the classification of expository texts is less error-prone in Chinese
(21.82%) than that in English (27.91%), while narrative texts achieve a bit higher
accuracy in English (87.27%) than in Chinese (83.33%).
4 Conclusion
In this study, we have employed vocabulary-independent properties, POS-
motifs, an unrepeated sequence of POS tags within the sentence boundaries, to
classify five different text types in Chinese and English. Datasets are character-
ized by six quantitative indices indicating variation or richness of POS-motifs,
namely, TTR, Hapax percentage, R1, Entropy, RR and Gini’s coefficient.
It is found that the richness of POS-motifs may function as an acceptable
indicator in classifying some text types in both Chinese and English. The results
from discriminant analysis indicate that the narrative vs. expository distinction
exist in both LCMC and Frown. The results from decision trees manifest that
though different variables are employed for splitting different types in LCMC
and Frown, expository text types of official (H) and academic (J) are the earlier
Quantitative Text Classification Based on POS-motifs | 83
ones to be split via the major rule of hapax percentage of POS-motifs, compared
with those of narrative text types which involve a bit complicated rules indicat-
ing their richness of POS-motifs. In addition, text types are more likely to be
misclassified into homogenous groups rather than opposite ones. The results
from random forests indicate that using POS-motifs richness as an indicator to
classify texts is acceptable in certain types. In LCMC, narrative types of news (A)
and fiction (K) are the poorly attributed ones, while expository ones like officials
(H) and academics (J) are mostly correctly classified. However in Frown, most
text types are poorly classified except for essays (G). The overall classification
result for attributing into individual text types in both LCMC and Frown is not
satisfactory, but acceptable if just distinguish into “narrative vs. expository”
dichotomy.
As POS has already attained a certain degree of abstraction for words, POS
sequences combining syntagmatic relations can be regarded as, to some extent,
crossing and straddling the boundaries of morphology and lexicology to reflect
genre-specific syntactic peculiarities. But such syntactic regularities have been
constrained a little bit by the indicator of POS-motifs in the current study, as
repeated POS will be automatically broken to form a new motif.
It is to be remarked that motifs are quite “legal” units of second order just as
other well known units like sentence, clause, compound, etc. They represent
vectors of “more primary” units like syllables, words, etc., but any units in lan-
guage are conventional definitions and may be stated and segmented different-
ly in various languages. Methodologically, they have the same status as other
units or properties.
Acknowledgement
This research is supported by the National Social Science Foundation of China
funder Grant # 15BYY098.
84 | Ruina Chen
References
Beliankou, A., Köhler, R., & Naumann, S. (2013). Quantitative properties of argumentation
motifs. In: I. Obradović, E. Kelih, R. Köhler (Eds.). Methods and Applications of Quantita-
tive Linguistics (pp. 35-43). Selected Papers of the 8th
International Conference on Quanti-
tative Linguistics (QUALICO). Belgrade: Academic Mind.
Biber, D. (1988). Variation across Speech and Writing. Cambridge: Cambridge University Press.
Biber, D. (1995). Dimensions of Register Variation: A Cross-linguistic Perspective. Cambridge:
Cambridge University Press.
Breiman, L. (2001). Random forests. Machine Learning, 45(1): 5–32.
Conrad, S., & Biber, D. (Eds.). 2001. Variation in English: Multi-dimensional Studies. New York:
Longman.
Finegan E. (2010).Corpus linguistic approaches to “legal language”: adverbial expression of
attitude and emphasis in Supreme Court opinions. In C. Malcolm, & A. Johnson (Eds.). The
Rutledge Handbook of Forensic Linguistics (pp. 65-77). Abingdon: Rutledge．
Grieve, J., Biber, D., Friginal, E., & Nekrasova, T. (2010). Variation among blogs: a multi-
dimensional analysis. In A. Mehler, S. Sharoff, & M. Santini (Eds.) Genres on the Web:
Corpus Studies and Computational Models (pp. 45–71). New York: Springer-Verlag.
Herdan, G. (1966). The Advanced Theory of Language as Choice and Chance. Berlin: Springer.
Hoover, L. D. (2002). Frequent word sequences and statistical stylistics. Literary and Linguistic
Computing, 17(2):35–42.
Hoover, L. D. (2003a). Frequent collocations and authorial style. Literary and Linguistic Compu-
ting, 18(3): 45–56.
Hoover, L. D. (2003b). Multivariate analysis and the study of style variation. Literary and Lin-
guistic Computing, 18(4):65–79.
Hoover, L. D. (2007). Corpus stylistics, stylometry, and the styles of Henry James. Style, 41(2):
174-203
Kubát, M., Matlach, M., & Čech, R. (2014). Quantitative Index Text Analyser (QUITA).
http://oltk.upol.cz/software.
McEnery, T., & Xiao, R. (2004). The Lancaster Corpus of Mandarin Chinese: a corpus for mono-
lingual and contrastive language study. In: Proceedings of the Fourth International Con-
ference on Language Resources and Evaluation (LREC) 2004 (pp. 1175-1178). Lisbon.
Hou, R., & Jiang, M. (2014). Analysis on Chinese quantitative stylistic features based on text
mining. Literary and Linguistic Computing: Digital Scholarship in the Humanities, (4):1–11.
Jurafsky, D., & Martin, J.H. (2009). Speech and Language Processing: An Introduction to Natu-
ral Language Processing, Computational Linguistics, and Speech Recognition, 2nd ed.
Upper Saddle River, NJ: Pearson Prentice Hall.
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In: J. Genzor,
& M. Bucková (Eds.), Favete linguis. Studies in honour of Viktor Krupa (pp.142–152). Brati-
slava: Academic Press.
Köhler, R. (2008a). Word length in text. A study in the syntagmatic dimension. In: S. Mislovičo-
vá (Ed.), Jazyk a jazykoveda v pohybe (pp. 416–421). Bratislava: VEDA.
Köhler, R. (2008b): Sequences of linguistic quantities. Report on a new unit of investigation.
Glottotheory, 1(1):115–119.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In: B.
Preisach, & D. Schidt-Thieme (Eds.), Data Analysis, Machine Learning and Applications.
Quantitative Text Classification Based on POS-motifs | 85
Proceedings of the Jahrestagung der Deutschen Gesellschaft für Klassifikation 2007 in
Freiburg (pp. 637-646). Berlin-Heidelberg: Springer.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F- and L-motifs as text characteristics. In: P. Grzybek, E. Kelih & J.
Mačutek (Eds.). Text and Language. Structures – Functions – Interrelations – Quantitative
Perspectives (pp. 81–89). Wien: Praesens.
Kredens, K.，& Coulthard M. (2012) Corpus linguistics in authorship identification. In P.M.
Tiersma, & L. M. Solan (Eds.).The Oxford Handbook of Language and Law (pp. 504-
516).Oxford: Oxford University Press．
Manning, C.D., & Schütze, H. (1999). Foundations of Statistical Natural Language Processing.
Cambridge, MA: MIT Press.
Popescu, I.I., & Altmann G. (2006). Some aspects of word frequencies. Glottometrics, 13, 23–
46.
Popescu, I.I., Altmann, G., Grzybek, P., Jayaram, B. D., Köhler, R., Krupa, V., Mačutek,J., Pustet,
R., Uhlířová, L., & Vidya, M. N. (2009). Word Frequency Studies. Berlin/New York: Mouton
de Gruyter.
Popescu, I.-I., Zörnig, P., Grzybek, P., Naumann, S., & Altmann, G. (2013). Some statistics for
sequential text properties. Glottometrics, 26, 50–94.
Sanada, H. (2010). Distribution of motifs in Japanese texts. In: P. Grzybek, E. Kelih, & J.
Mačutek (Eds.). Text and Language. Structures – Functions – Interrelations – Quantitative
Perspectives (pp. 183-193). Wien: Praesens.
Xiao, R. (2009). Multidimensional analysis and the study of world Englishes, World Englishes,
28 (4): 421–50.
Yu Fang
L-motif TTR for Authorship Identification in
Hongloumeng and Its Translation
Abstract: Previous studies have found that different authors have different writ-
ing styles, which can be shown in vocabulary richness. However, little research
has focused on whether such differences are shown in corresponding transla-
tions. In the present study, Hongloumeng and its two translations by Hawkes
and Yang Xianyi are selected as the study object. L-motif TTR is used to reevalu-
ate the authors of Hongloumeng through 15 selected chapters from the first 80
chapters and 15 selected chapters from the rest 40 chapters. Results show that
significant differences exist in vocabulary richness between the two parts, sug-
gesting that the two parts were written by two authors. Furthermore, we also
evaluate the quality of different translations: (a) both translators choose nearly
the same word to express the story; (b) Yang uses more varied words to avoid
repetition, whereas Hawkes prefers more simplified words; (c) In translating
culture-loaded words, Hawkes favors equal words in Western culture, while
Yang translates those words literally.
Keywords: L-motif TTR, authorship identification, Hongloumeng, translation
1 Introduction
Authorship identification has been the focus of many researchers due to literary
works of disputed or unknown authorship. The basic approach is to identify “a
stylistic fingerprint characteristic of the author, and then determine whether
this fingerprint is also present in a disputed work” (Juola and Baayen 2005: 59).
Such approach is usually based on one presupposition, that is, different authors
have different writing styles, which can be shown in their use of function words,
word collocation, sentence structure, etc. In measuring an author’s writing style,
vocabulary richness is one of the important indicators for authors clearly differ
in the sizes and structures of their vocabularies. Studies in this field usually
follow two lines — authorship attribution (Labbé 2007; Khmelev 2000) and
authorship verification (Koppel and Schler 2004; Iqbal et al 2010).
||
Yu Fang: Department of Linguistics, Zhejiang University, Hangzhou, China, fydiana@163.com
88 | Yu Fang
The earliest study of quantitative authorship attribution concerning vo-
cabulary richness can be traced back to Yule (1944) who proposed the index K.
Since then, a number of indexes have been applied into research. Coyotl-
Morales et al. (2006), considering both stylistic and topic features of texts, used
a set of word sequences that combine function words and content words to test
353 poems written by five modern writers. The result showed that such method
was appropriate to capture all writers’ distinct features, so it can handle the
attribution of short documents. Similarly, Argamon and Levitan (2005) selected
twenty novels written by five authors, which turned out that using the most
frequent function words in the corpus as features of stylistic text classification
gives a good discrimination for both author and nationality attribution tasks.
Though many studies on vocabulary richness have been carried out con-
cerning the style of original authors, there has been little interest in studying
the style of translators. One reason might be that the number of translated
works with disputed or unknown authorship is not as many as that of original
texts. Another major reason is that for a long time, many researchers have con-
sidered translation as a derivative rather than a creative activity, so translators
should simply reproduce as closely as possible the style of the original and not
have a style of his or her own. In recent years, however, more and more re-
searchers are aware that translation is “a rewriting of an original text” (Lefevere
1992:1), so studies concerning translators’ style have emerged. Baker (2000)
chose five texts translated by Peter Bush and three texts translated by Peter
Clark for their stylistic analysis. Two indicators--type-token ratio and average
sentence length--suggested that vocabulary richness in Peter Clark’s works was
lower than that in Peter Bush’s work. She also investigated the frequency and
patterning of SAY (the most frequent reporting verb in English) and found that
Peter Clark tended to use SAY in his works while Peter Bush did not. Fang & Liu
(2015a) used STTR and lambda to measure the vocabulary richness of two trans-
lated Hongloumeng and found that the vocabulary richness in the native speak-
er’s (Hawkes) version is no higher than that in the non-native speaker’s (Yang)
version, because Yang liked to create some new words to express culture-loaded
words.
Those studies show the powerfulness of vocabulary richness in attributing
authorship, both in original texts and in translations. However, there is little
research dealing with a literary work with different authors. If the work is sup-
posed to have more than one author, it can be divided into several parts accord-
ing to its authors and each part can be regarded as one individual text, then will
the differences of vocabulary richness exist among those parts? There is also
few research dealing with stylistic differences of one translator in translating a
text written by several authors. In other words, if the above-mentioned literary
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 89
work is translated by one person, will the differences of vocabulary richness in
the original text shown in translations of the translator? And if this work has
different translated versions, will the result be the same? That is to say, if origi-
nal texts have been proved to be written by different authors, will it reflect in
their translations? Moreover, if one translation reflects such difference, while
the other not, can we say the former one is a better version? In other words, can
this result assist in evaluating the quality of different translations?
Moreover, if we look deeper into the methodologies of previous studies, we
can find that they either focus on some distinctive words like the most frequent
verbs and function words or they only consider texts as a whole. In other words,
one commonality was shared: they did not take the sequential organization of a
text with respect to any linguistic unit into consideration. As a result, linguistic
motifs, originally called segment or sequence (Köhler 2006), a new unit for se-
quential analyses, was introduced. Linguistic motif, the longest continuous
sequence of equal or increasing values of linguist units, represents quantitative
properties of linguistic units and thus is useful for the comparison of author and
texts. Linguistic motif has four major types — L-motif, F-motif, P-motif and T-
motif, among which an L-motif refers to continuous series of equal or increasing
length values of morphs, word and so on. Köhler and Naumann (2008) calculat-
ed L-motif TTR of 66 poems and prose texts and fitted the values to Menzerath-
Altmann Law, and all cases had excellent fits.
In this paper, we will choose the original Hongloumeng and its two transla-
tions by David Hawkes and Yang Xianyi as the materials. L-motif TTR (Köhler
and Naumann 2008) is used to measure their vocabulary richness. Considering
the limitations and problems existing in previous research, we keep the follow-
ing research questions in mind:
Question 1. Was Hongloumeng, as many researchers suggest, written by two
authors? In other words, will L-motif TTR of the first 80 chapters be significantly
different from that of the last 40 chapters?
Question 2. If Hongloumeng was proven to be written by two authors, will
this difference be shown in the translation of one translator? More specifically,
will the difference of vocabulary richness exist in the translator’s work?
Question 3. According to the results of L-motif TTR we get from the first two
questions, that is, from the vocabulary richness point, can we evaluate the qual-
ity of different translations?
90 | Yu Fang
2 Materials and Method
Hongloumeng is one of the masterpieces of Chinese literature and one of the
Four Great Chinese Classical Novels. However, its author has been widely dis-
cussed and no agreement has been reached. Usually, people accept the idea
that it was written by two authors: Cao Xueqin wrote the first 80 chapters and
Gao E the rest chapters. Recently, some researchers have applied quantitative
method to this authorship identification. The statistical analysis of a literary text
justifies the traditional methodology to works which for a long time may have
received only impressionistic and subjective treatment. The most two repre-
sentative studies are carried out by Chen Bingzao in the University of Wisconsin
and Li Xianping in Fudan University. Calculating word-correlativity between
the first 80 chapters and the rest 40 chapters, Chen (1980) concluded that the
whole 120 chapters of Hongloumeng were written by the same author. While Li
(1987) extracted 47 function words and calculated their frequencies respectively
in each chapter. And then cluster analysis was used, which revealed that this
book was written by more than two authors.
Owing to the popularity of the original text, many translators have tried to
introduce it to other countries. Until now, there are nine complete or selective
English translations (Chen and Jiang 2003), and two of them are widely accept-
ed: The Story of the Stone translated by a British sinologist David Hawkes and
his son-in-law John Minford; A Dream of Red Mansions translated by a Chinese
translator Yang Xianyi and his wife Gladys Yang. To reduce the workload while
still reach the aim of the study, we randomly selected 15 chapters (Chapter 4, 12,
16, 24, 28, 36, 40, 44, 48, 52, 64, 68, 72, 76, 80) out of the first 80 chapters la-
beled as A and also 15 chapters (Chapter 82, 84, 88, 90, 92, 94, 96, 100, 104, 108,
110, 112, 114, 116, 118) out of the rest 40 chapters labeled as B.
To measure the vocabulary richness of Hongloumeng and also its transla-
tions, type-token ratio (TTR) is selected from many available indexes. TTR, indi-
cating the relationship “between the total number of running words in a corpus
and the number of different words used” (Olohan 2004: 80), is an important
indicator of vocabulary richness. It can reflect the writing style of an author for
“the writer has available a certain stock of words, some of which he/she may
favor more than others” (Holmes 1994: 91). Thus it turns out to be a quite power-
ful indicator for authorship identification.
Analogous to L-motif of words or other linguistic units, L-motif TTR refers to
continuous series of equal or increasing length values of different words. To get
the values of L-motif TTR, we need to output the number of different words N
times in an N-word text. For example, if we have a text as “Today is Sunday, and
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 91
we don’t need to go to school on Sunday.” The text has 13 words, so the L-motif
TTR is like:
token type
1 1
2 2
3 3
4 4
5 5
6 6
7 7
8 8
9 9
10 9
11 10
12 11
13 11
Herdan (1964) observed a near linear relationship between the size of vo-
cabulary V (types) and the total number of word tokens N in a text:
V= αN
β
(1)
Altmann (1980) gave a theoretical derivation of model (1), which is “the most
commonly used one in linguistics” (Köhler & Naumann, 2008: 642):
y= x a
(2)
Just like model (1), x represents the number of tokens, which is N, and y repre-
sents the number of types, which is V. And a is an empirical parameter. Model
(2) is expected to work with L-motif TTR (Köhler and Naumann 2008: 642).
Before getting values of L-motif TTR, word segmentation needs to be done
because Chinese, unlike English, has no space between each word. Segtag1
,
developed by Professor Shi Dongxiao in Xiamen University, is applied to the
original text of Hongloumeng. After the automatic segmentation, we check and
justify some results for accuracy. In both Chinese and English texts, punctua-
tion is deleted before measuring L-motif TTR.
||
1 It can be accessed from http://vdisk.w�i��.���/s/�������Vv���V.
92 | Yu Fang
NLREG2
, as a powerful statistical analysis program that can perform non re-
gression analysis, is used in this study and it can determine the values of pa-
rameters in an equation whose form the user specifies. In this study, values of L-
motif TTR of each chapter will firstly be calculated by a self-built program, and
we expect them to fit model (2). So NLREG will be applied to determine values of
parameter a.
After all the data are obtained, SPSS 20 is applied to carry out significance
analysis: values of parameter a in A and values of parameter a in B are com-
pared to discover whether Hongloumeng was written by different authors. Then
in the same way, p-values of parameter a in the two translations are tested. After
that, we will look at whether the results of translations are consistent with that
of the original text.
3 Results and Discussion
In this section, whether Hongloumeng has more than one author is tested first;
and then if the original text were written by two authors, how it showed in the
two translations; finally, we proceed to evaluate the quality of the two transla-
tions.
3.1 One or Two Authors in the Original Text
Using NRLEG, we can fit the values of L-motif TTR into model (2) and get the
values of parameter a. Due to the limitation of space, here we only take Chapter
44 as an example and Fig. 1 shows the excellent fit of this model to data from
this chapter.
||
2 It can be accessed from http://www.nlreg.com/.
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 93
Fig. 1: L-motif TTR of Chapter 44 in the original text
Goodness-of-fit was determined using the determination coefficient R2
, which
was 0.9792 in this case. Besides this one, other chapters all yielded excellent
fits. Tab. 1 shows the values of parameter a and R2
in all selected 30 chapters. As
we can see, R2
in nearly all chapters are above 0.9 and some are even above
0.98.
Tab. 1: The values of parameter a and R2
in the original text*
Chapter Parameter a R2
Chapter Parameter a R2
4 0.8846 0.9832 82 0.8481 0.9820
12 0.8771 0.9926 84 0.8470 0.9568
16 0.8807 0.9606 88 0.8455 0.9744
24 0.8464 0.9578 90 0.8475 0.9823
28 0.8438 0.9763 92 0.8464 0.9944
36 0.8535 0.9699 94 0.8446 0.9468
40 0.8424 0.9747 96 0.8504 0.9153
44 0.8459 0.9792 100 0.8584 0.9533
48 0.8579 0.9663 104 0.8497 0.9780
52 0.8518 0.9675 108 0.8464 0.8941
64 0.8643 0.9506 110 0.8515 0.9276
68 0.8577 0.9112 112 0.8455 0.9859
72 0.8499 0.9459 114 0.8595 0.9911
76 0.8661 0.9812 116 0.8567 0.9692
94 | Yu Fang
Chapter Parameter a R2
Chapter Parameter a R2
80 0.8684 0.9922 118 0.8522 0.9856
*The figures in the table are rounded to four decimal places
According to Tab. 1, the curves of those values are produced and are shown in
Fig. 2.
Fig. 2: The values of parameter a in the original text
Fig. 2 shows that the dash line is smoother than the full line, which means val-
ues of parameter a in the 15 chapters selected from the rest 40 chapters do not
differ from each other. This finding can also be verified by the exact data: values
of parameter a in A fluctuate between 0.84 and 0.89, while values in B fluctuate
between 0.84 and 0.86. We can also find that the full line is overall above the
dash line, though the divergence is not so wide. Therefore, a significance test is
needed for further discussion. On the basis of the results we have so far, we
propose the following hypothesis:
H0: The values of parameter a in A is not significantly different from that in B.
H1: The values of parameter a in A is significantly different from that in B.
Since both A and B have only 15 values, normal distribution tests are needed.
One sample Kolmogorov-Smirnov test shows values of parameter a in the two
groups are normally distributed: pA=0.726>0.05, pB=0.904>0.05, which ensures
0.82
0.83
0.84
0.85
0.86
0.87
0.88
0.89
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
parameter a
parameter a in A parameter a in B
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 95
the rationality of the independent sample t-test: t(28) = 2.509, p=0.018<0.05. The
result means that we have to reject H0 and the values of parameter in A is signif-
icantly different from that in B. To put it in another way, word usage in the two
parts is significantly different from each other: since t-value is positive, the
author tends to use more varied words in the first 80 chapters than that in the
last 40 chapters. From this point of view, we might conclude that the first 80
chapters and the rest 40 chapters are likely written by different authors.
3.2 How One Translator Presents a Work Written by Two
Authors
If Hongloumeng was written by different authors, we will further explore wheth-
er different vocabulary richness is shown in the two translations.
3.2.1 Hawkes’ translation
Just like the above part, we also use NRLEG to fit the values of L-motif TTR of
Hawkes’ translation into model (2). Fig. 3 shows the result of Chapter 44 in
Hawkes’ translation.
Fig. 3: L-motif TTR of Chapter 44 in Hawkes’ translation
96 | Yu Fang
The data is almost matched with the empirical curve, which is proven by the
determination coefficient R2
— 0.9778 in this case. Values of R2
reach above 0.90
in all selected 30 chapters, whose values are shown in Tab. 2.
Tab. 2: The values of parameter a and R2
in Hawkes’ translation*
Chapter Parameter a R2
Chapter Parameter a R2
4 0.8593 0.9745 82 0.8563 0.9539
12 0.8626 0.9879 84 0.8556 0.9537
16 0.8496 0.9677 88 0.8518 0.9726
24 0.8340 0.9577 90 0.8447 0.9678
28 0.8345 0.9777 92 0.8544 0.9829
36 0.8439 0.9669 94 0.8443 0.9716
40 0.8437 0.9804 96 0.8460 0.9346
44 0.8469 0.9778 100 0.8551 0.9668
48 0.8389 0.9758 104 0.8521 0.9704
52 0.8389 0.9744 108 0.8390 0.9389
64 0.8376 0.9546 110 0.8417 0.9486
68 0.8357 0.9519 112 0.8448 0.9649
72 0.8270 0.9060 114 0.8585 0.9875
76 0.8485 0.9879 116 0.8586 0.9693
80 0.8446 0.9882 118 0.8448 0.9719
*The figures in the table are rounded to four decimal places.
The curves of parameter a in this translation version are shown in Fig. 4.
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 97
Fig. 4: The values of parameter a in Hawkes’ translation
In Hawkes’ translation, both the full line and the dash line are smooth: values
of parameter a in A fluctuate between 0.82 and 0.86, while values in B fluctuate
between 0.83 and 0.86. However, contrary to the original text, the dash line is
overall above the full line in this version.
The values of parameter a in the two groups are also proven to be normally
distributed by one sample Kolmogorov-Smirnov test: pA=0.947>0.05,
pB=0.652>0.05. Then the independent sample t-test is carried out: t(28) = -2.286,
p=0.030<0.05. The result conforms to that in the original text and the values of
parameter in the first selected 15 chapters are significantly different from that in
the rest selected 15 chapters. Still, we should notice that t-value in this test is
negative, which means the translator tends to use more varied words in the last
40 chapters than that in the first 80 chapters. Of course, we can assume that
Hawkes has noticed the differences of word usage between the two parts and
showed such differences deliberately in his translation, but this needs clarifica-
tion in specific context.
3.2.2 Yang’s translation
Repeating the process of the above part, we use NRLEG to fit the values of L-
motif TTR in Yang’s translation. Fig. 5 shows the fitting result of Chapter 44 of
this version.
0.8
0.81
0.82
0.83
0.84
0.85
0.86
0.87
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
parameter a
parameter a in A parameter a in B
98 | Yu Fang
Fig. 5: L-motif TTR of Chapter 44 in Yang’s translation
Very similar to Hawkes’ translation, the determination coefficient R2
was 0.9779
in this case and reached above 0.92 in all selected 30 chapters, whose values are
shown in Tab. 3.
Tab. 3: The values of parameter a and R2
in Yang’s translation*
Chapter Parameter a R2
Chapter Parameter a R2
4 0.8575 0.9681 82 0.8514 0.9603
12 0.8740 0.9799 84 0.8431 0.9493
16 0.8599 0.9693 88 0.8530 0.9596
24 0.8471 0.9531 90 0.8502 0.9630
28 0.8498 0.9669 92 0.8506 0.9705
36 0.8595 0.9531 94 0.8425 0.9590
40 0.8494 0.9688 96 0.8505 0.9444
44 0.8490 0.9779 100 0.8555 0.9571
48 0.8523 0.9838 104 0.8514 0.9622
52 0.8503 0.9673 108 0.8458 0.9648
64 0.8459 0.9483 110 0.8480 0.9443
68 0.8428 0.9340 112 0.8495 0.9612
72 0.8450 0.9265 114 0.8579 0.9749
76 0.8526 0.9813 116 0.8583 0.9615
80 0.8574 0.9785 118 0.8480 0.9699
*The figures in the table are rounded to four decimal places
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 99
According to Tab. 3, we produce the curves of those values, which are shown in
Fig. 6.
Fig. 6: The values of parameter a in Yang’s translation
We can see that except the highest point, other points fluctuate around 0.85 for
both lines. In Yang’s translation, the largest value of parameter a in A is 0.8740
and the smallest value is 0.8428, while the largest value of parameter a in B is
0.8583 and the smallest value is 0.8425. Unlike data in Hawkes’ translation, the
two lines are intertwined with each other. In other words, the mean of two
groups may be equal.
After a normal distribution is confirmed: pA=0.726>0.05, pB=0.904>0.05, the
independent sample t-test is carried out: t(28) = 1.038, p=0.308>0.05. Different
from Hawkes’ translation, the values of parameter a in the two parts of Yang’s
translation have no significant difference, which is also different from the result
in the original text.
Getting L-motif TTR of all selected 30 chapters, we fit its values into model
(2) and carry out significance tests of parameter a. The test shows that the first
80 chapters and the rest 40 chapters were written by different authors,
which conforms to the result in most previous studies. The two translations
behave differently: the two parts of Hawkes’ translation show a significant
difference in vocabulary richness, which does not show in Yang’s transla-
tion. When different translation versions of the same text are provided, readers
usually tend to choose a better one to read, thus an evaluation of the quality of
translations is needed.
0.82
0.83
0.84
0.85
0.86
0.87
0.88
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
parameter a
parameter a in A parameter a in B
100 | Yu Fang
3.3 Evaluating the Quality of Translations
From the above sections, we can know that fitting values of L-motif TTR into
model (2) yields the parameter a, which has a direct link with vocabulary rich-
ness: the larger the parameter a, the richer the vocabulary richness is. Using
parameter a to measure vocabulary richness, can we offer a suggestion for eval-
uating the quality of the two translated Hongloumeng? Since the vocabulary
richness of two parts in Hawkes’ translation has significant difference, we will
also compare the two translations in two parts.
According to the data we get above, the curves of parameter a in the two
parts are shown in Fig. 7 and 8 respectively.
Fig. 7: Comparison of the values of parameter a in A
0.8
0.82
0.84
0.86
0.88
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
parameter a
parameter a in A of Hawkes' translation
parameter a in A of Yang's translation
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 101
Fig. 8: Comparison of the values of parameter a in B
In Fig. 7, we can see that except parameter a in Chapter 1, the values of Hawkes
version are lower than that of Yang’s version in other chapters. However, the
difference seems not so wide in B: from Fig. 8, we cannot judge which version’s
parameter a is larger. Till now, another significance test is needed.
H0: The vocabulary richness of the two translations is the same.
H1: The vocabulary richness of the two translations is significant different.
The independent sample t-test is conducted and a significant difference be-
tween Hawkes’ translation and Yang’s translation in the first selected 15 chap-
ters is found: t (28) = -3.067, p=0.005<0.05. But there is no significant difference
in the rest selected 15 chapters: p=0.789. Therefore, we will only compare the
first 15 chapters of the two translations in the following part.
Since the vocabulary richness in Hawkes’ translation is lower than that in
Yang’s translation in the first selected 15 chapters, we can guess Hawkes over-
used some words, which may decrease the readability of his translation, but at
the same time, make it more easily for readers to understand. To get a better
understanding, we put words into their specific context. For the wordlists are
long, here we only choose words whose frequencies are among Top 50, which is
shown in AppendixⅠ.
The two translations share almost the same words in the Top 50 wordlists
and most of them are function words like articles, prepositions and conjunc-
0.825
0.83
0.835
0.84
0.845
0.85
0.855
0.86
0.865
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
parameter a
parameter a in B of Hawkes' translation
parameter a in B of Yang's translation
102 | Yu Fang
tions. Besides function words, personal pronouns also have high frequencies,
which implies that the work is mainly about human activities and both transla-
tors prefer personal pronouns to characters’ name. Moreover, “her” and “she”
have higher frequencies than other personal pronouns, which means there are
more female characters than male characters in this novel. In addition, one
name is listed in both Top 50 wordlists: “Bao-yu” occurring 46 times in Hawkes’
translation and “Pao-yu” occurring 31 times per 10000 words in Yang’s transla-
tion, is the main character in this story. “Jia”, referring to one family, is also
frequently used, from which we can infer that the story happened in this family.
Among notional words, “said” attracts our attention. It ranks No. 51 in
Yang’s translation with the frequency of 30 times per 10000 words. However,
this word occurs 81 times per 10000 words in Hawkes version, nearly three
times higher. The usage of verb “said” can be classified into several types: first-
ly, it implies the meaning of “explained, suggested, protested” that refers to
speech content; secondly, it has the meaning of “exclaimed, shouted, cried”
that indicates speech act; thirdly, it means “asked, replied, told” that shows
word essence and fourthly, it can be used the same as “chuckled, smiled,
laughed” that conveys “said” meaning indirectly. Considering the high fre-
quency of “said” in Hawkes’ translation, we assume that he tended to use this
word in all possible situations. From pragmatics point of view, word meaning is
not abstract in the actual use of the language, so context is important in the
study of word usage. This is “especially true in literature for authors, who often
wield the weapon of word, to achieve their desired intention and effect” (Fang
and Liu 2015b: 78). In the following, several examples are selected to illustrate
the word usage.
e.g 1 (from Chapter 40)
The original text: 只见丰儿带了刘姥姥板儿进来，道：“大奶奶倒忙的紧。”
Hawkes’ version: Xi-feng’s maid Felicity, accompanied by Grannie Lin and little Ban-er,
arrived while they were in the midest of this activity. "You are very busy, Mrs. Zhu, " said
Grannie Liu.
Yang’s version: While she was doing this, Fenger arrived with Granny Liu and Baner.
"How busy you are, madam!" remarked Granny Liu.
This dialogue is extracted from Chapter 40, a dialogue between Granny Liu and
Li Wan. Meeting with each other in the morning, Grannie Liu said first, so these
words can be regarded as a kind of greeting. Here Hawkes translated the word
dao into “said Grannie Liu”, while Yang translated it as “remarked Granny Liu”.
Although “said” is not special in this situation, but “remaked”, which means
“to give a spoken statement of an opinion or thought”, is also not a good choice.
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 103
In this sense, Hawkes’ choice is more proper. And if some improvements can be
made, “Granny Liu greeting with these words” may be a better solution.
e.g 2 (from Chapter 72)
The original text: 鸳鸯因悄问: “你奶奶这两日是怎么了? 我看他懒懒的。” 平儿见问,因房
内无人,便叹道: “他这懒懒的也不止今日了,这有一月之前便是这样…”
Hawkes’ version: “Tell me, what’s been the matter with your mistress during these past
few days?” said Faithful. ‘She seems so lethargic.” Patience sighed. “It’s not just these last
few days,” she said, having first looked round to make sure that no one else was within
hearing.
Yang’s version: “What’s the matter with your mistress these days?” Yuanyang asked in a
low voice. “I’ve noticed she seems very listless.” As they were alone Pinger sighed, “She’s
been this way for some time, at least a month…”
This is a private conversation between Pinger and Yuanyang, who talked about
Xifeng’s illness. The original text used “悄问” to indicate Yuanyang’s low voice.
Hawkes translated it as “said Faithful”, which cannot convey such private situa-
tion, while Yang’s translation as “Yuanyang asked in a low voice” is better. The
reply of Pinger is described as “叹道” in the original text, which is translated
into two parts in Hawkes’ translation: “Patience sighed” and “she said”. And
Yang only translated as “sighed”, whose meaning is defined as “to breathe out
slowly and noisily, expressing tiredness, sadness, pleasure, etc.” and can be
used together with speech content, so it is appropriate to use it in this situation.
The features of word usage are not only embodied in Top 50 wordlist, but
also in some unfrequently used words, especially in those culture-loaded words.
e.g. 3 Amida (from Chapter 20)
The original text: 这一辈子我自然比不上你。我只保佑着明儿得一个咬舌的林姐夫，时
时刻刻你可听`爱'`厄'去。阿弥陀佛，那才现在我眼里！
Hawkes’ version: All I can say it’s that I hope you marry a lisping husband, so that you
have “ithee-withee” “ithee-withee” in your ears every minute of the day. Ah, Holy
Name! I think I can see that blessed day already before my eyes!
Yang’s version: I just pray that you’ll marry a husband who talks like me, so that you
hear nothing but‘love’ the whole day long. Amida Buddha! May I live to see that day!
These words were said by Xiangyun, who cannot pronounce “二” in Chinese
accurately and thus was laughed at by Daiyu. Frank and outspoken, she said
those words to rebut Daiyu. “阿弥陀佛” (Amida) in this sentence is a culture-
based term in Buddhism and Yang translated it as “Amida Buddha”, while
Hawkes translated it as “Holy Name”. It is easy for Chinese readers to under-
stand the meaning behind this term because most of them are familiar with
Buddhism. However, since the target readers of English version are English
104 | Yu Fang
readers, who are more familiar with Christianity, Hawkes replaced it by “Holy
Name”.
e.g. 4 (from Chapter 40)
The original text: 织女牛郎会七夕。
Hawkes’ version: On Seventh Night the lovers meet in heaven.
Yang’s version: The Weaving Maid and Cowherd meet in Heaven.
The Lady Dowager held a party in Grand View Garden and the extended families
played a drinking game in which everyone was required to say something relat-
ed to keywords given by the host. Aunt Xue got the keyword of “seven”, so she
blurted out the Chinese legend of “织女” and “牛郎” who are lovers but could
only meet with each other in 7th
, July of Chinese lunar calendar. Yang translated
the two names as “Weaving Maid” and “Cowherd” literally and did not give any
figure in the translation, so only readers with some Chinese culture background
could know the relationship between the two characters and know the figure
behind the sentence. But Hawkes just used “lovers” instead of the two names
and explained the date exactly, which is more suitable for English readers.
We evaluate the quality of the two translations from two aspects: the most
frequently used words and some unfrequently used words and reach the follow-
ing conclusions. Firstly, to express the whole story, the two translators share
most words as shown in Top 50 wordlists especially. Secondly, more varied
words are found in Yang’s translation and among them the usage of “said”
stands as a good example, but improvement could be made in both translations
concerning word choice. Thirdly, in translating culture-loaded words, Yang is
inclined to keep the original style as closely as possible, which may cause in-
comprehensibility for English readers; while Hawkes tries to find equal words in
Western culture, which may lose some information of the original text. Such
difference could be explained by the translators’ background: Hawkes, a fa-
mous British sinologist who had studied in Oxford and Beijing University, was
familiar with Chinese traditional culture and his aim is to make Hongloumeng
understandable for most Western readers. While Yang Xianyi, exposed to Chi-
nese culture since he was born and went to Oxford for British literature study,
must have a deeper understanding toward the classic than Hawkes and his aim
is to spread Chinese culture to the Western world.
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 105
4 Conclusion
Based on the above analyses, we came to conclusions corresponding to the
three research questions posed in the introduction section.
(1) The original text of Hongloumeng, as many researchers suggest, was
written by two authors. Fitting L-motif TTR into model (2), we get the values of
parameter a in these 30 selected chapters. Independent sample t-test shows
there is significant difference in the first 15 selected chapters and the rest 15
selected chapters and the overall values in the first part are larger than the sec-
ond part, which indicates the first part has higher vocabulary richness than the
second part.
(2) The two translations present this feature differently: significant differ-
ence has been found in the two parts of Hawkes’ translation through independ-
ent sample t-test, though the overall values in the first part are smaller than the
second part; there is no significant difference in the two parts of Yang’s transla-
tion. This result indicates translators’ subjectivity in handling the difference of
vocabulary richness in original text.
(3) On the basis of parameter a getting from L-motif TTR, we have tried to
evaluate the quality of different translations: (a) Top 50 wordlists show the two
translations share most words, which indicates that both translators choose
nearly the same word to express the story. (b) Yang used more varied words to
avoid repetition, while Hawkes chose more simplified words, which are shown
through the verb “said”. (c) In translating culture-loaded words, different strat-
egies are applied: Hawkes aims to ease readers’ burden and tries to find equal
words in Western culture, while Yang is eager to spread Chinese culture to other
countries and translates those word literally.
In the paper, we have tried to clarify the authorship identification of
Hongloumeng and explore whether the difference in the original text shown in
its translations. Some conclusions do come out after the investigation. This
study, however, still has its limitations. Firstly, due to the restrictions of time
and space, we only choose 30 chapters out of the whole book, which may affect
the accuracy of results. For further research, larger corpora could be built. Sec-
ondly, we have found that the significant difference in the two parts of Hawkes’
translation is not the same as the original text and no significant difference has
been found in Yang’s translation, but we did not offer reasons. Maybe we can
find a better way to carry out this task in the future.
106 | Yu Fang
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188.
Reference
Argamon, S., & Levitan, S. (2005). Measuring the usefulness of function words for authorship
attribution. Paper presented at Proceedings of ACH/ALLC Conference 2005, British Co-
lumbia..
Baker, M. (2000). Towards a methodology for investigating the style of a literary translator.
Target, 12(2), 241–266.
Chen, H. & Jiang, F. (2003). The translation of Hong Lou Meng into English: A descriptive study.
Chinese Translators Journal, 5, 46–52.
Coyotl-Morales, R. M., Villaseñor-Pineda, L., Montes-y-Gómez, M., & Rosso, P. (2006). Author-
ship attribution using word sequences. In Progress in Pattern Recognition, Image Analysis
and Applications (pp. 844–853). Heidelberg/New York: Springer Berlin Heidelberg.
Fang, Y & Liu H.T. (2015a). Comparison of vocabulary richness in two translated Hongloumeng.
Glottometrics, 31, 54–75.
Fang, Y & Liu H.T. (2015b). Probability distribution of interlingual lexical divergences in Chinese
and English: 道 (dao) and said in Hongloumeng. Glottometrics, 32, 63–87.
Holmes, D. I. (1994). Authorship attribution. Computers and the Humanities, 28(2), 87-106.
Herdan, G. (1964). Quantitative Linguistics. London: Butterworths.
Juola, P., & Baayen, R. H. (2005). A controlled-corpus experiment in authorship identification
by cross-entropy. Literary and Linguistic Computing, 20(Suppl), 59–67.
Iqbal, F., Khan, L. A., Fung, B., & Debbabi, M. (2010). E-mail authorship verification for forensic
investigation. In Proceedings of the 2010 ACM Symposium on Applied Computing (pp.
1591–1598). New York, NY: ACM.
Khmelev, D. V. (2000). Disputed authorship resolution through using relative empirical entropy
for markov chains of letters in human language texts. Journal of quantitative linguistics,
7(3), 201–207.
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In J. Genzor &
M. Bucková (Eds.), Favete linguis. Studies in honour of Victor Krupa (pp. 145-152). Brati-
slava: Slovak Academic Press.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F-and T-segments. In C.
Preisach, H. Burkhardt, L. Schmidt-Thieme & R. Decker (Eds.), Data Analysis, Machine
Learning and Applications (pp. 637–645). Berlin &Heidelberg: Springer.
Koppel, M., & Schler, J. (2004). Authorship verification as a one-class classification problem. In
Proceedings of the twenty-first international conference on Machine learning (pp. 62-68).
New York, NY: ACM.
Labbé, D. (2007). Experiments on authorship attribution by intertextual distance in English.
Journal of Quantitative Linguistics, 14(1), 33–80.
Lefevere A. (1992). Translation, Rewriting, and the Manipulation of Literary Fame. London and
New York: Routledge.
L-motif TTR for Authorship Identification in Hongloumeng and Its Translation | 107
Li X. (1987). A new study on Hongloumeng. Journal of Fudan University (social science edition),
(5):3–16．
Olahan, M. (2004). Introducing Corpora in Translation Studies. London: Routledge.
Yule, G. U. (1944). The Statistical Study of Literary Vocabulary. Cambridge, UK: Cambridge
University Press.
Appendix
Tab. 4: Top 50 words in the wordlist*
Rank
Hawkes Yang
Word Frequency Standard
frequency
Word Frequency Standard frequency
1 the 4 798 413 the 2 099 396
2 to 3 961 341 to 1 806 340
3 and 3 168 273 and 1 445 272
4 of 2 536 218 a 993 187
5 a 2 312 199 you 964 182
6 you 2 057 177 of 905 171
7 I 1 815 156 I 847 160
8 it 1 661 143 he 694 131
9 that 1 575 136 her 672 127
10 in 1 562 135 in 652 123
11 her 1 525 131 she 626 118
12 she 1 328 114 it 579 109
13 was 1 251 108 that 565 106
14 he 1 214 105 for 538 101
15 for 1 079 93 was 511 96
16 said 941 81 his 435 82
17 with 937 81 this 412 78
18 on 891 77 with 406 77
19 had 883 76 had 396 75
20 his 735 63 on 378 71
21 as 696 60 as 343 65
22 be 663 57 but 325 61
23 have 656 57 him 296 56
24 Jia 653 56 so 293 55
108 | Yu Fang
Rank
Hawkes Yang
Word Frequency Standard
frequency
Word Frequency Standard frequency
25 this 598 52 me 281 53
26 but 590 51 if 273 51
27 at 586 50 they 264 50
28 him 584 50 be 257 48
29 is 558 48 all 255 48
30 all 539 46 Jia 252 47
31 Bao-yu 531 46 have 237 45
32 they 516 44 is 233 44
33 if 513 44 at 220 41
34 so 508 44 out 219 41
35 me 506 44 your 213 40
36 what 466 40 up 209 39
37 them 457 39 not 206 39
38 when 449 39 we 199 38
39 about 441 38 when 191 36
40 one 432 37 what 191 36
41 out 429 37 can 190 36
42 up 419 36 now 189 36
43 from 407 35 from 188 35
44 there 403 35 no 183 34
45 now 399 34 my 179 34
46 not 399 34 then 177 33
47 by 397 34 by 174 33
48 your 372 32 one 173 33
49 were 369 32 lady 171 32
50 we 367 32 Pao-yu 165 31
*standard frequency = (frequency/token)*10000, all figures in the table are
rounded to the nearest integer.
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 109
Wei Huang
Length Motifs of Words in Traditional and
Simplified Chinese Scripts
Abstract: Based on the operational definition of word length motif, the present
study mainly investigates the rank frequency distribution and the length distri-
bution of word length motifs in Chinese texts. Results show that the rank fre-
quency distribution of word length motifs in both of simplified and traditional
Chinese texts can be fitted by the power law function � = ���
, and the parame-
ter a of this function in simplified texts is larger than that in traditional texts
while the parameter b in simplified texts is smaller than that in traditional texts.
The length distribution of word length in Chinese texts follows the Hyper-Pascal
model. The findings in this article indicate that the simplified and traditional
scripts are writing systems with a same self-organization principle and different
parameters.
Keywords: word length motif, Chinese character, simplification, traditional
Chinese, simplified Chinese
1 Introduction
Linguistic motifs are introduced to investigate the syntagmatic properties in
sequence of linguistic units and linguistic properties. Length motifs of word can
be immediately counted with characters in alphabetic scripts and with syllables
in syllabic scripts. Köhler (2015) has pointed out that motif is also helpful to
Chinese script if syllables are counted. Chen & Liang (2016) found that, if the
word length of Chinese is measured with syllables, the rank-frequency distribu-
tions of word length motifs can be fitted with power law function � = ���
, and
that the length distributions of word length motifs can be fitted with the Hyper-
Pascal function.
However, Chinese script is neither an alphabetic system nor a syllabic sys-
tem, and the syllable is formally not immediate constituents of Chinese charac-
ters. There is another method for defining word length in Chinese texts. As one
||
Wei Huang: Faculty of Linguistic Sciences, Language and Culture University, Beijing, China,
huangwei@blcu.edu.cn
110 | Wei Huang
of conclusions of the word length study on Chinese by Chen and Liu (2016), it is
the component which is the most appropriate word length measurement unit in
written Chinese.
In this article, we tend to construct and explore the word length motif in
Chinese script based on the construction theory of Chinese characters (Wang
2015), in which one character can be deconstructed into one or more compo-
nents, and components consist of one or more strokes. That is to say, the word
length motifs of Chinese script can be counted in numbers of components or
strokes. Based on this definition, we try to verify the hypothesis whether the
rank-frequency distribution and the length distribution of word length motifs
follow any laws. Since the definition of word length motif is related to structural
form of Chinese characters, does the simplification of Chinese script effect the
behaviour of word length motifs? In other words, is there some law in the rank-
frequency distribution or length distribution of word length motifs in Chinese
scripts? Do the traditional Chinese characters and simplified Chinese characters
differ significantly?
To operationally define the word length motifs with the structural unit of
Chinese scripts, the structure of Chinese character is briefly introduced in sec-
tion 2. This section also includes the definition of word length motifs of Chinese
words and the illustration of sample texts used in this study. Then the rank
frequency distribution and length distribution of word length motifs are pre-
sented and discussed in section 3, which is followed by a conclusion in section
4.
2 Materials and Method
2.1 Structural Complexity of Chinese Characters
The structural complexity of word can be simply defined as word length in al-
phabetic or syllabic scripts. Nevertheless, the complexity of Chinese characters
cannot be determined by syllable because any Chinese character (not word) has
only one syllable. In Chinese philology, a component is the immediate constitu-
ent of Chinese characters, and strokes comprise components. Then the structur-
al complexity of Chinese characters can be measured in two ways, one is the
numbers of strokes a Chinese character has, and the other is the number of
components it has, if the hierarchical relationship among components and the
connection of strokes are ignored.
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 111
For example, as illustrated in Fig.1 (a), the Chinese character ‘日’ (/ʐi/, sun
or day) has 4 strokes, and consists only one component (itself) from the view-
point of construction theory of Chinese character. A more complicated character
‘鬼’ (/kuei/, ghost) consists of only one component, however 9 strokes, as
shown in Fig.1 (b). The next two characters ‘春’ (/tʂuən/,spring) and ‘很’
(/xən/,very) each have 9 strokes and both of them consists of 2 components (‘’
and ‘日’, ‘彳’ and ‘艮’) (Fig.1 (c) and (d)). However, the structural relation-
ships between components in ‘春’ and ‘很’ differ. The former one is top-
bottom framework, while the latter is left-right framework. One more complex
case is illustrated in Fig.1 (e). The character ‘语’ (/y/,language) also has 9
strokes but consists 3 components, ‘讠’, ‘五’ and ‘口’. And there is a
hierarchical structure in the deconstruction of this character, it is ‘讠’ and
‘吾’ as left-right framework in the first level, and then ‘五’ and ‘口’ as
top-bottom framework in the second level. As a matter of fact, the hierarchical
relationship among components is more sophisticated in observation of thou-
sands of Chinese characters. As a starting stage in this article, the complexity of
Chinese character is limited to the length of character measured in the number
of components or strokes.
Fig. 1: Writing order of five Chinese characters
Moreover, the above examples do not involve the differences between simplified
Chinese character and traditional Chinese character. In general, the traditional
characters are more complex than the simplified characters from the structural
angle of view. The last example ‘语’ in Fig.1 (e) will be ‘語’ in traditional
writing system. The only difference between these two characters is the simpli-
fication of ‘訁’ to ‘讠’. Then the traditional character ‘語’ consists of 3
components and has 14 strokes. As to the first four example characters in Fig.1,
112 | Wei Huang
the simplified writings are the same to the traditional ones. In the 3,500 com-
mon Chinese characters, about 30% characters have different writing forms in
the traditional script system and simplified script system.
2.2 Definition of Word Length Motif
According to the general definition of motif, the complexity motif of Chinese
characters is a continuous series of equal or increasing numbers of components
or strokes. For instance, if the script complexity motif is defined with compo-
nents, the two sentences written in simplified Chinese characters
“春节越来越近了。”
(Spring Festival is coming.)
and
“假日里很多人都会选择参加旅游团出门旅游放松身心。”
(Many people choose to travel to relax by joining in a tour group.)
are represented by two sequences of script complexity motifs with length of 4
and 8 as
(2-2-3)(2-3)(2)(1)
and
(4)(1-1-2-2)(1-3-3-3-3-3)(2-3-4)(2-2)(1-3-4)(2-3)(1-1).
The punctuation is ignored and the motif does not cross sentences.
Based on the above definition of complexity motifs of Chinese characters,
the word length motif is defined as a continuous series of equal or increasing
numbers of components or strokes in sequence of word length. The above ex-
ample sentences can be segmented into word sequences as
“春节 越来越 近 了”
and
“假日 里 很多 人 都 会 选择 参加 旅游团 出门 旅游 放松 身 心” .
Then the word length sequences measured in components are
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 113
4-8-2-1
and
5-1-4-1-3-3-6-5-9-3-7-5-1-1.
Then the word length motifs measured in components are
(4-8)(2)(1)
and
(5)(1-4)(1-3-3)(6)(5-9)(3-7)(5)(1-1).
If the two example sentences above are written in traditional characters as
“春節 越來越 近 了”
and
“假日 裡 很多 人 都 會 選擇 參加 旅遊團 出門 旅遊 放鬆 身 心” ,
the word length motifs measured in components are
(5-8)(2)(1)
and
(5)(2-4)(1-3-4-10)(7-10)(3-7-7)(1-1).
2.3 Text Collections
To investigate the rank-frequency distribution of word length motifs measured
in components, the length distribution of this kind of motifs, and the difference
between simplified scripts and traditional scripts, 2 text collections have been
built. The simplified collection comprises 20 written texts from People’s Daily,
which are the most important official newspapers in China. There are 10 news
commentary texts and 10 news report texts. The length of text measured in Chi-
nese character tokens, as shown in Tab. 1, ranges from 835 to 1285, and 23591 in
total. The traditional collection is a transcription of the simplified one. The sim-
plified script in each text, which is segmented into word sequences by using an
automated segmentation tool first and then manually corrected, have been
114 | Wei Huang
automatically transcribed into traditional script by using a computer program1
.
Thus the two collections can be regarded as two versions with different coding
methods of same meanings.
Then the word length motifs measured in components or strokes have been
automatically formed and counted by computer programs. The statistical prop-
erties of the rank-frequency distribution and length distribution have been ex-
plored by using Altmann-Fitter and NLREG
Tab. 1: Character tokens and word tokens of simplified and traditional texts
Simplified text Traditional text Genre Character tokens Word tokens
NC01-S NC01-T News comment 1411 777
NC02-S NC02-T News comment 1283 717
NC03-S NC03-T News comment 1301 729
NC04-S NC04-T News comment 1046 583
NC05-S NC05-T News comment 1136 629
NC06-S NC06-T News comment 1731 945
NC07-S NC07-T News comment 1100 608
NC08-S NC08-T News comment 1106 652
NC09-S NC09-T News comment 1232 709
NC10-S NC10-T News comment 1213 662
NR01-S NR01-T News report 1264 689
NR02-S NR02-T News report 1082 631
NR03-S NR03-T News report 1378 784
NR04-S NR04-T News report 1169 630
NR05-S NR05-T News report 965 507
NR06-S NR06-T News report 855 451
NR07-S NR07-T News report 1032 571
NR08-S NR08-T News report 1285 696
NR09-S NR09-T News report 976 501
NR10-S NR10-T News report 1026 568
Total 23591 13039
||
1 http://jf.cloudtranslation.cc/s2t.html.
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 115
3 Results and Discussion
3.1 The Rank Frequency Distribution of Word Length Motifs
The types and tokens of word length motifs of texts in the two collections are
presented in Tab.2. There is an obvious trend that both the types and tokens of
word length motifs in traditional texts are more than those in simplified texts.
This can be explained by that in the traditional script the inventory of word
length measured in components numbers and the average word length are larg-
er than those in simplified script, which results in a more variegated situation in
motif construction.
The fittings of the power law function � = ���
to the rank frequency distri-
bution data of word length motifs in the 40 texts have been executed. As illus-
trated in Fig.2, the power law function � = ���������������
fits the data of text
NC01-S which is presented in Tab. 8 in Appendix with a goodness of fit
��
= ������, while the function � = ���������������
fits the data of text NC01-T
presented in Tab. 9 in Appendix with ��
= ������. Tab.3 presents the fitting
results including goodness of fit and estimated parameters for all 40 texts. The
smallest ��
is 0.8112 in the fitting of text NR06-T. Most of the fittings are good.
Tab. 2: Types and tokens of word length motifs in simplified and traditional texts
Text Word length
motif types
Word length
motif tokens
Text Word length
motif types
Word length
motif tokens
NC01-S 136 323 NC01-T 149 347
NC02-S 126 286 NC02-T 138 309
NC03-S 110 305 NC03-T 124 332
NC04-S 113 243 NC04-T 116 256
NC05-S 110 287 NC05-T 124 293
NC06-S 152 397 NC06-T 164 425
NC07-S 102 264 NC07-T 126 270
NC08-S 103 280 NC08-T 120 285
NC09-S 118 309 NC09-T 138 321
NC10-S 114 281 NC10-T 126 289
NR01-S 116 301 NR01-T 127 320
NR02-S 116 282 NR02-T 123 301
NR03-S 146 339 NR03-T 156 356
116 | Wei Huang
Text Word length
motif types
Word length
motif tokens
Text Word length
motif types
Word length
motif tokens
NR04-S 109 267 NR04-T 117 290
NR05-S 99 224 NR05-T 108 228
NR06-S 85 198 NR06-T 94 206
NR07-S 114 251 NR07-T 126 262
NR08-S 144 307 NR08-T 147 328
NR09-S 93 233 NR09-T 112 238
NR10-S 106 251 NR10-T 118 260
Fig. 2: Fitting the power law function � = ���
to the data in Tab. 8 and Tab. 9 in Appendix
Tab. 3: Fitting results of power law function to the rank frequency distribution data in 40 texts
Text a b R2
Text a b R2
NC01-S 25.5538 -0.6553 0.9353 NC01-T 29.6074 -0.6892 0.9207
NC02-S 24.0327 -0.6739 0.9723 NC02-T 24.4750 -0.6547 0.9125
NC03-S 30.9806 -0.7059 0.9392 NC03-T 23.8989 -0.5991 0.8501
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 117
Text a b R2
Text a b R2
NC04-S 21.8246 -0.6787 0.9332 NC04-T 22.9481 -0.6877 0.9500
NC05-S 27.4533 -0.6910 0.9276 NC05-T 24.5368 -0.6678 0.9549
NC06-S 35.2202 -0.7033 0.9518 NC06-T 36.8436 -0.7041 0.9616
NC07-S 25.9210 -0.6980 0.9732 NC07-T 21.4917 -0.6581 0.9653
NC08-S 26.6382 -0.6836 0.9534 NC08-T 21.4920 -0.6236 0.9264
NC09-S 27.9860 -0.6768 0.9366 NC09-T 24.0700 -0.6377 0.9351
NC10-S 27.4722 -0.6987 0.9286 NC10-T 24.0115 -0.6557 0.9034
NR01-S 34.8297 -0.7670 0.9176 NR01-T 33.4988 -0.7359 0.9105
NR02-S 29.7844 -0.7415 0.9694 NR02-T 25.1604 -0.6525 0.9103
NR03-S 30.6794 -0.7108 0.9657 NR03-T 30.5598 -0.7039 0.9587
NR04-S 30.0606 -0.7603 0.9780 NR04-T 33.8980 -0.7806 0.9697
NR05-S 18.5857 -0.6210 0.8653 NR05-T 16.7627 -0.5975 0.8918
NR06-S 21.4034 -0.7041 0.9395 NR06-T 17.0723 -0.6102 0.8112
NR07-S 21.5529 -0.6657 0.9359 NR07-T 23.5812 -0.7108 0.9846
NR08-S 25.3171 -0.6933 0.9828 NR08-T 25.1014 -0.6589 0.9425
NR09-S 23.4808 -0.7061 0.9833 NR09-T 23.8575 -0.7368 0.9755
NR10-S 27.0468 -0.7378 0.9751 NR10-T 19.4866 -0.6168 0.9190
M 26.79 -0.6986 M 25.12 -0.6691
SD 4.36 0.0348 SD 5.33 0.0495
The paired-sample t-test shows that the difference of parameter a between sim-
plified texts (M = �����, SD = ����) and traditional texts (M = �����, SD = ����)
reaches significance (�(��) = �����, p = �����), and the difference of parameter b
between simplified texts (M = −������, SD = ������) and traditional texts
(M = −������, SD = ������) is also significant (�(��) = �����, p = �����). Both
the parameter a and the parameter b of the fitting function differ significantly
between simplified texts and traditional texts. The parameter a in simplified
texts is larger than that in traditional texts, however, the parameter b in simpli-
fied texts is smaller than that in traditional texts.
Hence, it can be summarized that although the structural complexity of
simplified Chinese characters is smaller than that of traditional Chinese charac-
ters in general, the simplification of character structure does not affect the sta-
tistical characteristics in syntagmatic behaviour of texts. The simplified and
traditional scripts are writing systems with same self-organization principle and
different parameters.
118 | Wei Huang
3.2 The Length Distribution of Word Length Motifs
The length distribution data of word length motifs can be derived from the rank
frequency distributions of word length motifs. For example, the length distribu-
tion of word length motifs in text NC01-S and NC01-T are presented in the first
two columns of Tab.4 (a) and (b) respectively.
According to previous studies, the Hyper-Pascal model can be used to de-
scribe the length distribution of word length motif (Köhler, 2006; Köhler &
Naumann, 2008). To verify the universality of this hypothesis, we fit the Hyper-
Pascal model to the length distribution data of word length motifs in the texts of
our two collections by using the Altmann-Fitter. As illustrated in Tab.4 (a) and
Tab.4 (b), the Hyper-Pascal model fit the data of text NC01-S and NC01-T very
well. The fitting results can also be seen in Fig.3 and Fig.4.
With all of the 40 texts in simplified and traditional Chinese scripts, the Hy-
per-Pascal model fits the length distribution of word length motifs very well, as
presented in Tab. 10 in Appendix. The three parameters (k, m and q) of Hyper-
Pascal model in the fitting results are shown in Tab.5.
Tab. 4: Length distribution of word length motifs of text NC01-S and NC01-T
Motif Length Frequency NP[i] Motif Length Frequency NP[i]
1 76 83.25 1 101 102.55
2 114 112.48 2 136 134.34
3 83 70.81 3 66 63.17
4 36 34.02 4 28 27.38
5 9 14.15 5 11 11.51
6 2 5.38 6 1 4.76
7 2 1.92 7 2 1.95
8 0 0.66 8 0 0.79
9 1 0.32 9 1 0.32
10 1 0.22
χ2
= 6.87 χ2
= 3.52
p(χ2
) = 0.0762 p(χ2
) = 0.4756
C = 0.0213 C = 0.0101
DF = 3 DF = 4
R2
= 0.9343 R2
= 0.9987
k = 2.7747 k = 0.3049
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 119
Motif Length Frequency NP[i] Motif Length Frequency NP[i]
m = 0.5209 m = 0.0915
q = 0.2537 q = 0.3933
(a) NC01-S (b) NC01-T
Fig. 3: Fitting the Hyper-Pascal model to the length distribution of word length motifs in text
NC01-S
120 | Wei Huang
Fig. 4: Fitting the Hyper-Pascal model to the length distribution of word length motifs in text
NC01-T
Tab. 5: Parameters of Hyper-Pascal model in fitting to length distribution of word length motifs
TxtID k m q TxtID k m q
NC01-S 2.7747 0.5209 0.2537 NC01-T 0.3049 0.0915 0.3933
NC02-S 4.3971 0.8523 0.2397 NC02-T 0.7317 0.1942 0.3626
NC03-S 0.1850 0.0502 0.4331 NC03-T 0.0607 0.0124 0.3730
NC04-S 0.5440 0.1470 0.3995 NC04-T 1.0176 0.2098 0.3099
NC05-S 0.3574 0.0587 0.3141 NC05-T 0.6119 0.0981 0.2783
NC06-S 1.2863 0.2884 0.3221 NC06-T 0.7710 0.1776 0.3280
NC07-S 0.4440 0.0989 0.3628 NC07-T 0.5409 0.1069 0.3322
NC08-S 1.1609 0.2333 0.3077 NC08-T 1.4512 0.2495 0.2695
NC09-S 0.5495 0.1090 0.3413 NC09-T 1.0195 0.1688 0.2719
NC10-S 0.8979 0.2111 0.3550 NC10-T 1.5226 0.3068 0.2851
NR01-S 8.7507 1.3239 0.1515 NR01-T 1.9629 0.5282 0.2752
NR02-S 0.0317 0.0099 0.4311 NR02-T 0.5650 0.1619 0.3307
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 121
TxtID k m q TxtID k m q
NR03-S 18.6884 1.5861 0.0859 NR03-T 1.5236 0.3730 0.2848
NR04-S 2.5127 0.6326 0.2856 NR04-T 1.8349 0.5207 0.2899
NR05-S 0.6394 0.1624 0.3501 NR05-T 0.0783 0.0221 0.4158
NR06-S 0.4763 0.1231 0.3759 NR06-T 0.4076 0.1289 0.3763
NR07-S 1.4679 0.3404 0.2962 NR07-T 0.3033 0.0782 0.3623
NR08-S 0.9452 0.1844 0.3077 NR08-T 1.0637 0.1780 0.2528
NR09-S 0.4241 0.0960 0.3276 NR09-T 1.1250 0.2303 0.2636
NR10-S 0.0268 0.0075 0.4327 NR10-T 0.0564 0.0116 0.3725
M 2.3280 0.3518 0.3187 M 0.8476 0.1924 0.3214
SD 4.2392 0.4256 0.0860 SD 0.5722 0.1433 0.0485
122 | Wei Huang
Fig. 5: Parameter k, m and q of Hyper-Pascal model in fitting to length distribution of word
length motifs
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 123
Although there are several outliers of parameter k as illustrated in Fig.5(a) and
also in parameter m as shown in Fig.5(b), according to the paired-sample t-test,
there is no significant difference between simplified texts (M = ������, SD =
������) and traditional texts (M = ������, SD = ������) on the parameter k of
Hyper-Pascal model in fitting to length distribution of word length (�(��) = �����,
p = ����� ). Besides, neither the parameter m, which has a mean value
M = ������ (SD = ������) in simplified texts and M = ������ (SD = ������) in
traditional texts that derives a �(��) = ����� and p = �����, nor the parameter q,
which has a mean value M = ������ (SD = ������) in simplified texts and
M = ������ (SD = ������) in traditional texts that derives a �(��) = −����� and
p = �����, differs significantly between the two collections. Thus we can con-
clude that the length distribution of word length motifs measured with number
of components in Chinese texts follows the Hyper-Pascal model, and that there
is no significant difference between simplified and traditional Chinese scripts
on the length distribution of word length motifs.
Now that the Hyper-Pascal model can fit the length distribution of word
length motifs in both simplified and traditional texts, and the 3 parameters in
the model do not behave differently in simplified and traditional collections,
what is the difference between simplified and traditional scripts on the word
length motifs measured with the number of components? It sounds unreasona-
ble that although the definition of word length motif is related to the structural
complexity of characters, the simplification of Chinese scripts does not affect
the statistical characteristics of word length motif. We went a step further by
computing the differences of 3 parameters of simplified and traditional samples.
The second column in Tab.6 is the number of different characters between the
simplified and traditional text pair, and the last 3 columns are respectively the
difference of parameter k, m and q defined as parameter in simplified samples
minus that in traditional samples. The relationship of number of traditional
characters and the 3 parameters is computed with the Pearson correlation,
which is shown in Tab.7. The difference of parameter m is significantly related
to the number of traditional characters with r = �����, p = ����� (2 tailed). Thus
we have a reason to hypothesize that the length of a text and the number of
traditional Chinese characters in the text may affect the statistical characteris-
tics of the length distribution of word length motifs. This hypothesis needs to be
tested with more multifarious texts in the future.
124 | Wei Huang
Tab. 6: Differences on parameters between simplified and traditional text pairs
Text pair Number of different
characters
Difference of
parameter k
Difference of
parameter m
Difference of
parameter q
NC01-S/NC01-T 531 2.4698 0.4294 -0.1396
NC02-S/NC02-T 477 3.6654 0.6581 -0.1229
NC03-S/NC03-T 425 0.1243 0.0378 0.0601
NC04-S/NC04-T 369 -0.4736 -0.0628 0.0896
NC05-S/NC05-T 349 -0.2545 -0.0394 0.0358
NC06-S/NC06-T 515 0.5153 0.1108 -0.0059
NC07-S/NC07-T 367 -0.0969 -0.0080 0.0306
NC08-S/NC08-T 340 -0.2903 -0.0162 0.0382
NC09-S/NC09-T 460 -0.4700 -0.0598 0.0694
NC10-S/NC10-T 434 -0.6247 -0.0957 0.0699
NR01-S/NR01-T 448 6.7878 0.7957 -0.1237
NR02-S/NR02-T 373 -0.5333 -0.1520 0.1004
NR03-S/NR03-T 486 17.1648 1.2131 -0.1989
NR04-S/NR04-T 431 0.6778 0.1119 -0.0043
NR05-S/NR05-T 298 0.5611 0.1403 -0.0657
NR06-S/NR06-T 218 0.0687 -0.0058 -0.0004
NR07-S/NR07-T 379 1.1646 0.2622 -0.0661
NR08-S/NR08-T 408 -0.1185 0.0064 0.0549
NR09-S/NR09-T 362 -0.7009 -0.1343 0.0640
NR10-S/NR10-T 370 -0.0296 -0.0041 0.0602
Tab. 7: Pearson correlations of number of traditional characters and differences of parameters
Number of
traditional
characters
Difference of
parameter k
Difference of
parameter m
Difference of
parameter q
number of
traditional
characters
Correlation 1 .405 .491* -.399
Sig. (2-tailed) .077 .028 .081
N 20 20 20 20
*. Correlation is significant at the 0.05 level (2-tailed).
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 125
4 Conclusion
In this article the word length motif is operationally defined with the number of
components which is the immediate constituent in Chinese scripts. The rank
frequency distribution and the length distribution of word length motifs have
been investigated. The preliminary findings are that, (1) the rank frequency
distribution of word length motifs can be fitted by the power law function
� = ���
, and the parameter a of this function in simplified texts is larger than
that in traditional texts while the parameter b in simplified texts is smaller than
that in traditional texts; and (2) the length distribution of word length in Chi-
nese texts follows the Hyper-Pascal model, however, there is no significant
difference between simplified and traditional Chinese script on the length dis-
tribution of word length motifs.
What limits the current research also promises future studies. On the one
hand, the length of sample texts in this study is small. The maximum is no more
than eight hundreds of words. Besides, the genre of sample texts is restricted to
news report and news review for the sake of homogeneity of the samples. On the
other hand, the traditional collection used in this study is not originally tradi-
tional scripts but transcription from simplified texts word by word. Thus, the
following questions should be answered by enhancing the diversity of sample
texts in the future. How about the rank frequency distribution and the length
distribution of word length motifs in long Chinese texts? How about it in other
genres of Chinese texts? And how about it in the originally traditional Chinese
scripts such as texts from Hong Kong and Taiwan where the traditional Chinese
is widely used today or in the texts of classical Chinese literature?
Acknowledgement
This research was supported, in part, by the National Social Science Foundation
of China under Grant No. 13CYY022, and by the China Postdoctoral Science
Foundation under Grant No. 2016M590529.
126 | Wei Huang
References
Chen, H., & Liu, H. (2016). How to measure word length in spoken and written Chinese. Journal
of Quantitative Linguistics, 23 (1), 5–29.
Chen, H., & Liang, J. (2016). Chinese word length motif and its evolution. (This volume)
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In: G. Jozef &
B. Martina (Eds.), Favetelinguis. Studies in honour of Viktor Krupa (pp. 145–152). Bras-
tislava: Slovak Academic Press.
Köhler, R. (2015). Linguistic motifs. In: G.K. Mikros & J. Mačutek (Eds.), Sequences in Language
and Text (pp. 89–108). Berlin, Boston: de Gruyter.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In: C.
Preisach, H. Burkhardt, L. Schmidt-Thieme & R. Decker (Eds.), Data Analysis, Machine
Learning and Applications (pp. 635–646). Berlin, Heidelberg: Springer.
Wang, N. (2015). Hanzi Gouxingxue Daolun (Introduction to construction theory of Chinese
character). Beijing: The commercial Press.
Appendix
Tab. 8: The rank frequency distributions of word length motifs in text NC01-S
Rank Motif Frequency
1 4 21
2 5 18
3 6 16
4 3-6 14
5 3 12
6 3-5 11
7 4-4 8
8 3-7 7
9 4-5 6
10 2-6 6
11 3-4-6 6
12 3-5-5 5
13 4-6 5
14 1-3 4
15 2-4-6 4
16 3-3-4 4
Rank Motif Frequency
17 2 4
18 3-8 4
19 6-7 3
20 6-6 3
21 7 3
22 4-8 3
23 3-4 3
24 1-3-7 3
25 5-5 3
26 3-6-6 3
27 1-5 3
28 2-8 3
29 5-6-6 3
30 2-4 3
31 3-9 2
32 5-7 2
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 127
Rank Motif Frequency
33 2-3-6 2
34 3-4-6-8 2
35 1-3-6 2
36 4-5-8 2
37 1-6 2
38 2-7 2
39 1-1-5 2
40 3-3-3-5 2
41 3-3-5 2
42 3-10 2
43 3-6-7 2
44 5-5-5 2
45 5-9 2
46 1-2-4 2
47 2-5-6 2
48 2-3-7 2
49 3-4-6-6-6 2
50 3-3-6 2
51 3-4-5 2
52 4-4-5-7 2
53 7-7-7-7 2
54 3-3 2
55 3-6-6-8 2
56 3-6-8 2
57 4-6-7 1
58 2-3-4 1
59 4-9 1
60 3-4-7-7 1
61 1-5-5-5-5 1
62 2-3-6-6-6 1
63 6-8 1
64 2-3-3-4 1
65 2-3-4-6-6-6-7 1
66 3-8-8-8 1
67 1-3-4 1
Rank Motif Frequency
68 1-3-3-4 1
69 9 1
70 2-6-7 1
71 3-3-9 1
72 1-1-1-3-3-5 1
73 1-2-2-3 1
74 2-3-5 1
75 2-2-7 1
76 1-3-5 1
77 2-3-4-4-5-6 1
78 1-5-6 1
79 1-3-3-6 1
80 2-6-6-7 1
81 3-5-8 1
82 1-4 1
83 4-7-7 1
84 1-1-1-4 1
85 1-3-6-6 1
86 1-2-5 1
87 2-3-13 1
88 3-5-6 1
89 1-2-5-6-6 1
90 2-7-8 1
91 8 1
92 3-3-3-6 1
93 3-7-7 1
94 3-3-3-7 1
95 2-5 1
96 2-2-5-5 1
97 1-3-9 1
98 2-4-5-5 1
99 3-6-9 1
100 4-4-4 1
101 2-8-8 1
102 3-3-8 1
128 | Wei Huang
Rank Motif Frequency
103 2-3-4-5 1
104 3-4-4-4 1
105 5-6 1
106 2-3-4-6-7 1
107 5-5-8 1
108 3-5-5-6 1
109 4-4-5-5-6 1
110 5-8 1
111 2-5-7 1
112 1-2-2-2-3-3-3-4-5 1
113 3-5-5-7 1
114 2-4-4-7 1
115 3-3-6-7 1
116 4-5-6 1
117 2-5-6-8 1
118 3-4-7 1
119 1-4-8 1
Rank Motif Frequency
120 1-1 1
121 3-3-5-6-10 1
122 4-5-5-6 1
123 3-3-4-6 1
124 4-4-4-5 1
125 3-4-4-5-5-5-6 1
126 4-5-6-6-8 1
127 1-2 1
128 4-4-6-8 1
129 1-2-2-6 1
130 2-2 1
131 3-4-4 1
132 3-5-6-7 1
133 1-7 1
134 1-4-6 1
135 4-7 1
136 2-4-5 1
Tab. 9: The rank frequency distributions of word length motifs in text NC01-T
Rank Motif Frequency
1 6 24
2 7 21
3 4 19
4 5 18
5 3-7 13
6 3 10
7 3-6 8
8 4-7 7
9 3-8 7
10 6-8 7
11 6-7 6
12 3-5 5
Rank Motif Frequency
13 5-6 4
14 2-6 4
15 5-8 4
16 2-7 4
17 2-8 4
18 8 4
19 4-5 4
20 7-7 4
21 3-10 4
22 6-11 3
23 3-4 3
24 2-5 3
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 129
Rank Motif Frequency
25 1-3 3
26 1-7 3
27 4-4 3
28 6-6 3
29 3-5-8 2
30 3-3-11 2
31 7-10 2
32 2-7-7 2
33 3-3-6 2
34 4-5-8 2
35 3-5-5 2
36 1-7-7 2
37 5-7 2
38 2-11 2
39 4-6 2
40 3-5-5-7 2
41 3-11 2
42 1-3-7 2
43 5-5 2
44 7-7-11 2
45 1-5 2
46 9 2
47 4-6-7 2
48 2 2
49 5-5-6 2
50 3-3-5 2
51 2-3-7 2
52 3-5-6 2
53 1-9 1
54 6-7-10 1
55 5-8-8 1
56 3-4-4-6 1
57 2-7-8 1
58 4-4-5-8 1
59 4-11 1
Rank Motif Frequency
60 6-7-7 1
61 4-9 1
62 1-3-3-4 1
63 1-5-7 1
64 1-1-4-5 1
65 1-2-2-2-3-3-
3-4-5
1
66 3-3-5-6-10 1
67 4-4-4-6 1
68 3-6-6 1
69 6-10 1
70 7-7-8 1
71 2-4-6-8 1
72 4-5-6 1
73 5-6-6-8 1
74 2-6-6-10 1
75 3-4-9-9 1
76 6-9 1
77 2-3-4 1
78 3-4-4-7 1
79 3-3-3-8 1
80 3-9 1
81 5-7-7-8 1
82 2-4-7-7 1
83 8-10 1
84 1-4-6 1
85 5-10 1
86 1-3-7-10 1
87 1-2-2-7-7 1
88 1-2-4-5-6 1
89 3-6-7-7 1
90 3-3-6-11 1
91 4-5-5-8 1
92 1-2-5 1
93 2-4-6 1
94 3-3-4-6-7-7- 1
130 | Wei Huang
Rank Motif Frequency
7
95 6-6-7-7-10 1
96 3-4-7 1
97 6-7-11 1
98 1-4-8 1
99 7-9 1
100 1-1 1
101 1-6 1
102 2-5-7 1
103 2-2-7-7 1
104 2-6-8 1
105 2-6-6-7-9 1
106 1-1-1-3-3-8-
8-10-10-10
1
107 2-3-4-6 1
108 1-2-2-3-5 1
109 1-6-7 1
110 3-4-10 1
111 5-6-8 1
112 1-4-4-7 1
113 3-3-4 1
114 2-2-5-7-7-8 1
115 2-3-4-6-6-6-
11
1
116 8-8 1
117 1-1-6 1
118 3-5-8-9 1
119 4-6-8 1
120 4-8 1
121 2-2 1
122 4-6-7-8-9 1
123 2-5-7-8 1
124 5-5-5 1
125 1-3-6 1
126 2-4-5 1
127 3-4-5-6 1
Rank Motif Frequency
128 2-3-4-5-11 1
129 3-4-6 1
130 2-3-6 1
131 2-8-8 1
132 5-5-7-11 1
133 1-3-4-6-8 1
134 4-5-7-8-13 1
135 5-5-8-9 1
136 10 1
137 2-3-5 1
138 3-3-7 1
139 3-3-13 1
140 1-7-8 1
141 4-7-8 1
142 1-2 1
143 3-3-4-6-8 1
144 3-4-6-7 1
145 1-5-6 1
146 4-7-11 1
147 1-4 1
148 6-6-9 1
149 1-3-11 1
Length Motifs of Words in Traditional and Simplified Chinese Scripts | 131
Tab. 10: The results of fitting Hyper-Pascal model to the length distribution of word length
motif in simplified and traditional Chinese texts
TxtID χ2
P(χ2
) C DF R2
N x-max
NC01-S 6.87 0.0762 0.0213 3 0.9343 323 9
NC02-S 7.30 0.1209 0.0255 4 0.9793 286 8
NC03-S 2.83 0.5859 0.0093 4 0.9951 305 9
NC04-S 3.25 0.5173 0.0134 4 0.9958 243 9
NC05-S 2.12 0.3461 0.0074 2 0.9947 287 6
NC06-S 3.21 0.3602 0.0081 3 0.9924 397 7
NC07-S 1.14 0.7672 0.0043 3 0.9974 264 8
NC08-S 3.38 0.3361 0.0121 3 0.9860 280 7
NC09-S 2.18 0.5365 0.0070 3 0.9969 309 7
NC10-S 8.38 0.0151 0.0298 2 0.9862 281 6
NR01-S 13.00 0.0046 0.0432 3 0.9605 301 9
NR02-S 2.21 0.3311 0.0078 2 0.9945 282 6
NR03-S 9.58 0.0225 0.0283 3 0.9606 339 7
NR04-S 7.82 0.0499 0.0293 3 0.9775 267 10
NR05-S 0.66 0.8825 0.0029 3 0.9978 224 7
NR06-S 4.53 0.2099 0.0229 3 0.9911 198 7
NR07-S 2.54 0.4675 0.0101 3 0.9912 251 7
NR08-S 6.70 0.0821 0.0218 3 0.9916 307 7
NR09-S 0.75 0.6887 0.0032 2 0.9985 133 6
NR10-S 6.41 0.0931 0.0256 3 0.9901 251 7
NC01-T 3.52 0.4756 0.0101 4 0.9987 347 10
NC02-T 2.05 0.7272 0.0066 4 0.9976 309 8
NC03-T 2.86 0.2390 0.0086 2 0.9977 332 6
NC04-T 3.59 0.1663 0.0140 2 0.9919 256 6
NC05-T 2.90 0.2345 0.0099 2 0.9934 293 6
NC06-T 7.90 0.0193 0.0186 2 0.9958 425 6
NC07-T 1.83 0.9073 0.0068 3 0.9953 170 7
NC08-T 3.47 0.1761 0.0122 2 0.9877 285 6
NC09-T 5.38 0.0678 0.0168 2 0.9829 321 6
NC10-T 5.10 0.0779 0.0177 2 0.9894 289 6
NR01-T 7.09 0.0690 0.0222 3 0.9887 320 8
NR02-T 1.91 0.3839 0.0064 2 0.9984 301 6
132 | Wei Huang
TxtID χ2
P(χ2
) C DF R2
N x-max
NR03-T 4.35 0.2265 0.0122 3 0.9920 156 9
NR04-T 6.71 0.0819 0.0231 3 0.9394 290 8
NR05-T 5.45 0.1415 0.0239 3 0.9939 228 7
NR06-T 0.66 0.8830 0.0032 3 0.9983 206 7
NR07-T 1.65 0.4389 0.0063 2 0.9976 262 6
NR08-T 6.58 0.0372 0.0201 2 0.9923 328 6
NR09-T 1.58 0.2093 0.0066 1 0.9916 138 5
NR10-T 3.50 0.1735 0.0135 2 0.9965 260 6
Yingqi Jing, Haitao Liu
Dependency Distance Motifs in 21 Indo-
European Languages
Abstract: This paper applies the notion of linguistic motif to investigating the
linear arrangement of dependency distance (DD) in Indo-European and its im-
plicational meanings in language typology. A series of DD-motifs operating in a
decreasing, increasing or equal magnitude are introduced. We first describe the
frequency distribution of DD-motifs, and observe a preference for decreasing
DD-motifs in human languages. Moreover, we further investigate the role of DD-
motifs in controlling the syntactic complexity. The results show that serializing
DD values in the same order of magnitude can more or less restrict the structural
complexity, and it may be a useful method to realize the DD minimization in
natural languages. Finally, we explore the value of DD-motifs in language ty-
pology. Our classification experiments reveal that adding the harmonic property
and DD-motifs into dependency direction can improve the classification results.
Keywords: DD-motifs, Indo-European languages, structural complexity,
language typology
1 Introduction
One of essential traits of the speech is its linear nature, and each element is
produced in succession (Saussure 1960: 70). To understand the underlying syn-
tactic structure of a linear sentence, a tree diagram is conventionally adopted
(Mel'čuk 1988). According to Tesnière (1959: 19 [2015]), speaking a language
involves transforming structural order to linear order, whereas understanding a
language involves transforming linear order to structural order. The relation-
ship between linear and structural order is a central topic in formal syntax.
Dependency grammar provides a class of formal descriptions of how pairs
of words link in sentences via unequal syntactic relations (Mel'čuk 1988, Hud-
son 2007). With these syntactic dependencies, we can understand a sentence
||
Yingqi Jing: Department of Comparative Linguistics, University of Zurich, Zürich, Switzerland,
yingqi_jing@163.com
Haitao Liu: Department of Linguistics, Zhejiang University, Hangzhou, China, htliu@163.com
134 | Yingqi Jing – Haitao Liu
structure by attaching each unit onto a tree diagram, or generate an utterance
by projecting the syntactic structure on a timeline. So far, a wide range of evi-
dence suggests that the linear distance between syntactically linked units
(called “dependency distance”, DD) tends to be minimized in human languages
due to universal cognitive constraints that limit the language processing and
producing difficulty (Ferrer-i-Cancho 2004, Liu 2008, Gildea and Temperley
2010, Futrell et al. 2015). Many quantitative researches attribute the DD minimi-
zation either to the high proportion of adjacent dependencies, or to the rarity of
dependency crossing by comparing the natural languages with random lan-
guages (Liu 2008, Ferrer-i-Cancho 2013, 2014). Here we think that the randomiz-
ing process can also change the DD sequence, which may play certain role in
controlling the general structure complexity of languages. Moreover, the DD
also has a typological meaning, and Jing (2016) found that the DD can contrib-
ute to the improvement of classification effects in word order typology. To pro-
ceed, the current study attempts to see whether the sequential arrangement of
DD values can be embedded into the language classification.
Specifically, we concentrate on investigating the linear placement patterns
of DD and their implications for language typology. For instance, given the
statistical tendency for DD minimization, what kinds of DD sequence can signif-
icantly reduce the structural complexity of languages? Will the distribution of
DD sequence contribute to the improvement of language classification effects?
To answer these questions, the current study applied the notion of motifs to
invetigating the sequential arrangement of DD in Indo-European languages. The
concept of motifs was first put forward by Köhler and Naumann (2008, 2009,
2010) to refer to “the longest continuous sequence of equal or increasing values
representing a quantitative property of a linguistic unit”. This notion of linguis-
tic motifs was initially inspired by Boroda’s F-motiv in musical pieces (Boroda
1982), and can be defined for any linguistic unit (e.g., word, phrase, clause, etc.)
in any order (increasing, decreasing or equal).
Here we introduce a new concept called dependency distance motif (DD-
motif) to represent the continuous sequence of DD. In so doing, a number of
methodological issues or questions remain to be answered. First, due to the
differences between head-initial (HI) and head-final (HF) languages, which DD
operation, performing in an increasing, decreasing or equal magitude, best
faciliates the quantitative analysis of the DD-motifs? Second, will DD-motifs
following the same order of magnitude lessen the structural complexity of a
language? Finally, if continuous DD sequences are preferred, will the distribu-
tion of DD-motifs provide a better classification result in Indo-European?
Dependency Distance Motifs in 21 Indo-European Languages | 135
The rest of this manuscript will first introduce how we can calculate the
DD-motifs in a sentence and estimate their structural complexity. 21 Indo-
European language dependency treebanks from HArmonized Multi-LanguagE
Dependency Treebank (HamleDT) 2.0 are also introduced in this section (Zeman
et al. 2012). Section 3 presents the DD-motif results in Indo-European and dis-
cusses its value in language typology. The last section is the conclusion.
2 Methods and Materials
Dependency grammar recognizes words as basic syntactic elements, linked via
binary asymmetrical relations (Mel'čuk 1988, Hudson 2007, Liu 2009). In a de-
pendency graph (as in Fig. 1), the directed arcs often pointing from the head
word to its modifier indicate their dependency relations and directionality. If
the head word precedes its dependent in the word string, a HI structure will be
formed. Otherwise, it is HF when the head follows the dependent. Dryer (1992,
1996) have observed a harmonic head ordering in natural languages by describ-
ing the co-occurring word order features with the order of verb and object. Jing
(2016) found that HI or HF dependencies tend to cluster together in the linear
sequence. This harmonic property has become a central regularity in word order
typology.
Fig. 1: Dependency graph of a sample sentence
Here we focus on another key concept, dependency distance (DD), which
measures the linear distance between syntactically linked words in a sentence.
The notion of DD was first put forward by Heringer et al. (1980: 187) and defined
by Hudson (1995: 16) as “the distance between words and their parents”. To
understand or parse a sentence, a word is restored in the memory until it at-
taches with its head (Gibson 1998). The memory load increases with the length
of a dependency (Hudson 2010), and the DD of human languages is generally
considered to be minimized due to the constraints of working memory (Ferrer-i-
Cancho 2004, Liu 2008, Futrell et al. 2015). Liu (2008) proposed the mean de-
pendency distance (MDD) as a metric for measuring language processing diffi-
culty. This distance-based method for estimating structural complexity is widely
used, but it can only measure the distance between syntactically related units
136 | Yingqi Jing – Haitao Liu
without considering their head orderings or the sequences of DD values. The
present research attempts to investigate the linear arrangement of DD and its
influence upon the structural complexity of languages. We are seeking to know
whether serializing the DD sequences in the same order of magnitude can more
or less affects the syntactic complexity of natural languages.
To this end, we apply the definition of motifs from quantitative linguistics
to segmenting the DD sequence into continuous and dynamic units. This con-
cept is specifically designed to explore the sequential arrangement of any lin-
guistic elements in a text. A linguistic motif can be defined as “the longest con-
tinuous sequence of equal or increasing values representing a quantitative
property of a linguistic unit” (Köhler and Naumann 2008, 2009). But we can
also generate motifs in other ways, and Köhler and Naumann (2010) suggest
that it would be appropriate to go from the last item in the word string to the
first unit when analysing a left-branching/head-final language. Here we assume
the word order of a sentence going from left to right, and manipulate the order
of magnitude by cutting the DD sequence in three different ways: decreasing,
increasing, and equal magnitudes. These operations can not only consider the
variations between HI and HF languages, but also avoid double counting the
same dependencies. Thus, a decreasing DD-motif is a continuous series of re-
ducing DD values, and an increasing or equal DD-motif can be defined accord-
ingly. Moreover, we also segment the DD sequence with respect to the depend-
ency direction (HI or HF). A continuous sequence of DD with the same head
direction (HD) is grouped as a consistent HD-motif. It serves as a baseline for
comparing with the structural complexity of DD-motifs.
To be more specific, the DD sequences and DD-motifs of the above sample
sentence “The report is to be released today” can be formed in the following
way. We first split the sentence into two parts by the root word, since it has no
governor. Second, we can calculate the DD values for each word, and record
them sequentially within each domain. Note that we distinguish between the HI
and HF dependencies with the symbol -/+. Third, having obtained the DD se-
quences, we can further generate the DD-motifs according to a decreasing, in-
creasing or equal order. Likewise, we can form the consistent HD-motifs by
simply cutting the DD sequences in terms of the dependency direction.
Dependency Distance Motifs in 21 Indo-European Languages | 137
In the end, we can extract 5 decreasing DD-motifs, 5 increasing DD-motifs, 4
equal DD-motifs, and 4 consistent HD-motifs from the sentence in Fig. 1. But the
current study only focuses on investigating the distribution and structural com-
plexity of DD-motifs with at least two elements. Therefore, 1 decreasing DD-
motif, 1 increasing DD-motif, 2 equal DD-motifs, and 2 consistent HD-motifs
meet our requirements. We can even look at the dependency direction for each
DD-motif. For example, the decreasing DD-motif (-2, -1) is consistently HI, and
the increasing DD-motif (+1, -2) has a mixed head ordering.1
To estimate the structural complexity of the DD-motifs, we adopt the metric
of MDD from Liu (2008) and calculate the average value for DD-motifs. This
MDD of motifs = sum(DD-motifs)/n, length(DD-motif) >1 (2)
procedure can be expressed with formula (2).
In this formula, n represents the total number of elements in DD-motifs,
whose length is required to be more than 1 word. With this formula, the MDD of
consistent HD-motifs in the sample sentence is (1+1+2+1)/ 4= 1.25.
In order to conduct a comparative research on the distribution of DD-motifs
in natural languages, we selected 21 Indo-European language dependency tree-
banks from HamleDT 2.0. They include: 5 Slavic languages (Bulgarian, Czech,
Russian, Slovak, Slovenian), 5 Romance languages (Catalan, Spanish, Italian,
Portuguese, Romanian), 5 Germanic languages (Danish, German, English,
Dutch, Swedish), 3 Indo-Iranian languages (Bengali, Persian, Hindi), 2 Hellenic
languages (Greek, Ancient Greek), and 1 Italic language (Latin). These depend-
ency treebanks (or dependency conversions of other treebanks) have been har-
||
1 The term “consistent” or “mixed” in this study is used to describe the head direction of a
dependency. A consistently decreasing DD-motif denotes that all elements in the motif have
the same head ordering, and their DD values are arranged in a decreasing magnitude. A mixed
DD-motif refers to that those elements in the motif encompass different dependency directions.
DD sequences: (+1, +1) (-1, +1, -2, -1) (1)
Decreasing DD-motifs: (+1) (+1) (-1) (+1) (-2, -1)
Increasing DD-motifs: (+1) (+1) (-1) (+1, -2) (-1)
Equal DD-motifs: (+1, +1) (-1, +1) (-2) (-1)
Consistent HD-motifs: (+1,+1) (-1) (+1) (-2, -1)
138 | Yingqi Jing – Haitao Liu
monized to the Prague Dependency Treebank annotation style (Zeman et al.
2012). All punctuation and sentences with less than three words were rejected.
3 Results and Discussion
In this section, we begin our investigation of DD sequences by describing the
frequency distribution of varying DD-motifs in our sample languages, and we
further compare the MDDs of each type of motifs with that of consistent HD-
motifs. This exploration can reveal the linear placement patterns of DD in real
texts and their influences upon the structural complexity of human languages.
Furthermore, we will probe into the role of DD-motifs in language typology.
3.1 Preference for Decreasing DD-motifs
With these dependency treebanks, we computed the number of varying DD-
motifs with either consistent or mixed head ordering. We expect our operations
can not only reveal the linear regularities of DD but also confirm the tendency
for harmonic head ordering (Vennemann 1974, Hawkins 1994, Dryer 1992). This
suggests that we should maximize the amount of DD-motifs with the same de-
pendency direction and minimize the mixed DD-motif frequency.
Tab. 1 reports the frequencies of the DD-motifs in 21 Indo-European lan-
guages. We first take a look at the total number of DD-motifs in three different
orders: decreasing, increasing and equal. The largest figure among the three
types of DD-motifs is in boldface. We can see that 11 languages prefer the de-
creasing DD-motifs, which is slightly higher than that of increasing DD-motifs
(10 languages), and the amount of equal DD-motifs is always the least. It seems
that the DD sequences with the same magnitude are not preferred in real texts,
which is likely due to the selection for a flatter structure in a sentence (Hawkins
1994, Jing and Liu 2015). Because equal DD-motifs largely occur in a chain of
adjacent dependencies, which may in turn bring about deeper syntactic struc-
tures.
Next, we conduct a detailed comparison of DD-motifs with either consistent
or mixed head ordering between three groups. For numerals, decreasing DD-
motifs have the largest number of consistent dependencies in 20 languages,
while increasing DD-motifs owns the most mixed dependencies in 19 languages.
This suggests that arranging DD values in a decreasing magnitude can maximal-
ly satisfy the constraints of harmonic principle, and can have less mixed DD-
Dependency Distance Motifs in 21 Indo-European Languages | 139
motifs than operating in an increasing order. It is worth mentioning that though
equal DD-motifs can keep the mixed dependencies at the lowest level, the num-
ber of consistent dependencies is always the least among the three groups.
In addition, we compare the number of mixed DD-motifs with those of con-
sistent dependency direction within groups. The larger number is put in the
box, and we can find that for decreasing DD-motifs all our sample languages
except for Persian are inclined to have a consistent head ordering, whereas a
tendency for mixed DD-motifs is observed in 19 languages when serializing the
DD values in an increasing order. No significant difference between mixed and
consistent head ordering is reported in equal DD-motifs. Thus, based on the
previous analysis of the distribution of DD-motifs between groups and within
groups, we can conclude that human languages prefer a decreasing DD ar-
rangement, since the decreasing DD-motifs can to the most extent preserve the
consistent dependencies and restrict the mixed dependencies.
Intriguingly, a roughly complementary distribution of dependency direc-
tion between decreasing and increasing DD-motifs is also revealed in Tab. 1.
That means suppose a language tends to have consistently decreasing DD-
motifs, it is likely to encompass more mixed DD-motifs when performed in an
increasing order. Exceptionally, Bengali and Hindi, two completely HF lan-
guages, exhibit a consistent head ordering for DD-motifs in any order, and Per-
sian shows a preference for mixed DD-motifs in any order. Further investiga-
tions are needed.
Tab. 1: Frequencies of the DD-motifs in 21 Indo-European languages
Lang. Decreasing Increasing Equal
Con Mix Total Con Mix Total Con Mix Total
Bulgarian (bg) 28077 18344 46421 14794 30828 45622 11176 12696 23872
Czech (cs) 196084 147026 343110 103957 244511 348468 68275 82653 150928
Russian (ru) 74843 58219 133062 39860 91315 131175 33595 35936 69531
Slovak (sk) 117658 87587 205245 62820 140725 203545 40563 46102 86665
Slovenian (sl) 4533 3363 7896 2311 5152 7463 1190 1602 2792
Catalan (ca) 72314 45128 117442 37938 79225 117163 19380 29861 49241
Spanish (es) 79353 48940 128293 39917 89909 129826 21106 33749 54855
Italian (it) 12665 6220 18885 6752 11563 18315 5581 5101 10682
Portuguese (pt) 36110 19740 55850 18491 39822 58313 10547 17266 27813
140 | Yingqi Jing – Haitao Liu
Romanian (ro) 5190 2492 7682 2992 3091 6083 3354 1579 4933
Danish (da) 16141 8196 24337 9591 15872 25463 6090 4543 10633
German (de) 93662 74215 167877 50478 130213 180691 28840 30948 59788
English (en) 79275 48470 127745 35264 93529 128793 20279 26556 46835
Dutch (nl) 30346 15086 45432 14567 33873 48440 10628 10020 20648
Swedish (sv) 30983 19776 50759 18494 34837 53331 10265 10097 20362
Bengali (bn) 1385 159 1544 755 429 1184 498 61 559
Persian (fa) 18110 29529 47639 13976 27604 41580 10883 12974 23857
Hindi (hi) 51667 14200 65867 53575 13987 67562 33715 6740 40455
Greek (el) 10994 8656 19650 4569 15889 20458 2391 4396 6787
Ancient Greek (grc) 38639 34074 72713 26316 44823 71139 15389 11943 27332
Latin (la) 6901 6472 13373 5380 7180 12560 2881 2316 5197
3.2 Syntactic Complexity for DD-motifs
The preference for decreasing DD-motifs is evidenced by the biased frequency
distribution in various language texts. But another question arises: can the
linear arrangement of DD values in the same order of magnitude (DD-motif)
reduce the structural complexity of languages?
In formal syntax, a substantial evidence points to the universal constraints
of DD minimization in human languages (Ferrer-i-Cancho 2004, Liu 2008,
Gildea and Temperley 2010, Futrell et al. 2015). Many previous researches at-
tribute this universal property either to the high proportion of adjacent depend-
encies, or to the rarity of dependency crossing (Liu 2008, Ferrer-i-Cancho 2013,
2014). These two factors are obviously intertwined with each other, since the
shorter DD aligns with lower probability of crossing (Ferrer-i-Cancho and
Gómez-Rodríguez 2015). A recent study claims that chunking may play an im-
portant role in reducing DD (Lu et al. 2016).
These studies on DD minimization are typically conducted by comparing
the structure complexity of human languages with that of random languages.
Two algorithms, either randomizing dependency relations or changing word
order, are applicable in the literature (Liu and Hu 2008, Futrell et al. 2015). It is
revealed that natural languages have significantly shorter DD than random
languages. Here we argue that those algorithms can not only randomize the
linear distance of a dependency, but also alter the sequential arrangement or
rhythmic patterns of DD values. We think that comparing natural languages
Dependency Distance Motifs in 21 Indo-European Languages | 141
with random languages can be revealing in certain scenarios, but this paradigm
may also bring about some confounding variables. If the placement pattern of
DD values is one factor that contributes to the minimization of DD, how can we
simply attribute this property to the effects of projectivity or chunking before
the exclusion of DD arrangement? More importantly, it is possible to generate
random languages without changing the DD of a sentence. Fig. 2 and Fig. 3
construct two random dependency graphs of the sample sentence with the same
DD value, but it is hard to tell why these two dependency analyses are not pre-
ferred in natural languages. This suggests that the linear distance of a depend-
ency though tends to be minimized, yet the DD minimization is not optimal in
human languages (Gildea and Temperley 2010, Futrell et al. 2015) and is not the
only mechanism that shapes the complex structure of human languages.
One observable fact is that the two random languages owns different DD se-
quences with the natural languages, which leads to our hypothesis that the
linear arrangement of DD values may play a certain role in controlling the gen-
eral structural complexity of languages. Specifically, we wonder whether ar-
ranging DD values in the same order of magnitude can to some extent contrib-
ute to the DD minimization.
To examine our hypothesis, we evaluate the structural complexity of DD-
motifs by measuring their average values of DD (as in formula 2). We first calcu-
late the MDD of consistent HD-motifs, which can be seen as a baseline, since the
elements in a consistent HD-motif have the same dependency direction, and
their DD values are not placed in the same order of magnitude. After that, we
compare it with the MDDs of consistently decreasing, increasing or equal motifs
so that the effects of dependency direction can be constrained.
Fig. 2 Dependency graph of the sample sentence with random syntactic relations
Fig. 3: Dependency graph of the sample sentence with random word order
142 | Yingqi Jing – Haitao Liu
Fig. 4: MDD values for consistently increasing, decreasing or equal motifs
We are expecting that in some languages DD-motifs operating in the same order
of magnitude can have MDD values below the baseline.
Fig. 4 shows the average values for consistently increasing, decreasing or
equal DD-motifs. The step line represents the baseline value of consistent HD-
motifs. We can see that consistently equal DD-motifs in all 21 languages exhibit
significantly lower MDD values than the baseline. It is largely caused by the fact
that most consistently equal DD-motifs exist in a chain of adjacent dependen-
cies, because their average DD values fluctuate between 1 and 1.5. Moreover,
compared with increasing DD sequence, the decreasing DD-motifs have more
capability to reduce the structural complexity, since we have observed 6 lan-
guages with lower MDD values for consistently decreasing motifs than the base-
line, while only 1 language (Latin) shows a lower average value for consistently
increasing DD-motifs. This result can be further confirmed by a comparison
between the MDDs of varying motifs. 18 languages except for Bengali, Hindi and
Latin have lower MDD of consistently decreasing motifs than that of consistent-
ly increasing motifs. This finding also echoes the previous distributional prefer-
ence for decreasing DD-motifs, which we believe is probably the side-effect of
the principle of least effort, since human languages favor the sequential ar-
rangement with lower structural complexity (Zipf 1949).
Therefore, we can draw the following conclusion: the DD-motifs can more
or less contribute to reducing the structural complexity of natural languages
and serializing the DD values in the same order of magnitude may be a useful
method to realize the DD minimization.
Dependency Distance Motifs in 21 Indo-European Languages | 143
3.3 Classification Effects for the Skewness of DD-motifs
The above discussion explored the linear arrangement patterns of DD values
and their influences upon the general structural complexity of languages. In
this part, we are seeking to investigate the role of DD-motifs in language typolo-
gy.
Modern word order typology is largely concerned with depicting and classi-
fying languages in terms of the placement of certain types of dependencies, say
the order of verb and object (Greenberg 1963, Lehmann 1973, 1978, Vennemann
1974). Tesnière (1959: 32) distinguished between HI and HF languages according
to the direction of linearization, and Liu (2010) adopted 20 large-scale depend-
ency treebanks to classify languages by their skewness of dependency direction.
Jing (2016) probed into the possibility to improve the language classification
results by setting the DD as the weight of dependency direction.
These researches have outlined a new dependency treebank-based method
to investigate the distribution of dependency direction in human languages, but
this approach assumes that each dependency (HI or HF) is independent to each
other, disregarding the harmonic property of certain dependencies and the
sequential arrangement of DD values. We consider that this model may ignore
syntagmatic information in language typology.
To optimize the metric of dependency direction, we proceed by incorporat-
ing the property of harmonic head ordering and DD placement patterns into our
method. We are trying to see whether the skewness of consistent HD-motifs or
the distribution of DD-motifs placing in the same order of magnitude can im-
prove the classification results in Indo-European languages.
We first compare our classification by the skewness of consistent HD-motifs
in Fig. 6 with Jing’s (2016) classification result in Fig. 5. We can find that two
Hellenic languages (Greek, Ancient Greek) are put together on the continuum,
though they have shown a huge difference in the dependency direction, which
may indicate the word order change from Ancient Greek to modern Greek (Tay-
lor 1994, Liu and Xu 2012). But if we focus on the skewness of continuous HI or
HF dependencies, these two languages seem to preserve the similar harmonic
head ordering magnitude in the linear sequence. Also, we have witnessed a
closer placement of Bulgarian to its genealogical relative, Russian. This poten
144 | Yingqi Jing – Haitao Liu
Fig. 5: Language classification result with the skewness of dependency direction (from Jing
2016)
Fig. 6: Language classification with the skewness of consistent HD-motifs
tial shift illustrates that adding the harmonic property into dependency direc-
tion can be a useful way to enhance the language classification effects. Notice
that Danish is wrongly clustered with Romance languages, which is partly due
to its special annotation by setting the determiner as the head of a noun phrase.
Dependency Distance Motifs in 21 Indo-European Languages | 145
Fig. 7: Language classification with the skewness of consistently decreasing DD-motifs
The normalization process in HamleDT still preserves some of this attribute.
In order to reveal the significance of DD sequence in language typology, we
also conducted detailed classifications with the skewness of consistently de-
creasing, increasing or equal DD-motifs. There are 3! = 6 combinations for the
three types of DD-motifs. The optimal classification result is observed for the
skewness of decreasing DD-motifs. As shown in Fig. 7, arranging DD sequences
in a decreasing order can have a better performance in distinguishing between
Romance and Slavic, even though dependency direction often does not work
well in classifying languages with free word order (Liu 2010). This improvement
we believe can be attributed to the function of consistent DD sequence in the
same order of magnitude, since the new metric can give full consideration to the
three characters: dependency direction, harmonic property, and sequential
arrangement of DD. Interestingly, all these precious metrics work poorly in
differentiating between Slavic and Germanic languages. Further researches are
necessary to resolve this thorny issue.
146 | Yingqi Jing – Haitao Liu
4 Conclusion
The current study has conducted a large-scale quantitative analysis of the DD-
motifs in Indo-European languages. We concentrate on depicting the frequency
distribution and structural complexity of DD-motifs in real texts. We also inves-
tigate the value of this unit in language typology. Some major findings are
summarized here.
First of all, having described the frequency distribution of three types of
DD-motifs (decreasing, increasing or equal) in Indo-European, we find that the
segmentation of DD sequence in a decreasing magnitude can maximally pre-
serve the consistent dependencies and effectively control the mixed dependen-
cies. A general preference for decreasing DD-motifs is also observed. Secondly,
we further explore the effects of DD-motifs in controlling the MDD of natural
languages. By comparing with the MDD of consistent HD-motifs, we have shown
that arranging the DD values in a same order of magnitude can more or less ease
the language processing difficulty. To be specific, the equal DD-motifs are most
capable of lessening the structural complexity of human languages. The de-
creasing DD-motifs have more ability to restrict the syntactic complexity than
the increasing DD-motifs do. Thirdly, DD-motifs are also useful in improving the
language classification results. The skewed distribution of consistent HD-motifs
can have a better performance in classifying our sample languages than the
metric of dependency direction, and an optimal classification result is observed
for consistently decreasing DD-motifs.
More quantitative studies applying the DD-motifs to other language fami-
lies will enrich our findings, or similar researches based on more harmonic
language texts will shed some new light on our work.
Acknowledgement
This work is supported by a Ph.D. grant from the China Scholarship Council
(201606320224) and the National Social Science Foundation of China under
Grant No. 11&ZD188.
Dependency Distance Motifs in 21 Indo-European Languages | 147
References
Boroda, M. (1982). Häuñgkeitsstrukturen Musikalischer Texte. In J. Orlov, M. Boroda, G. Moisei
& I. Nadarejšvili (Eds.), Sprache, Text, Kunst: Quantitative Analysen (pp. 231-262). Bochum:
Studienverlag Dr. N. Brockmeyer.
Dryer, M. S. (1992). The Greenbergian Word Order Correlations. Language, 68, 81–138.
Dryer, M. S. (1996). Word Order Typology. In J. Jacobs (Ed.), Handbook on Syntax, Vol. 2 (pp.
1050–1065). Berlin/New York: Walter de Gruyter Publishing.
Ferrer-i-Cancho, R. (2004). Euclidean Distance between Syntactically Linked Words. Physical
Review E, 70(5), 056135.
Ferrer-i-Cancho, R. (2013). Hubiness, Length and Crossings and their Relationships in Depend-
ency Trees. Glottometrics, 25: 1–21.
Ferrer-i-Cancho, R. (2014). A Stronger Null Hypothesis for Crossing Dependencies. Europhysics
Letters, 108(5), 58003.
Ferrer-i-Cancho, R., & Gómez-Rodríguez, C. (2015). Crossings as a Side Effect of Dependency
Lengths. http://arxiv.org/abs/1508.06451
Futrell, R., Mahowald, K., & Gibson, E. (2015). Large-scale Evidence of Dependency Length
Minimization in 37 Languages. PNAS, 112(33), 10336–10341.
Gibson, E. (1998). Linguistic Complexity: Locality of Syntactic Dependencies. Cognition, 68(1),
1–76.
Gildea, D., & Temperley, D. (2010). Do Grammars Minimize Dependency Length? Cognitive
Science, 34(2), 286–310.
Greenberg, J. H. (1963). Some Universals of Grammar with Particular Reference to the Order of
Meaningful Elements. In J. H. Greenberg (Ed.), Universals of Human Language (pp. 73-113).
Cambridge, MA: MIT Press.
Hawkins, J. A. (1994). A Performance Theory of Order and Constituency. Cambridge: Cambridge
University Press.
Heringer, H.-J., Strecker, B., & Wimmer, R. (1980). Syntax: Fragen, Lösungen, Alternativen.
München: Fink.
Hudson, R. (1995) Measuring Syntactic Difficulty. Unpublished paper.
http://dickhudson.com/wp-content/uploads/2013/07/Difficulty.pdf
Hudson, R. (2007). Language Networks: The New Word Grammar. Oxford: Oxford University
Press.
Hudson, R. (2007). An Introduction to Word Grammar. Cambridge: Cambridge University Press.
Jing, Y., & Liu, H. (2015). Mean Hierarchical Distance: Augmenting Mean Dependency Distance.
In Proceedings of the Third International Conference on Dependency Linguistics (Depling
2015) (pp. 161–170). Uppsala, Sweden.
Jing, Y. (2016). Harmonic Head Ordering and Language Classification Optimization (Un-
published Master’s thesis). Zhejiang University, P. R. China.
Köhler, R., & Naumann, S. (2008). Quantitative Text Analysis Using L-, F-and T-Segments. In C.
Preisach, H. Burkhardt, L. Schmidt-Thieme & R. Decker (Eds.), Data Analysis, Machine
Learning and Applications (pp. 637–645). Berlin & Heidelberg: Springer.
Köhler, R., & Naumann, S. (2009). A Contribution to Quantitative Studies on the Sentence Level.
In R. Köhler (Ed.), Issues in Quantitative Linguistics (pp. 34–57). Lüdenscheid: RAM-Verlag.
Köhler, R., & Naumann, S. (2010). A Syntagmatic Approach to Automatic Text Classification.
Statistic Properties of F- and L-motifs as Text Characteristics. In P. Grzybek, E. Kelih & J.
148 | Yingqi Jing – Haitao Liu
Mačutek (Eds.), Text and Language. Structures, Functions Interrelations, Quantitative
Perspectives (pp. 81–89). Wien: Praesens.
Lehmann, W. (1973). A Structural Principle of Language and its Implications. Language, 49,
47–66.
Lehmann, W. (1978). The Great Underlying Ground Plans. In W. Lehmann (Ed.), Syntactic Typol-
ogy: Studies in the Phenomenology of Language (pp. 3-56). Austin: University of Texas
Press.
Liu, H. (2008). Dependency Distance as a Metric of Language Comprehension Difficulty. Journal
of Cognitive Science, 9(2), 159–191.
Liu, H., & Hu, F. (2008). What Role does Syntax Play in a Language Network? Europhysics Let-
ters, 83(1), 226-234.
Liu, H. (2009). Dependency Grammar: from Theory to Practice. Beijing: Science Press.
Liu, H. (2010). Dependency Direction as a Means of Word-order Typology: a Method Based on
Dependency Treebanks. Lingua, 120(6), 1567–1578.
Liu, H., & Xu, C. (2012). Quantitative Typological Analysis of Romance Languages. Poznań
Studies in Contemporary Linguistics, 48(4), 597–625.
Lu, Q., Xu, C., & Liu, H. (2016). Can Chunking Reduce Syntactic Complexity of Natural Lan-
guages? Complexity. doi: 10.100/cplx.21779
Melʹčuk, I. (1988). Dependency Syntax: Theory and Practice. Albany: State University Press of
New York.
Saussure, F. (1960). Course in General Linguistics. London: P. Owen.
Taylor, A. (1994). The Change from SOV to SVO in Ancient Greek. Language Variation and
Change, 6(1), 1–37.
Tesnière, L. (1959). Éléments de Syntaxe Structural. Paris: Klincksieck.
Tesnière, L. (2015). Elements of Structural Syntax. Translated by T. Osborne & S. Kahane. Am-
sterdam: John Benjamins.
Vennemann, T. (1974). Theoretical Word Order Studies: Results and Problems. Papiere zur
Linguistik, 7, 5–25.
Zeman, D., Mareček, D., Popel, M., Ramasamy, L., Štěpánek, J., Žabokrtský, Z., & Hajič, J.
(2012). HamleDT: To Parse or not to Parse? In Proceedings of LREC-2012 (pp. 2735–2741).
İstanbul, Turkey.
Zipf, G. (1949). Human Behavior and the Principle of Least Effort: An Introduction to Human
Ecology. Cambridge, Mass: Addison-Wesley Press.
Appendix
Hamle DT 2.0 provides a collection of the 21 Indo-European language treebanks
used in our study. They are BulTreeBank/CoNLL 2006, Prague Dependency
Treebank, SynTagRus, Slovak Treebank, Slovene Dependency Treebank, An-
Cora-CA, AnCora-ES, Italian Syntactic-Semantic Treebank/CoNLL 2007, CoNLL
2006 (Floresta Sintá(c)tica), Resurse pentru Gramaticile de Dependenta, CoNLL
2006, TIGER Corpus/CoNLL 2009, Penn Treebank/CoNLL 2007, CoNLL 2006
(Alpino), CoNLL 2006 (Talbanken05), Hyderabad Dependency Treebank/ICON
Dependency Distance Motifs in 21 Indo-European Languages | 149
2010, Persian Dependency Treebank, Hyderabad Dependency Tree-
bank/COLING 2012, Greek Dependency Treebank/CoNLL 2007, Ancient Greek
Dependency Treebank, Latin Dependency Treebank. The following documents
are related with these treebanks.
Afonso, S., Bick, E., Haber, R., & Santos, D. (2002). Floresta Sintá(c)tica: A Treebank for Portu-
guese. In LREC-2002, Las Palmas, Spain.
Bamman, D., & Crane, G. (2011). The Ancient Greek and Latin Dependency Treebanks. In C.
Sporleder, A. Bosch & K. Zervanou (Eds.), Language Technology for Cultural Heritage,
Theory and Applications of Natural Language Processing (pp. 79-98). Berlin/Heidelberg:
Springer.
Bejček, E., Hajičová, E., Hajič J., Jínová, P., Kettnerová V., Kolářová, V., Mikulová M., Mírovský J.,
Nedoluzhko A., Panevová J., Poláková L., Ševčíková M., Štěpánek J., & Zikánová S. (2013).
Prague Dependency Treebank 3.0. http://hdl.handle.net/11858/00-097C-0000-0023-1AAF-
3. Charles University in Prague, ÚFAL, Praha, Czechia.
Boguslavsky, I., Grigorieva, S., Grigoriev, N., Kreidlin, L., & Frid, N. (2000). Dependency Tree-
bank for Russian: Concept, Tools, Types of Information. In Proceedings of the 18th Confer-
ence on Computational Linguistics, Vol. 2 (pp. 987–991). ACL, Morristown, NJ, USA.
Brants, S., Dipper, S., Hansen, S., Lezius, W., & Smith, G. (2002). The TIGER Treebank. In Pro-
ceedings of the Workshop on Treebanks and Linguistic Theories. Sozopol, Bulgaria.
Călăcean, M. (2008). Data-driven Dependency Parsing for Romanian. Uppsala Universitet,
Uppsala, Sweden.
Džeroski, S., Erjavec, T., Ledinek, N., Pajas, P., Žabokrtsky, Z., & Žele, A. (2006). Towards a
Slovene Dependency Treebank. In Proceedings of LREC-2006 (pp. 1388–1391), ELRA, Ge-
nova, Italy.
Husain, S., Mannem, P., Ambati, B. R., & Gadde, P. (2010). The ICON-2010 Tools Contest on
Indian Language Dependency Parsing. In Proceedings of ICON-2010 Tools Contest on Indi-
an Language Dependency Parsing (pp. 1-8). ICON.
Kromann, M. T., Mikkelsen, L., & Lynge, S. K. (2004). Danish Dependency Treebank.
http://code.google.com/p/copenhagen-dependency-treebank/. København, Denmark.
Marcus, M. P., Marcinkiewicz, M. A., & Santorini, B. (1993). Building a Large Annotated Corpus
of English: The Penn Treebank. Computational linguistics, 19(2), 313–330.
Montemagni, S., Barsotti F., Battista M., Calzolari N., Corazzari O., Lenci A., Zampolli A., Fanci-
ulli F., Massetani M., Raffaelli R., Basili R., Pazienza M., Saracino D., Zanzotto F., Mana N.,
Pianesi F., & Delmonte R. (2003). Building the Italian Syntactic-Semantic Treebank. In A.
Abeillé (Ed.), Building and Using Parsed Corpora (pp.189-210). Dordrecht: Kluwer.
Nivre, J., Nilsson, J., & Hall, J. (2006). Talbanken05: A Swedish Treebank with Phrase Structure
and Dependency Annotation. In Proceedings of LREC (Vol. 6), Genova, Italy.
Prokopidis, P., Desipri, E., Koutsombogera, M., Papageorgiou, H., & Piperidis, S. (2005). Theo-
retical and Practical Issues in the Construction of a Greek Dependency Treebank. In Pro-
ceedings of TLT-2005 (pp. 149–160). Barcelona, Spain.
Rasooli, M. S., Moloodi A., Kouhestani M., & Minaei-Bidgoli B. (2011). A Syntactic Valency
Lexicon for Persian Verbs: The First Steps towards Persian Dependency Treebank. In 5th
Language and Technology Conference (LTC): Human Language Technologies as a Chal-
lenge for Computer Science and Linguistics (pp. 227–231). Poznań, Poland.
150 | Yingqi Jing – Haitao Liu
Rosa, R., Masek, J., Marecek, D., Popel, M., Zeman, D., & Zabokrtský, Z. (2014). HamleDT 2.0:
Thirty Dependency Treebanks Stanfordized. In Proceedings of LREC-2014 (pp. 2334–2341).
ELRA, Reykjavík, Iceland.
Simov, K., & Osenova P. (2005). Extending the Annotation of BulTreeBank: Phase 2. In Proceed-
ings of TLT-2005 (pp. 173–184). Barcelona, Spain.
Šimková, M., & Garabík R. (2006). Синтаксическая разметка в Словацком национальном
корпусе. In Tруды международной конференции Корпусная лингвистика – 2006 (pp.
389–394). Sankt-Peterburg: St. Petersburg University Press.
Taulé, M., Antònia M., & Recasens M. (2008). AnCora: Multilevel Annotated Corpora for Catalan
and Spanish. In Proceedings of LREC-2008. ELRA, Marrakech, Morocco.
Van der Beek, L., Bouma, G., Malouf, R., & Van Noord, G. (2002). The Alpino Dependency Tree-
bank. Language and Computers, 45(1), 8–22.
George K. Mikros, Ján Mačutek
Word Length Distribution and Text Length:
Two Important Factors Influencing
Properties of Word Length Motifs
Abstract: Word length motifs and their properties in Modern Greek blogs are
investigated. We observe that the parameter of the type-token relation decreases
with the increasing text length, and that the parameter values in real and ran-
domized texts are not significantly different. Relations between the number of
motifs in a text and the text length is modelled by the linear function for Modern
Greek and Ukrainian texts, with the parameter of the model being in both lan-
guages inversely proportional to the mean word length.
Keywords: word length motifs, text length, word length.
1 Introduction
In linguistics, motifs are understood as continuous sequences of values of some
language units. In this paper we focus exclusively on word length motifs, i.e.,
on sequences of words with non-decreasing lengths, with word length meas-
ured in the number of syllables (for on overview of linguistic motifs in general cf.
Köhler 2015).
Word length motifs (henceforward denoted only as motifs) were introduced
by Köhler (2006), who was inspired by a study of a sequential structure of note
duration in music by Boroda (1982). Motifs were later investigated from a theo-
retical point of view by Köhler (2008a, 2008b), Mačutek (2009, 2015), Sanada
(2010), Mačutek & Mikros (2015), and Milička (2015). Hints at possible applica-
tions of motif properties (mainly) to an automatic text classification can be
found in Köhler & Naumann (2008, 2010), and also in some of the abovemen-
tioned papers.
This paper, using data material which consists of 1000 blogs written in
||
George K. Mikros: Department of Italian Language and Literature, National and Kapodistrian
University of Athens, Athens, Greece, gmikros@isll.uoa.gr
Ján Mačutek: Department of Applied Mathematics and Statistics, Comenius University,
Bratislava, Slovakia, jmacutek@yahoo.com
152 | George K. Mikros – Ján Mačutek
Modern Greek (cf. Section 2 for data description), tries to provide some insight
into two related areas. First, we follow the line of research presented by Milička
(2015). In that paper, a question is asked whether the word length distribution
observed in a text determines motif properties (or, as the author says, whether
motif properties are inherited from the word length distribution). It was shown
that motif frequencies in real texts differ from those in randomized texts (in
which the word length distribution is preserved, but the order in which
particular lengths occur is random). This finding indicates that motif properties
carry some information which is not included in the word length distribution.
On the other hand, type-token relations for motifs (cf. Mačutek 2015) in real and
randomized texts are basically the same (statistically speaking, they are not
significantly different, cf. Section 3).
Second, we concentrate on several interrelations among motif properties,
namely the type-token relation, text length (measured in the number of words),
and the number of motifs observed in the text. It seems that text length plays
a very important role (cf. Section 4).
Computations were performed using the software environments R (www.r-
project.org) and NLREG (www.nlreg.com).
2 Data Description
During last decade Internet evolved from a static field of simple information
provision into a digital carrier of language production characterized by interac-
tivity and dynamic configuration of the online textual content.
Blogs are among the most known web tools that have transformed web
communication and overcame the unidirectionality of standard online commu-
nication. Until 2011 worldwide have been created approximately 181 million
blogs producing 900,000 posts every day which are being read by 77% of inter-
net users1
. Since many blogs are important information nodes and attract many
more readers than most of the traditional printed media, they can exert influ-
ence in language usage and produce linguistic innovations accelerating linguis-
tic change. For this reason, blog’s language usage has started to attract atten-
tion and become a challenging research subject in the linguistic community.
||
1 Source: NM Incite.
Word Length Distribution and Text Length: Two Important Factors […] | 153
Blogs represent a new text genre with interesting characteristics. They com-
bine personal views, news and reporting on current events (Mishne, 2007).
Their structure is a hybrid containing both monologue and dialogue features. At
the same time, they are both log entries reflecting personal opinions and open
calls for public discussion (Nilsson, 2003). Mishne (2007, p. 34) studied in detail
various properties of linguistic usage in English blogs and showed that they
present increased usage of personal pronouns and words relating to personal
surroundings emerging personal experience. Furthermore, he examined the
linguistic complexity of the blogs using the perplexity measure (Brown et al.,
1992) and the out-of-vocabulary rate (OOV) and found that their linguistic struc-
ture was more complex than most of the similar written genres (e.g. personal
correspondence). Increased perplexity, according to Mishne (2007), equates
with increased irregularity in linguistic usage (i.e. free-form sentences, de-
creased compliance with grammatical rules etc.). In addition, blogs presented
increased OOV rates, meaning that blog texts exhibit a topical diffused vocabu-
lary, with many neologisms, possible typographical errors and increased level
of references to named entities from the blogger’s personal environment.
Another interesting characteristic of the blog’s linguistic structure is its
equilibrium between spoken and written language. Sentence construction in
blogs is highly variable using selectively structures from both spoken and writ-
ten norms (Chafe & Danielewicz, 1987). Equally important effect in language
usage in blogs is the age of the bloggers. Half of them are aged 18-342
. For this
reason, formality in language usage is decreased, with shorter that average
sentence lengths and lower readability scores in the most famous readability
formulas (like, e.g., Gunning-Fog, Flesch-Kincaid, SMOG; cf. Bailin & Grafstein
2016).
Since there was not any social media corpus available for Modern Greek, we
decided to compile ours from scratch. For this reason, we harvested the Greek
blogosphere from September 2010 till August 2011 and manually collected 100
Greek blogs, their authors being equally divided to 50 male and 50 female blog-
gers. Since topics can induce significant bias into stylometric measurements
(Mikros & Argiri, 2007), we decided to explore only a part of the collected corpus,
using blogs that share the same topic. In this study we used 20 blogs (10 male
and 10 female authors) with the common topic (personal affairs), making the
total of 1,000 blog posts counting 406,460 words. For each author we collected
||
2 Source: The Social Media Report, Q3, 2011, NM Incite, Nielsen.
154 | George K. Mikros – Ján Mačutek
50 most recent blog posts. The posts contain from 24 (the minimum) to 3652 (the
maximum) words, the mean text length being 406.46 words.
All measurements related to syllable length have been performed using a
customized PERL script we developed for this research. Syllable counting in
Modern Greek is a complex task since its orthography is considered deep, with
complex correspondences between graphemes and phonemes. Moreover, a
large number of inconsistent phoneme-grapheme correspondences have been
created due to the diglossia existent in the Greek language for over a century.
Our syllable counter works using a two-stage algorithm. In the first pass,
the software performs a broad phonemic transcription of the written text using
the basic grapheme to phoneme rules for Modern Greek. In the second stage,
there is a hand-made lexicon with phonetic transcriptions of the 20,000 most
frequent words of Modern Greek. The software checks the phonetic transcrip-
tions produced in the first stage and corrects them if it finds a difference be-
tween its list and the relative phonetic transcription produced by the rules. A
series of reliability tests of our script showed that its final phonemic transcrip-
tions have a 95-98% accuracy, a performance that guarantees that our motif
length measurements are reliable as well.
3 Type-token Relation for Motifs Revisited
The type-token relation is, when motifs are considered (cf. Wimmer 2005 for a
comprehensive paper on type-token relations in linguistics), the relation be-
tween the number of all motifs (tokens) and different motifs (types) observed in
a text. Mačutek (2015) analyzed 70 Ukrainian texts of seven different genres and
showed that the development of the relation can be modelled by the function
�(�) = ��
, (1)
with �(�) being the number of different motifs among the first � motifs; � is a
parameter.
Our results confirm that model (1) is appropriate also for the motif type-
token relation in Modern Greek texts, which, given that motifs behave similarly
to their basic units, i.e., words, and that the type-token relation for words is one
of established language laws, is not surprising.
However, a much more interesting finding can be reported. Milička (2015)
investigated motif frequencies in both real and randomized Czech and Arabic
texts. He concluded that there were statistically significant differences in motif
Word Length Distribution and Text Length: Two Important Factors […] | 155
frequencies in these two types of texts. We randomized 100 times each of the
first ten of the blog posts from our corpus. The process of randomization was
performed in two different ways. First, sequences of word lengths were
permuted and each permutation was considered to correspond to a new,
randomized text, i.e., word length frequencies were preserved, with only the
order of the words changed. Second, we generated random numbers from the
distributions of word length in the texts, i.e., the type of the word length
distribution was preserved, with word length frequencies fluctuating randomly.
Then motifs were constructed in the randomized texts. Both approaches to the
randomization lead to word length sequences for which not only model (1)
described the type-token relation for motifs sufficiently well, but they also
yielded practically the same values of the parameter 0 (statistical tests did not
reveal significant differences).
One could object that 100 randomization are not reliable enough; however,
the values of the parameter 0 seem to be very stable, and, in addition, one is
likely to reject (almost) any statistical hypothesis if the sample size is very large
(cf. Mačutek & Wimmer 2013). The same result was obtained for a long Modern
Greek text (novel The mother of the dog by Matesis, containing 47,852 words, cf.
Mačutek & Mikros 2015); here, because of time consuming computations,
randomizations were performed only ten times.
Hence, it seems that the parameter of the model for the type-token relation
is influenced more by language and by text length (cf. Section 4) than by the
author or by the genre (attempts to apply the parameter values to an automatic
authorship attribution did not bring convincing results).
It is to be noted that not all motif properties in real texts coincide with those
in randomized texts. In addition to already mentioned motif frequencies
(Milička 2015), also the parameters of the Menzerath-Altmann law in such texts
differ (Mačutek & Mikros 2015). Moreover, even in the cases where properties of
real and randomized texts are the same, one cannot exclude the possibility that
an apparent randomness is in fact a result of an interference of several
competing language laws (which can „cancel“ each other).
4 Interrelations among Some Motif and Text
Properties
Within the framework of quantitative linguistics, language units and their prop-
erties are not considered to be isolated entities, but, on the contrary, it is sup-
156 | George K. Mikros – Ján Mačutek
posed that they influence each other. One of the aims of linguistic research is to
build a network of established (i.e., mathematically modelled and statistically
tested) interrelations between its particular elements (i.e., units and their prop-
erties). Such an approach to building a linguistic theory is known as synergetic
linguistics (cf. Köhler 2005).
One of the first attempts to present interrelations among properties of lin-
guistic motifs was presented by Mačutek (2015), based on an analysis of
Ukrainian texts. We confirm these results on new data material3
from another
language.
Analogously to words, also for motifs it holds that the value of the
parameter b in model (1) tends to decrease with the increasing text length (see
Fig. 1). This behaviour is easy to explain – the maximum of word length
observed in a text is usually relatively low and long words occur but seldom,
hence motifs containing long words appear only rarely, and those in which
there are several long words are possible, but the probability of their occurrence
is close to zero. It means that new types of motifs appear, once the usual ones
(consisting of relatively short words) are already observed, more and more
seldom as the text gets longer. At the present we do not try to derive
a mathematical model for this relation. The trend is quite obvious, but the
values of b exhibit a relatively high variability. Therefore we suppose that text
length is not the only important factor, and a more complicated mathematical
model with more than one explanatory variable (e.g., also some parameters of
the word length distribution) will probably be needed to fit the data sufficiently
well.
||
3 Numerical values of properties discussed in this section can be sent upon request. Given the
size of the data material (1000 texts), it is not possible to present them in this paper.
Word Length Distribution and Text Length: Two Important Factors […] | 157
Fig. 1: Relation between text length in words and value of parameter b in type-token relation
(Modern Greek blogs).
According to Mačutek (2015), there is also a relation between the number of all
motifs in a text and the text length. The number of all motifs, seemingly,
increases linearly with the increasing text length. He also formulated
a hypothesis that, if the relation really is linear, the slope of the linear function
would be inversely proportional to the mean word length in the text. The linear
model without an intercept,
�(�) = ��, (2)
gives a very good fit also for Modern Greek blogs, see Fig. 2. This model not only
fits data very well, but it also has two additional advantages.
First, it is a special case of a very general model suggested by Wimmer &
Altmann (2005), which is currently considered as one common mathematical
expression of (nearly) all language laws. Admittedly, linguists prefer power
laws (Naranan & Balasubrahmanyan 2005) over linear functions, but, again,
a linear model is a special case of a power law for certain parameter values
158 | George K. Mikros – Ján Mačutek
(which is the case of our data – fitting the function �(�) = ���
yields values of b
very close to 1).
Second, the only parameter of model (2) is directly interpretable in terms of
word length. Denote MWL the mean word length (measured in the number of
syllables) in the corpus and ��(�) the number of motifs in a text consisting of x
words. The fit (in terms of the determination coefficient ��
, cf. Mačutek &
Wimmer 2013) remains very good if the parameter a is estimated not by the
classical least square method, but as the inverse proportion of the MWL. We
thus obtain the model
��(�) =
�
���
�, (3)
with x being the number of words in a text. The line in Fig. 2 represents model
(3). For the corpus of Modern Greek blogs we have ��� = 2��06, which leads
to the model ��(�) = 0����� with ��
= 0�996.
Fig. 2: Relation between text length in words and number of all motifs in text (Modern Greek
blogs).
Word Length Distribution and Text Length: Two Important Factors […] | 159
The same behaviour - the linear relation between the number of motifs in a text
and the text length, with the inverse proportion of the MWL from the corpus
substituted as the value of parameter a - can be observed also in 70 Ukrainian
texts of seven different genres (cf. Mačutek 2015). The MWL in the Ukrainian
corpus is 2.660 and after inserting the value into (3) we have ��(�) = 0��76�,
with ��
= 0�9765. The data and the line representing model (3) can be seen in
Fig. 3.
Fig. 3: Relation between text length in words and number of all motifs in text (70 Ukrainian
texts, cf. Mačutek 2015).
For the sake of simplicity, the MWL was computed from the corpora of Modern
Greek and Ukrainian texts, respectively. Similar results are obtained if one
computes the MWL for each text separately, i.e., if the value of the parameter in
model (3) is specific for each text. Hereby we do not claim that the MWL does
not differ among genres or authors. The good fit for the MWL value from a cor-
pus can be caused by a relatively homogenous language material.
160 | George K. Mikros – Ján Mačutek
This approach, however, is not directly applicable to five very long Modern
Greek texts (cf. data in Mačutek & Mikros 2015, text lengths range from 9,761 to
77,692). The linear function (2) fit data excellently, but not if we take the
�
���
= 0�482 from the corpus of the five novels as the parameter. On the other
hand, the model
��(�) = 0�482��
(4)
with � = 0�985 achieves ��
= 0�9988.
With respect to this different pattern of behaviour of long texts we remind
that word length seems to decrease with the increasing text length (cf. Kelih
2012 for an analysis of a Russian novel and its translation to Bulgarian). This
aspect has not been sufficiently investigated yet, and it deserves a closer atten-
tion. The linear model may not be sufficient if a text is long, but it seems that
even in such a case the inverse proportion remains an important player. Any-
way, the question is whether we can consider a long novel to be one text, one
whole, or whether it is more appropriate to see in it a collection of several short-
er texts (e.g., chapters).
Mačutek (2015) speculated over the nature (linear vs. non-linear) of the
relation between the number of all motifs and the number of different motifs in
texts. We can conclude that the relation is non-linear (Fig. 4), and that the
function �(�) = ���
yields a good fit (� = ���98, � = 0�52, ��
= 0�9���). We
postpone attempts to interpret the parameters until data from more languages
are available.
Word Length Distribution and Text Length: Two Important Factors […] | 161
Fig. 4: Relation between number of all motifs and number of different motifs (Modern Greek
blogs).
5 Conclusion
The type-token relation for word length motifs behaves analogously to that for
words (its parameter decreases with the increasing text length), confirming thus
once more the expectations that motifs display properties similar to the units of
which they consist (words here). The parameters of the type-token relations for
motifs in real and randomized text attain the same value for texts analyzed in
this paper.
The number of motifs is determined by the mean word length (namely, by
its inverse proportion) in relatively short texts. In long novels, the suggested
mathematical model has two parameters, the inverse proportion of the mean
word length being one of them.
162 | George K. Mikros – Ján Mačutek
Based on analyses of Modern Greek and Ukrainian texts we allow ourselves
to state (at least tentatively) that the word length distribution is a crucial factor
influencing the type-token relation for word length motifs. If the distribution is
fixed, even randomized texts do not differ from real ones as far as the type-
token relation is concerned. The text length, together with the mean word
length in the text (which is given by the word length distribution), determine
the number of all motifs which occur in the text.
Acknowledgement
J. Mačutek was supported the grant VEGA 2/0047/15.
References
Bailin, A., & Grafstein, A. (2016). Readability: Text and Context. London: Palgrave Macmillan.
Boroda, M.G. (1982). Häufigkeitsstrukturen musikalischer Texte. In: J.K. Orlov, M.G. Boroda &
I.Š. Nadarejšvili (Eds.), Sprache, Text, Kunst. Quantitative Analysen (pp. 231–262).
Bochum: Brockmeyer.
Brown, P.F., deSouza, P.V., Mercer, R.L., Della Pietra, V.J., & Lai, J.C. (1992). Class-based n-
gram models of natural language. Computational Linguistics, 18(4), 467–479.
Chafe, W., & Danielewicz, J. (1987). Properties of spoken and written language. In R. Horowitz &
J. S. Samuels (Eds.), Comprehending Oral and Written Language (pp. 83–113). New York:
Academic Press.
Kelih, E. (2012). On the dependency of word length on text length. Empirical results from Rus-
sian and Bulgarian parallel texts. In: S. Naumann, P. Grzybek, R. Vulanović & G. Altmann
(Eds.), Synergetic Linguistics. Text and Language as Dynamic Systems (pp. 67–80). Wien:
Praesens.
Köhler, R. (2005). Synergetic linguistics. In: R. Köhler, G. Altmann & R.G. Piotrowski (Eds.),
Quantitative Linguistics. An International Handbook (pp. 760–774). Berlin, New York: de
Gruyter.
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In: J. Genzor
& M. Bucková (Eds.), Favete Linguis. Studies in Honour of Viktor Krupa (pp. 145–152). Bra-
tislava: Slovak Academic Press.
Köhler, R. (2008a). Sequences of linguistic quantities. Report on a new unit of investigation.
Glottotheory, 1(1), 115–119.
Köhler, R. (2008b). Word length in text. A study in the syntagmatic dimension. In S. Mislovičo-
vá (Ed.), Jazyk a jazykoveda v pohybe (pp. 416–421). Bratislava: Veda.
Köhler, R. (2015). Linguistic motifs. In: G.K. Mikros & J. Mačutek (Eds.), Sequences in Language
and Text (pp. 89–108). Berlin, Boston: de Gruyter.
Word Length Distribution and Text Length: Two Important Factors […] | 163
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In: C.
Preisach, H. Burkhardt, L. Schmidt-Thieme & R. Decker (Eds.), Data Analysis, Machine
Learning and Applications (pp. 635–646). Berlin, Heidelberg: Springer.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F- and L-motifs as text characteristics. In: P. Grzybek, E. Kelih & J.
Mačutek (Eds.), Text and Language. Structures, Functions, Interrelations, Quantitative
Perspectives (pp. 81–89). Wien: Praesens.
Mačutek, J. (2009). Motif richness. In: Köhler, R. (Ed.), Issues in Quantitative Linguistics (pp.
51–60). Lüdenscheid: RAM-Verlag.
Mačutek, J. (2015). Type-token relation for word length motifs in Ukrainian texts. In: A. Tuzzi, M.
Benešová & J. Mačutek (Eds.), Recent Contributions to Quantitative Linguistics (pp. 63–73).
Berlin, Boston: de Gruyter.
Mačutek, J., & Mikros, G.K. (2015). Menzerath-Altmann law for word length motifs.In: G.K.
Mikros & J. Mačutek (Eds.), Sequences in Language and Text (pp. 125–131). Berlin, Boston:
de Gruyter.
Mačutek, J., & Wimmer, G. (2013). Evaluating goodness-of-fit of discrete distribution models in
quantitative linguistics. Journal of Quantitative Linguistics, 20(3), 227–240.
Milička, J. (2015). Is the distribution of L-motifs inherited from the word length distribution? . In:
G.K. Mikros & J. Mačutek (Eds.), Sequences in Language and Text (pp. 133–145). Berlin,
Boston: de Gruyter.
Mikros, G.K., & Argiri, E.K. (2007). Investigating topic influence in authorship attribution. In: B.
Stein, M. Koppel & E. Stamatatos (Eds.), Proceedings of the SIGIR 2007 International
Workshop on Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection
(vol. 276, pp. 29–35). Amsterdam: CEUR.
Mishne, G. (2007). Applied Text Analytics for Blogs. Amsterdam: University of Amsterdam.
Naranan, S., & Balasubrahmanyan, V.K. (2005). Power laws in statistical linguistics and related
systems. In: R. Köhler, G. Altmann & R.G. Piotrowski (Eds.), Quantitative Linguistics. An
International Handbook (pp. 716-738). Berlin, New York: de Gruyter.
Nilsson, S. (2003). The Function of Language to Facilitate and Maintain Social Networks in
Research Weblogs. Umeå: Umeå Universitet.
Sanada, H. (2010). Distribution of motifs in Japanese texts. In: P. Grzybek, E. Kelih & J. Mačutek
(Eds.), Text and Language. Structures, Functions, Interrelations, Quantitative Perspectives
(pp. 181–193). Wien: Praesens.
Wimmer, G. (2005). The type-token relation. In: R. Köhler, G. Altmann & R.G. Piotrowski (Eds.),
Quantitative Linguistics. An International Handbook (pp. 361–368). Berlin, New York: de
Gruyter.
Wimmer, G., & Altmann, G. (2005). Unified derivation of some linguistic laws. In: R. Köhler, G.
Altmann & R.G. Piotrowski (Eds.), Quantitative Linguistics. An International Handbook (pp.
791–807). Berlin, New York: de Gruyter.
Yaqin Wang
Quantitative Genre Analysis Using
Linguistic Motifs
Abstract: Linguistic motif is the longest continuous sequence of equal or in-
creasing values representing a quantitative property of a linguistic unit. The
present study makes an attempt towards a quantitative investigation on charac-
teristics of different genres by using the notion of linguistic motif, i.e., L-, F-
motifs. The rank frequency distributions of L-, F-motifs across genres were mod-
eled by Zipf-Mandelbrot distribution. Parameters of Zipf-Mandelbrot model and
certain attributes related to motifs were compared. Results show that parame-
ters of Zipf-Mandelbrot model and the proportion of hapax legomena of the F-
motifs to size of the F-motif tokens can separate several genres from others. It
reveals that both L- and F-motifs can be employed as indicators for genre classi-
fication.
Keywords: linguistic motif, L-motifs, F-motifs, genre classification, Zipf-
Mandelbrot distribution
1 Introduction
The linguistic motif is a new unit for sequential analyses in quantitative linguis-
tics. It is defined as the longest continuous sequence of equal or increasing
values representing a quantitative property of a linguistic unit (Beliankou et al.
2013; Köhler, 2015). There are various versions of motifs on the basis of numeri-
cal variables (Köhler, 2015: 90).
An L-motif is a continuous series of equal or increasing length values (e.g.
of morphs, words or sentences).
An F-motif is a continuous series of equal or increasing frequency values
(e.g. of morphs, words or syntactic construction types).
A P-motif is a continuous series of equal or increasing polysemy values (e.g.
of morphs or words).
A T-motif is a continuous series of equal or increasing polytextuality values
||
Yaqin Wang: Department of Linguistics, Zhejiang University, Hangzhou,
China,wyq322@126.com
166 | Yaqin Wang
(e.g. of morphs, words or syntactic construction types).
When the word length is measured in terms of syllables, an example of L-
motif of a sentence is shown below:
Serious conversation about alcohol is reserved for wine and spirits.
(3-4) (2-3) (1-2) (1-1-1-2)
Motifs can also be formed from the perspective of categorical variables in
linguistics. The types of relations in argumentation structure (Beliankou et al.
2013) or POS (part of speech) tags are example of categorical variable. For in-
stance, an R-motif is an uninterrupted sequence of unrepeated elements. A D-
motif is an uninterrupted depth-first path of elements in a tree structure (Köh-
ler, 2015).
Following the definition, on the one hand, the text can be segmented ex-
haustively and without ambiguity. On the other hand, motifs are scalable in
relation to appropriate granularity. This means that they can be applied itera-
tively: It is possible to form motifs on the basis of length or frequency values etc.
of motifs. Thus, length motifs of frequency motifs can be formed as well as fre-
quency motifs of word length motifs. More importantly, motifs usually display a
rank-frequency distribution of the Zipf-Mandelbrot type. This indicates that they
follow certain regularity.
Zipf-Mandelbrot’s law (Mandelbrot, 1953), a power law distribution on
ranked frequency modified on the basis of Zipf’s law, is defined as:
�(�) =
�
(���)�
(C > 0, a > 0, b > -1, r ) (1)
Here, f(r) is the frequency of a word in rank r, a and b are parameters and C
is a normalizing constant. Parameters of Zipf-Mandelbrot probability distribu-
tion lead to specified curves of different groups. They have been used as a pre-
dictor for cross-linguistic comparison (Bentz et al. 2014a; 2014b) and diachronic
language change (Koplenig, 2015).
A number of studies have been performed on the basis of various versions
of motifs (Köhler, 2006, 2008a, 2008b, 2015; Köhler& Naumann 2008, 2009,
2010; Mačutek, 2009; Sanada, 2010; Milička, 2015). The empirical parameters of
Zipf-Mandelbrot distribution, for instance, applied to the distribution of fre-
quency motifs of word frequencies, are used for the comparison of authors,
texts, and text sorts and for classification purposes (Köhler& Naumann, 2010).
Köhler and Naumann (2010) investigated five different text sorts in relation to L-
motifs and F-motifs with 55 texts (including 10 poems, 10 narrative texts, 10
juridical, 10 scientific, and 15 journalistic texts with varying text lengths). Sev-
eral attributes, namely, parameters of Zipf-Mandelbrot law for F-motifs, hapax
Quantitative Genre Analysis Using Linguistic Motifs | 167
legomena of L-motifs, LL-motifs, LF-motifs and FF-motifs, Ord’s criteria I and S,
etc., representing these 55 texts were scrutinized. They suggested a larger num-
ber of texts and more text types for further studies to determine whether syn-
tagmatic features provide useful information for text classification.
Thus, the current research tends to explore characteristics of different gen-
res (similar to the concept of text type) by using the notion of linguistic motif
based on a corpus of more quantities of texts and more genres. Motifs in the
sense of numerical data, i.e. L-, F-motifs are investigated to examine genre clas-
sification purpose of motifs. Several attributes related to motif properties are
explored. Here are major research objectives of the current study:
1) Parameters a and b of Zipf-Mandelbrot (hereafter referred to as ZM)
model fitting to L-motifs;
2) Proportion of hapax legomena of the L-motifs to the size of total L-motif
tokens (hereafter referred to as hapax (L));
3) Parameters a and b of ZM model fitting to F-motifs;
4) Proportion of hapax legomena of the F-motifs to the size of total F-motif
tokens (hereafter referred to as hapax (F)).
2 Methods and Data
The BNC is a corpus of about 100 million words of contemporary spoken and
written British English. It is a microcosm of current British English in its entirety,
including various kinds of text types (Burnard, 2000). The written texts of the
BNC are composed of two sorts of texts, namely, imaginative and informative
texts. The informative texts include eight domains, namely, applied science, arts,
belief, commerce, leisure, natural science, social science, and world affairs. These
nine domains can also be referred to genres with different subject fields (Lee,
2001). In the present study, two terms “domain” and “genre” are used inter-
changeably.
The BNC corpus is all in encoding format, in which all elements are delim-
ited by the use of tags. Nine samples of clean texts without any tags, of which
each sample contains 10,000 tokens, were extracted from the corpus. The ZM
model was first fitted to rank frequency distributions of motif tokens of nine
samples. This gives rise to nine sets of parameters of ZM model from nine gen-
res. However, it is hard to conduct a statistical test on these parameters (each
genre has only one set of parameter). Thus, to observe differences or similarities
of genre motifs closely, we then extracted 20 texts from each genre. In total,
there are 180 documents and each text has approximately 2000 words.
168 | Yaqin Wang
Regarding L-motifs, quanteda package (Benoit & Nulty, 2015) in R project
was first employed for computing syllabic lengths of words. Then Perl programs
were written and run to analyze each document according to the definition of L-
motifs.
F-motifs in texts from a corpus can be determined with respect to two dif-
ferent methods of frequency count: The frequency values for words can be
counted on the basis of the occurrences of the words in the given text or with
respect to the complete corpus (Köhler & Naumann, 2010). In the current study,
the first method was employed and F-motifs are based on frequency values of
words in the given genre. For instance, the sentence Serious conversation about
alcohol is reserved for wine and spirits was extracted from leisure genre. We then
determined the frequency value of each token based on the corpus of leisure.
The result is shown below:
Tab. 1: Frequency values of Serious conversation about alcohol is reserved for wine and spirits
serious 15
conversation 4
about 245
alcohol 7
is 1095
reserved 2
for 1118
wine 8
and 3352
spirits 3
Thus, F-motifs of this sentence are: (15) (4-245) (7-1095) (2-1118) (8-3352) (3). F-
motifs of all the genres were formed by Perl programs.
Then, rank-frequency distributions of motifs across genres were determined
and fitted by Zipf-Mandelbrot distribution by Altmann Fitter (Altmann-Fitter,
2013). As an example, Fig. 1 shows the model fitting of L-motifs of one text. Pro-
portions of hapax legomena of the L-, F-motifs with respect to the size of total L,
F-motif tokens were also computed for further comparison.
Quantitative Genre Analysis Using Linguistic Motifs | 169
Fig. 1: The rank frequency distribution of L-motifs modeled by the Zipf-Mandelbrot distribution
(df=253, χ2
= 66.5670, R2
=0.9934). (Note: the x-axis represents the rank number and the y-axis
represents corresponding frequencies. F(x) is the observed frequency value and NP (x) is the
predicted value. )
3 Results and Discussion
In this section, parameters of ZM model fitting to L-, F-motifs and the proportion
of hapax legomena in terms of L-, F-motifs were discussed. Comparison among
different genres in relation to two kinds of motifs was carried out.
3.1 L-motifs
Regarding genre L-motifs, we first investigated parameters a and b of ZM model
and then hapax (L).
3.1.1 ZM parameters
The probability distributions of total motif tokens of different genres are dis-
played in Fig. 2 with 20 highest observed frequencies and predicted values. As
Fig. 2 shows, the shape of model approximation varies across genres. However,
170 | Yaqin Wang
Fig. 2: Rank frequency distributions of genre L-motifs fitted by Zipf-Mandelbrot model with the
highest 20 frequency L-motifs. (Note: Dots represent their observed frequency and lines repre-
sent correspondingly predicted value by ZM model. In this and following tables and figures,
genre names hereafter are represented by abbreviations. Applied science is referred to as APP,
arts is ART, belief is BEL, commerce is COM, imaginative is IMA, leisure is LEI, natural science is
NAT, social science is SOC, world affairs is WOR.)
It is hard to separate one genre from the other only from the graph. Parameters
in Tab. 2, therefore, help to illustrate the difference. Imaginative has the small-
est a and b, social science the highest b and world affairs the highest a. Models
fit are all excellent according to R2
value in the table (R2
> 0.9). The value of pa-
rameters a and b are associated with the extent of steepness and deviation from
original slope of the distribution (Bentz et al. 2014a), indicating that the distri-
bution of L-motifs varies across genres.
Tab. 2: Parameters of ZM model for genre L-motifs
Genre a b R2
APP 2.0349 6.7351 0.9934
ART 2.0583 8.1541 0.9883
BEL 2.1811 9.8666 0.9893
COM 2.0572 7.5729 0.984
IMA 1.9927 6.1238 0.9886
0
200
400
600
800
1000
1200
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
FREQUENCY
RANK
APP
ART
BEL
COM
IMA
LEI
NAT
SOC
WOR
APP
ART
BEL
COM
IMA
LEI
NAT
SOC
WOR
Quantitative Genre Analysis Using Linguistic Motifs | 171
Genre a b R2
LEI 2.1253 8.4802 0.9881
NAT 2.1073 7.9594 0.9924
SOC 2.2273 10.7526 0.9868
WOR 2.2386 9.7588 0.9842
In order to study whether statistical significance exists among L-motifs distribu-
tions, we fitted ZM model to rank frequency distributions of L-motifs of the ex-
tracted 180 texts and copied all of the parameters. Fig. 3 shows the distribution
of parameters a and b for different genres. It can be seen that parameter a rang-
es from 1 to 2.8 and parameter b from 0.1 to 16. Visually, some parameters from
belief and applied science are around the top right corner. Most points are clus-
tered together in the middle with parameter a ranging from 1.6 to 2.2 and pa-
rameter b ranging from 4 to 10.
Fig. 3: The distribution of ZM parameters a and b for different genres
ANOVA tests on parameters a and parameters b across genres were carried out
respectively and the result shows that parameters are significantly different
from each other (pa = 0.004, pb = 0.027). Tab. 3 shows that regarding parameters
a, on the one hand, imaginative genre is significantly different from other seven
genres (except world affairs, marginally significant with respect to leisure and
natural science). The difference between world affairs and four genres (applied
science, belief, commerce, social science) is also statistically significant. On the
other hand, significant differences also exist across genres in terms of parame-
0
2
4
6
8
10
12
14
16
1 1.2 1.4 1.6 1.8 2 2.2 2.4 2.6 2.8
Parameter
b
Parameter a
APP ART BEL COM IMA LEI NAT SOC WOR
172 | Yaqin Wang
ters b. It reveals that difference across genres in terms of distribution of L-motifs
is statistically significant.
Tab. 3: p value of ANOVA test for parameter a/b of nine genre distributions
(a) p value of ANOVA test for parameter a
APP ART BEL COM IMA LEI NAT SOC WOR
APP 1 0.153 0.927 0.318 0* 0.073 0.077 0.389 0.003*
ART 0.153 1 0.181 0.667 0.033* 0.71 0.731 0.569 0.107
BEL 0.927 0.181 1 0.364 0.001* 0.088 0.094 0.442 0.003*
COM 0.318 0.667 0.364 1 0.011* 0.422 0.439 0.889 0.042*
IMA 0 0.033* 0.001* 0.011* 1 0.077 0.073 0.007* 0.597
LEI 0.073 0.71 0.088 0.422 0.077 1 0.977 0.346 0.214
NAT 0.077 0.731 0.094 0.439 0.073 0.977 1 0.361 0.204
SOC 0.389 0.569 0.442 0.889 0.007* 0.346 0.361 1 0.03*
WOR 0.003* 0.107 0.003* 0.042* 0.597 0.214 0.204 0.03* 1
(b) p value of ANOVA test for parameter b
APP ART BEL COM IMA LEI NAT SOC WOR
APP 1 0.031* 0.289 0.343 0.001* 0.016* 0.127 0.214 0.005*
ART 0.031* 1 0.27 0.225 0.235 0.797 0.527 0.358 0.485
BEL 0.289 0.27 1 0.911 0.023* 0.174 0.637 0.853 0.073
COM 0.343 0.225 0.911 1 0.017* 0.142 0.56 0.767 0.057
IMA 0.001* 0.235 0.023* 0.017* 1 0.352 0.07 0.036* 0.623
LEI 0.016* 0.797 0.174 0.142 0.352 1 0.374 0.24 0.659
NAT 0.127 0.527 0.637 0.56 0.07 0.374 1 0.774 0.184
SOC 0.214 0.358 0.853 0.767 0.036* 0.24 0.774 1 0.107
WOR 0.005* 0.485 0.073 0.057 0.623 0.659 0.184 0.107 1
Note: The sign” *” after a number means statistical significance (p < 0.05).
3.1.2 Hapax (L)
Fig. 4 shows hapax (L) and parameter a/b distribution. It indicates that alt-
hough commerce and arts in Fig. 4 (b) tend to be distributed towards the right
side, points in both 4(a) and 4(b) scatter around and it is hard to tell one genre
Quantitative Genre Analysis Using Linguistic Motifs | 173
from others. An ANOVA test on hapax (L) reveals no significant difference
among hapax-legomena distribution across genres (p = 0.218).
(a) Hapax (L) and parameter a
(b) Hapax (L) and parameter b
Fig. 4: Parameter a/b of ZM model and hapax (L) distribution
3.2 F-motifs
Likewise, parameters a and b of ZM model fitting to F-motifs are first discussed
and then hapax (F) is also displayed.
3.2.1 ZM parameters
The probability distributions of genre F-motifs fitted by ZM model are displayed
in Fig. 5. Compared with L-motifs distributions, the difference between F-motifs
distributions is more obvious. It shows that world affairs distribution and imagi-
native distribution are different from other genres with the former lying above
0
5
10
15
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11
Parameter
a
Hapax (L)
APP ART BEL COM IMA LEI NAT SOC WOR
0
1
2
3
0.04 0.05 0.06 0.07 0.08 0.09 0.1 0.11
Parameter
b
Hapax (L)
APP ART BEL COM IMA LEI NAT SOC WOR
174 | Yaqin Wang
others and the latter below all the others. Parameters in Tab. 4 account for this
phenomenon that imaginative distribution has a smaller a value while world
affairs has a larger one. In contrast, world affairs distribution has the lowest
value of parameter b while the other has the highest value. This mirrors the
finding mentioned above that similar to L-motifs, the rank-frequency distribu-
tion of F-motifs is different for nine genres. R2
value in Tab. 4 also demonstrates
that the model fitting is quite good with respect to F-motifs distributions (R2
>
0.9).
Fig. 5: Rank frequency distribution of genre F-motifs fitted by Zipf-Mandelbrot model with the
highest 20 frequency F-motifs. (Note: Dots represent their observed frequency and lines repre-
sent correspondingly predicted value by ZM model)
Tab. 4: Parameters of ZM model for genre F-motifs
Genre a b R2
APP 0.7732 9.0839 0.9517
ART 0.7423 7.4796 0.9536
BEL 0.7516 6.9954 0.9735
COM 0.7594 10.8768 0.974
IMA 0.7466 13.5478 0.9694
0
10
20
30
40
50
60
70
80
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20
Frequency
Rank
APP
ART
BEL
COM
IMA
LEI
NAT
SOC
WOR
APP
ART
BEL
COM
IMA
LEI
NAT
SOC
WOR
Quantitative Genre Analysis Using Linguistic Motifs | 175
Genre a b R2
LEI 0.746 7.8332 0.969
NAT 0.7713 11.976 0.9766
SOC 0.7491 9.0893 0.9591
WOR 0.7868 6.6123 0.9614
The ZM distributions’ parameters are displayed in Fig. 6. Compared with L-
motifs, ZM parameters are irregularly distributed in relation to F-motifs. Param-
eters a are ranging from 0.4 to 0.7 while parameters b from 0 to 45.
Fig. 6: The distribution of ZM parameters a and b for different genres
ANOVA tests among genre parameters were carried out and the result shows
that parameters are significantly different from each other (pa < 0.01, pb = 0.01).
For instance, Tab. 4 shows that a value of imaginative distribution is significant-
ly different from six genres (except arts and leisure, marginally significant in
terms of social science). World affairs distribution is significantly different from
five genres (except applied science, belief and natural science). To summarize
briefly, F-motifs distributions are significantly different across genres.
0
5
10
15
20
25
30
35
40
45
0.4 0.5 0.6 0.7
Parameter
b
Parameter a
APP ART BEL COM IMA LEI NAT SOC WOR
176 | Yaqin Wang
Tab. 5: p value of ANOVA test for parameter a/b of nine genre distributions
(a) p value of ANOVA test for parameter a
APP ART BEL COM IMA LEI NAT SOC WOR
APP 1 0.012* 0.691 0.387 0.001* 0.013* 0.646 0.097 0.176
ART 0.012* 1 0.004* 0.097 0.401 0.981 0.003* 0.387 0.000*
BEL 0.691 0.004* 1 0.207 0.000* 0.004* 0.951 0.040* 0.339
COM 0.387 0.097 0.207 1 0.013* 0.101 0.186 0.423 0.027*
IMA 0.001* 0.401 0.000* 0.013* 1 0.388 0.000* 0.089 0.000*
LEI 0.013* 0.981 0.004* 0.101 0.388 1 0.003* 0.400 0.000*
NAT 0.646 0.003* 0.951 0.186 0.000* 0.003* 1 0.035* 0.370
SOC 0.097 0.387 0.040* 0.423 0.089 0.400 0.035* 1 0.003*
WOR 0.176 0.000* 0.339 0.027* 0.000* 0.000* 0.370 0.003* 1
(b) p value of ANOVA test for parameter b
APP ART BEL COM IMA LEI NAT SOC WOR
APP 1 0.675 0.133 0.600 0.595 0.005* 0.712 0.675 0.380
ART 0.675 1 0.278 0.917 0.911 0.015* 0.430 0.402 0.195
BEL 0.133 0.278 1 0.327 0.331 0.173 0.062 0.055 0.018*
COM 0.600 0.917 0.327 1 0.994 0.020* 0.372 0.346 0.162
IMA 0.595 0.911 0.331 0.994 1 0.020* 0.368 0.342 0.159
LEI 0.005* 0.015* 0.173 0.020* 0.020* 1 0.001* 0.001* 0.000*
NAT 0.712 0.430 0.062 0.372 0.368 0.001* 1 0.960 0.610
SOC 0.675 0.402 0.055 0.346 0.342 0.001* 0.960 1 0.646
WOR 0.380 0.195 0.018* 0.162 0.159 0.000* 0.610 0.646 1
3.2.2 Hapax (F)
The indicator hapax (F) together with ZM parameters are then discussed. It can
be seen that in Fig. 7(a) parameters a and hapax (F) separate leisure texts and
belief texts from others and 7(b) distinguishes arts and leisure from the other
seven genres. An ANOVA test on hapax (F) was carried out and the result shows
that genre hapax(F)s are significantly different from each other (p < 0.05). Tab. 5
demonstrates the significant difference among genres. For instance, hapax (F)s
of leisure is significantly different from other seven genres (except imaginative).
Quantitative Genre Analysis Using Linguistic Motifs | 177
(a) Hapax (F) and parameter a
(b) Hapax (F) and parameter b
Fig. 7: Parameter a/b of ZM model and hapax (F) distribution.
Tab. 6: p value of ANOVA test for hapax (F) of nine genre distributions
APP ART BEL COM IMA LEI NAT SOC WOR
APP 1 0.018* 0.035 0.954 0.000* 0.000* 0.017* 0.579 0.054
ART 0.018* 1 0.000* 0.015* 0.162 0.038* 0.000* 0.068 0.000*
BEL 0.035* 0.000* 1 0.040* 0.000* 0.000* 0.770 0.008* 0.854
COM 0.954 0.015* 0.040* 1 0.000* 0.000* 0.019* 0.540 0.062
IMA 0.000* 0.162 0.000* 0.000* 1 0.491 0.000* 0.001* 0.000*
LEI 0.000* 0.038* 0.000* 0.000* 0.491 1 0.000* 0.000* 0.000*
NAT 0.017* 0.000* 0.770 0.019* 0.000* 0.000* 1 0.003* 0.634
0
0.2
0.4
0.6
0.8
0.4 0.45 0.5 0.55 0.6 0.65 0.7
Parameter
a
Hapax (F)
APP ART BEL COM IMA LEI NAT SOC WOR
0
20
40
60
0.4 0.45 0.5 0.55 0.6 0.65 0.7
Parameter
b
Hapax (F)
APP ART BEL COM IMA LEI NAT SOC WOR
178 | Yaqin Wang
APP ART BEL COM IMA LEI NAT SOC WOR
SOC 0.579 0.068 0.008* 0.540 0.001* 0.000* 0.003* 1 0.014*
WOR 0.054 0.000* 0.854 0.062 0.000* 0.000* 0.634 0.014* 1
4 Conclusion
The present study investigates genre classification purposes of L-motifs and F-
motifs by comparison of hapax legomena and parameters of Zipf-Mandelbrot
model.
It can be concluded that attributes related to motifs, i.e., parameters of ZM
model, proportion of hapax legomena of the F-motifs to the size of the F-motif
tokens, can separate some genres from others. Parameters are different for gen-
re distributions, which result in their different shape of approximation. Regard-
ing L-motifs, imaginative genre has the smallest a and b, whereas social sciences
has the highest b and world affairs the highest a. Likewise, parameter a in genre
distributions for F-motifs has smaller value for imaginative distribution while a
larger one for world affairs. By contrast, world affairs has the lowest value of
parameter b while the imaginative distribution the highest value. For both two
kinds of motifs, there exists significant difference among parameters of ZM
models.
When it comes to hapax legomena, hapax (L) cannot separate genres from
each other. By contrast, the indicator hapax (F), combined with parameters of
ZM model can distinguish certain genres, such as leisure and belief, from others.
It is, however, still worthy of further study how and why some genres differ
from others. Other motif properties, such as R-motifs and D-motifs, can also be
studied in terms of classification purposes in future research.
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188.
Quantitative Genre Analysis Using Linguistic Motifs | 179
References
Altmann-Fitter, 2013. Altmann-Fitter User Guide. The Third Version. Downloadable at.
http://www.ram-verlag.eu/wp-content/uploads/2013/10/Fitter-User-Guide.pdf (2014-11-
29).
Beliankou, A., Köhler, R., & Naumann, S. (2013). Quantitative properties of argumentation
motifs. In: I. Obradović, E. Kelih, & R. Köhler (Eds.). Methods and Applications of Quantita-
tive Linguistics. Selected Papers of the 8th
International Conference on Quantitative Lin-
guisttics (QUALICO) in Belgrade, Serbia, April 26-29, 2012: 35–43. Belgrade: Academic
Mind.
Bentz, C., Kiela, D., Hill, F., & Buttery, P. (2014a). Zipf's law and the grammar of languages: A
quantitative study of Old and Modern English parallel texts. Corpus Linguistics and Lin-
guistic Theory, 10(2), 175–211.
Bentz, C., Verkerk, A., Kiela, D., Hill, F., & Buttery, P. (2014b). Adaptive communication: Lan-
guages with more non-native speakers tend to have fewer word forms. PloS one, 10(6),
e0128254.
Burnard, L. (2000). Reference guide for the British National Corpus (world edition).
Benoit, K. & Nulty, P. (2015). Quanteda: Quantitative Analysis of Textual Data. R package
version 0.9.0-1. http://CRAN.R-project.org/package=quanteda
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In: J. Genzor,
& M. Bucková (Eds.), Favete linguis. Studies in honour of Viktor Krupa: 142–152. Bratislava:
Academic Press.
Köhler, R. (2008a). Word length in text. A study in the syntagmatic dimension. In: S. Mislovičo-
vá (Ed.), Jazyk a jazykoveda v pohybe: 416-421. Bratislava: VEDA
Köhler, R. (2008b). Sequences of linguistic quantities. Report on a new unit of investigation.
Glottotheory, 1(1), 115–119.
Köhler, R. (2015). Linguistic Motifs. Sequences in Language and Text, 69, 89-108.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In: B.
Preisach, & D. Schidt-Thieme (Eds.), Data Analysis, Machine Learning and Applications.
Proceedings of the Jahrestagung der Deutschen Gesellschaft für Klassifikation 2007 in
Freiburg: 637–646. Berlin-Heidelberg: Springer.
Köhler, R., & Naumann, S. (2009). A contribution to quantitative studies on the sentence lev-
el. Issues in Quantitative Linguistics, 34–45.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F- and L-motifs as text characteristics. In: P. Grzybek, E. Kelih, & J.
Mačutek (Eds.). Text and Language. Structures – Functions – Interrelations – Quantitative
Perspectives: 81–89. Wien: Praesens.
Koplenig, A. (2015). Using the parameters of the Zipf–Mandelbrot law to measure diachronic
lexical, syntactical and stylistic changes–a large-scale corpus analysis. Corpus Linguis-
tics and Linguistic Theory. doi: 10.1515/cllt-2014-0049
Lee, D. (2001). Genres, Registers, Text types, Domains, and Styles: Clarifying the Concepts and
Navigating a Path through the BNC Jungle. Language Learning & Technology, 5(3), 37–72.
http://llt.msu.edu/vol5num3/lee/
Mačutek, J. (2009): Motif richness. In: R. Köhler (Ed.), Issues in Quantitative Linguistics: 51-60.
Lüdenscheid: RAM-Verlag.
180 | Yaqin Wang
Mandelbrot, B. (1953). An informational theory of the statistical structure of language. Com-
munication theory, 84, 486–502.
Milička, J. (2015). Is the Distribution of L-Motifs Inherited from the Word Lengths Distribu-
tion? Sequences in Language and Text, 69, 133-146.
Sanada, H. (2010). Distribution of motifs in Japanese texts. In: P. Grzybek, E. Kelih, J. Mačutek
(Eds.). Text and Language. Structures-Functions-Interrelations-Quantitative Perspectives:
183–193. Wien: Praesens.
Jingqi Yan
The Rank-frequency Distribution of Part-of-
speech Motif and Dependency Motif in the
Deaf Learners’ Compositions
Abstract: This paper makes an explorative study of part-of-speech (POS) motif
and dependency motif using the treebanks of deaf students’ writing in three
learning stages. Firstly, the fitting to the POS motif data with Zip-Mandelbrot
distribution yields a good result. However, the dependency motif distribution
fits well by the Popescu-Altmann-Köhler function instead. Then, a further ob-
servation of the top-ranking motifs finds that the most frequent motifs are
formed among the most frequently-used word classes and dependencies. Lastly,
the hapax-type ratio for both motif types shows an extremely heavy proportion
of hapax, which may indicate the discreteness of sentence structure. Through
the three analyses, some changes of sentence structure and the development of
syntax are witnessed across the language learning course.
Keywords: motif, dependency relations, POS, rank-frequency distribution
1 Introduction
Part-of-speech(POS) and syntactic relations are two important quantitative
properties in linguistics. These two properties are inter-related and contain rich
information about sentence structure and syntax of a text, which have been
discussed by many researchers (Liu, 2007, 2009a; Vulanovic & Köhler, 2009;
Tuzzi, Popescu, & Altmann, 2010; Vincze, 2013; Chen, 2016).The majority of
these studies were concerned with the rank-frequency distribution of these
linguistic units themselves in different corpora and it has been proved that the
distribution follows a certain distribution pattern. These quantitative studies,
however, were based on paradigmatic information only and they failed to con-
sider the linear sequential structure, which is a crucial property of language.
An unambiguous and objective linguistic unit hereby is introduced to pro-
vide more sequential and syntagmatic information of a text. Motif, originally
||
Jingqi Yan: Department of Linguistics, Zhejiang University, Hangzhou, China,
jqyan@zju.edu.cn
182 | Jingqi Yan
called segment or sequence, refers to the longest sequence of monotonously
increasing numbers which represents a certain quantitative property of a lin-
guistic unit (Köhler, 2006). Current studies have mainly focused on the study of
Length motif (L- motif) counted in the linguistic property such as syllables (Köh-
ler, 2006, 2008a, b; Köhler & Naumann, 2008, 2009, 2010). It was proved that
motifs, like other linguistic units, display a certain rank-frequency distribution.
Yet such distribution was only testified on limited motif types, especially for L-
motif based on syllables.
A quantitative analysis of R-motif based on categories of POS and syntactic
relations is proposed in this study to testify the distribution of R-motifs and even
more to approach a new perspective to discover the sequence development in
language learning on a framework of dependency grammar. Despite the differ-
ent understandings of syntactic relations, dependency relations based on de-
pendency grammar are used in this paper. Dependency grammarians believe
that a sentence structure in a text is built on the relations between two words
(Mel’cuk, 2003; Nivre, 2006; Hudson, 2007; Liu, 2009b). Inspired by the core
assumption, quantitative syntax studies based on dependency grammar were
conducted (Jiang & Liu, 2015; Liu, 2008a, b). However, only a few research ex-
amined the syntax development. Ninio (2010a, b,2014) has conducted a couple
of research on syntactic development in language learning based on the theory
of dependency grammar and the result showed that the adjacent syntactic struc-
tures are more preferably learned for children. In this paper, we may delve
deeper into the quantitative study of syntax development from dependency
grammar with the help of treebank. As previous studies on quantitative syntax
analysis based on a treebank are mainly about the frequency properties of the
respective linguistic units themselves (Liu, 2009a; Liu, Zhao, & Li, 2009), the
sequence construction and even further, the development or some changes of
sequence construction in the process of language learning still remains an open
issue.
The texts chosen in this paper were collected from the samples of Chinese
deaf writing in different grades. We chose deaf written compositions as our
samples to observe syntax development because most deaf individuals have a
delayed acquisition of language (Svirsky, et al., 2000). Most deaf individuals,
with the absence of acoustic access to language, are constrained by the insuffi-
cient infiltration of language at the young age. Ninety percent of deaf children
are born in the hearing family, so in most cases, they cannot get enough input
of any language form, whether signed or spoken (Johnson & Erting, 1989).
Therefore, the mastery of sign language and the systematic learning of written
language are mainly received among the deaf communities after school. Their
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 183
lexical-semantic and syntactic knowledge, unlike their hearing counterparts
who may have already finished the construction of syntax at the age of 3 to 4,
are slow to develop (Gheitury, Ashraf, & Hashemi, 2013). However, this does not
mean that they do not own innate language capacity. The innate knowledge of
syntax still exists in the deaf disregard of the different modality of language
learning from the hearing children (Meier, 1991). In this case, we may find the
deaf an interesting subject of research because their natural acquisition of syn-
tax is delayed or somewhat achieved in a slow-motion picture which may help
us better observe the changes of syntax.
As Köhler and Naumann (2008) have hypothesized, motifs, like other basic
linguistic units, follow a monotonously decreasing frequency distribution. We
therefore believe that the motif or sequence structure of POS and dependency
relations may also follow a lawful distribution pattern. In addition, changes of
rankings of the two motif types are inter-related and can explain some syntactic
developmental trajectories. Two research questions are included:
(1) Will the R-motif counted in POS and dependency relations of the deaf
leaners’ written treebank abide by Zipf-Mandelbrot distribution as pro-
posed by previous researches (Köhler & Naumman, 2008, 2010; Köhler,
2008a)?
(2) Will the result of statistical motif analysis distinguish any differences of
syntactic complexity of among texts? Or put it in a more detailed way,
according to the frequency distribution of R-motif, can we observe
some changes in the sequence structure in different grade level of the
writing?
2 Method and Data
Treebank is considered as a valid resource for syntax analysis of dependency
grammar. It is based on the core concepts that 1) the two syntactic-related lin-
guistic units form a binary relation with a specific direction; 2) the relations
between the two units are asymmetrical, one as the governor and the other as
the dependency; 3) the relations are labeled and linked by a dependency rela-
tion (Liu, 2007, 2009b). Samples are mainly students’ compositions and diaries.
One advantage of treebank is that it contains not only word class information
but also dependency types. This will provide abundant information on both
lexical and syntactic level. In the building of the treebank, three basic elements
are supporting each other, namely the governor, the dependent and the de-
pendency relations. We built a manually annotated treebank of the composi-
184 | Jingqi Yan
tions written by deaf students from the 4th
grade in the primary stage to the 3rd
grade in the senior high stage in one of the Chinese Special Education Schools.
Under the guidance of the Chinese dependency syntax, we built a Chinese Tree-
bank of the deaf students’ compositions from those 9 grades. We then grouped
the texts into three sub-treebanks of three learning stages: primary, junior and
senior. The final treebank includes 594 sentences and 10380 word tokens. For
the general information of each stage, see Tab. 1. The consistency of the amount
of words in every sub-treebank is guaranteed. The treebank is at hand with
complete POS and dependency relation tagging.
Tab. 1: General data information for the deaf writing treebank
We take Example (1) to analyze the dependency structure. Tab. 2 presents the
format of Example (1) in the treebank of deaf writing samples. From the table, it
is noticed that the POS of each word and the dependency types are marked. In
the dependency relation, two syntactically related words in the sentence form a
dependency type. A dependency relation has a one-way direction. There must
be a word functioning at a higher syntactic hierarchy and it serves as a governor
to govern the other word. The other word therefore is treated as a dependent. To
clarify the relations, the dependency structure is illustrated in Fig.1. Following
the rule, the treebank of the 3 sub-corpora was built. It is specially noted that in
this treebank, we did not annotate any language errors. Neither did we try to
correct the error. The reason is to try to retain the syntactic structure as it is so
that the linguistic features of a language learners’ writing may best and objec-
tively be reserved.
Ex (1)
Wo xihuan kan Wangxiaoer zhe ge gushi
I like read Wangxiaoer the QUANTIFIER story
‘I like reading the story of Wangxiaoer’
Stage Text numbers Word token numbers Sentence numbers
Primary 24 3139 187
Junior 12 3459 212
Senior 16 3782 196
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 185
Tab. 2: Annotation of a sample sentence in the deaf treebanking
Word
number
Word POS Governor
word
number
Governor
word
POS of
governor
word
Dependency
relation
1 Wo r 2 xihuan v subj
2 Xihuan v 8 . bjd s
3 kan v 2 xihuan v obj
4 Wangxiaoer nr 5 zhe r epa
5 zhe r 6 ge q qc
6 ge q 7 gushi n atr
7 gushi n 3 kan v obj
8 . Bjd
Fig. 1: The dependency structures of Ex(1)
To convert the above POS and dependency relations in the treebank into motifs,
it is better to review the definition of R-motif proposed by Köhler (2015) first. R-
motif is based on categorical sequences and is an uninterrupted sequence of
unrepeated elements. POS and dependency relations can both be considered
categorical. Therefore for the sentence flow “Wo xihuan kan Wangxiaoer zhe ge
gushi. Wangxiaoer fang niu. Guizi lai le.”, the segmentation into R-motif pre-
sented as a sequence of POS is as follows:
(Wo xihuan) (kan Wangxiaoer zhe ge gushi.)
(Wangxiaoer fang) (niu.) (Guizi lai le.)
(R V) (V NR R Q N)(NR V)(N)(N V UM)…
‘I like reading the story of Wangxiaoer. Wangxiaoer is
looking after the cows. The enemy soldiers came.’
The R-motif presented as a sequence of dependency relations is as the following:
186 | Jingqi Yan
(Subj S Obj Epa Qc Atr) (Obj Subj S)(Obj Subj S Esa)
As the motifs are formed and counted, we may come out with several statistical
results. The software Altman-fitter was used to calculate the best-fit frequency
distribution.
3 Results and Discussion
Section 2 presents the building of dependency treebank and illustrates the motif
formation procedures based on POS and dependency relations. In this section,
we will firstly investigate the frequency distribution of POS R-motif and De-
pendency R-motif. We attempt to find some distribution regularities in the two
motifs and also discover several sequence ordering patterns in the top-ranking
and bottom-ranking lists.
3.1 Frequency Distribution Analysis
In this sub-section, we may firstly corroborate the Zipfian frequency distribution
fitting to the POS R-motif and Dependency R-motif. The Zipf-Mandelbrot distri-
bution was fitted to the two motifs and other possible distributions were dis-
cussed as well.
3.1.1 Fitting of the R-motif on the basis of POS
Köhler (2015) has conducted some examinations and he found that motifs dis-
play a rank-frequency distribution of the Zipf-Mandelbrot type. It is believed
that motifs have the similar quantitative behavior to other linguistic units. We
first investigated and fitted the distributions of the R-motif of POS tags. Thirteen
main parts of speech are included in the treebank. They are: A-
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 187
Fig. 2: Fitting Zipf-Mandelbrot distribution as fitted to the data from three stages
attributor, V-verb, A-adjective, R-pronoun, D-adverb, C-conjunction, M-
numeral, P-preposition, U-Auxiliary and Q-quantifier.
Fig. 2 gives a Bi-logarithmic impression of the fitting of Zipf-Mandelbrot dis-
tribution to the three sub-treebanks of different stages. The results were general-
ly good (R2
>0.95) in the three distributions and the determination coefficient (R2
)
was getting better with the growth of grades. The individual fitting data of the
three sub-treebanks is presented in Tab. 3.
Tab. 3: Fitting the data by Zip-Mandelbrot distribution for the POS R-motif from the three sub-
treebanks
Stages P(X2
) DF R2
A b n
Primary 1 265 0.95 0.99 0.59 341
Junior 1 299 0.96 1.00 0.78 394
188 | Jingqi Yan
Stages P(X2
) DF R2
A b n
Senior 1 321 0.97 1.01 0.77 426
3.1.2 Fitting of the R-motif on the basis of dependency relations
We then testified the data of R-motif based on dependency relations to the Zipf-
Madelbrot distribution. The 32 types of dependency relations annotated here
followed Liu’s categorization (2007). Tab. 4 shows the distribution results.
Concerning the statistical result, the Zipf-Mandelbrot distribution in the
three sub-treebanks yielded not very good fitting (R2
<0.75). The result conforms
to Liu’s study (2007) in which the frequency distribution of dependency types
did not quite fit the Zipf-like power-law curve either. Similarly, the fitting of the
rank frequency distribution of the dependency motifs was not as good as that of
other linguistic units.
Tab. 4: Fitting the data by Zipf-Mandelbrot distribution for the dependency R-motifs from the
three sub-treebanks
Stages P(X2)
DF R2
A B n
Primary 1 395 0.75 0.66 1.82 552
Junior 1 433 0.70 0.64 2.28 608
Senior 1 464 0.72 0.66 1.79 657
Popscu, Altmann and Köhler (2010) proposed the Popescu-Altmann-Köhler
(PAK) function to capture heterogeneity in texts in the fitting of language units.
It is argued that despite the general Zipf-like distribution pattern, a text also
composes of several sub-sequences. Vincze (2013) conducted the PAK function
fitting to the dependency relations in Szeged treebank and yielded good results.
We hypothesize that the dependency R-motifs may also display a PAK func-
tion(y=1+a*exp(-x/p)+b*exp(-x/q)) as a stratified structure. The fitting curves of
the three stages are presented in Fig. 3 and the fitting data is shown in Tab. 5,
where most determination coefficient R2
are larger than 0.95. The fittings for the
three stages are all very good.
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 189
Tab. 5: Fitting the data by PAK distribution in the three sub-treebanks
Stages a b p q R2
Primary 5.81 55.66 28.80 2.19 0.99
Junior 46.68 4.26 2.80 35.21 0.98
Senior 56.61 5.11 2.75 32.93 0.98
Observing from the curve in Fig.3, a clear stratification is found in all stages.
The discrete components which are displayed as outliers in the whole curve
represent the most frequent classes. Most quantities of motifs, on the contrary,
are low in frequency, where the long tail of the curve indicates the heavy pro-
portion of hapax legommena. This condition is shown as a long tail in the fitting
curve which indicates the heavy proportion of hapax legommena. We may dis-
cuss the long-tail feature in the next section.
Based on the frequency distribution of the POS R-motif and Dependency R-
motif, different fitting results were found. The fitting of the frequency distribu-
tion of POS-based R-motifs to the Zipf-Mandelbrot distribution yielded a good
result. The higher the learning stage , the better the fitting of the curve . The
frequency distribution of dependency R-motifs however, did not seem to abide
by the Zipfian law. But the stratification of the motifs was found via a good fit-
ting of Popescu-Altman-Köhler distribution. Therefore, not all motif types fit the
Zipf-Mandelbrot distribution as the previous research found. Motifs on the basis
of dependency relations may have a variant distribution behavior and the
goodness-of-fit may vary by the language skills of the text.
190 | Jingqi Yan
Fig. 3: Fitting PAK function as fitted to the data from three stages
3.2 The Top Ranked R-motifs
The fitting of the frequency distribution alone can only provide a macro image
of the language property. In an attempt to get a closer view about the ranking
distribution of the two motif types, in this section, the top motif presentation
was listed and compared among the three learning stages. We attempted to
discover the top motif sequence in both motif types.
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 191
3.2.1 Ranking list of POS-based R-motifs
We listed and compared the top-20 POS-based R-motifs in the three learning
stages. Tab. 6 presents the ranking results. We can observe that in the primary
stage, the POS which make up the most frequent sequence include noun (N),
verb (V), pronoun (R), auxiliary (U), and adjective (A) and conjunction (C).
While in the junior and senior stages, adjective does not appear in the R-motif
and adverb plays a greater role in the top ranking motifs instead. It is also no-
ticeable that the ranking of noun and verb changes too. These changes go in
line with the frequency distribution of part of speech.
Tab. 6: The top ranking lists of POS R-motif distribution
Rank Primary Junior Senior
1 N V V
2 N+V V+N V+N
3 V N N
4 V+N N+V N+V
5 V+R V+R V+D
6 N+U V+D V+R
7 N+R+V V+U D+V
8 V+U+N N+D+V V+N+U
9 V+U+R D N+D+V
10 N+A N+U V+U+N
11 N+V+A N+V+R V+U
12 A+N+V V+U+N N+U
13 N+V+U D+V R+V
14 V+N+R V+N+R N+V+U
15 N+C V+R+D V+N+D
16 N+D+A V+U+R V+U+R
17 V+A+N V+R+N U+N
18 N+D+V V+N+D N+R+V
19 N+V+R V+N+U V+R+N
20 V+U+A+N N+V+U V+R+D
192 | Jingqi Yan
Fig. 4: The frequency distribution of POS
Fig. 4 lists the frequency of each POS in the three stages. The most frequently
appeared POS are almost all included in the top ranking lists of POS R-motif,
which may indicate that the most frequent word classes may also more prefera-
bly be combined together into one linear sequence. When one observes the
proportion changes in the three stages, it is noticed that the frequency of noun
and adjective in the primary are relatively higher than the other two stages,
while the frequencies of adverb and verb go higher in the junior and senior
stages. The POS R-motif ranking has the corresponding changes as in the POS
ranking.
3.2.2 Ranking list of dependency-based R- motifs
With the similar conduct, the 20 most frequent R-motifs were listed here to ob-
serve more vividly about the motif distribution characteristics. Table 7 presents
the top 20 ranking R-motif based on dependency relations. The main dependen-
cy relations included in the top R-motifs are object (OBJ), attribute (ATR), sub-
ject (SUBJ), “de” construction (DEC), adverbial (ADVA), clause relation (CR),
main governor (S), which are all high frequency dependency relations.
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 193
Tab. 7: The top ranking list of dependency R-motif distribution
Ranking Primary Junior Senior
1 OBJ ADVA ADVA
2 ATR OBJ OBJ
3 SUBJ ADVA+CR ATR
4 ATR+DEC ATR ADVA+CR
5 ADVA ADVA+CR+SUBJ ATR+OBJ
6 OBJ+CR ADVA+SUBJ ATR+DEC
7 SUBJ+CR ATR+OBJ CR
8 SUBJ+CR+OBJ ADVA+S ADVA+SUBJ
9 ADVA+CR OBJ+SUBJ+CR ATR+OBJ+ADVA
10 SUBJ+CR+ESA COOR ADVA+CR+SUBJ
11 SUBJ+S DEC+ATR DEC
12 SUBJ+S+ATR OBJ+ADVA ATR+ADVA+DEC
13 CR+SUBJ ADVA+CR+ESA CR+OBJ
14 SUBJ+ADVA+
CR
ADVA+CR+OBJ EPA
15 ATR+OBJ+DEC ADVA+POBJ OBJ+CR
16 ATR+SUBJ+CR CR+ESA SUBJ+ADVA
17 ATR+SUBJ+S CR+OBJ ADVA+ATR+DEC
18 CR SUBJ ADVA+CR+OBJ
19 CR+OBJ+SUBJ SUBJ+ADVA+CR ADVA+DEC+ATR
20 EPA ADVA+ATR ADVA+S+ATR
194 | Jingqi Yan
Fig. 5: The frequency distribution of dependency relations
Variations of the motif distribution are found among three stages. In the prima-
ry stage, the uninterrupted sequences reflecting the elementary order in Chinese
take a large proportion, e.g. SUBJ+CR,SUBJ+S, SUBJ+ADVA+CR, SUBJ+S+OBJ,
SUBJ +CR+ESA etc. In the junior and senior stage, more modifying relations are
added before subject, object and head. In addition, one can also discover that
the main modifying relation in the primary stage is attribute, which accounts for
a big proportion. While coming to the junior and senior stage, the adverbial
becomes a more frequent modifying relation. The changes are also reflected in
the frequency distribution of dependency relations (See Fig. 5). In Fig. 5, the
frequency distribution of the main dependency relations are compared among
stages. Remarkable changes are seen in adverbial and subject as the frequency
of adverbial increases greatly in junior and senior stages and the subject use
decreases steadily with the growth of stages. The changes may indicate some
structure development in the deaf students’ syntax.
When we compare the ranking result of POS R-motifs with that of depend-
ency R-motifs, several connections are found. We may find that the presentation
of the two motifs shares similar rules. For all the sub-treebank of different stag-
es, the most frequent types of POS and dependency relations have more chance
to form the frequent motifs. The POS of a word contains much syntactic infor-
mation. Each word class has its syntactic function and has its own preferable
combination to another word class. On this basis, the POS motif is interrelated
with dependency relation motif.
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 195
The possible developmental property of a text is also observed. Seen from
the motif structure changes, it is found that attribute appears early in the prima-
ry stage but in the junior and senior stage, adverbial modifier is more frequently
combined with other dependency relations. The POS motif reveals the similar
pattern, with adjective appearing frequently as a component in the motif in the
primary stage and adverb a more frequent combining component in the junior
and senior stage. These results are roughly in line with previous research on
word class learning, showing that among the four main word classes of noun,
verb, adverb and adjective, noun are the best known word class and adverb the
least known, with verbs and adjectives in between (Phillips, 1981; Rodgers,
1969). This might explain the frequentappearance of attribute structure at early
ages and the adverbial structure appearing at later stages. In the course of
learning, students are more inclined to learn attribute first and adverbial struc-
ture may be considered as a more advanced one.
Seen from the motif sequence itself, the structure of the sentence in stu-
dents’ writing is relatively simple with the repetition of basic order of SVO. This
demonstrates the elementary order in Chinese, i.e., Subject-Verb–Object struc-
ture. This SVO structure appears in the top R-motif frequency ranking which is
presented in the dependency motif forms as SUBJ+CR, SUBJ+S,
SUBJ+ADVA+CR, SUBJ+S+OBJ, SUBJ +CR+ESA etc., and also revealed in the
POS motif, like V+N, N+V, V+NR, N+V+R. As was discussed before, R-motif
forms in the condition that the sequence is uninterrupted by unrepeated de-
pendency relations or POS. Therefore in one extreme circumstance when the
text is composed only of SVO sentences, the text will follow the most basic se-
quence as (SVO) (SVO) (SVO)...As a consequence, the text will be quite easy in
structure with only one motif type presented as SVO. In the actual language use,
the SVO structure is often interrupted and departed when additional modifying
relations join in in order to accommodate a more complicated communicative
need. This will explain why the dependency R-motifs are more structured at an
early stage but scattered with more dependency relations combined in the se-
quence at the later stage. The discreteness of motifs in the junior and senior
stage is possible to predict a more complex language use as more dependency
relations are added to the SVO sequence.
3.3 Hapax Ratio Comparisons of R-motif
In the previous section, weobserved the long-tail characteristic of the distribu-
tion curve, which indicates the extremely heavy weight of hapax legomena in
the dependency R-motif distribution. A hapax legomonon is a word type occur-
196 | Jingqi Yan
ring only once in a text. Researchers have found that hapax legomena can indi-
cate some characteristics of a text (Plag, Dlaton-Puffer, & Baayen,1999) and thus
can be a good indicator of the number of neologisms, of authorship and of the
synthetism/analytism of language and also of vocabulary richness (Popescu &
Altmann, 2008, Lindquist, 2007). In this study, we extend the definition of ha-
pax legommonon to a motif type counted by one occurrence. Comparing the
hapax proportion in relation to the whole motif types among the three stages, a
significant steady decrease among stages was found in the POS R-
motifs(X2
(2,1162)=6.331, p=0.042) but not inthe dependency R-motifs.
Tab. 8: Hapax-token ratio of POS R-motif
Primary Junior Senior
Hapax 304 269 215
Motif types 426 394 342
Motif tokens 1324 1230 1114
Hapax/type(%) 71.36 68.27 62.87
Tab. 9: Hapax-token ratio of dependency relation R-motif
In Tab. 8 and Tab. 9, the hapax-type ratio of dependency R-motifsis significantly
higher than that of POS motifs. Such discrepancy is probably due to the varie-
ties of dependency types and POS. As was mentioned before, there are 13 POS
types and 32 dependency types in total. The more varities of dependency types
may contribute to more combinations of motif sequences. With the few depden-
cy motifs to be indentical in types, a majority of motifs take on their own se-
quences, thus forming a high proportion of hapax.
When it comes to the developmental changes, the hapax-type ratio of POS
R-motif decreases monotonously with the growth of stages whereas the depend-
ency R-motif does not reach a significant decline. When the hapax-type ratio is
Primary Junior Senior
Hapax 580 523 471
Motif types 657 608 552
Motif tokens 943 860 810
Hapax/type(%) 88.28 86.02 85.33
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 197
calculated by word types, a higher hapax-type ratio of the text may have the
implication of a richer vocabulary and higher information density(Fox, 1999).
Therefore the ratio is supposed to be increasing with the growth of stages as
students achieve better vocabulary and syntax along with education. However,
when the hapax-type ratio is counted by R-motif, the situation may be different.
With the growth of learning stages, the hapax-type ratio for POS R-motif is de-
creasing instead (See Tab. 8). The POS R-motif reflects the sequence of a sen-
tence or text, as previously indicated; and there is always a syntactic rule un-
derneath considering the structure or the sequence of a sentence, so the
alignment of the POS sequence in the text is constrained by the grammatical
rules. In a well-organized text, the patterns of the sequence in the text are more
likely to be fixed with a small amount of variance of sequence. In the deaf chil-
dren’s writing, many POS errors are found, which may cause the chaos of the
sequence. We calculated the POS errors among three stages and found a signifi-
cant decrease in POS errors (X2
(2,55)=9.153,p=0.01). In the primary stage, students’
language capacity is not yet well developed, so they tend to make more POS
mistakes, which leads to the interruption of the original POS sequence. As a
consequence, more abnormal motifs occur. With the development of vocabulary
use, fewer POS errors are made, so the occurrences of abnormal motifs decrease
accordingly. While for dependency R-motifs, the hapax-type ratio seems to be
more influenced by the sequential complexity of syntactic relations. With 32
types of dependency relations, the sequence or the alignment may vary much in
types. The complexity may somewhat weaken the influence of syntactic matura-
tion to obtain more organized sequences. Still we see a decreasing trend from
the date. Therefore, it should be noted that the hapax-type ration for motifs may
be affected by both the sequence density and the number of motif types. The
question remains for future research as to how the two factors influence the
hapax-type ratio of motifs.
4 Conclusion
The advantages for motif studies are that the segmentation is unambiguous and
objective and thus a syntagmatic linear description of language can be yielded.
In the present study, properties of part-of-speech and dependency relations are
taken into account as the elements to form motif for the first time. The texts
chosen vary in the educationlal stage too. Here, a comprehensive motif study
bears several remarkable results. Motifs based on different language properties
may not always follow the Zipf-Mandelbrot distribution as proposed by Köhler.
198 | Jingqi Yan
The fitting result for the texts of different language maturity may differ too.
Motif studies using the unit of POS and dependency relation can have some
implications for the syntax development in the language learning. It is proposed
here that the distribution features of motifs can be a good indicator of the lan-
guage lmaturity of a text.
Future researches may further testify the frequency distribution of motifs
considering more linguistic units. A second level analysis on the Length of the
R-motifs is open for further studies. More quantitative features are yet to be
studied like the possible stratification features in motifs. Moreover the implica-
tions for text level classifications in motif studies need more quantitative prove
from other writing samples and other languages as well by using a larger corpus
or a control hearing corpus.
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188.
References
Chen, R. (2016). Quantitative text classification based on POS-motifs. (this volume)
Fox, R. (1997). Management ergolect as a separate language variety. Studia romanica et angli-
ca Zagrabiensia, 42, 113-126.Gheitury, A., Ashraf, V., & Hashemi, R. (2014). Investigating
deaf students’ knowledge of Persian syntax: Further evidence for a critical period hypoth-
esis. Neurocase, 20(3), 346–354.
Hudson, R. (1984). Word Grammar. Oxford: Blackwell.
Jiang, J., & Liu, H. (2015). The effects of sentence length on dependency distance, dependency
direction and the implications–Based on a parallel English–Chinese dependency tree-
bank. Language Sciences, 50, 93–104.
Johnson, R., & Erting, C. (1989). Ethnicity and socialization in a classroom for Deaf children. In
C. Lucas (Ed.), The sociolinguistics of the Deaf community (pp. 41–84). New York, NY: Aca-
demic Press.Köhler, R. (2006). The frequency distribution of the lengths of length se-
quences. In: J. Genzor & M. Bucková (Eds.), Favete linguis. Studies in honour of Victor
Krupa (pp.145-152). Bratislava: Slovak Academic Press.
Köhler, R. (2008a). Word length in text. A study in the syntagmatic dimension. In: S. Mislovičo-
vá. (Ed.), Jazyk a jazykoveda v pohybe (pp. 416–421). Bratislava, VEDA: Vydavatel'stvo
SAV.
Köhler, R. (2008b). Sequences of linguistic quantities report on a new unit of investiga-
tion. Glottotheory, 1(1), 115–119.
The Rank-Frequency Distribution of Part-of-speech Motif and […] | 199
Köhler, R. (2015). Linguistic motifs. Sequences in Language and Text, 69, 89.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F-and T-segments. In: C.
Preisach et al.(Eds.), Data Analysis, Machine Learning and Applications (pp. 637-645).
Heidelberg, Berlin: Springer.
Köhler, R., & Naumann, S. (2009). A contribution to quantitative studies on the sentence level.
In R. Köhler (Ed.), Issues in Quantitative Linguistics (pp.34–45). Lüdenscheid: RAM-Verlag.
Köhler, R., & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F-and L-motifs as text characteristics. In: P. Grzybek, E. Kelih and J.
Mačutek (Eds.). Text and Language (pp.81–89). Wien: Praesens.
Liu, H. (2007). Probability distribution of dependency distance. Glottometrics, 15, 1–12.
Liu, H. (2008a). Dependency distance as a metric of language comprehension difficul-
ty. Journal of Cognitive Science, 9(2), 159–191.
Liu, H. (2008b). The complexity of Chinese syntactic dependency networks. Physica A: Statisti-
cal Mechanics and its Applications, 387(12), 3048–3058.
Liu, H. (2009a).Probability distribution of dependencies based on Chinese dependency tree-
bank. Journal of Quantitative Linguistics, 16 (3), 256–273.
Liu, H. (2009b). Dependency Grammar: From Theory to Practice. Beijing: Science Press.
Liu, H., Zhao, Y. & LI. W. (2009). Chinese syntactic and typological properties based on de-
pendency syntactic treebanks. Poznań Studies in Contemporary Linguistics, 45(4), 509–
523.
Lindquist, H. (2007). Viewpoint-wise The spread and development of a new type of adverb in
American and British English. Journal of English linguistics, 35(2), 132–156.
Meier, R. (1991). Language acquisition by deaf children. American Scientist, 79(1), 60–70.
Mel’cuk, I. (2003). Levels of dependency in linguistic description: Concepts and problems.
Dependency and valency. An International handbook of contemporary research, 1, 188–
229.
Nivre, J. (2006). Inductive dependency parsing. Dordrecht, Netherlands: Springer.
Ninio, A. (2010a). Predicate and argument. In: P. Hogan (Ed.), The Cambridge encyclopedia of
the language sciences (p. 659). Cambridge: Cambridge University Press.
Ninio, A. (2010b). Two-word stage. In: P. Hogan (Ed.), The Cambridge encyclopedia of the lan-
guage sciences (pp. 877–878). Cambridge: Cambridge University Press.
Ninio, A. (2014) Syntactic development: dependency grammar perspective. In P. Brooks, V.
Kempe, & J. Golson (Eds.), Encyclopedia of language development. Thousand Oaks,
CA: SAGE Publications.
Plag, I., Dalton-Puffer, C., & Baayen, H. (1999). Morphological productivity across speech and
writing. English language and linguistics, 3(02), 209–228.
Popescu, I., & Altmann, G. (2008). Hapax legomena and language typology. Journal of Quanti-
tative Linguistics, 15(4), 370–378.
Popescu, I., Altmann, G., & Köhler, R. (2010). Zipf’s law—another view. Quality & Quanti-
ty, 44(4), 713–731.
Phillips, T. (1981). Difficulties in foreign language vocabulary learning and a study of some of
the factors thought to be influential. Birkbeck College, University of London: MA Project.
Rodgers, T. (1969). On measuring vocabulary difficulty: An analysis of itemvariables in learning
Russian-English vocabulary pairs. International Review of Applied Linguistics, 7, 327–343.
Svirsky, M., Robbins, A., Kirk, K., Pisoni, D., & Miyamoto, R. (2000). Language development in
profoundly deaf children with cochlear implants. Psychological science, 11(2), 153–158.
200 | Jingqi Yan
Tuzzi, A., Popescu, I., & Altmann, G. (2010). Quantitative Analysis of Italian Texts. Lüdenscheid,
Germany: RAM-Verlag.
Vincze, V. (2013). Domain differences in the distribution of parts of speech and dependency
relations in Hungarian. Journal of Quantitative Linguistics, 20(4), 314–338.
Vulanović, R., & Köhler, R. (2009). Word order, marking, and parts-of-Speech systems. Journal
of Quantitative Linguistics, 16(4), 289–306.
Jiang Yang
Quantitative Properties of Polysemy Motifs
in Chinese and English
Abstract: Polysemy is a fundamental and universal property of human lan-
guages. This paper explores the basic statistics and fundamental quantitative
properties of the polysemy motifs in Chinese and English. Results show that the
Chinese and English rank-frequency polysemy motif distributions fit the Zipf-
Mandelbrot distribution with different parameters; the length motif distribu-
tions of the Chinese and English polysemy motifs fit the mixed negative binomi-
al distribution; the micro analysis of the lengths of the Chinese and English
polysemy motifs reveals many language-specific characteristics; the meanings
of English polysemes are of relatively high flexibility compared with Chinese;
and the English polysemes are more context-dependant than the Chinese ones.
Keywords: polysemy motif, contrastive study, distribution, length, flexibility,
context-dependant
1 Introduction
Polysemy refers to the linguistic phenomenon in which a single lexical item is
associated with more than one related senses. As a large number of word forms
have multiple meanings, polysemy is pervasive in natural languages. According
to previous statistics, over 40% of English words have more than one meaning
(Traxler, 2011) while approximately 20% of lexical entries in The Contemporary
Chinese Dictionary (5th Edition) are polysemes (Wang, 2009).
Polysemy is a fundamental and universal property of human languages.
The questions concerning polysemy, along with other semantic characteristics
of language (particularly e.g. monosemy, homonymy, synonymy, antonymy,
hyponymy), have drawn great attention of researchers from linguistics, psy-
chology and cognitive science. For example, polysemy brings with it semantic
ambiguity, and the resolution of the latter (often referred to as “word sense
disambiguation”) has become a relatively independent research issue and an
||
Jiang Yang: School of Foreign Studies, Hunan University of Science and Technology, Xiangtan,
China, yangjiang@hnust.edu.cn
202 | Jiang Yang
open question for a long time in computational linguistics (Ide, 1998). Moreo-
ver, it is also worthwhile to mention that to encode semantic relations between
words and provide richer lexical knowledge for Natural Language Processing
tasks, structured knowledge resources such as WordNet, HowNet, Roget’s The-
saurus and BabelNet have been constructed besides the on-going compilation of
traditional dictionaries.
Polysemy is closely connected to other language properties. The “Law of
Meaning” theory (Zipf, 1945) states that there exists a power-law relationship
between the more frequent words and the less frequent words. The more fre-
quent words have more senses than the less frequent words. The synergetic
model of language (Köhler, 2005a) has proved the particular relationships
among polysemy, synonymy and other language characteristics such as fre-
quency, word length, inventory sizes of language units and functional loads of
language units. Polysemy can be viewed as a language property ruled by a
complex mechanism emerging as a result of intricate interrelations among
communication requirements (Köhler, 2005b).
Albeit that a number of prior studies have been conducted on polysemy, it
remains unexplored from the quantitative perspective. Just as Köhler (2015) puts
it, quantitative linguistics has been concerned with units, properties and their
relations mostly in a way where syntagmatic (or taken to mean “sequential”)
behaviour of the objects under study is ignored. Therefore, linguistic motif, a
new linguistic unit, is proposed (Köhler, 2006, 2015) to explore the sequential
structures with respect to all kinds of linguistic units and properties from a syn-
tagmatic point of view. On the basis of the notion of linguistics motif, this paper
deals with polysemy motifs in Chinese and English in an effort to examine the
quantitative properties of the two languages under discussion.
Two types of resources are used in this paper: the sense dictionaries and the
polysemy motif corpora for both Chinese and English respectively. Specifically,
the Chinese sense dictionary is sourced from The Contemporary Chinese Diction-
ary (5th Edition) and the English sense dictionary from The American Heritage
Dictionary (4th Edition). The original dictionaries contain a total of 68,764 and
75,685 lexical entries respectively while the derived sense dictionaries include
only 62,985 and 71,696 lexical entries respectively for the lexical entries with
same word form but different parts of speech are incorporated by increasing one
sense for each part of speech. With regard to the corpora, a sample of the Peo-
ple’s Daily Corpus with 2,531,511 words is used as the Chinese polysemy motif
corpus and the English one is a 2,787,208 word corpus sampling from the British
National Corpus. The polysemy motif data for Chinese and English are extracted
from the corpora with the aid of a program which automatically retrieves each
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 203
word’s sense number in the sense dictionaries. In case of unknown words,
punctuations and other special characters, their returned values are all as-
signed as “1”.
The remainder of the paper is organised as follows. The definition of the
polysemy motif and an overview of the Chinese and English polysemy motifs are
presented in Section 2. Section 3 explores the distribution property of the poly-
semy motifs in Chinese and English. Section 4 mainly investigates the probabil-
ity distribution properties of the lengths of polysemy motifs. Section 5 illustrates
the contrastive properties of the Chinese and English languages. The conclusion
is given in Section 6.
2 Polysemy Motifs in Chinese and English
According to Köhler (2015), linguistic motif is defined as the longest continuous
sequence of equal or increasing values representing a quantitative property of a
linguistic unit. The investigation of linguistic motifs focuses on the sequential
structure of linguistic expressions in general texts. The introduction of this new
unit into quantitative linguistics endeavours to find an alternative method
which can give information about the sequential organisation of a text with
respect to any linguistic unit and to any of its properties without relying on a
specific linguistic approach or grammar.
A polysemy motif (P-motif hereafter) is a continuous series of equal or in-
creasing polysemy values (Köhler, 2015). In this paper, the polysemy values are
confined to those of the words in a given language, thus the so-called “polyse-
my values” refer to the total sense numbers of the given words in a specific lan-
guage. For example, the following two sentences can be represented by a se-
quence of P-motifs respectively according to the above definition:
(1) 农业 是 国民经济 的 基础 ， ８亿 农民 的 富足 、 稳定 至关重要 。
(Agriculture is the foundation of the national economy. The prosperity and
stability of the 800 million peasants is of great concern.)
(1-15) (1-10) (2) (1-1-1-10) (1-1-4) (1-1)
(2) And then he withdrew after four sets against the former champion Mi-
chael Chang.
(8-11-11-16-19) (6-153) (13-22) (11-12) (1-1-1)
It is notable that in Köhler’s definition of motif, sentential boundaries are bro-
ken up and the motifs may span two or more sentences. In other words, theoret-
204 | Jiang Yang
ically, it is possible for a text, no matter how many sentences there are in it, to
have only one P-motif, starting from the very beginning and stopping at the
end. Different from that, in this paper we put a constraint on P-motifs: any P-
motif should stop at the end of a sentence. Meanwhile, all punctuations have
the same default polysemy value, i.e., “1”. We hold that language is different
from music in that utterances have natural stops while a sequence of notes in
music may last forever. We assume that this little revision of P-motif will not
affect the mode and functionality of the whole linguistic motif system.
Tab. 1 shows some basic information about the P-motif data obtained from
the polysemy motif corpora.
Tab. 1: Basic information about the P-motif data used in this paper
Language Sentential
Sequence
P-motif
Token
P-motif
Type
Average P-
motif Length
Average Poly-
semy Value
Chinese 101,095 989,440 9,363 2.5585 2.4404
English 129,341 1,414,757 72,734 1.97 4.7599
We observe that while the sizes of the Chinese and English polysemy motif cor-
pora are roughly equal and so are the numbers of sentential sequences, the P-
motif tokens in both languages show a difference to a small degree, with the P-
motif token number in English slightly greater than that in Chinese. This is re-
lated to the average P-motif length (APL) as the P-motif token number is in in-
versed proportion to APL. Given the size of a corpus, the greater the APL is, the
less the P-motif token number is.
The most prominent difference between Chinese and English P-motif given
in Tab. 1 is their P-motif types. To make a reasonable contrast, it is necessary to
consider not only the P-motif types, but also the corresponding P-motif tokens.
We borrow the concept of type/token ratio (TTR) from corpus linguistics to deal
with the issue. The TTR of the Chinese P-motifs is 0.9463% while that of the
English P-motifs is 5.1411%. A high TTR indicates a large amount of P-motif
variation and a low one indicates relatively little P-motif variation. Therefore,
the P-motifs are less varied in Chinese than in English.
The average polysemy value (APV) in Chinese (2.4404) is nearly twice less
than that in English (4.7599). Compared with those in the sense dictionaries (the
APVs in the Chinese and English sense dictionaries are 1.3489 and 3.4701 re-
spectively), the values increase accordingly to a significant extent, indicating
polysemes are actively used in both languages. As to the APV differences be-
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 205
tween Chinese and English, we believe it is a matter of difference regarding
language properties. Although polysemy is a universal linguistic phenomenon
in human languages, different languages may display more or less individuali-
ties in this respect. Chinese and English are no exception.
3 Distribution of Polysemy Motifs in Chinese and
English
In terms of the advantageous properties of motifs, Köhler (2015) hypothesizes
that motifs display a rank-frequency distribution of the Zipf-Mandelbrot type,
i.e. they behave in this respect in a way similar to other more intuitive units of
linguistic analysis. He tests the length motifs (L-motif) of word syllables by us-
ing a short text which is one of the end-of-year speeches of Italian presidents.
His examination proves that fitting the Zipf-Mandelbrot distribution to the L-
motif data yields an excellent result. In this part, we test the Zipf-Mandelbrot
distribution of P-motifs.
The Altmann Fitter (http://www.ram-verlag.biz/altmann-fitter/) is em-
ployed as a fitting tool to conduct the experiment. Since the size of the English
P-motif data shown in Tab. 1 is too large for Altmann Fitter to process, we sam-
ple a smaller English polysemy motif corpus with a size of 362,745 words from
the larger one to represent the English P-motif data. The Chinese and English P-
motif data (incomplete) we actually use for the test are given in Tab. 2 and 3
respectively.
Tab. 2: Rank-frequency distribution of Chinese P-motifs
Rank P-motif Frequency Rank P-motif Frequency
1 (2) 67,594 11 (5) 17,720
2 (1) 46,540 12 (1-1-2) 16,619
3 (1-10) 41,942 13 (1-4) 15,710
4 (3) 36,900 14 (6) 14,023
5 (1-2) 36,587 15 (1-6) 13,689
6 (1-1) 29,375 16 (1-5) 13,648
7 (4) 24,564 17 (8) 12,962
8 (1-3) 22,769 18 (1-1-1) 11,259
9 (1-8) 20,572 19 (2-10) 10,897
206 | Jiang Yang
Rank P-motif Frequency Rank P-motif Frequency
10 (1-1-10) 18,697 20 (1-14) 10,837
Tab. 3: Rank-frequency distribution of English P-motifs obtained from a small corpus
Rank P-motif Frequency Rank P-motif Frequency
1 (1) 12,756 11 (10) 2,098
2 (22) 8,427 12 (16) 1,986
3 (1-1) 4,066 13 (4) 1,968
4 (21) 3,105 14 (30) 1,927
5 (8) 2,893 15 (26) 1,872
6 (29) 2,552 16 (9) 1,802
7 (6) 2,210 17 (7) 1,600
8 (11) 2,205 18 (19) 1,478
9 (12) 2,173 19 (14) 1,466
10 (15) 2,136 20 (3) 1,432
The results yielded from Altmann Fitter show that both the Chinese and English
rank-frequency P-motif data fit the Zipf-Mandelbrot distribution. The detailed
fitting results are as follows: for the Chinese P-motif data, the estimated param-
eters a = 1.5775, b = 7.9524, n = 9363, and the chi-square test related statistics X2
= 11792.973, P(X2
) = 0.0000, DF = 9359, C = 0.0119, R2
= 0.9917; for the English P-
motif data, a = 1.0852, b = 1.1955, n = 19954, X2
= 7733.4472, P(X2
) = 0.0000, DF =
16633, C = 0.042, R2
= 0.9586. Fig. 1 and 2 are the log-log plots of the fit for both
languages respectively.
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 207
Fig. 1: Log-log graph of Zipf-Mandelbrot distribution fitting to Chinese P-motif data
Fig. 2: Log-log graph of Zipf-Mandelbrot distribution fitting to English P-motif data
It should be noted that the chi-square test is sensitive to the sample size, i.e. the
size of the calculated chi square is directly proportional to the size of the sam-
ple. That both of the Chinese and English P-motif data sets used in this paper
are very large samples makes the chi-square test invalid. The rejection of the
chi-square test owes to the fact that the chi-square value linearly increases with
208 | Jiang Yang
an increase of the sample size and linguistic samples tend to be rather large
(Antić et al, 2005). Thus, here the goodness-of-fit is evaluated by the coefficient
of discrepancy (C) instead of the probability of chi-square (P(X2
)). In contempo-
rary linguistics, the discrepancy coefficient meets broad acceptance, as it has
the advantage of not depending on the degrees of freedom. One speaks of a
good fit for C < 0.02, and of a very good fit for C < 0.01 (Antić et al, 2005). In
addition, the coefficient of determination (R2
), a number that indicates how well
the data fit a statistical model, is also a statistic worthy of consideration on the
basis of C in the context of the testing of hypotheses. Consequently, for the Chi-
nese P-motifs rank-frequency distribution, it is a good fit according to the C and
R2
statistics, and for the English one, it is acceptable for its discrepancy coeffi-
cient (C = 0.042) is not too larger than 0.02 and its coefficient of determination
(R2
= 0.9586) indicates a very good fit. Observing the log-log plots of the fit can
bring an intuitive impression, which supports our statement as well.
In addition to the pervious works by Köhler and Naumann (2009), Mačutek
(2009) and Köhler (2015), here from the perspective of the P-motif, we have
tested Köhler’s hypothesis (2006) that the frequency distribution of linguistic
motifs is similar to the distribution of the basic linguistic units. On this basis, it
is promising to apply P-motifs to potential polysemy related studies in linguis-
tics.
4 Lengths of Polysemy Motifs in Chinese and
English
Linguistic motifs have another advantageous property, i.e. motifs are scalable
with respect to granularity. One and the same definition can be iteratively ap-
plied: it is possible to form motifs on the basis of a certain kind of motifs (Köh-
ler, 2015). On the basis of P-motifs, the lengths of P-motifs are determined. Con-
sequently, the length motifs (L-motifs) of the Chinese and English P-motifs can
be formed. It is noteworthy that the English P-motif data set used here is a full
version of all P-motifs from the English polysemy motif corpus. Tab. 4 shows the
top 20 most frequent English P-motifs in the data set.
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 209
Tab. 4: Top 20 most frequent English P-motifs in the English polysemy motif corpus
Rank P-motif Frequency Rank P-motif Frequency
1 (1) 92,403 11 (10) 16,286
2 (22) 75,201 12 (26) 15,924
3 (1-1) 26,908 13 (9) 15,055
4 (21) 22,641 14 (16) 14,187
5 (8) 21,388 15 (4) 14,101
6 (11) 19,814 16 (30) 13,636
7 (29) 18,825 17 (13) 11,748
8 (6) 17,268 18 (7) 11,565
9 (15) 17,054 19 (1-22) 11,256
10 (12) 16,334 20 (3) 10,754
Tests show that the distributions of the L-motifs of the Chinese and English P-
motifs are both fitted to the mixed negative binomial distribution. The results
are given in Tab. 5 and the bi-logarithmic graphs of the fit for both languages
are shown in Fig. 3 and 4 respectively.
Tab. 5: Results of fitting the mixed negative binomial distribution to the L-motifs of the Chinese
and English P-motifs
Language k p1 p2 a X2
DF C R2
Chinese 1.9616 0.5723 0.0432 0.9994 8552.3892 34 0.0086 0.9924
English 6.9288 0.627 0.8813 0.016 9996.1098 15 0.0071 0.9943
210 | Jiang Yang
Fig. 3: Bi-logarithmic graph of the mixed negative binomial distribution fitting to Chinese L-
motif data
Fig. 4: Bi-logarithmic graph of the mixed negative binomial distribution fitting to English L-
motif data
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 211
A mixed distribution is the result of a combination of two processes (Andrei and
Köhler, 2013). A plausible linguistic interpretation for the processes in probabil-
ity distributions is to regard them as the diversification processes (Altmann,
1991, 2005). For the discussion of the P-motifs and the lengths of the P-motifs,
what of concern is the combination of two polysemy related diversifications,
which result in the negative binomial distribution but with different parameters.
The two processes in a mixed distribution can be assumed in this case as one
process concerns the diversification of polysemy in the P-motifs and the other a
consequence of the diversification of the length of the P-motifs. Similar to what
Andrei and Köhler (2013) have done for the test of fitting the same probability
distribution to the length data of depth motifs, in order to justify that the mixed
negative binomial distribution is a model of the lengths of P-motifs, it is neces-
sary to test the hypothesis that the distribution of polysemy in the P-motifs is
compatible with the negative binomial distribution.
The “polysemy” we discuss here actually refers to the sense types of words.
For example, in the P-motif “(4-5-5-6-15)”, the sense type 4 occurs once and its
frequency is “1” in this P-motif. So are the sense type 6 and 15. For the sense
type 5, its frequency is “2”. Other sense types do not occur in the example,
therefore, their frequencies are all “0”.
We test all the sense types occurring in the polysemy motif corpora, all but
the sense type 1 abide by the negative binomial distribution with very good
results. Tab. 6 lists the full data of sense type 3 in Chinese and English P-motifs.
A portion of the fitting results for both languages are provided in Tab. 7 and 8.
Tab. 6: Distribution data of sense type 3 in Chinese and English P-motifs
Chinese English
Frequency Type Frequency Frequency Type Frequency
0 850800 0 1359801
1 131050 1 54075
2 7211 2 864
3 353 3 15
4 16 4 2
212 | Jiang Yang
Tab. 7: Results of fitting the negative binomial distribution to the distributions of sense type 2-
6 in the Chinese P-motifs
Sense Type X2
C DF R2
P N
2 1883.58 0.0019 4 0.9996 0.9992 989440
3 704.24 0.0007 2 1.0000 0.9996 989440
4 171.17 0.0002 2 1.0000 0.9994 989440
5 6.79 0.0000 2 1.0000 0.9748 989440
6 59.65 0.0001 2 1.0000 0.9996 989440
Tab. 8: Results of fitting the negative binomial distribution to the distributions of sense type 2-
6 in the English P-motifs
Sense Type X2
C DF R2
P N
2 3.09 0.0000 1 1.0000 0.9997 1414757
3 42.98 0.0000 1 1.0000 0.9996 1414757
4 173.61 0.0001 1 1.0000 0.9996 1414757
5 0.35 0.0000 1 1.0000 0.9941 1414757
6 541.14 0.0004 2 1.0000 0.9997 1414757
The distribution of the sense type 1 is in accord with other distributions other
than the negative binomial distribution. The reason why it is an exception is
still unknown to us. It may be because that we break the continuousness of P-
motifs by sentences, or that the treatment of assigning “1” to punctuations is
wrong, or it just presents a matter of fact. Anyhow, we would rather view it as
an outlier, which does not affect the general distributions.
So far, a general description of the lengths of P-motifs in Chinese and Eng-
lish has been presented, i.e. the mixed negative binomial distribution is a model
of the lengths of P-motifs. This property is common in both Chinese and Eng-
lish. From the micro perspective, however, certain differences related to the
length of P-motifs between the languages can be found.
To sum up, for the most frequent P-motifs, e.g. the P-motifs shown in Tab. 2
and 4, the Chinese P-motifs with a length of 2 are in a dominant position where-
as the English P-motifs with a length of 1 are in the majority. What’s more, as is
mentioned in Section 2, the average P-motif length in Chinese (APL = 2.5585) is
greater than that (APL = 1.97) in English. As for the least frequent P-motifs, e.g.
the P-motifs with frequencies less than or equal to 10, the APL in Chinese is
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 213
about 13.6231, much greater than that in English (APL = 7.264). In this case, the
sum of the least frequent P-motifs in Chinese accounts for approximately 10% of
the total Chinese P-motifs while in English the corresponding proportion is
approximately 34%. Moreover, the maximum lengths of P-motifs in both lan-
guages are different. In Chinese that number is 73 with a frequency of 1 while in
English it is 38 with a frequency of 39. Some of the above mentioned differences
are to be linguistically interpreted in Section 5, and the remaining is to be inves-
tigated in future studies.
5 Contrastive Properties of Chinese and English
from Perspective of Polysemy Motif
The primary goals of the current research are: (1) to explore some fundamental
quantitative properties of P-motifs in human languages and (2) to unveil, by
means of P-motifs, the linguistic properties of the discussed languages per se
related to polysemy. As the foregoing discussion has aimed at the first issue, we
endeavour to respond to the second one in this part.
In general, compared with Chinese polysemes, the meanings of English
polysemes are of relatively high flexibility. There are several supportive evi-
dences for this. The most important one is that the average sense value of the
English polysemes is larger than the Chinese polysemes not only in dictionaries,
but also in actual use. A survey of the histories of both languages in lexically
semantic evolution will lead to the judgement that English words are dynamic
whereas Chinese words are relatively conservative in terms of meaning. Besides,
the English P-motifs are much more varied than the Chinese P-motifs. It is a
reflection of polysemy diversification. What’s more, there are more P-motif to-
kens in English than in Chinese, taking the size of the corpus for each into ac-
count. And the average P-motif lengths are quite different. Word meaning flexi-
bility, as we assume, may have possible connections to another linguistic
property, i.e. complexity.
English polysemes are more context-dependant than Chinese polysemes. It
is proved by a further exploration into the top 20 most frequent P-motifs in both
languages. On the one hand, most of the top 20 most frequent Chinese P-motifs
are with a length of 2 or 3, almost all of which start with a value of “1”, indicat-
ing that in these P-motifs, a polyseme is followed by a monoseme. When going
back to look for what follows the polyseme, we find that most of the successors
are monosemes. This finding results in that the observed polysemes occur be-
214 | Jiang Yang
tween monosemes. However, the case for the English P-motifs is quite different.
Most of the top 20 most frequent English P-motifs are with a length of 1. While
most of their successors are monosemes, their predecessors, considering the
construction method of P-motifs, are no doubt also polysemes with senses more
than theirs. Undoubtedly, considering our treatment to punctuations in sen-
tences, the observed polyseme occurs in company with its preceding polyseme.
We know that the meaning of a word is specific when observed from its context,
no matter how many senses the word has. In other words, the static and poten-
tially multiple sense choices of a word in mind are resolved by its association
with the dynamic and actual accompanying words in reality. For human beings,
although resolving ambiguity in context is a mental process with unbelievable
speed, the combination of polysemes is still a big obstacle in understanding the
exact meaning. The context, under this circumstance, is a principal source of
information for human beings to seek help from. In this sense, we believe that
English polysemes are more context-dependant than Chinese polysemes. This
finding may be useful for studies or practitioners in language learning, disam-
biguation, dictionary compilation, etc.
6 Conclusion
Linguistic motif is a recently proposed new unit for quantitative investigations
of sequential structures from the syntagmatic viewpoint. Focusing on polysemy
motifs, we have explored in this paper the basic statistics and the fundamental
quantitative properties of the polysemy motifs in Chinese and English. On such
basis, we have further discussed the language-specific properties related to
polysemy in Chinese and English.
Our study shows: (1) Both of the Chinese and English rank-frequency poly-
semy motif distributions fit the Zipf-Mandelbrot distribution with different pa-
rameters; (2) Both of the length motif distributions of the Chinese and English
polysemy motifs fit the mixed negative binomial distribution; (3) The micro
analysis of the lengths of the Chinese and English polysemy motifs results in
revealing many language-specific characteristics; (4) The meanings of English
polysemes are of relatively high flexibility compared with Chinese polysemes;
(5) English polysemes are more context-dependant than Chinese polysemes.
This study is the first attempt exploring polysemy motifs under the general
framework of linguistic motif, it has undoubtedly left a few questions unan-
swered and many issues untouched. The research field of polysemy motif awaits
more strength to be put and more efforts to be made.
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 215
Acknowledgement
This research was supported by the Scientific Research Fund of Hunan Provin-
cial Education Department under Grant #14B068.
References
Altmann, G. (1991). Modeling diversification phenomena in language. In: U. Rothe (Ed.): Diver-
sification Processes in Language: Grammar (pp. 33–46). Hagen: Rottmann.
Altmann, G. (2005). Diversification processes. In: R. Köhler, G. Altmann & R. G. Piotrowski
(Eds.), Quantitative Linguistics: An International Handbook (pp. 646–658). Berlin; New
York: de Gruyter.
Andrei, B., Köhler R. & Naumann, S. (2013). Quantitative properties of argumentation motifs. In:
I. Obradović, E. Kelih & R. Köhler (Eds.): Methods and Applications of Quantitative Linguis-
tics (pp. 33–43). Belgrade: Academic Mind.
Antić, G., Grzybek P. & Stadlober E. (2005). Mathematical apects and modifications of Fuck’s
Generalized Possion Distribution (GPD). In: R. Köhler, G. Altmann, & R. G. Piotrowski (Eds.),
Quantitative Linguistics: An International Handbook (pp. 158-180). Berlin; New York: de
Gruyter.
Čech, R., Mačutek, J., Žabokrtský, Z. & Horák, A. (2015). Polysemy and synonymy in syntactic
dependency networks. Digital Scholarship in the Humanities Advance, 7, 189–200.
Ide, N. & Véronis, J. (1998). Word Sense Disambiguation: The State of the Art. Computational
Linguistics, 24(1), 1–40.
Köhler, R. & Naumann, S. (2009). A contribution to quantitative studies on the sentence level.
In: R. Köhler (Ed.), Issues in Quantitative Linguistics (pp. 34–57). Lüdenscheid: RAM-
Verlag.
Köhler, R. (2005a). Synergetic linguistics. In: R. Köhler, G. Altmann & R. G. Piotrowski (Eds.),
Quantitative Linguistics: An International Handbook (pp. 760–774). Berlin; New York: de
Gruyter.
Köhler, R. (2005b). Properties of lexical units and systems. In: R. Köhler, G. Altmann & R. G.
Piotrowski (Eds.), Quantitative Linguistics: An International Handbook (pp. 305–313). Ber-
lin; New York: de Gruyter.
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In: J. Genzor,
& M. Bucková (Eds.), Favete linguis: Studies in Honour of Viktor Krupa (pp. 145–152). Bra-
tislava: Slovak Academic Press.
Köhler, R. (2015). Linguistic motifs. In: G. Mikros & J. Mačutek (Eds.), Sequences in Language
and Text (pp. 89–108). Berlin: de Gruyter.
Mačutek, J. (2009). Motif richness. In: R. Köhler (Ed.), Issues in Quantitative Linguistics (pp.
51–60). Lüdenscheid: RAM-Verlag.
Traxler, M. (2011). Introduction to Psycholinguistics: Understanding Language Science (pp.
1934). New Jersey: Wiley-Blackwell.
216 | Jiang Yang
Wang, H. (2009). Cíyì, cícháng, cípín -- Xiàndài Hànyǔ Cídiǎn (dìwǔbǎn) duōyìcí jìliàng fēnxī
[Polysemous Words: Meaning, Length and Frequency]. Studies of the Chinese Language,
329(2), 120–130.
Zipf, G. K. (1945). The meaning frequency relationship of words. Journal of General Psychology,
33,251–266.
Zipf, G. K. (1949). Human Behavior and the Principle of Least Effort. Cambridge: Addison-
Wesley.
Cong Zhang
The Words and F-motifs in the Modern
Chinese Versions of the Gospel of Mark
Abstract: This paper investigates the words and F-motifs in six modern Chinese
versions of the Gospel of Mark from the year 1855 to 2010. We found that the
rank-frequency of F-motifs in each text follows the power law. Results of fre-
quency comparison displayed that both the types and tokens of the words and
F-motifs in all versions increase with the evolution of the Chinese language.
Results of length comparison showed that, on the one hand, monosyllabic
words decrease with the evolution of the Chinese language and both word
length of disyllabic words and average word length increase with it; on the oth-
er hand, the trend of F-motifs is not very obvious. The hierarchical cluster anal-
ysis proved that the length of F-motifs can be used to distinguish Classical Chi-
nese from other two kinds of Chinese, but cannot distinguish between Beijing
Mandarin and Modern Mandarin.
Key words: Words; F-motifs; Chinese; the Gospel of Mark
1 Introduction
The linguistic motif means the sequence or segment (Köhler 2006, 2008a, b;
Köhler & Naumann 2008, 2009, 2010) and is a relatively new indicator for quan-
titative linguistics. Köhler (2015) defined six types of motifs altogether, includ-
ing L-motif, F-motif, P-motif, T-motif, R-motif and D-motif. The purpose of the
current research is to examine the F-motif in the Modern Chinese versions of the
Gospel of Mark.
The definition of F-motif is a continuous series of equal or increasing fre-
quency values (e.g. of morphs, words or syntactic construction types) (Köhler
2015). F-motif, formed by word frequency, carries linear sequential features of
word frequency in a text. By means of the calculation of F-motifs, we can quan-
titatively present the linear sequential features of word frequency in each text.
And by comparing the difference between frequency and length of words and F-
||
Cong Zhang: School of Humanities, Zhejiang University, Hangzhou, China,
zhangco1079@163.com
218 | Cong Zhang
motifs, we could obtain more information with respect to the relations between
them, reveal some characteristics and find the scope of application about F-
motifs. What’s more, combined with the investigation of some modern Chinese
versions of the Gospel of Mark from the year 1855 to 2010, we could, to some
extent, describe the evolution process of modern Chinese over the last 155 years
with words and F-motifs.
To display the details of F-motif, we take the first three verses of Chapter 1
of the Gospel of Mark in Delegate’s Version (The Delegate’s Version Translation
Committee 1855, p.18) as examples:
马可 福音 传
The Book of Mark
第一 章
Chapter one
上帝 子 耶稣 基督 福音 之 始 也。
The beginning of the gospel about Jesus Christ, the Son of God.
先知 载 曰，我 遣 我使，在 尔 前， 备 尔 道。
It is written in Isaiah the prophet: “I will send my messenger ahead of you, who will prepare
your way.”
野 有 声，呼 云，备 主 道，直 其 径。(Mark. 1. 1-3)
– “a voice of one calling in the desert, ‘Prepare the way for the Lord, make straight paths for
him.’” (Mark. 1. 1-3)1
The words in these verses have been segmented manually. We replace each
word with its frequency in the Gospel of Mark, and obtain the F-motifs of the
three verses according to the above-given definition:
(1-9) (8) (1-16-53) (25-262) (12) (9-345) (4-104) (8) (4-260) (180) (20-180) (15-39-186) (13) (6-
186) (20) (4-87) (3-25) (20) (6-18-20) (1-119) (1)
To make the distribution of this notion more understandable, we demonstrate
the F-motifs in the following figure.
||
1 English version of these verses are from The Holy Bible (New International Version), 1984.
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 219
Fig. 1: The first three verses of Chapter 1 of the Gospel of Mark in Delegate’s Version
In Fig. 1, each point represents one word. The x-axis is their order of appearance
in the text, and the y-axis is their frequency of appearance in the text. From the
very beginning of the x-axis to the end, the points, which are connected by con-
tinuous series of straight lines that are parallel to the x axis or ascend in coordi-
nate system, represent a F-motif; and every descending line means the end of a
former F-motif and the beginning of a new F-motif.
With regard to texts under investigation, we choose the Gospel of Mark due
to the following two reasons:
First of all, the Bible is the most important book for Christianity and is the
most widely published book till today. Although Christianity is not a main-
stream religion in China, these versions have a great influence upon the modern
evolution of the Chinese language (Jiang, 2003). The Gospel of Mark is regarded
as the earliest of Gospels, and it is one of the most vital part of the Bible.
Secondly, the work of translating the Bible into Chinese runs throughout
the whole modern evolutionary process of the Chinese language, and all ver-
sions of the Gospel of Mark are parallel texts, i.e. each telling us the same story
of Jesus and conveying the same meaning. Furthermore, in order to help more
people understand the Bible so as to promote Christianity, the languages used
in these versions are all standard written or spoken Chinese at the time of their
publication. All our investigations, discussions and conclusions are based on
this premise.
220 | Cong Zhang
The versions of the Gospel of Mark under investigation are translated in
three kinds of Chinese: Classical Chinese, Beijing Mandarin and Modern Manda-
rin. In these three kinds of Chinese, Classical Chinese has a very long history in
China, and it is significantly different from Beijing Mandarin and Modern Man-
darin in many ways and is gradually replaced by Beijing Mandarin in the early
20th century. Beijing Mandarin is the main foundation of Modern Mandarin, so
they are similar in many aspects. Roughly, the evolution order of these three
kinds of Chinese is Classical Chinese - Beijing Mandarin - Modern Mandarin.
Admittedly, there have been some studies with respect to the evolution of
Chinese language (Chen, Liang & Liu, 2015; Liu & Liu, 2011; Wang, 2004; Wu,
2014; Xu, 2007). However, among these studies, a quantitative investigation of
F-motif and its relationship with words in parallel texts are still lacking. To an-
ticipate, our investigation may fill the blank in this respect.
The goal of the present study aims to answer the following questions:
Question 1. What are the features of the rank-frequency distribution of F-
motifs in all versions of the Gospel of Mark?
Question 2. What is the relation between the Frequency of words and F-
motifs in all versions of the Gospel of Mark?
Question 3. What are the trends of words and F-motifs in the modern Chi-
nese versions of the Gospel of Mark?
Question 4. Does the length of F-motifs have the function to differentiate
between these three kinds of Chinese?
2 Materials and Methods
The Gospel of Mark under investigation includes six versions. The detailed in-
formation of these versions is presented in Tab. 1.
Tab. 1: The information of each version of the Gospel of Mark
Versions of the Gospel of Mark Abbreviation Publication Date Language
Delegate’s Version D1855 1855 Classical Chinese
Union Wenli Version UW1919 1919 Classical Chinese
Peking Colloquial Version PC1872 1872 Beijing Mandarin
Union Mandarin Version UM1908 1908 Beijing Mandarin
New Chinese Version NC1976 1976 Modern Mandarin
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 221
Versions of the Gospel of Mark Abbreviation Publication Date Language
Contemporary Chinese Version CC2010 2010 Modern Mandarin
We compare words with F-motifs in two aspects:
1. Frequency. We calculate the types and tokens of them in each version of the
Gospel of Mark respectively, and observe their trend in the evolution of the
Chinese language; moreover, we employ the correlation analysis to detect
the relation between their types and tokens.
2. Length. We compute their normalized static length, normalized dynamic
length, average static and dynamic length, and observe their trend in the
evolution of the Chinese language. The length of words in each version is
measured in Chinese characters, e.g. the word length of “的” is 1, and the
word length of “耶稣” is 2.
The normalized static length of each word or F-motif class can be calculat-
ed with the following formula:
NSLi =
��
∑ ��
�
���
(1)
where n refers to the number of different length classes, and Pi refers to the
word or F-motif types of length class i.
The normalized dynamic length of each word or F-motif class can be calcu-
lated with the following formula:
NDLi =
��
∑ ��
�
���
(2)
where n refers to the number of different length classes, and Ti refers to the
word or F-motif tokens of length class i.
The average static length can be calculated with the following formula:
ASL =
∑ ����
�
���
∑ ��
�
���
(3)
where n refers to the number of different length classes, Ci refers to the length
class i, and Pi refers to the word or F-motif types of length class i.
The average dynamic length can be calculated with the following formula:
ADL =
∑ ����
�
���
∑ ��
�
���
(4)
222 | Cong Zhang
where n refers to the number of different length classes, Ci refers to the length
class i, and Ti refers to the word or F-motif tokens of length class i.
3 Empirical Results and Discussion
3.1 Fitting the Power Law
The premise of making comparison between types and tokens of words and F-
motifs is that they both follow the similar distribution. Prior studies show that
the rank-frequency distribution of words in most languages follows the power
law, and some tests have shown that the rank-frequency distributions of motifs
are similar to the distributions of the basic units (Köhler 2015). However, as
Chinese is different from many Indo-European languages, it is still in need of
test whether the rank-frequency distribution of words in Chinese follows the
power law. Therefore, we fit the power function y=axb
to the rank-frequency
distribution of F-motifs in each text. Then, we take logarithms of both the rank
and frequency in the rank-frequency distribution of F-motifs in each text, and fit
a linear model y= a*x + b to them.
Tab. 2: The fitting results of each version of the Gospel of Mark
Versions of the
Gospel of Mark
Fitting y=ax-b
R2
Taking logarithms and
Fitting y=ax+b
R2
D1855 y=108.3538x-0.5976
0.9077 y=-0.7078x+ 5.0136 0.9124
UW1919 y=94.9383x-0.5590
0.8281 y=-0.7257x+ 5.1554 0.899
PC1872 y=130.0856x-0.5833
0.8483 y=-0.7417x+5.4154 0.9106
UM1908 y=120.7706x-0.5708
0.831 y=-0.7336x+5.3695 0.9098
NC1976 y=165.5140x-0.6237
0.9217 y=-0.7558x+5.5319 0.9186
CC2010 y=157.8115x-0.6146
0.9046 y=-0.7329x+5.3993 0.9144
The results in Tab. 2 show that here all versions of the Gospel of Mark follow the
power law, suggesting that the view of Köhler (2015) holds true in the F-motifs
of Chinese as well, and hence we can regard F-motif as a basic linguistic unit.
Then we propose our next questions: as a basic unit formed by word frequency,
are there trends concerning the frequency of words and F-motifs in the evolu-
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 223
tion of Chinese? What is the relation between the frequency of words and F-
motifs in the modern Chinese versions of the Gospel of Mark?
3.2 The Types and Tokens of Words and F-motifs
In this section, types and tokens of words and F-motifs are presented. By com-
paring the types and tokens of F-motifs with the types and tokens of words in
each text, we attempt to find out the trends of them as well as the connection
between them.
Tab. 3: The types and tokens of words and F-motifs in each text
Versions of the Gospel of
Mark
Types of words Tokens of words Types of F-
motifs
Tokens of F-
motifs
D1855
(Classical Chinese)
1573 9050 1852 4467
WU1919
(Classical Chinese)
1544 9525 1919 4747
PC1872
(Beijing Mandarin)
1739 12209 2305 6042
MU1908
(Beijing Mandarin)
1804 12293 2354 6052
NC1976
(Modern Mandarin)
1932 12690 2326 6317
CC2010
(Modern Mandarin)
1973 12906 2462 6408
As shown in Tab. 3, on the premise that all the texts convey the same meaning,
the types and tokens of words are increasing from Classical Chinese to Beijing
Mandarin to Modern Mandarin. The types and tokens of F-motifs exhibit the
same trend (except for fewer types of F-motifs in NC1976 than those of F-motifs
in MU1908). Xu suggested that, during the replacement of Classical Chinese and
its development to Modern Mandarin, Ancient Chinese Vernacular (which Bei-
jing Mandarin belongs to) has increased sharply in the attributes, adverbials
and complements etc. for the purpose of expressing more precise notions and of
having more diverse ways to express thoughts (2007). Evidently, our findings
offer a quantitative evidence to this view.
224 | Cong Zhang
It can also be seen in Tab. 3 that all the types of F-motifs are greater than
the types of words in the same text, and all the tokens of F-motifs are less than
the tokens of words in the same text, which means that language evolution has
little impact on the relationship between words and F-motifs. On this basis, we
employ the correlation analysis to detect the relationship between types of
words and F-motifs, tokens of words and F-motifs respectively. The results are
as follows:
Tab. 4: The results of the correlation analysis
Types of words Types of F-motifs
Types of words Pearson Correlation 1 .922**
Sig. (1-tailed) 0.004
Types of F-motifs Pearson Correlation .922**
1
Sig. (1-tailed) 0.004
Tokens of words Tokens of F-motifs
Types of words Pearson Correlation 1 .999**
Sig. (1-tailed) 0
Types of F-motifs Pearson Correlation .999** 1
Sig. (1-tailed) 0
**. Correlation is significant at the 0.01 level (1-tailed).
According to Tab. 4, the correlation coefficient between types of words and
types of tokens is 0.922, and the correlation coefficient between tokens of words
and tokens of tokens is 0.999, both of which are positively correlated. Because
F-motif is formed by the frequency of words, whose result means that the types
of F-motifs increase or decrease with the types of words, and the tokens of F-
motifs increase or decrease with the tokens of words.
3.3 The Length of Words and F-motifs
A recent study (Chen, Liang & Liu, 2015) suggests that Chinese has the disyllabic
trend, and the word length of Chinese increases with the evolution of Chinese.
However, no prior research has tested it in parallel texts before, nor has previ-
ous effort tested the length of F-motifs, either. Here in this section our question
is: What are the trends of the length of words and F-motifs in the modern Chi-
nese versions of the Gospel of Mark?
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 225
Tab. 5: The normalized word length distribution of all the texts
Versions of the Gospel of
Mark
Static class 1 Static class 2 Average static word length
D1855
(Classical Chinese)
0.555 0.3846 1.5238
WU1919
(Classical Chinese)
0.581 0.3608 1.4974
PC1872
(Beijing Mandarin)
0.3145 0.6176 1.7769
MU1908
(Beijing Mandarin)
0.3137 0.6175 1.7766
NC1976
(Modern Mandarin)
0.2707 0.6625 1.8131
CC2010
(Modern Mandarin)
0.2636 0.665 1.8312
Versions of the Gospel of
Mark
Dynamic class
1
Dynamic class 2 Average dynamic word length
D1855
(Classical Chinese)
0.7898 0.1855 1.2408
WU1919
(Classical Chinese)
0.8228 0.1535 1.2069
PC1872
(Beijing Mandarin)
0.5964 0.3768 1.4361
MU1908
(Beijing Mandarin)
0.6091 0.3661 1.421
NC1976
(Modern Mandarin)
0.583 0.3896 1.4495
CC2010
(Modern Mandarin)
0.5672 0.4032 1.4709
We use class i to represent all those words and F-motifs whose length equal i
(measured in characters). Thus the word length of all versions ranges from class
1 to 7, and mainly centralizes on class 1 and 2, so in Tab. 5 we just present the
data of class 1, 2 and the average word length of each version. In Chinese, one
syllable is roughly one character, hence, the disyllabic trend can be described
as the increase of class 2. As can be seen in Tab. 5, from Classical Chinese to
Beijing Mandarin to Modern Mandarin, both the static and dynamic word class 1
decrease, both the static and dynamic word class 2 increase and both the aver-
226 | Cong Zhang
age static and dynamic word length increase, thus confirming the conclusion of
Chen, Liang, & Liu (2015) in parallel texts.
Tab. 6: The main parts of normalized F-motif length distribution of all the texts
Versions of the
Gospel of Mark
Static class
1
Static class
2
Static class
3
Static class
4
Average static
F-motif length
Motif-D1855
(Classical Chinese)
0.0335 0.4303 0.3888 0.1156 2.6944
Motif-UW1919
(Classical Chinese)
0.0349 0.432 0.3814 0.1256 2.6858
Motif-PC1872
(Beijing Mandarin)
0.0325 0.4095 0.4056 0.121 2.7197
Motif-UM1908
(Beijing Mandarin)
0.0336 0.3959 0.4053 0.1355 2.74
Motif-NC1976
(Modern Mandarin)
0.0327 0.396 0.4089 0.1307 2.7429
Motif-CC2010
(Modern Mandarin)
0.0309 0.4013 0.418 0.1239 2.7185
Versions of the
Gospel of Mark
Dynamic
class 1
Dynamic
class 2
Dynamic
class 3
Dynamic
class 4
Average dy-
namic F-motif
length
Motif-D1855
(Classical Chinese)
0.3024 0.4493 0.1867 0.0481 2.026
Motif-UW1919
(Classical Chinese)
0.3033 0.4626 0.1727 0.0508 2.0065
Motif-PC1872
(Beijing Mandarin)
0.3105 0.4358 0.1923 0.0495 2.0207
Motif-UM1908
(Beijing Mandarin)
0.3123 0.427 0.1927 0.0565 2.0312
Motif-NC1976
(Modern Mandarin)
0.316 0.4353 0.1873 0.0495 2.0089
Motif-CC2010
(Modern Mandarin)
0.3141 0.4285 0.1987 0.0487 2.014
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 227
Compared to words, the F-motifs length of all versions displayed much broader
distributions ranging from 1 to 16. Table 6 displays the data of main static and
dynamic F-motif lengths as well as the average static and dynamic F-motif
lengths. As can be seen from Tab. 6, the static lengths of F-motifs are mainly
distributed in class 2, 3 and 4; the dynamic lengths of F-motifs are mainly dis-
tributed in class1, 2 and 3. This means in class 1, F-motifs have few types but
many tokens, and in class 4, F-motifs have few tokens but many types. Unlike
word length, there is no obvious trend in the lengths of F-motifs. Then do the
lengths of F-motifs have the function to differentiate between the three kinds of
Chinese? We group the four static and dynamic length classes of F-motifs to-
gether and test it by the hierarchical cluster analysis. The result is displayed as
follows:
Fig. 2: Hierarchical cluster analysis of the length of F-motifs in each version
As shown in Fig. 2, like the frequency indicator, the length of F-motifs can dis-
tinguish Classical Chinese from the other two kinds of Chinese, but cannot dis-
tinguish Beijing Mandarin from Modern Mandarin. One of the contributing
causes is that Beijing Mandarin and Modern Mandarin are more similar to each
228 | Cong Zhang
other. Another possible reason is that word order, which has a great influence
on F-motif, is an important means to express grammatical meaning in Chinese,
but the evolution of word order is very complicated rather than following a line-
ar variation.
4 Conclusion
Based on the six versions of the Gospel of Mark in three kinds of Chinese, on the
premise that all the texts convey the same meaning, the conclusions are as fol-
lows:
The rank-frequency distributions of F-motifs in three kinds of Chinese fol-
low the power law.
The frequency of words and F-motifs are positively correlated. The types of
F-motifs increase or decrease with the types of words, and the tokens of F-motifs
increase or decrease with the tokens of words.
Overall, both the types and tokens of words and F-motifs increase from
Classical Chinese to Beijing Mandarin to Modern Mandarin (except fewer types
of F-motifs in NC1976 than those of F-motifs in MU1908). Specifically, some
trends can be observed in word length: from Classical Chinese to Beijing Man-
darin to Modern Mandarin, both the static and dynamic word class 1 decrease,
both the static and dynamic word class 2 increase and both the average static
and dynamic word length increase. On the other hand, there is no obvious trend
in the length of F-motifs.
By means of the hierarchical cluster analysis, we find that the length of F-
motifs can distinguish Classical Chinese from the other two kinds of Chinese,
but cannot distinguish Beijing Mandarin from Modern Mandarin.
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188.
The Words and F-motifs in the Modern Chinese Versions of the Gospel of Mark | 229
Reference:
Chen, H., Liang, J., & Liu, H. (2015). How does word length evolve in written Chinese? PLoS ONE,
10(9), e0138567. http://doi.org/10.1371/journal.pone.0138567
International Bible Society. (1984). The Holy Bible (New International Vesion). Zondervan.
Jiang, X. (2003). The translation of the bible and its influence on Chinese. Foreign Language
Teaching and Research, 4, 301–305.
Köhler, R. (2006). The frequency distribution of the lengths of length sequences. In J. Genzor, &
M. Bucková (Eds.), Favete linguis. Studies in honour of Viktor Krupa (pp. 145–152). Brati-
slava: Slovak Academic Press.
Köhler, R. (2008a). Word length in text. A study in the syntagmatic dimension. In S. Mis-
lovičová (Ed.), Jazyk a jazykoveda v pohybe (pp. 416–421). Bratislava: Veda.
Köhler, R. (2008b). Sequences of linguistic quantities. Report on a new unit of investigation.
Glottotheory, 1(1). 115–119.
Köhler, R., & Naumann, S. (2008). Quantitative text analysis using L-, F- and T-segments. In C.
Preisach, H. Burkhardt, L. Schmidt-Thieme & R. Decker (Eds.), Data Analysis, Machine
Learning and Applications (pp. 635–646). Berlin & Heidelberg: Springer.
Köhler, R. & Naumann, S. (2009). A contribution to quantitative studies on the sentence level.
In R. Köhler (Ed.), Issues in Quantitative Linguistics (pp. 34–57). Lüdenscheid: RAM-Verlag.
Köhler, R. & Naumann, S. (2010). A syntagmatic approach to automatic text classification.
Statistical properties of F- and L-motifs as text characteristics. In P. Grzybek, E. Kelih & J.
Mačutek (Eds.), Text and Language. Structures, functions, interrelations, quantitative per-
spectives (pp. 81–89). Wien: Praesens.
Köhler, R. (2015). Linguistic Motifs. In G. Mikros, & J. Mačutek (Eds). Sequences in Language
and Text (pp. 89–108). Berlin, Boston: DE GRUYTER.
Liu, B., & Liu, H. (2011). A Study on the Evolution of the Verbal Syntactic Valence Based on
Corpus. Language Teaching and Language Studies, 6, 83–89.
The Delegate’s Version Translation Committee. (1855). The New Testament (Delegate’s Version).
Hong Kong: Ying Wa College.
Wang, L. (2004). A Manuscript of Chinese Language History. Beijing: The Chinese Publishing
House.
Wu, C. (2014). “Beijing Mandarin” and the Modern Change of Chinese. Shandong: Shandong
Education Press.
Xu, S. (2007). The History of Chinese Dialect. Beijing: Peking University Press.
Hongxin Zhang, Haitao Liu
Motifs of Generalized Valencies
Abstract: The generalized valency patterns cover both obligatory arguments
and optional adjuncts of valency carriers in authentic texts. Such a valency is
defined as the number of all the dependents of the valency carrier. With a motif
defined as the longest sequence with non-decreasing values, this paper chooses
two news-genre dependency treebanks, one in Chinese and one in English and
examines the motifs of generalized valencies in them and their lengths. They
both are found to abide by the right truncated modified Zipf-Alekseev distribu-
tion. In addition, the Hyperpoisson model captures the interrelation between
motif lengths and length frequencies. These research findings validate valency
motifs as basic language entities and as results of a diversification process.
Keywords: generalized valency, dependency treebank, distribution, motif,
length of motifs, diversification
1 Introduction
The linguistic term valency has its meaning derived from the definition of va-
lency in chemistry. Tesnière (1959) introduced into linguistics the idea of valen-
cy, based on which he developed modern dependency syntactic theory (Liu
2009a). Valency denotes the requirements for words (e.g. verbs) to take certain
words as their complements. As verbs play a pivotal role in sentences, verb
valency usually draws the most attention. Different verbs may take particular
kinds and forms of complements, which gives rise to various valen-
cy/complementation patterns (Köhler 2012). For instance, as is shown in Fig. 1,
“give” is a tri-valency verb and “read”, a bi-valency one. Both “gave” and “read”
are governors and those directly beneath them are their dependents.
||
Hongxin Zhang: Department of Linguistics, Zhejiang University, Hangzhou, China,
mariazhang@yeah.net
Haitao Liu: Department of Linguistics, Zhejiang University, Hangzhou, China, htliu@163.com
232 | Hongxin Zhang – Haitao Liu
Fig. 1: Examples of verb valencies
Prior research in this field has conducted quantitative investigations into verb
valency in a number of languages. Köhler (2005) pioneered the investigation
into the quantitative properties of German verb valency and argued that German
verb valency patterns are regularly distributed. Liu (2011) discovered that the
valency of a verb is a function of verb length, and that greater valency is both a
result of more frequent occurrences and of greater polysemy. Liu and Liu (2011)
carried out a diachronic study of Chinese verb valency. Gao, Zhang and Liu
(2014) incorporated the idea of verb valency into the synergetic lexical model of
Chinese verbs.
Čech, Pajas and Mačutek (2010) proposed a full verb valency approach
without making the distinction between complements (obligatory arguments)
and adjuncts (optional arguments) since a clear criterion for distinguishing the
two is missing. Using the full-valency approach, Čech and Mačutek (2010) ob-
served that in Czech, shorter verb length goes with more verb complementation
patterns.
Most quantitative methods approach language issues in the mass, applying
a bag-of-words model and addressing paradigmatic information only, namely,
unordered sets of elements, inventories of units, and properties. They generally
ignore language in the line, or the syntagmatic/sequential behavior of the ob-
jects: “the organization of the linguistic elements and of the property values of
these elements in the course of the text” (Köhler, Naumann: 81 2010).
Čech, Vincze and Altmann (in this volume) used a Czech text and a Hungar-
ian one to examine the motifs of full verb valencies, where motif study is a way
to study language in the line. In their study, a motif is defined as the longest
continuous sequence of equal or increasing values as a motif of valencies repre-
sents a numerical sequence. An example of the motifs goes like this: 1-1-2, 0-1,
0-2-3, 2, 1-1-2. The second motif begins with a valency of 0 as it is smaller than 2,
the last element in the first motif; similarly, the third motif also begins with 0.
The motifs in Čech et al. (in this volume) are found to abide by the Zipf-
Motifs of Generalized Valencies | 233
Mandelbrot distribution. Their study shows that valency motifs are normal lan-
guage entities.
But valency is not unique to verbs only. Though not giving an explicit defi-
nition, Tesnière (1959) discussed the valency of other word classes using the
same way he analyzed verb valency. Herbst (1988) defined valency as the re-
quirement for a word on its complements. This word is usually a verb, noun or
adjective. The complements can either be obligatory arguments or optional
adjuncts. He (Herbst 1988) examined a valency model for nouns in English and
in his valency dictionary of English (Herbst et al. 2004) incorporated verbs,
nouns, and adjectives as governors.
Liu (2009a) proposed the idea of Generalized Valency Pattern (GVP), which
is graphically presented in Fig. 2. If we understand valency as the potential of
words to combine with other words to form bigger linguistic units, GVP express-
es the ability for a word (class) to govern other words (word classes) or be gov-
erned by another word (class).
Fig. 2: A Generalized Valency Pattern, adapted from Liu (2009a: 68)
In GVP (Fig. 2), W stands for a word (class), C1 through C3 are the complements
which are required to complete or specify the meaning of W, and A1 through A3,
adjuncts to explain or restrict W. G is a word (or word class) that can potentially
act as the governor of W. As is suggested by this graph, a word can take multiple
dependents but can have only one as its governor in a sentence.
Once W occurs in an authentic text, it opens some slots to be filled in. To
put it in other words, when the valency potential becomes specific slots, the
quantities and types of arguments can be predicted. Simultaneously, this action
for W to enter an authentic text also reveals whether W can satisfy the needs to
act as a dependent of another word (class). Whether these two types of binding
will actually take place depends on whether the binding requirements from
syntax, semantics and pragmatics are met.
234 | Hongxin Zhang – Haitao Liu
Alongside the proposal of GVP, Liu (2009b) examined the probability distri-
bution of dependencies based on a Chinese dependency treebank and thereby
proposed PVP (Probabilistic Valency Pattern Theory) with the idea of probabili-
ties further incorporated. In his paper, Liu examined all word classes as gover-
nors, all word classes as dependents, then specifically focused on verbs as gov-
ernors and nouns as dependents.
To put it simply, the generalized valency patterns incorporate all arguments
and adjuncts, either obligatory or optional. They also cover the ability of a word
to govern other words (acting as a governor) and to be governed (acting as a
dependent). Different from the full valency approach of Čech et al. (2010), the
GVP approach extends full valency of verbs to all running words, thus enabling
an analysis of the dynamic feature of word chains from the perspective of valen-
cies.
Most of the previous studies focus on verbs only. To our best knowledge, no
existing study covers generalized valencies of all running words in authentic
texts, thus rendering it impossible to understand the valency structures cover-
ing all words in dynamic language use.
In this study, we will take the GVP approach and examine generalized va-
lencies of words by examining how words in authentic texts act as valency car-
riers. Valency is quantified as the number of all the arguments (obligatory and
optional alike) that a word takes, or in other words, the number of all its de-
pendents. Since the valency of each running word can easily be extracted from
dependency-annotated treebanks, we can rewrite sentences of a text as motifs–
numerical sequences of generalized valencies. If motifs are eligible language
entities, they are expected to behave alike. Like other intuitive and basic lan-
guage entities, users don’t use all of the valency motifs to the same degree,
either consciously or unconsciously. Köhler (2015) argued that motifs are ex-
pected to display a rank-frequency distribution of the Zipf-Mandelbrot type. In
addition, if motifs are basic language units, the interrelation between motif
lengths and length frequencies is expected to be captured by a linguistically
meaningful model.
We are going to examine the motifs of generalized valencies in both English
and Chinese to investigate the similarities and differences between the two
languages.
We pose the following research questions:
Question 1: Are motifs of generalized valencies regularly distributed?
Question 2: Are motif lengths regularly distributed?
Question 3: What is the interrelation between motif lengths and length frequencies?
Motifs of Generalized Valencies | 235
Whether valency motifs can claim a linguistic status is dependent on the an-
swers to the previous questions.
In the remainder of this paper, the next section addresses materials along
with the method of examining valencies. Section 3 details the research findings.
Finally Section 4 presents conclusions and proposals for future work.
2 Materials and Method
To conduct a cross-language study, we choose as our research materials two
corpora or syntax treebanks manually parsed with dependency relations. Both
are of the news genre. One is the English part of the Prague Czech-English De-
pendency Treebank 2.0 (PCEDT 2.0) (Hajič et al. 2012), which contains the entire
Wall Street Journal (WSJ) Section of Penn Treebank (Linguistic Data Consortium,
1999) and bears a size of 1.174 million words with 2,499 stories. Designed by
Peking University, Peking University Multi-view Chinese Treebank 1.0 (PMT 1.0)
(Qiu et al. 2014) is our choice for the Chinese dependency treebank. Bearing a
size of 336 thousand words with 14,463 sentences, PMT 1.0 contains all the arti-
cles of People’s Daily (a notable Chinese daily) from January 1st to January 10th,
1998. Both treebanks use similar annotation schemes except for some depend-
ency types which are unique to only one of them, for instance the infinitive “to”
in English. Though the Chinese corpus has a much smaller size, it is sufficient
for discovering the valency patterns and subsequent valencies in our study.
Two news treebanks parsed with similar annotations render it possible to
conduct a cross-language comparison, but that’s not enough. To minimize the
influences from different sizes of corpora and thus to focus on the differences
only resulting from languages per se, we need to compare treebanks with equiv-
alent sizes. Furthermore, to make sure the research findings are not a result of
random fluctuations, we divide the Chinese treebank into 6 sub-corpora, each
with about 50 thousand words, including punctuation marks. As for the English
treebank, the much larger size invites a random sampling of 6 sub-corpora from
a total of 23, all with sizes of about 50 thousands words.
Both original corpora have nodes of punctuation marks. To examine the de-
pendency between words only, we exclude punctuation marks from our discus-
sion by assigning their roles to their governors. The final resulting 12 sub-
corpora (E1 to E6 in English and C1 to C6 in Chinese) all range in sizes of 43,200
plus words.
236 | Hongxin Zhang – Haitao Liu
Through these procedures, the research materials in this study are basically
sub-corpora of the same size and of the same genre, annotated with similar
dependency schemes.
Having settled the research materials, we tend to obtain the generalized valency
or the number of all dependents of the running words from the sub-corpora
through programming.
For instance in Sentence 1 (Fig. 3) of the English treebank, “will” bears two
dependents, and “old”, only 1. We thus define them as having valencies of 2 and
1, respectively. For those words which appear as terminal leafs without any
“offsprings”, we define them as having a valency of 0.
(1) Pierre Vinken, 61 years old will join the board as a nonexecutive direc-
tor Nov. 29.
Fig. 3: Dependency tree for Sentence (1)
Tab. 1 presents the valency patterns and relevant valencies for Sentence (1).
Tab. 1: Valencies for words in Sentence (1)
Node POS Valency pattern Valency
Pierre NN 0
Vinken NN Pierre+Vinken (governor)+old 2
61 CD 0
years NN 61+years (governor) 1
old JJ years+old (governor) 1
Motifs of Generalized Valencies | 237
Node POS Valency pattern Valency
will MD Vinken+will (governor)+join 2
join VB join (governor)+board+as+Nov. 3
the DT 0
board NN the+board (governor) 1
as IN as (governor)+director 1
a DT 0
nonexecutive JJ 0
director NN a+nonexecutive+director (governor) 2
Nov. NN Nov. (governor)+29 1
29 CD 0
From the valency patterns extracted from the treebanks, it can be observed that
a governor can be a verb, a noun, an adjective, an adverb, a preposition, a con-
junction, the infinitive “to” etc. so long as it occurs with at least a daughter leaf.
Fig. 3 presents some typical examples.
We now move on to the motifs of valencies. As valencies are numerical, a
motif in this study is thus the longest continuous sequence of equal or increas-
ing valencies. The following motifs of valencies stand for the previous sentence:
0-2
0-1-1-2-3
0-1-1
0-0-2
1
0
The first motif stops at a valency of 2 as the next valency 0 is smaller. In most
cases, we can see a motif begins with a valency of 0, indicating that it embodies
at least one node without dependents. Such a mode might reflect some mecha-
nism which governs word behavior in a sequence.
The time series of the sample sentence (Fig. 4) of the valencies seems to
suggest a special kind of syntactic rhythm. This influences the dependency
distance between the dependency-related words, which in turn will affect the
comprehension difficulty of the syntactic structures (Liu 2008). This rhythm
might reflect the balance between the effort of the speaker to minimize the pro-
duction effort and the equilibrating force of the hearer to minimize the compre-
hension load.
238 | Hongxin Zhang – Haitao Liu
Fig. 4: A time series of generalized valencies in the sample sentence
This special kind of syntactic rhythm can be partly captured by the motif length,
which is defined as the number of elements a motif has. Therefore, the motif
lengths for the sample sentence are 2, 4, 3, 3, 1 and 1, which are reflected in Fig.
4.
With these algorithms, we collect the rank-frequency distribution data for
the motifs and their lengths. The data will be ranked in a descending frequency
(Freq) so that the highest frequency has Rank 1.
3 Results and Discussion
In this section, we take turns to examine the distribution patterns of motifs and
their lengths in the first two sub-sections, addressing the first two research
questions. These will be followed by a discussion of diversification processes.
We argue that both motifs and their lengths result from diversification. Finally
Section 3.4 discusses the link between motif lengths and their frequencies, ad-
dressing the third research question.
This study employs Altmann-Fitter 3.1.3 for fitting the data to relevant func-
tions and distributions.
Motifs of Generalized Valencies | 239
3.1 Distribution of Valency Motifs
Initially, we examine the distribution of valency motifs. Tab. 2 summarises the
motif types and number of motifs together with the types of valencies in the two
sets of treebanks. Some typical types of motifs will be available in Tab. 3 and
Tab. 5.
Tab. 2: A summary of rank-frequency distribution data for Motifs
Data Motif Type Number of Motifs Valency Type
motif-E1 173 17642 10
motif-E2 173 17547 11
motif-E3 173 17723 9
motif-E4 174 17503 10
motif-E5 180 17645 11
motif-E6 180 17513 10
motif-C1 254 16126 14
motif-C2 253 16102 11
motif-C3 243 16082 11
motif-C4 256 15957 14
motif-C5 238 16200 14
motif-C6 249 16132 14
Evident from Tab. 2, there are more occurrences of motifs in English sub-
corpora. One possible reason is that there are less types of valencies in English
(9-11) than in Chinese (11-14). The number of motifs closely correlates with the
motif types with Pearson correlation = -0.992 (P-Value = 0.000), and bears an
acceptable correlation with valency types with Pearson correlation = 0.791 (P-
Value = 0.002). We argue that in English it’s statistically more likely for a motif
to come across a smaller value of valency and terminate there. Moreover, the
average of generalized valencies is 1.77 in Chinese and 1.72 in English, which
seems to suggest that Chinese is more compact than English. This can be anoth-
er possible cause for the divergence of motif numbers in the two languages.
To save space, we only list the top ten motifs here. More details are availa-
ble in the Appendix. In the English sub-corpora (Tab. 3), the top ten share a lot
of motif patterns. Particularly for the first four, the rankings of motifs are identi-
cal. These top ten (Tab. 3 & Tab. 4) take up an almost uniform percentage (73-
74%), with each ranking weighing similarly across the six sub-corpora. All these
240 | Hongxin Zhang – Haitao Liu
suggest the homogeneity of data and indicate motifs of valencies as an im-
portant property of the English news genre in this study. Whether these patterns
are universal across genres in English necessitates a study on other genres.
Tab. 3: Rank-frequency distribution data for English motifs (top ten)
(R.=Rank, Freq=Frequency)
R.
E1 E2 E3 E4 E5 E6
Freq Motif Freq Motif Freq Motif Freq Motif Freq Motif Freq Motif
1 2995 1 2923 1 3114 1 2873 1 2966 1 2985 1
2 2505 0+2 2437 0+2 2612 0+2 2484 0+2 2346 0+2 2387 0+2
3 2090 0+1 2001 0+1 1970 0+1 2078 0+1 2169 0+1 1979 0+1
4 1194 0+0+
2
1264 0+0+
2
1224 0+0+
2
1254 0+0+
2
1243 0+0+
2
1238 0+0+
2
5 917 0+0+
3
879 0+0+
3
940 0+0+
3
857 0+1+
1
942 0+1+
1
854 0+1+
1
6 811 0+1+
1
796 0+1+
1
770 0+1+
1
816 0+0+
3
864 0+3 837 0+0+
3
7 760 0+1+
2
784 0+3 759 0+3 794 0+3 834 0+0+
3
767 0+3
8 747 0+3 710 0+1+
2
709 1+1 710 0+1+
2
643 0+1+
2
755 0+1+
2
9 705 1+1 688 1+1 703 0+1+
2
674 1+1 635 1+1 696 1+1
10 384 0+0+
0+3
339 0+0+
0+3
369 0+0+
0+3
351 0+0+
0+3
316 0+0+
1
312 0+1+
3
Tab. 4: Percentages for top 10 motifs
Rank E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
1 17 17 18 16 17 17 16 18 17 17 17 17
2 14 14 15 14 13 14 11 10 10 10 11 10
3 12 11 11 12 12 11 9 8 8 8 9 8
4 7 7 7 7 7 7 7 8 7 8 8 7
5 5 5 5 5 5 5 5 6 5 5 5 5
Motifs of Generalized Valencies | 241
Rank E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
6 5 5 4 5 5 5 5 5 5 5 5 5
7 4 4 4 5 5 4 4 4 4 4 4 4
8 4 4 4 4 4 4 4 3 3 3 3 3
9 4 4 4 4 4 4 3 3 3 3 3 3
10 2 2 2 2 2 2 3 3 3 3 3 3
sub-
total
74 73 74 74 73 73 67 67 66 66 67 67
A similar trend is observed in the Chinese data (Tab. 4 & Tab. 5), where the first
seven are completely the same across the six sub-corpora. The top ten also ac-
count for similar percentages (66-67%), but much smaller than the English fig-
ures. This is to a certain degree understandable since generally, there are less
motif types in English and the distribution is expected to be more concentrated.
Tab. 5: Rank-frequency distribution data for Chinese motifs (top ten) (R. = Rank）
R.
C1 C2 C3 C4 C5 C6
Freq Motif Freq Motif Freq Motif Freq Motif Freq Motif Freq Motif
1 2650 0+1 2849 0+1 2781 0+1 2684 0+1 2729 0+1 2760 0+1
2 1726 0+2 1555 0+2 1612 0+2 1627 0+2 1722 0+2 1686 0+2
3 1422 1 1299 1 1334 1 1273 1 1420 1 1360 1
4 1124 0+1+1 1231 0+1+1 1204 0+1+1 1201 0+1+1 1223 0+1+
1
1124 0+1+1
5 869 0+3 908 0+3 794 0+3 847 0+3 853 0+3 882 0+3
6 777 0+0+2 751 0+0+2 791 0+0+2 759 0+0+2 817 0+0+
2
810 0+0+2
7 657 0+0+1 659 0+0+1 585 0+0+1 660 0+0+1 617 0+0+
1
640 0+0+1
8 578 0+0+3 510 0+4 540 0+0+3 510 0+4 560 0+0+
3
563 0+0+3
9 492 0+4 495 0+0+3 473 0+1+1
+1
492 0+1+2 450 0+4 505 0+1+2
1
0
474 0+1+2 492 0+1+2 472 0+1+2 487 0+0+3 442 0+1+
2
447 0+4
242 | Hongxin Zhang – Haitao Liu
The sub-corpora in both languages exhibit interesting similar trends and also
consistent differences. Besides the aforementioned differences, another distinc-
tion between the two sets of data is the ranking order of the motifs, particularly
for the first three (43% in English and 36% in Chinese of the total) which are
completely reversed. The motif “1” accounts for quite different percentages in
the two languages: 16-17% in English but only 8-9% in Chinese. On the contrary,
the motif “0+1” is more prevalent in Chinese (16-18%) than in English (11-12%).
Following the definition of motifs, for a motif “0+1” to occur individually, the
next valency has to be 0 since it’s is the only possible valency smaller than 1.
This observation suggests that it’s more likely for Chinese structures to be orga-
nized in a pattern where a word with one dependent appears between two
words without any dependents.
Another interesting observation is that unlike the English data, the “0+4”
pattern occurs as one of the top ten in the Chinese sub-corpora except for the
third one, where it ranks No. 11. One possible reason is that the valency of 4
accounts for only 2.4-2.6% in English, smaller than the figure in Chinese (3.1-
3.2%).
To understand the causes for all the afore-mentioned differences will call
for a follow-up study of relevant valency patterns and of the contributing factors
for their uneven distributions, which will be addressed in a parallel study of this
project. For instance, we might have to examine which two possible word clas-
ses might be combined to constitute the “0+1” motif.
We fit the two sets of data in this study to the right truncated modified Zipf-
Alekseev (a,b; n = x-max,α fixed) model. The results represent excellent fits as
listed in Tab. 6. Fig. 5 presents the fitting for E1 data.
Tab. 6: Fitting the motif data to the right truncated modified Zipf-Alekseev model
Input data R² a b n α N
Motif-E1 0.9891 0.1724 0.2865 172 0.1698 17641
Motif-E2 0.9918 0.2873 0.2558 172 0.1666 17546
Motif-E3 0.9929 0.288 0.2618 172 0.1757 17722
Motif-E4 0.9905 0.3058 0.2549 173 0.1642 17502
Motif-E5 0.9891 0.3276 0.2491 179 0.1681 17644
Motifs of Generalized Valencies | 243
Input data R² a b n α N
Motif-E6 0.991 0.2841 0.2554 179 0.1705 17512
Motif-C1 0.9989 0.2055 0.2325 253 0.1643 16125
Motif-C2 0.9965 0.1442 0.2405 252 0.1769 16101
Motif-C3 0.9968 0.1635 0.2385 242 0.1729 16081
Motif-C4 0.9972 0.164 0.2366 255 0.1682 15956
Motif-C5 0.9978 0.1953 0.236 237 0.1685 16199
Motif-C6 0.9989 0.1776 0.2375 248 0.1711 16131
Fig. 5: Fitting motif length data of E1 to the right truncated modified Zipf-Alekseev model
(f[x]:empirical frequency, NP[x]:theoretical frequency)
It's worth noticing that the right truncated modified Zipf-Alekseev model is
merely one of the several that capture the given data. For instance, the Zipf-
Mandelbrot model as suggested by Köhler (2015) and employed in Čech, Vincze
and Altmann (in this volume) also produces excellent fitting results with R²
ranging from 0.984-0.9962. This model is quite well proven and is recognized as
an excellent one for modeling the motif distribution (Köhler 2015).
In a word, the motifs of generalized valencies are found to be distributed
regularly, which constitutes an affirmative answer to the first research question
and corroborates that motifs of generalized valencies behave like other basic
language entities.
244 | Hongxin Zhang – Haitao Liu
3.2 Distribution of Motif Lengths
Having analysed how the motifs of valencies are distributed, we need to take a
close look at valencies before we examine the lengths of valency motifs.
Fig. 6 and Fig. 7 are the time series plots of valencies for the first 200 words
of E1 and C1, respectively, which are believed to be typical miniatures of the
whole language. As predicted, the time series reflect a certain rhythm of valency
since it’s impossible for language users to attach equal importance to all the
words.
Fig. 6: A time series of the valencies of the first 200 running words of E1
Motifs of Generalized Valencies | 245
Fig. 7: A time series of the valencies of the first 200 running words of C1
As can be seen from the two plots, it’s not very common for two words with the
same valency to be neighbors (52 occurrences in English and 45 in Chinese out
of a total of 200). For quite a big proportion of neighboring same-value valen-
cies, both neighbors have no dependents. But the statistics here differs between
the two languages. In English, 65% (34 out of 52 cases) go with neighboring 0-
valencies but in Chinese, only 49% (22 out of 45). In other such neighboring
same-valency cases for both languages, usually the words have a valency of 1.
For a node to have several dependents is to elaborate this node and make it
more explicit, which can partly explain why it’s rare (about 10% for both lan-
guages) for words to have more than 3 dependents. But the valency of 2 enjoys
different proportions: 15% in English and 10% in Chinese.
The differences in valencies will be partly reflected in the time series plots
of motif lengths. Fig. 8 and Fig. 9 are such plots of the first 200 motif lengths in
E1 and in C1, respectively. The lengths seem to reflect a rhythm somewhat dif-
ferent from that of valencies. Obviously, the length of 1 is less prevalent than
lengths of 2 and 3 in both languages. More motifs seem to bear a length of 4 in
Chinese than in English, with even more occurrences than the length of 1 in
Chinese. Adjacent identical values (with 2 or 3 accounting for 85% in Chinese
and 95% in English for all such neighbors) are more common in English (73
occurrences out of a total of 200) than in Chinese (47 cases). Particularly, it’s
quite noticeable that adjacent identical values of 2 are more common in English
246 | Hongxin Zhang – Haitao Liu
(accounting for 78% of all such neighbors). These interesting observations are
worthy of further exploration and interpretation.
Fig. 8: A time series of the lengths of the first 200 valency motifs in E1
Fig. 9: A time series of the lengths of the first 200 valency motifs in C1
Motifs of Generalized Valencies | 247
Tab. 7 and Tab. 8 summarize the data of valency lengths for the 12 sub-corpora.
We find that for Rankings 1 through 9, each length value is uniform throughout
the sub-corpora in the very language. In both languages, except for a length
value of 1, the rest lengths are ordered in an increasing ranking. In English, the
motif length of 1 ranks the 3rd
(accounting for 18-19%) while in Chinese, the 4th
(taking up a percentage of 9-10%). This is an important contributing factor for
the motif lengths in Chinese (2.68 on average) to be consistently bigger than
that in English (2.46 on average). The uneven distribution of motif length 1
might be attributable to the relevant valency patterns.
Tab. 7: Rank-frequency data of motif lengths (English)
Rank Length
E1 E2 E3 E4 E5 E6
Freq Freq Freq Freq Freq Freq
1 2 6659 6561 6651 6636 6672 6467
2 3 5039 5133 5043 5021 5159 5128
3 1 3240 3146 3344 3086 3197 3185
4 4 2069 2066 2086 2117 1987 2058
5 5 516 516 474 519 515 547
6 6 97 108 101 100 90 101
7 7 17 13 17 17 17 24
8 8 4 2 5 4 6 2
9 9 1 1 2 1
Total 17641 17546 17722 17502 17644 17512
Average
length
2.45 2.47 2.44 2.47 2.45 2.47
Tab. 8: Rank-frequency data of motif lengths (Chinese) (L= length, Freq = frequency)
Rank
C1 C2 C3 C4 C5 C6
Freq L Freq L Freq L Freq L Freq L Freq L
1 6523 2 6631 2 6485 2 6467 2 6593 2 6621 2
2 4824 3 4874 3 4920 3 4868 3 4961 3 4939 3
3 2195 4 2173 4 2244 4 2144 4 2107 4 2050 4
4 1595 1 1443 1 1486 1 1416 1 1567 1 1498 1
5 705 5 728 5 715 5 793 5 727 5 772 5
248 | Hongxin Zhang – Haitao Liu
6 218 6 199 6 168 6 192 6 188 6 183 6
7 50 7 41 7 54 7 56 7 40 7 53 7
8 10 8 11 8 8 8 11 8 11 8 7 8
9 3 9 1 9 1 9 4 9 2 9 3 9
10 1 30 3 10 2 13 2 11
11 1 40 1 11 1 10 1 12
12 1 82 1 14
13 1 15
total 16125 16101 16081 15956 16199 16131
Aver.
length
2.68 2.69 2.69 2.71 2.67 2.68
We fit the length data (Tab. 7 and Tab. 8) to the right truncated modified Zipf-
Alekseev model. The results (Tab. 9) represent excellent fittings with R² all
above 0.97, and constitute an affirmative answer to the second research ques-
tion. The common distribution pattern for motif lengths further testifies to the
fact that motifs behave like all other linguistic units. Tab. 10 and its graphic
representation (Fig. 10) illustrate the fitting of C1 data.
Tab. 9: Fitting length data to the right truncated modified Zipf-Alekseev distribution (x-max =
length types)
Input data R² a b n α N x-max
Motif-length-E1 0.9735 0.0214 0.9759 8 0.3775 17641 8
Motif-length-E2 0.9763 0.1119 0.9625 9 0.3739 17546 9
Motif-length-E3 0.9706 0.009 1.0037 9 0.3753 17722 9
Motif-length-E4 0.975 0.1418 0.9332 9 0.3792 17502 9
Motif-length-E5 0.9774 0.089 0.9883 9 0.3781 17644 9
Motif-length-E6 0.9751 0.1186 0.9367 8 0.3693 17512 8
Motif-length-C1 0.9937 0.191 0.9452 11 0.4045 16125 11
Motif-length-C2 0.9957 0.1935 0.9504 9 0.4118 16101 9
Motif-length-C3 0.9951 0.213 0.9471 9 0.4033 16081 9
Motif-length-C4 0.9957 0.2171 0.9487 12 0.4053 15956 12
Motif-length-C5 0.9936 0.2265 0.9469 11 0.407 16199 11
Motif-length-C6 0.9943 0.2268 0.9515 13 0.4105 16131 13
Motifs of Generalized Valencies | 249
Tab. 10: Fitting motif length data of C1 to the right truncated modified Zipf-Alekseev model
(f[i]:empirical frequency, NP[i]:theoretical frequency)
x[i] f[i] NP[i] x[i] f[i] NP[i]
1 6523 6523 7 50 170.72
2 4824 4935.12 8 10 100.13
3 2195 2298.55 9 3 60.82
4 1595 1107.02 10 1 38.08
5 705 563.96 11 1 24.48
6 218 303.1
Fig. 10: Graphic representation of Tab. 10
3.3 The Diversification Process
The Zipf’s law (Zipf 1935, 1949) initiates a discussion on the diversification pro-
cess. Originally a law on word frequencies, this empirical law observes that in
speech and text, word frequency decays as a power law of its rank. Zipf (1949)
proposed “the least effort” principle as the theoretical explanation for this law.
This principle results from two competitive economic principles. From the
speaker’s perspective, to produce utterance with the least effort, he would tend
to reduce the number of words. As a consequence, quite a lot of words will bear
multiple meanings, and in an extreme case, all meanings will be carried by just
250 | Hongxin Zhang – Haitao Liu
one word. Resulting in less words in the inventory, this speaker economic force
constitutes the unification force.
In contrast, from the listener’s perspective, to comprehend with the least ef-
fort, each word carrying only one distinct meaning might be preferred. This
way, more words might be required in the inventory, giving rise to the diversifi-
cation force.
The Zipf's law indicates a consequence of reaching a balance between the
previous two opposing forces. This balance is actually the simultaneous mini-
mization of both efforts, which optimizes the cost of communicative transac-
tions between the two parties.
With both forces of unification and its opposite, diversification (or collec-
tively called Zipfian processes) in play, language evolves. But beyond the field
of language laws, the Zipf’s law governs various fields and is recognized as a
general law of human behaviors.
In the case of generalized valencies, we regard them as a result of a diversi-
fication process.
A concept quite known in biology, the diversification process occurs at var-
ious levels of language as a process of enlarging the number of meanings or
forms of any linguistic entity, accounting for an enormous scope of phenomena
(Altmann 1991). For instance, words class membership can be enlarged through
deviation (e.g. composition) or without formal change (e.g. to head). Words can
also acquire different meanings and become polysemous or acquire connota-
tions by connecting with other words.
Altmann (2005) hypothesized that an entity diversifies in one direction, re-
sulting in unequal frequencies of the diversified entities, which can be ordered
according to a decreasing frequency. He argues that the fitting of such rank-
frequency distributions (usually Zipf’s law related) can constitute a criterion
deciding whether the taxonomies of the diversified entities are theoretically
prolific or not.
Altmann (1991) suggested some models for diversification process, one of
which is the right truncated modified Zipf-Alekseev model, a well-known Zipf’s
law-related distribution to model the ranking law of diversified entities (Köhler
2012). A linguistic derivation of the model is available in Liu (2009b.)
The fitting of valency motifs (cf. Section 3.1) and their lengths (cf. Section
3.2) to the right truncated modified Zipf-Alekseev model suggests them as a
result of diversification process and verifies that the motifs and motif lengths
can be placed, at least with regard to the distribution, in the list of other basic
linguistic units.
Motifs of Generalized Valencies | 251
3.4 Frequency and Length
This subsection addresses the third research question concerning the interrela-
tion between motif lengths and frequencies of these lengths. Tab. 11 and Tab. 12
present the data reordered according to length.
Tab. 11: Length and frequency for English sub-corpora
Length
E1 E2 E3 E4 E5 E6
Freq Freq Freq Freq Freq Freq
1 3240 3146 3344 3086 3197 3185
2 6659 6561 6651 6636 6672 6467
3 5039 5133 5043 5021 5159 5128
4 2069 2066 2086 2117 1987 2058
5 516 516 474 519 515 547
6 97 108 101 100 90 101
7 17 13 17 17 17 24
8 4 2 5 4 6 2
9 1 1 2 1
Total 17641 17546 17722 17502 17644 17512
Tab. 12: Length and frequency for Chinese sub-corpora (L=length)
C1 C2 C3 C4 C5 C6
L Freq L Freq L Freq L Freq L Freq L Freq
1 1595 1 1443 1 1486 1 1416 1 1567 1 1498
2 6523 2 6631 2 6485 2 6467 2 6593 2 6621
3 4824 3 4874 3 4920 3 4868 3 4961 3 4939
4 2195 4 2173 4 2244 4 2144 4 2107 4 2050
5 705 5 728 5 715 5 793 5 727 5 772
6 218 6 199 6 168 6 192 6 188 6 183
7 50 7 41 7 54 7 56 7 40 7 53
8 10 8 11 8 8 8 11 8 11 8 7
9 3 9 1 9 1 9 4 9 2 9 3
30 1 10 3 10 1 11 2
40 1 11 1 13 2 12 1
252 | Hongxin Zhang – Haitao Liu
C1 C2 C3 C4 C5 C6
82 1 14 1
15 1
Total 1612
5
1610
1
1608
1
1595
6
1619
9
16131
We fit the data to the Hyperpoisson (a, b) model which is in a number of cases
used to model the interrelation between two language qualities (Köhler 2015).
The fitting proves to be excellent (Tab. 13) with all R² values above 0.993. The
interrelation between motif lengths and their frequencies further corroborates
the linguistic status of the valency motifs.
Tab. 13: Modeling the length-frequency relation with the Hyperpoisson function
Input data R² a b N
Length-Frequency-E1 0.9959 0.9966 0.4377 17641
Length-Frequency-E2 0.9946 0.9974 0.4249 17546
Length-Frequency-E3 0.9956 1.0029 0.4553 17722
Length-Frequency-E4 0.9969 0.9987 0.4223 17502
Length-Frequency-E5 0.9951 0.9838 0.425 17644
Length-Frequency-E6 0.9939 1.0213 0.4475 17512
Length-Frequency-C1 0.998 1.0082 0.2465 16125
Length-Frequency-C2 0.997 0.9737 0.2119 16101
Length-Frequency-C3 0.9986 0.9909 0.2271 16081
Length-Frequency-C4 0.9974 1.0089 0.2209 15956
Length-Frequency-C5 0.9985 0.9843 0.234 16199
Length-Frequency-C6 0.9974 0.9827 0.2223 16131
The differences between the values for Parameter b of the two languages are
quite pronounced, which seems to suggest that b can act as an indicator setting
the two languages apart.
Motifs of Generalized Valencies | 253
4 Conclusion
This study employs the idea of Generalized Valency Patterns (GVP), which ex-
tends the idea of full valency of verbs to all running words in large scale authen-
tic texts. A valency is defined as the number of all dependents of a word and a
motif, as the longest sequence with equal or increasing values. The GVP frame-
work enables an analysis of the valency of all the running words and therein of
the dynamic feature of word chains from the perspective of valencies.
This project chooses two news-genre dependency-annotated treebanks, one
in English and one in Chinese, divides the data into 6 sub-corpora of equivalent
sizes in each language, and examines the motifs of generalized valencies and
their lengths. The sub-corpora for each language are found to behave homoge-
neously.
The study yields the following research findings:
a. Valency motifs in both languages are regularly distributed, abiding by
the same right truncated modified Zipf-Alekseev model.
b. The rank-frequency distribution of motif lengths also follow the right
truncated modified Zipf-Alekseev model.
c. The Hyperpoisson model captures the interrelation between motif
lengths and length frequencies.
d. The previous findings corroborate the linguistic status of valency mo-
tifs and suggest them as a result of a diversification process.
We need to pursue what contributes to the differences and similarities between
the two languages. For instance, the most obvious difference is the different
numbers of motifs generated from equivalent sizes of original data. To this end,
we need to further examine the valency patterns and the consequent valencies,
which calls for further studies.
This study focuses on news genre of two languages. Whether the research
findings in this study can be more universal features remains unknown. Follow-
up studies involving more diversified languages need to be carried out to an-
swer this question. Will other languages have similar valencies? Will they have
similar valency patterns? Or lengths of valency motifs? Will they behave more
like English or Chinese, or quite differently from these two languages? These are
intriguing questions awaiting us.
254 | Hongxin Zhang – Haitao Liu
Acknowledgement
This work was supported by the National Social Science Foundation of China
under Grant No. 11&ZD188. We are deeply indebted to Zicheng Huang and Haiqi
Wu, who helped with data for the research.
References
Altmann, G. (2005). Diversification processes. In: R. Köhler, G. Altmann, & R. Piotrowski (Eds.),
Quantitative Linguistics. An International Handbook (pp. 648–659). Berlin: de Gruyter.
Altmann, G. (1991). Modelling diversification phenomena in language. In: U. Rothe (Ed.), Diver-
sification Processes in Language: Grammar (pp. 33–46). Hagen: Rottmann.
Čech, R., Vincze, V., & Altmann, G. (2016). On motifs and verb valency. (this volume)
Čech, R., Pajas, P., & Mačutek, J. (2010). Full valency. Verb valency without distinguishing
complements and adjuncts. Journal of Quantitative Linguistics, 17(4), 291–302.
Čech, R., & Mačutek, J. (2010). On the quantitative analysis of verb valency in Czech. In: P.
Grzybek, E. Kelih, & J. Mačutek (Eds.), Text and Language. Structure, Functions, Interrela-
tions (pp. 21–29). Wien: Preasen Verlag.
Gao, S., Zhang, H., & Liu, H. (2014). Synergetic properties of Chinese verb valency. Journal of
Quantitative Linguistics, 21(1), 1–21.
Hajič, J., Hajičová, E., Panevová, J., Sgall, P., Cinková S., Fučíková, E., Mikulová, M., Pajas, P.,
Popelka, J., Semecký, J., Šindlerová, J., Stĕpánek, J., Toman, J., Urešová, Z., & Žabokrtský,
Z. (2012). Prague Czech-English Dependency Treebank 2.0 LDC2012T08. DVD. Philadelphia:
Linguistic Data Consortium.
Herbst, H. (1988). A valency model for nouns in English. Journal of Linguistics, 24(2), 265–301.
Herbst, T., David, H., Ian, F.R., & Dieter, G. (2004). A Valency Dictionary of English: A Corpus-
Based Analysis of the Complementation Patterns of English Verbs, Nouns and Adjectives.
Berlin, New York: Mouton de Gruyter.
Köhler, R. (2005). Quantitative untersuchungen zur valenz deutscher verbena. Glottometrics, 9,
13–20.
Köhler, R. (2012). Quantitative Syntax Analysis. Berlin, New York: de Gruyter.
Köhler, R. (2015). Linguistic motifs. In: M. Georgios, & J. Mačutek (Eds.), Sequences in Lan-
guage and Text (pp. 89–108). Berlin, Boston: de Gruyter.
Liu, B., & Liu, H. (2011). A Corpus-based study on the evolution of the Chinese verbal syntactic
valence. Language Teaching and Linguistic Studies, 6, 83–89.
Liu, H. (2008). Dependency distance as a metric of language comprehension difficulty. Journal
of Cognitive Science, 9(2), 159–191.
Liu, H. (2009a). Dependency Grammar: From Theory to Practice. Beijing: Science Press.
Liu, H. (2009b). Probability distribution of dependencies based on Chinese dependency tree-
bank. Journal of Quantitative Linguistics, 16(3), 256–273.
Liu, H. (2011). Quantitative properties of English verb valency. Journal of Quantitative Linguis-
tics, 18(3), 207–233.
Motifs of Generalized Valencies | 255
Qiu, L., Zhang, Y., Jin, P., & Wang, H. (2014). Multi-view Chinese treebanking. In: Proceedings
of the 25th International Conference on Computational Linguistics (COLING) (pp. 257-268).
Dublin, Ireland, August. http://klcl.pku.edu.cn/ShowNews.aspx?id=137
Tesnière, L. (1959). Eléments de la Syntaxe Structurale. Paris: Klincksieck.
Zipf, G.K. (1949). Human Behavior and the Principle of Least Effort. Cambridge, Mass.: Addison-
Wesley.
Zipf, G.K. (1935). The Psycho-biology of Language. An Introduction to Dynamic Philology. Bos-
ton: Houghton Mifflin.
Appendix
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
1 2995 2923 3114 2873 2966 2985 2650 2849 2781 2684 2729 2760
2 2505 2437 2612 2484 2346 2387 1726 1555 1612 1627 1722 1686
3 2090 2001 1970 2078 2169 1979 1422 1299 1334 1273 1420 1360
4 1194 1264 1224 1254 1243 1238 1124 1231 1204 1201 1223 1124
5 917 879 940 857 942 854 869 908 794 847 853 882
6 811 796 770 816 864 837 777 751 791 759 817 810
7 760 784 759 794 834 767 657 659 585 660 617 640
8 747 710 709 710 643 755 578 510 540 510 560 563
9 705 688 703 674 635 696 492 495 473 492 450 505
10 384 339 369 351 316 312 474 492 472 487 442 447
11 291 297 301 307 301 305 410 403 429 421 414 409
12 288 281 287 267 278 279 321 313 363 283 373 334
13 263 275 261 262 270 278 276 270 326 274 296 325
14 260 267 259 244 266 262 235 265 287 243 227 241
15 245 264 255 232 248 249 211 229 224 240 208 234
16 233 223 216 220 215 219 199 224 220 224 201 202
17 221 216 212 211 213 213 182 162 192 184 175 168
18 210 216 201 196 206 212 170 149 155 151 150 146
19 209 214 199 194 204 194 151 142 151 147 146 144
20 164 184 162 180 176 194 148 137 146 138 146 140
21 151 180 156 142 154 163 142 129 146 136 134 139
22 119 131 141 141 120 134 135 127 130 127 131 129
23 106 118 109 128 113 116 124 121 122 126 125 117
24 100 109 106 105 107 112 113 118 107 121 113 114
256 | Hongxin Zhang – Haitao Liu
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
25 93 101 87 100 104 92 109 116 106 118 111 110
26 84 92 86 97 101 82 106 106 103 114 107 100
27 73 87 85 92 92 82 90 88 101 104 101 99
28 72 73 78 91 81 79 87 81 91 90 85 85
29 69 71 77 77 74 71 87 80 80 89 85 78
30 69 69 67 68 73 63 86 78 80 78 82 77
31 64 57 62 64 64 59 79 74 75 77 78 73
32 61 51 62 62 64 55 74 72 72 76 78 72
33 55 47 57 56 58 54 73 72 72 69 76 71
34 49 47 49 48 48 54 64 66 61 68 72 71
35 44 47 49 47 44 49 64 62 60 63 71 68
36 44 43 41 41 41 48 53 55 54 58 68 66
37 40 41 40 37 36 45 48 49 51 51 55 60
38 35 39 38 36 36 42 47 49 51 50 45 56
39 34 35 38 36 35 38 47 48 50 49 43 46
40 33 34 30 31 35 34 45 44 49 47 41 45
41 31 34 30 30 33 33 42 43 40 47 40 43
42 28 30 28 29 33 31 42 38 38 47 39 37
43 28 29 27 29 32 30 41 37 36 41 37 37
44 27 29 27 28 29 27 39 36 35 40 35 36
45 27 28 26 28 28 27 39 34 34 37 34 33
46 25 26 26 27 27 26 38 34 33 36 32 31
47 24 26 25 26 25 26 36 33 32 34 31 30
48 21 26 25 25 25 23 35 32 32 32 30 29
49 21 22 24 25 24 22 32 31 31 31 29 28
50 19 20 18 24 24 21 31 31 31 31 28 27
51 17 18 17 22 20 19 30 30 29 30 28 26
52 17 17 16 20 19 19 29 29 28 29 28 26
53 16 16 15 19 19 19 29 28 26 27 28 25
54 15 16 14 18 18 18 26 27 26 27 27 25
55 15 16 14 17 18 18 25 26 25 26 26 24
56 15 15 13 17 17 17 25 25 23 24 26 24
57 14 14 13 17 16 17 24 25 23 24 25 24
58 14 14 13 15 15 15 24 25 23 24 23 22
Motifs of Generalized Valencies | 257
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
59 13 14 13 14 14 15 23 25 22 23 22 21
60 13 14 12 13 13 14 22 24 21 23 22 20
61 12 13 12 13 12 14 20 23 21 22 20 20
62 12 13 11 12 11 12 20 22 21 22 20 20
63 12 12 11 12 11 11 20 21 19 21 19 19
64 12 12 10 10 11 10 18 20 19 21 19 18
65 11 12 10 9 11 10 18 20 18 20 18 18
66 11 12 9 9 10 10 18 19 17 18 16 17
67 10 11 9 9 10 10 17 18 16 18 16 17
68 10 11 9 8 10 9 15 18 16 18 16 17
69 10 11 9 8 9 9 14 17 15 17 15 15
70 10 9 8 8 9 9 14 17 15 16 15 14
71 9 9 7 8 8 8 14 16 15 16 14 14
72 9 8 7 7 8 8 14 15 15 12 14 13
73 8 8 7 7 8 7 14 15 14 11 13 13
74 8 8 7 7 8 7 13 13 14 11 13 13
75 7 7 7 7 7 7 12 13 13 10 13 12
76 6 7 6 6 7 7 12 13 13 10 12 12
77 6 7 6 6 7 7 11 13 12 10 11 12
78 5 7 6 6 7 6 10 13 11 10 11 11
79 5 7 6 6 7 6 10 13 11 10 11 11
80 5 6 6 5 6 6 10 11 11 10 11 11
81 5 6 5 5 6 6 10 11 11 10 10 11
82 4 6 5 5 6 5 9 11 10 10 10 11
83 4 6 5 5 6 5 9 10 10 10 10 11
84 4 5 5 4 6 5 9 10 10 10 10 10
85 4 5 5 4 5 5 9 10 10 10 9 10
86 4 5 5 4 5 5 8 10 9 9 9 10
87 4 5 5 4 5 5 8 9 9 8 9 9
88 4 5 4 4 5 5 8 9 9 8 9 9
89 4 4 4 4 4 4 7 9 9 8 8 9
90 4 4 4 4 4 4 7 8 8 7 8 9
91 4 4 4 4 4 4 7 8 8 7 8 9
92 4 4 4 4 4 4 7 8 8 7 7 9
258 | Hongxin Zhang – Haitao Liu
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
93 3 4 4 4 4 4 7 8 8 7 7 9
94 3 4 4 4 4 4 6 7 7 7 7 8
95 3 4 4 4 4 4 6 7 7 7 7 8
96 3 4 4 4 4 4 6 7 7 7 7 8
97 3 4 4 4 4 4 6 7 6 7 7 7
98 3 4 3 4 4 4 6 6 6 6 6 7
99 3 4 3 3 3 3 6 6 6 6 6 7
100 3 3 3 3 3 3 6 6 6 6 6 7
101 3 3 3 3 3 3 6 6 6 6 6 7
102 3 3 3 3 3 3 6 5 6 6 6 7
103 3 3 3 3 3 3 6 5 6 6 6 7
104 3 3 2 3 3 3 5 5 6 6 6 6
105 3 3 2 3 3 3 5 5 5 6 5 6
106 3 3 2 3 3 3 5 5 5 6 5 6
107 3 3 2 3 3 3 5 5 5 6 5 5
108 2 2 2 3 3 3 5 5 5 5 5 5
109 2 2 2 3 2 3 5 5 5 5 5 5
110 2 2 2 3 2 3 5 5 5 5 5 5
111 2 2 2 3 2 3 5 5 5 5 5 5
112 2 2 2 3 2 3 5 5 5 5 5 5
113 2 2 2 2 2 3 5 5 4 5 4 4
114 2 2 2 2 2 3 4 5 4 4 4 4
115 2 2 2 2 2 3 4 5 4 4 4 4
116 2 2 2 2 2 2 4 5 4 4 4 4
117 2 2 2 2 2 2 4 4 4 4 4 4
118 2 2 2 2 2 2 4 4 4 4 4 4
119 2 2 2 2 2 2 4 4 4 4 4 4
120 2 2 2 2 2 2 4 4 4 4 4 4
121 2 2 2 2 2 2 4 4 4 4 4 3
122 2 2 2 2 2 2 4 4 4 4 4 3
123 2 2 2 2 2 2 4 4 3 4 4 3
124 2 2 1 2 2 2 4 4 3 4 4 3
125 1 2 1 2 2 2 3 4 3 4 4 3
126 1 2 1 2 2 2 3 3 3 3 4 3
Motifs of Generalized Valencies | 259
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
127 1 2 1 2 1 2 3 3 3 3 3 3
128 1 2 1 1 1 2 3 3 3 3 3 3
129 1 2 1 1 1 2 3 3 3 3 3 3
130 1 2 1 1 1 2 3 3 3 3 3 3
131 1 2 1 1 1 2 3 3 3 3 3 3
132 1 2 1 1 1 2 3 3 3 3 3 3
133 1 2 1 1 1 2 3 3 3 3 3 3
134 1 1 1 1 1 2 3 3 3 3 3 3
135 1 1 1 1 1 1 3 3 3 3 3 3
136 1 1 1 1 1 1 3 3 3 3 3 2
137 1 1 1 1 1 1 3 3 3 2 3 2
138 1 1 1 1 1 1 3 3 3 2 3 2
139 1 1 1 1 1 1 3 3 2 2 3 2
140 1 1 1 1 1 1 2 3 2 2 2 2
141 1 1 1 1 1 1 2 3 2 2 2 2
142 1 1 1 1 1 1 2 2 2 2 2 2
143 1 1 1 1 1 1 2 2 2 2 2 2
144 1 1 1 1 1 1 2 2 2 2 2 2
145 1 1 1 1 1 1 2 2 2 2 2 2
146 1 1 1 1 1 1 2 2 2 2 2 2
147 1 1 1 1 1 1 2 2 2 2 2 2
148 1 1 1 1 1 1 2 2 2 2 2 2
149 1 1 1 1 1 1 2 2 2 2 2 2
150 1 1 1 1 1 1 2 2 2 2 2 2
151 1 1 1 1 1 1 2 2 2 2 2 2
152 1 1 1 1 1 1 2 2 2 2 2 2
153 1 1 1 1 1 1 2 2 2 2 2 2
154 1 1 1 1 1 1 2 2 2 2 2 2
155 1 1 1 1 1 1 2 2 2 2 2 2
156 1 1 1 1 1 1 2 2 2 2 2 2
157 1 1 1 1 1 1 2 2 2 2 2 2
158 1 1 1 1 1 1 2 2 2 2 2 2
159 1 1 1 1 1 1 2 2 2 2 2 2
160 1 1 1 1 1 1 2 2 2 2 2 2
260 | Hongxin Zhang – Haitao Liu
Rank Frequency
E1 E2 E3 E4 E5 E6 C1 C2 C3 C4 C5 C6
161 1 1 1 1 1 1 2 2 2 2 2 2
162 1 1 1 1 1 1 2 2 2 2 2 2
163 1 1 1 1 1 1 2 2 2 2 2 2
164 1 1 1 1 1 1 2 2 2 2 2 2
165 1 1 1 1 1 1 2 2 2 2 1 2
166 1 1 1 1 1 1 2 2 1 2 1 2
167 1 1 1 1 1 1 2 2 1 2 1 2
168 1 1 1 1 1 1 2 2 1 2 1 2
169 1 1 1 1 1 1 2 2 1 2 1 1
170 1 1 1 1 1 1 2 2 1 2 1 1
171 1 1 1 1 1 1 2 2 1 2 1 1
172 1 1 1 1 1 1 2 2 1 1 1 1
173 1 1 1 1 1 1 2 2 1 1 1 1
174 1 1 1 2 2 1 1 1 1
175 1 1 2 2 1 1 1 1
176 1 1 1 2 1 1 1 1
177 1 1 1 2 1 1 1 1
178 1 1 1 2 1 1 1 1
179 1 1 1 1 1 1 1 1
180 1 1 1 1 1 1 1 1
181 1 1 1 1 1 1
... ... ... ... ... ...
1
until
Rank
254
1
until
Rank
253
1
until
Rank
243
1
until
Rank
256
1
until
Rank
238
1 until
Rank
249
Index of Names
Abeillé, A. 149
Afonso, S. 149
Allerton, D. J. 14, 22
Altmann, G. 12–13 20, 22–24, 37, 47, 53–55,
59–60, 72, 85, 89, 91, 114, 118, 155, 157,
162–163, 168, 179, 181, 188, 196, 199–
200, 205–206, 211, 215, 232, 238, 243,
250, 254,
Ambati, B. R. 149
Andrei, R. 211, 215
Antić, G. 208, 215,
Antònia, M. 150
Argamon, S. 88, 106
Argiri, E. K. 153, 163
Baayen, R. H. 87, 106, 196, 199
Bailin, A. 153, 162
Baker, M. 88, 106
Balasubrahmanyan, V. K. 157, 163
Ballmer, Th.T. 22
Bamman, D. 149
Barsotti, F. 149
Basili, R. 149
Battista, M. 149
Bejček, E. 149
Beliankou, A. 14, 22, 67, 84, 165–166, 179
Benešová, M. 11, 163
Benoit, 168, K. 179
Bentz, C. 166, 170, 179
Beyer, A. P. 1
Biber, D. 65, 84
Bick, E. 149
Boguslavsky, I. 149
Borchers, H. W. 7, 11
Boroda, M. G. 13, 22–23, 134, 146, 151, 162
Bosch, A. 149
Bouma, G. 150
Brants, S. 149
Breiman, L. 80, 84
Brooks, P. 199
Brown, P. F. 65, 68, 153, 162
Bucková, M. 23, 60, 84, 106, 162, 179, 198,
215, 229
Burkhardt, H. 23, 106, 126, 147, 163, 229
Burnard, L. 167, 179
Buttery, P. 179
Călăcean, M. 149
Calzolari, N. 149
Čech, R. 14, 23, 69, 84, 215, 232, 234, 243,
254
Chafe, W. 153, 162
Chen, B. 90
Chen, H. 37–38, 49–50, 59, 65, 90, 106, 109–
110, 126, 181, 198, 220, 224, 226, 229
Chen, R. 65, 181, 198
Cinková, S. 254
Conrad, S. 65, 84
Constantoudis, V. 60
Corazzari, O. 149
Coulthard, M. 66, 85
Coyotl-Morales, R. M. 88, 106
Crane, G. 149
Dalton-Puffer, C. 199
Danielewicz, J. 153, 162
David, H. 89, 90, 254
Debbabi, M. 106
Decker, R. 106, 126, 147, 163, 229
Della Pietra, V. J. 162
Delmonte, R. 149
Desipri, E. 149
deSouza, P. V. 162
Diakonos, F. K. 60
Dieter, G. 254
Dipper, S. 149
Dryer, M. S. 135, 138, 146
Dunn, M. 60
Džeroski, S. 149
Ebeling, W. 37, 59
Erjavec, T. 149
Erting, C. 182, 198
Fanciulli, F. 149
Fang, Y. 87–88, 102, 106
Ferrer-i-Cancho, R. 134–135, 140, 147
Finegan, E. 66, 84
Fox, R. 197–198
Frid, N. 149
Friginal, E. 84
Fučíková, E. 254
Fung, B. 106
262 | Index of Names
Futrell, R. 134–135, 140, 147
Gadde, P. 149
Gao, S. 232, 254
Garabík, R. 150
Garas, A. 59
Garcia, D. 38, 59
Genzor, J. 23, 60, 84, 106, 162, 179, 198, 215,
229
Georgios, M. 254
Gibson, E. 60, 135, 147
Gildea, D. 134, 140–141, 147
Golson, J. 199
Gómez-Rodríguez, C. 140, 147
Grafstein, A. 153, 162
Gray, R. D. 60
Greenberg, J. H. 143, 147
Greenhill, S. J. 60
Grieve, J. 65, 84
Grigoriev, N. 149
Grigorieva, S. 149
Grzybek, P. 11, 23–24, 60, 85, 147, 162–163,
179–180, 199, 215, 229, 254
Haber, R. 149
Hajič, J. 148–149, 235, 254
Hajičová, E. 149, 254
Hall, J. 84, 149
Hansen, S. 149
Hausser, J. 4, 11
Hawkins, J. A. 138, 147
Herbst, H. 233, 254
Herdan, G. 65, 84, 91, 106
Heringer, H. J. 135, 147
Hill, F. 179
Hogan, P. 199
Holmes, D. I. 90, 106
Hoover, L. D. 66, 84
Horák, A. 215
Horowitz, R. 162
Hou, R. 66, 84
Hřebíček, L. 24
Hu, F. 140, 148
Huang, W. 109
Huang, Z. 254
Hudson, R. 133, 135, 147, 182, 198
Husain, S. 149
Ian, F. R. 254
Ide, N. 202, 215
Iqbal, F. 88, 106
Jacobs, J. 146
Jayaram, 23, 60, 85
Jiang, F. 90, 106
Jiang, J. 37, 59, 182, 198
Jiang, M. 66, 84
Jiang, X. 219, 229
Jiang, Y. 201
Jin, P. 255
Jing, Y. 133–135, 138, 143–144, 147
Jínová, P. 149
Johnson, R. 84, 182, 198
Jozef, G. 126
Juola, P. 87, 106
Jurafsky, D. 65, 84
Kahane, S. 148
Kalimeri, M. 37, 60
Karamanos, K. 60
Kelih, E. 11, 22–24, 60, 84–85, 147, 160,
162–163, 179–180, 199, 215, 229, 254
Kempe, V. 199
Kettnerová, V. 149
Khan, L. A. 106
Khmelev, D. V. 87, 106
Kiela, D. 179
Kirk, K. 199
Kleinow, T. 7, 11
Kleinow, T., 11
Köhler, R. 1, 11, 14, 16, 17, 22–24, 37–38, 40,
46–47, 49, 58, 60, 66–67, 84–85, 89, 91,
106, 109, 118, 126, 134, 136, 147, 151,
156, 162–163, 165–166, 168, 179, 181–
183, 185–186, 188–189, 197–200, 202–
203, 205, 208, 211, 215, 229, 231–232,
234, 243, 250, 252, 254
Kolářová, V. 149
Koplenig, A. 166, 179
Koppel, M. 88, 106, 163
Kouhestani, M. 149
Koutsombogera, M. 149
Kredens, K. 66, 85
Kreidlin, L. 149
Kromann, M. T. 149
Krupa, V. 85
Kubát, M. 69, 84
Labbé, D. 87, 106
Lai, J. C. 162
Index of Names | 263
Ledinek, N. 149
Lee, D. 167, 179
Lefevere, A. 88, 107
Lehmann, W. 143, 147
Lenci, A. 149
Levitan, S. 88, 106
Lezius, W. 149
Liang, J. 37, 59, 109, 126, 220, 224, 226, 229
Lindquist, H. 196, 199
Liu, B. 224, 229, 232, 254
Liu, H. 37, 59–60, 88, 102, 106, 110, 126,
134–135, 137–138, 140, 143, 145, 147–
148, 181–183, 188, 198–199, 220, 224,
226, 229, 231, 232–234, 237, 250, 254
Lönngren, L. 12
Lucas, C. 198
Lupea, M. 23
Lynge, S. K. 149
Mačutek, J. 1, 11, 14, 23–24, 60, 85, 126, 147,
151–152, 154–160, 162–163, 166, 179–
180, 199, 208, 215, 229, 232, 254
Mahowald, K. 147
Malcolm, C. 84
Malouf, R. 150
Mana, N. 149
Mandelbrot, B. 13, 17–18, 22, 165–170, 174,
178–181, 183, 186–189, 197, 201, 205–
207, 214, 233–234, 243
Mannem, P. 149
Manning, C. D. 65, 85
Marcinkiewicz, M. A. 149
Marcus, M. P. 149
Mareček, 148, 149
Martin, J. H. 65, 84
Martina, B. 126
Masek, J. 149
Massetani, M. 149
Matlach, M. 69, 84
McEnery, T. 68, 84
Mehler, A. 84
Meier, 183, 199
Meľčuk, I. 23, 148, 182, 199
Mercer, R. L. 162
Mikkelsen, L. 149
Mikros, G. K. 11, 14, 23, 60, 126, 151, 153, 155,
160, 162–163, 215, 229
Mikulová, M. 149, 254
Milička, J. 1, 11, 14, 23, 38, 60, 151–152, 154–
155, 163, 166, 180
Minaei-Bidgoli, B. 149
Mírovský, J. 149
Mishne, G. 153, 163
Mislovičová, S. 23, 60, 84, 162, 179, 198, 229
Miyamoto, R. 199
Moisei, G. 146
Moloodi, A. 149
Montemagni, S. 149
Montes-y-Gómez, M. 106
Motter, A. E. 59
Nadarejšvili, I. Š. 23, 162, 164
Naranan, S. 157, 163
Naumann, S. 1, 11, 14, 22–23, 37, 46, 60, 66–
67, 84–85, 89, 91, 106, 118, 126, 134,
136, 147, 151, 162–163, 166, 168, 179,
182–183, 199, 208, 215, 217, 229, 232
Nedoluzhko, A. 149
Nekrasova, T. 84
Nilsson, S. 149, 153, 163
Nivre, J. 149, 182, 199
Nulty, P. 168, 179
Obradović, I. 22, 84, 179, 215
Olahan, M. 107
Ondrejovič, S. 24
Orlov, J. K. 23, 146, 162
Osborne, T. 148
Osenova, P. 149
Pajas, P. 23, 149, 232, 254
Panevová, J. 149, 254
Papadimitriou, C. 37, 60
Papageorgiou, H. 60, 149
Pazienza, M. 149
Phillips, T. 195, 199
Pianesi, F. 149
Piantadosi, S. 38, 60
Pierrehumbert, J. B. 59
Piotrowski, R. G. 23–24, 162–163, 215, 254
Piperidis, S. 149
Pisoni, D. 199
Plag, I. 196, 199
Poláková, L. 149
Popel, M. 148, 149
Popelka, J. 254
Popescu, I. 12, 20, 23, 57, 60, 66, 70, 72, 85,
181, 188–189, 196, 199–200
264 | Index of Names
Pöschel, T. 37, 59
Preisach, B. 60,85, 179
Preisach, C. 23, 106, 126, 147, 163, 199, 229
Prokopidis, P. 149, 179
Pustet, J. 23, 60, 85
Raffaelli, R. 149
Ramasamy, L. 148
Rasooli, M. S. 149
Recasens, M. 150
Robbins, A. 199
Rodgers, T. 195, 199
Rosa, R. 149
Rosso, P. 106
Rothe, U. 215, 254
Samuels, J. S. 162
Sanada, H. 14, 24, 67, 85, 151, 163, 166, 180
Santini, M. 84
Santorini, B. 149
Santos, D. 149
Saracino, D. 149
Saussure, F. 133, 148
Schidt-Thieme, D. 60, 85, 179
Schler, J. 88, 106
Schmidt-Thieme, L. 23, 106, 126, 147, 163,
229
Schütze, H. 65, 85
Schweitzer, F. 59
Semecký, J. 254
Ševčíková, M. 149
Sgall, P. 254
Sharoff, S. 84
Šimková, M. 150
Simov, K. 149
Šindlerová, J. 254
Smith, G. 149
Solan, L. M. 85
Sporleder, C. 149
Stadlober, E. 215
Stamatatos, E. 163
Stein, B. 163
Štěpánek, J. 148–149, 254
Strecker, B. 147
Strimmer, K. 11
Svirsky, M. 182, 199
Tatar, D. 23
Taulé, M. 150
Taylor, A. 143, 148
Temperley, D. 134, 140–141, 147
Tesnière, L. 133, 143, 148, 231, 233, 255
Tiersma, P. M. 85
Tily, H. 60
Toman, J. 254
Traxler, M. 201, 215
Tuzzi, A. 3, 11–12, 163, 181, 200
Uhlířová, R. 23, 60, 85
Urešová, Z. 254
Van der Beek, L. 150
Van Noord, G. 150
Vennemann, T. 138, 143, 148
Verkerk, A. 179
Véronis, J. 215
Vidya, M. N. 23, 60, 85
Villaseñor-Pineda, L. 106
Vincze, V. 14, 24, 181, 188, 200, 232, 243,
254
Vulanović, R. 162, 200
Wang, H. 201, 216, 255
Wang, L. 220, 229
Wang, N. 110, 126
Wang, Y. 165
Wimmer, G. 17, 20, 24, 147, 154–155, 157–158,
163
Wimmerová, S. 24
Wu, C. 220, 229,
Wu, H. 254
Xiao, R. 65, 68, 84–85
Xu, C. 143, 148
Xu, S. 220, 223, 229
Yan, J. 61, 181
Yang, J. 201
Yang, X. 87–90, 97–99, 101–105, 107
Yule, G. U. 88, 107
Žabokrtský, Z. 148–149, 215, 254
Zampolli, A. 149
Zanzotto, F. 149
Žele, A. 149
Zeman, D. 135, 138, 148–149
Zervanou, K. 149
Zhang, C. 217
Zhang, H. 217, 232, 254,
Zhang, Y. 255
Zhao, Y. 182, 199
Zikánová, S. 149
Index of Names | 265
Zipf, G. 13, 17–18, 22, 142, 148, 165–170, 174,
178–179, 183, 186–189, 197, 199, 201–
202, 205–207, 214, 216, 231–232, 234,
242–243, 248–250, 253, 255
Zörnig, P. 85
Subject Index
Acquisition 182–183, 199
Adjective 187, 191–192, 195, 233, 237
Adjunct 14, 23, 231–234, 254
Adverbial 84, 192, 194, 195
Algorithm 140, 154, 238
Altmann-Fitter 47, 53–55, 114, 118, 168, 179,
205–206, 238
Ambiguity 166, 201, 214
Annotation 68, 138, 143, 235
Attribute 3, 37–38, 77, 80, 134, 140–141, 143,
165–167, 178, 192, 194–195, 223
Authorship 47, 85, 87–88, 90, 105–106, 155,
163, 196
– attribution 87–88, 155, 163
– verification 87, 106
Autosemantics 70
Auxiliary 70, 187, 191
Average polysemy value 204
Bible 219, 229
Blog 84, 151–155, 157–158, 161
Chinese 37–41, 49–50, 55–59, 65–69, 74, 82,
84, 90–91, 103–106, 109–113, 123, 125–
126, 182, 184, 194–195, 198–199, 201–
214, 216–229, 231–235, 239, 241–242,
245, 247, 251, 253–255
– ancient 37–38, 55–59, 223
– character 109–113, 117, 123, 126, 221
– classical 125, 217, 220, 223, 225, 227–228
– modern spoken 37, 39
– script 109–110, 118, 123, 125
– simplified 109–112, 117
– traditional 109–111, 117–118, 123, 125, 131
– written 37–41, 45–47, 49, 58–59, 110, 126,
229
Christianity 104, 219
Chunking 140–141
Classification 13, 65–67, 69, 72, 74–77, 80,
82–83, 106, 133–134, 143–146, 165–167,
178
Coefficient 51, 65, 70, 72–73, 82, 93, 95, 98,
158, 187–188, 208, 224
– determination 93, 95, 98, 158, 187–188
– Gini’s 65, 70, 72, 82
Complement 14, 23, 139, 223, 231–233
Component 20, 110–115, 123, 125, 189, 195
Comprehension load 237
Computational linguistics 65, 202
Conjunction 101, 187, 191, 237
Construction 110–111, 115, 126, 153, 165–166,
182–183, 192, 214, 217
Corpus 1, 3–11, 61, 65, 67–67, 70, 75, 88, 90,
148–149, 153, 155, 158–160, 167–168,
198, 202, 204–206, 208–209, 213, 235
– British National (BNC) 167, 179, 202
– DNA 1, 5, 7, 9, 10
Correlation 37–38, 51–52, 59–60, 70, 80,
123–124, 221, 224, 239
Czech 13, 16, 137, 139, 154, 232, 235, 254
Decision tree 65, 68–69, 76, 80, 82
Dependency 23, 59–60, 133, 135, 138, 147–
150, 181–182, 185–186, 189, 199, 235–
236, 254
– adjacent 134, 138, 140, 142
– consistent 138–139, 146
– direction 133, 135–139, 141, 143–146, 198
– grammar 133, 135, 182
– mixed 138–139, 146
– structure 184
– treebank 59–60, 135, 137–138, 148, 186,
198–199, 231, 234–235
– type 183–184, 188, 196, 235
Dependency distance (DD) 133–143, 145–146,
198–199, 237
– arrangement 139, 141
– mean (MDD) 135, 137–138, 141–142, 146
– minimization 133–134, 140–142
– sequence 134, 136, 138, 145
– value 136, 141–142
– motif, 133–140, 142–143, 145–146
Diachronic 37–38, 40–41, 61, 166, 179, 232
Dialogue 39, 102, 153
Dichotomy 65, 74, 80, 83
Diglossia 154
Discrepancy 196, 208
Discriminant 65, 67, 69, 72–77, 82
Distribution 13–14, 16–19, 23–25, 37–38, 41,
43–61, 63, 66–67, 71–72, 80, 84, 94, 99,
106, 109–110, 113–114, 118, 123, 125–
268 | Subject Index
126, 128, 133–134, 137–140, 143, 145–
146, 152, 155, 162–163, 165–179, 181–
183, 186–195, 198–201, 203, 205–206,
208–212, 214–215, 218, 220, 222, 225,
227–229, 231, 234, 238–243, 247–248,
250, 253–254
– rank frequency 109–110, 115–116, 118, 125–
126, 128, 165, 167, 169–171, 174, 188
– Zeta 18–19
– Zipf-Mandelbrot 13, 17–18, 22, 165–166,
168–169, 183, 186–189, 197, 201, 205–
207, 214, 233
Diversification 211, 213, 215, 231, 238, 249–
250, 253–254
Entropy 1, 4–5, 9–11, 37, 57–59, 65, 70–73,
76–77, 82
Error 39, 80–82, 153, 184, 197
– rate 80, 82
Fiction 68–70, 74–77, 80–81, 83
Flexibility 201, 213–214
Freiburg-Brown corpus of American English
(Frown) 65, 68, 73–77, 79–82
Frequency 13–14, 16–25, 34–38, 41–48, 50–
56, 58–61, 63, 65–67, 69–72, 84–85, 88,
101–102, 106–110, 113–116, 118, 125–
126, 128, 133, 138, 140, 145–146, 152,
154–155, 162, 165–171, 174, 179, 181–183,
186, 188–192, 194–195, 198, 201–202,
205–206, 208–209, 211–222, 224, 227–
229, 231, 234, 238–243, 247, 249–253,
255–260
Full valency frame 15–16
Function 18–21, 37, 47, 65, 73, 82, 87–90,
101, 106, 109, 115, 117, 125, 145, 151, 154,
157–158, 160, 181, 188, 190, 194, 220,
222, 227, 232, 252
– Hyper-Pascal 37, 46–49, 53, 55–59, 109
– Lorentzian 13, 19
Genre 3, 60, 66, 83, 114, 125, 153–155, 159,
165, 167–178, 231, 235–236, 240, 253
Governor 136, 183–185, 192, 231, 233–237
Grapheme 154
HArmonized Multi-LanguagE Dependency
Treebank (Hamledt) 135, 137, 143, 148–
149
Hapax 65, 70, 73, 76–77, 82–83, 165–169,
172–173, 176–178, 181, 189, 195–197
– legomena 165–169, 178, 195–196
– percentage 65, 70, 73, 76, 82–83
Harmonic head ordering 135, 138, 143
Harmonic property 133, 135, 143, 145
Head ordering 135–139, 143
Head-final (HF) 134–136, 139, 143
Head-initial (HI) 134–137, 143
H-point 70–71
Hungarian 13, 16, 19, 21–22, 24–25, 34, 36,
200, 232
Hurst-exponent 1, 4, 7–11
Hyper-Pascal model 46, 57–58, 109, 118–120,
122–123, 125, 131
Index 88, 90
Indices 65, 67, 69–70, 81–82
Information density 197
Lancaster Corpus of Mandarin Chinese (LCMC)
65, 68, 73–78, 80–82, 84
ICTCLAS 39
Language 2–4, 13, 20, 37–39, 45, 49, 57, 60,
65, 67–68, 83–84, 102, 106, 111, 133–
140, 142–146, 148, 151–157, 159, 162,
166, 180–182, 184, 188–190, 195–199,
201–205, 214–215, 217, 219–221, 224,
231–232, 234–235, 243–244, 247, 250,
252–254
– capacity 183, 197
– classification 134, 143
– entities 13, 57, 231, 233–234, 243
– evolution 224, 250
– Germanic 137, 145
– Hellenic 137, 143
– Indo-European 133–134, 138–139, 143, 145,
222
– Indo-Iranian 137
– processing, 134, 135, 146
– random 134, 140–141
– Romance 137, 143
– Slavic 137
– typology 37, 133–135, 138, 143, 145–146,
199
Law 13, 24, 37, 42, 44–46, 50–51, 57–60, 90,
109–110, 115–116, 125, 154–155, 157, 166,
183, 188–189, 202, 217, 222, 228, 249–
250
– Menzerath-Altmann 89, 155, 163
– Zipf’s 166, 199, 249–250
Subject Index | 269
Lemmatization 66
Length 2–5, 10, 13–14, 16, 19–23, 35–38, 40,
41, 45–61, 63, 66, 84, 88–90, 106, 109–
115, 118–120, 122–123, 125–126, 131, 135,
137, 151–152, 154–158, 161–162, 165–
166, 179, 198, 201, 204–205, 208, 211–
215, 217, 220–222, 224–229, 231–232,
234, 238, 243, 245, 247–249, 251–253
– average 20–21, 35–36, 247
– average word 115, 217, 225
– distribution 37–38, 41, 45–49, 52–56, 58–
60, 66, 109–110, 113–114, 118–120, 122–
123, 125, 131, 151–152, 155–156, 162,
225–226
– mean word 49, 151, 157–158, 161–162
– normalized dynamic 221
– value 16, 40, 89–90, 165
Lexicon 57, 154
Linear 15, 73–76, 91, 133–136, 138, 140–141,
143, 151, 157, 159–160, 181, 192, 197,
207, 217, 222, 228
– arrangement 133, 136, 140–141, 143
– distance 134–135, 140–141
– order 133
– regularity 138
Linguistic 1, 3, 13–14, 17–18, 20, 23–25, 74,
76, 96–97, 101, 103, 124, 153, 155, 157,
176, 178, 182, 190–191, 193, 195, 210,
213–216, 219–220, 232–234, 236–241,
244, 247, 250, 252, 255, 262, 271, 275–
277, 279, 295, 297–299, 301
– entity 22, 250
– property 66, 109, 182, 213
– structure 153
– unit 13, 16–17, 19, 22, 89–90, 109, 134, 136,
165, 181–183, 186, 188, 198, 202–203,
208, 222, 233, 248, 250
Logarithm 4, 187, 209–210, 222
Lorenz curve 72
Lyapunov stability 11
Magnitude 2, 133–134, 136–138, 140–143,
145–146
Markov Chain 11
Minimization 133–134, 140–142, 147, 250
Morpheme 16
Motif 1–2, 4, 7, 11, 13–25, 34–38, 41–56, 58–
61, 63, 65, 67, 69–70, 72, 81–85, 109–
110, 112–115, 118–120, 122–123, 125–126,
128, 1331–147, 151–152, 154–163, 165–
175, 178–183, 185–186, 188–192, 194–
199, 201–206, 208–209, 211–215, 217–
218, 220–225, 227–229, 231–232, 234–
235, 237–246, 248, 250, 252–254
– DD 133–143, 145–146
– F 165–166, 168–169, 173–175, 178, 217–
218, 222–224, 227, 228
– FVS 15–19, 34
– linguistic 89, 109, 133–134, 136, 151, 156,
165, 167, 202–204, 208, 214, 217
– L 1–3, 6–7, 16–17, 87, 89–92, 95, 97–100,
105, 165–175, 178, 182, 205, 208–210,
217
– P 89, 165, 203–209, 211–214, 217
– pattern 37, 59, 239
– Polysemy 201–205, 208–209, 211, 213–214
– POS 65–72, 81–83, 181, 194–196
– HD 136–138, 141–144, 146
– R 166, 178, 182–198, 217
Natural language 3, 65, 133–137, 140–142,
146, 201–202
– processing 65, 84–85, 202
Neologism 153, 196
NLREG 92, 114, 152
Non-terminal node 76
Normal phenomenon 47
Noun 191–192, 195, 233–234, 237
Object 87, 135, 143, 155, 192, 194–195, 202,
232
Order 1–11, 13, 19, 37–38, 51, 60, 67, 75, 83,
111, 133–137, 139–143, 145–146, 148,
152, 155, 171, 194–195, 200, 211, 219–
220, 228, 242
Out-of-vocabulary (OOV) rate, 153
Paradigm 3, 65, 141, 181, 232
Part of speech (POS) 65–72, 77, 81–83, 166,
181–187, 189, 191–192, 194–196, 198,
236
– sequence 65–66, 83
Pattern 37, 58–59, 88, 106, 134, 138, 140–
141, 143, 160, 181, 183, 186, 188, 195,
197, 231–240, 242, 247–248, 253
Pearson correlation 123–124, 224, 239
People’s Daily 113, 202, 235
Perl program 168
270 | Subject Index
Persistency 1, 7, 11
Phoneme-grapheme correspondence 154
Phoneme 154
Polysemy 165, 201–205, 208–209, 211, 213–
214, 232
Power law 37, 42, 44–45, 50–51, 57–59, 109,
115–116, 125, 157, 166, 217, 222, 228,
249
– function 37, 42, 44–45, 50–51, 57–59, 109,
115–116, 125
Probabilistic valency pattern theory 234
Projectivity 141
Pronoun 15, 102, 163, 187, 191, 252
Quantitative 1, 11, 14, 23–24, 65–67, 70, 72,
80–82, 84, 88–90, 106, 134, 136, 145–
146, 155, 163, 165, 179, 181–182, 186,
198–199, 201–203, 213–215, 217, 220,
223, 229, 232, 254
– property 67, 89, 134, 136, 165, 181–182,
201–203, 213–214, 232
Quantitative Index Text Analyser (QUITA) 69,
84
R programming 4, 69
Random forest 65, 68–69, 80–84
Randomization 155
Regularity 37–38, 49, 83, 135, 138, 153, 166,
186
Richness 65, 67, 70–72, 82–83, 87–90, 95,
99–101, 105–106, 196
– vocabulary 87–90, 95, 99–101, 105, 196
Segmentation 39–40, 68, 91, 113, 146, 185,
197
Sentence 2, 14–15, 23, 40, 65–66, 69, 82–83,
87–88, 103–104, 133, 135–138, 141, 153,
166, 168, 179, 181–182, 184–185, 195,
197–199, 204, 215, 229, 233, 237–238
Sequence 14–16, 37–38, 41, 65–66, 72, 77,
82, 89, 109, 112, 134–136, 142–143,
145–146, 165–166, 182–183, 185–186,
190–192, 195, 197, 203–204, 217, 231–
232, 237, 253
Sequential arrangement 134, 136, 140, 142–
143, 145
Simplification 109–111, 117, 123
Simplified script 112–113, 115
Spectrum 13–14, 17–19, 22, 34
Statistics 70, 201, 206, 208, 214, 245
Strength 80, 214
Stroke 110–112, 114
Structural complexity 110, 117, 123, 133–138,
140–143, 145–146
Structural order 133
Stylometrics 66
Subject 15–16, 152, 167, 183, 192, 194–195
SVO structure 195
Syllable 3, 16, 41, 83, 109–110, 151, 154, 158,
166, 182, 205, 225
Synchronic 37–38, 41, 58
Synsemantics 70
Syntactic 1–2, 7, 13, 15, 83, 133–136, 138,
140, 146–149, 165–166, 181–184, 194,
197, 199, 217, 231, 237–238
– complexity 133, 140, 136, 146, 183
– dependency 15, 133
– property 7
– relation 7, 15, 133, 181–182, 197
– rhythm 237–238
– scope 2
– structure 7, 133–134, 138, 182, 184, 237
Syntagmatic 3, 11, 37–38, 60, 65–66, 83, 109,
117, 143, 167, 181, 197, 202, 214, 232
System 5, 11, 45–46, 49, 58, 109, 111–112,
117, 182, 204, 219
– alphabetic 109
Szeged treebank 188
Terminal node 76
Test 13, 19, 45, 49, 58–59, 76, 88, 94, 97, 99,
101, 115, 117, 123, 154–155, 167, 171–173,
175–177, 205–206, 209, 211, 222, 227
– ANOVA 171–173, 175–177
– Chi-square 206–207
– Kolmogorov-Smirnov (KS) 94, 97
– T 45, 49, 58–59, 94–95, 97, 99, 101, 105,
117, 123
Text 3–4, 11–12, 22–24, 37–49, 60, 65–69,
74, 76, 77, 80, 82–83, 85, 88–89, 91,
106, 109–110, 113–118, 123, 125, 131, 138,
140, 145–146, 151–156, 159–163, 166–
168, 171, 176, 179–180, 182, 184, 188,
197, 203, 219, 220, 223–226, 228, 231,
234, 253
– classification 37, 60, 65, 85, 88, 147, 151,
163, 167, 179, 198–199, 229
– expository 77, 80, 82
Subject Index | 271
– length 151–152, 154–162, 166
– narrative 77, 80, 82–83, 166
– randomized 151–152, 155, 161–162
– simplified 109, 114–115, 117, 123, 125
– traditional 109, 114–115, 117, 123–125
– type 65, 67–69, 74–77, 80–83, 167
Threshold 10
Token 11, 70, 88, 90–91, 108, 151–152, 154–
155, 157, 161–163, 168, 184, 196, 204
Transcription 113, 125, 154
Translation 13, 25, 34, 36, 40, 87–90, 92,
95–106, 160, 218
Treebank 5, 135, 137–138, 143, 148, 181,
183–189, 198, 199, 231, 234–237, 239,
253–254
Type 13, 16–17, 41, 44–55, 59–61, 65–69, 71,
73, 76–82, 87–92, 94, 100–104, 115–116,
131–132, 138, 143, 145–146, 154–156,
159–160, 165–166, 168–169, 173, 176–
177, 179–184, 188–191, 194, 196–197,
202, 204, 211, 213–217, 219–224, 227–
231, 233–235, 238–241, 247–249, 255–
256, 260–264, 268–269, 278–280,
284–285, 287, 295
– entity 57–58
Type-token ratio (TTR) 65, 69–70, 73, 82, 87–
93, 95, 97–100, 105, 204
Type-token relation 151–152, 154–155, 157,
161–163
Typology 133–135, 138, 143, 145–146
Utterance 134, 204, 249
Valency 13–14, 22, 24, 149, 231, 233–234,
236, 239, 253–254
– frame 15–16, 22
– generalized 231, 234, 236
– motif 13–14, 22, 231, 233–235, 239, 244,
246, 250, 252–253
Value 1, 7, 9–11, 16–17, 45, 49–51, 54, 56–58,
64, 66, 74, 76, 80–82, 87, 89–101, 103–
115, 119, 133–134, 136–143, 146, 151,
153–158, 160–166, 168–169, 176, 181–
186, 188, 193–194, 197–199, 201–207,
217, 231–232, 238–240, 245, 252–253,
256, 275, 277, 292, 299–300
Variable 22, 73, 76–77, 81–82, 141, 153, 156,
165–166
Variance 73–74, 197
Verb 13–16, 22, 88, 102, 105, 135, 143, 187,
191, 192, 195, 231–233, 237, 254
– valency 13–14, 22, 231–233, 254
Word 2–3, 11, 16, 19, 22–23, 37–61, 63, 65–
68, 70–71, 83–85, 87–91, 95, 97, 99,
101–107, 109–110, 112–115, 118–120,
122–123, 125–126, 128, 131, 133–138,
140, 143, 145–148, 151–163, 165–168,
179–185, 192, 194–195, 197–203, 205,
211, 213–225, 227–229, 231–237, 242–
245, 249–250, 253
– culture-loaded 87–88, 103–105
– frequency 65–66, 85, 166, 217, 222, 249
– order 37, 134–136, 140, 143, 145, 228
– segmentation 39, 68, 91
– sequences 37, 84, 88, 106, 112–113
– token 39–40, 91, 114, 184
– type 195, 197
– length 11, 23, 37–61, 63, 66–67, 109–110,
112–115, 118–120, 122–123, 125–126,
128, 131, 151–152, 155–158, 160–163,
166, 202, 217, 221, 224–225, 227–229
– motif 38, 41, 45–46, 58, 109–110, 115, 118,
123, 125
Wordlist 101–105
Working memory 135
Writing style 87, 90
Zipf-Mandelbrot (ZM) model 165, 167, 169–
171, 173–174, 177–178, 243
